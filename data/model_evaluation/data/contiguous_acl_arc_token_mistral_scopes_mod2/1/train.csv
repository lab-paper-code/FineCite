token_context,word_context,seg_context,sent_cotext,label
"['framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991), LFS(Iordanskaja et al., 1992) , and JOYCE(Rambow and Korelsky, 1992) .', 'The framework was originally developed for the realization of deep-syntactic structures in NLG( #AUTHOR_TAG )']","['framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991), LFS(Iordanskaja et al., 1992) , and JOYCE(Rambow and Korelsky, 1992) .', 'The framework was originally developed for the realization of deep-syntactic structures in NLG( #AUTHOR_TAG )']","['framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991), LFS(Iordanskaja et al., 1992) , and JOYCE(Rambow and Korelsky, 1992) .', 'The framework was originally developed for the realization of deep-syntactic structures in NLG( #AUTHOR_TAG )']","['framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991), LFS(Iordanskaja et al., 1992) , and JOYCE(Rambow and Korelsky, 1992) .', 'The framework was originally developed for the realization of deep-syntactic structures in NLG( #AUTHOR_TAG )']",0
"['', 'Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent( Kittredge and Lavoie , 1998 ) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework( #AUTHOR_TAG )']","['', 'Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent( Kittredge and Lavoie , 1998 ) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework( #AUTHOR_TAG )']","['', 'Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent( Kittredge and Lavoie , 1998 ) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework( #AUTHOR_TAG )']","['', 'Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent( Kittredge and Lavoie , 1998 ) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework( #AUTHOR_TAG )']",2
"['o II a failli pleuvoir.', 'More details on how the structural divergences described in( #AUTHOR_TAG ) can be accounted for using our formalism can be found in( Nasr et al. , 1998 )']","['o II a failli pleuvoir.', 'More details on how the structural divergences described in( #AUTHOR_TAG ) can be accounted for using our formalism can be found in( Nasr et al. , 1998 )']","['o II a failli pleuvoir.', 'More details on how the structural divergences described in( #AUTHOR_TAG ) can be accounted for using our formalism can be found in( Nasr et al. , 1998 )']","['o II a failli pleuvoir.', 'More details on how the structural divergences described in( #AUTHOR_TAG ) can be accounted for using our formalism can be found in( Nasr et al. , 1998 )']",0
"['', 'Our work extends directions taken in systems such as Ariane( Vauquois and Boitet , 1985 ) , FoG( Kittredge and Polguere , 1991 ) , JOYCE ( Rambow and#AUTHOR_TAG ) , and LFS( Iordanskaja et al. , 1992 ) .', 'Although it adopts the general']","['', 'Our work extends directions taken in systems such as Ariane( Vauquois and Boitet , 1985 ) , FoG( Kittredge and Polguere , 1991 ) , JOYCE ( Rambow and#AUTHOR_TAG ) , and LFS( Iordanskaja et al. , 1992 ) .', 'Although it adopts the general']","['', 'Our work extends directions taken in systems such as Ariane( Vauquois and Boitet , 1985 ) , FoG( Kittredge and Polguere , 1991 ) , JOYCE ( Rambow and#AUTHOR_TAG ) , and LFS( Iordanskaja et al. , 1992 ) .', 'Although it adopts the general principles']","['', 'Our work extends directions taken in systems such as Ariane( Vauquois and Boitet , 1985 ) , FoG( Kittredge and Polguere , 1991 ) , JOYCE ( Rambow and#AUTHOR_TAG ) , and LFS( Iordanskaja et al. , 1992 ) .', '']",2
"['framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG( Kittredge and Polguere , 1991 ) , LFS( Iordanskaja et al. , 1992 ) , and JOYCE ( Rambow and#AUTHOR_TAG ) .', 'The framework was originally developed for the realization of deep-syntactic structures in NLG']","['framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG( Kittredge and Polguere , 1991 ) , LFS( Iordanskaja et al. , 1992 ) , and JOYCE ( Rambow and#AUTHOR_TAG ) .', 'The framework was originally developed for the realization of deep-syntactic structures in NLG']","['framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG( Kittredge and Polguere , 1991 ) , LFS( Iordanskaja et al. , 1992 ) , and JOYCE ( Rambow and#AUTHOR_TAG ) .', 'The framework was originally developed for the realization of deep-syntactic structures in NLG']","['framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG( Kittredge and Polguere , 1991 ) , LFS( Iordanskaja et al. , 1992 ) , and JOYCE ( Rambow and#AUTHOR_TAG ) .', 'The framework was originally developed for the realization of deep-syntactic structures in NLG']",2
"[""of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger( #AUTHOR_TAG ) ) and parsers generally aim to produce a tree spanning each sentence""]","[""of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger( #AUTHOR_TAG ) ) and parsers generally aim to produce a tree spanning each sentence""]","[""of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger( #AUTHOR_TAG ) ) and parsers generally aim to produce a tree spanning each sentence""]","['', ""Part of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger( #AUTHOR_TAG ) ) and parsers generally aim to produce a tree spanning each sentence""]",0
"['is a multilingual enhancement of COCKTAIL( #AUTHOR_TAG ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information .', '']","['is a multilingual enhancement of COCKTAIL( #AUTHOR_TAG ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information .', '']","['is a multilingual enhancement of COCKTAIL( #AUTHOR_TAG ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information .', '']","['both languages, we resolved coreference by using SWIZZLE,our implementation of a bilingual coreference resolver.', 'SWIZZLE is a multilingual enhancement of COCKTAIL( #AUTHOR_TAG ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information .', 'When COCKTAIL was applied separately on the English and the Ro- manian texts, coreferring links were identified for each English and Romanian document respectively.', 'When aligned referential expressions corefer with non-aligned anaphors, SWIZZLE derived new heuris- tics for coreference.', '']",2
"['', 'The acquisition of such knowledge is time-consuming, difficult, and error-prone.', 'Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX( #AUTHOR_TAG ) ,( Kennedy and Boguraev , 1996 )( Kameyama , 1997 ) ) .', '']","['', 'The acquisition of such knowledge is time-consuming, difficult, and error-prone.', 'Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX( #AUTHOR_TAG ) ,( Kennedy and Boguraev , 1996 )( Kameyama , 1997 ) ) .', '']","['', 'The acquisition of such knowledge is time-consuming, difficult, and error-prone.', 'Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX( #AUTHOR_TAG ) ,( Kennedy and Boguraev , 1996 )( Kameyama , 1997 ) ) .', '']","['', 'The acquisition of such knowledge is time-consuming, difficult, and error-prone.', 'Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX( #AUTHOR_TAG ) ,( Kennedy and Boguraev , 1996 )( Kameyama , 1997 ) ) .', '']",0
"['. is-part re- lations) and hypernyms (e.g. is-a relations).', 'Consistency checks implemented for this class of coref- erence are conservative: either the adjuncts must be identical or the adjunct of the referent must be less specific than the antecedent.', 'Table 1 lists the top performing heuristics of COCKTAILfor pronominal and nominal coreference.', 'Examples of the heuristics operation on the MUC data are presented presented in Table 2.', 'Details of the top performing heuristics of COCKTAIL were reported in( #AUTHOR_TAG )']","['is-part re- lations) and hypernyms (e.g. is-a relations).', 'Consistency checks implemented for this class of coref- erence are conservative: either the adjuncts must be identical or the adjunct of the referent must be less specific than the antecedent.', 'Table 1 lists the top performing heuristics of COCKTAILfor pronominal and nominal coreference.', 'Examples of the heuristics operation on the MUC data are presented presented in Table 2.', 'Details of the top performing heuristics of COCKTAIL were reported in( #AUTHOR_TAG )']","['. is-part re- lations) and hypernyms (e.g. is-a relations).', 'Consistency checks implemented for this class of coref- erence are conservative: either the adjuncts must be identical or the adjunct of the referent must be less specific than the antecedent.', 'Table 1 lists the top performing heuristics of COCKTAILfor pronominal and nominal coreference.', 'Examples of the heuristics operation on the MUC data are presented presented in Table 2.', 'Details of the top performing heuristics of COCKTAIL were reported in( #AUTHOR_TAG )']","['third class of heuristics resolves coreference by coercing nominals.', 'Sometimes coercions involve only derivational morphology - linking verbs with their nominalizations.', 'On other occasions, coercions are obtained as paths of meronyms (e.g. is-part re- lations) and hypernyms (e.g. is-a relations).', 'Consistency checks implemented for this class of coref- erence are conservative: either the adjuncts must be identical or the adjunct of the referent must be less specific than the antecedent.', 'Table 1 lists the top performing heuristics of COCKTAILfor pronominal and nominal coreference.', 'Examples of the heuristics operation on the MUC data are presented presented in Table 2.', 'Details of the top performing heuristics of COCKTAIL were reported in( #AUTHOR_TAG )']",0
"['', 'For this research , we used a coreference resolution system (( #AUTHOR_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference .', '']","['', 'For this research , we used a coreference resolution system (( #AUTHOR_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference .', '']","['', 'For this research , we used a coreference resolution system (( #AUTHOR_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference .', '']","['', 'Nevertheless, recent results show that knowledge-poor methods perform with amazing accuracy (cf.', '(Mitkov, 1998) ,(Kennedy and Boguraev, 1996)(Kameyama, 1997) ).', 'For example, CogNIAC(Baldwin, 1997) , a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference.', 'For this research , we used a coreference resolution system (( #AUTHOR_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference .', '']",5
"['will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc.', 'Each component will return a confidence measure of the reliability of its prediction , c.f.( #AUTHOR_TAG ) .', 'The results from each component are evaluated to determine the final category of the word']","['will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc.', 'Each component will return a confidence measure of the reliability of its prediction , c.f.( #AUTHOR_TAG ) .', 'The results from each component are evaluated to determine the final category of the word']","['deal with these issues we propose a multicomponent architecture where individual components specialize in identifying one particular type of unknown word.', 'For example, the misspelling identifier will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc.', 'Each component will return a confidence measure of the reliability of its prediction , c.f.( #AUTHOR_TAG ) .', 'The results from each component are evaluated to determine the final category of the word']","['deal with these issues we propose a multicomponent architecture where individual components specialize in identifying one particular type of unknown word.', 'For example, the misspelling identifier will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc.', 'Each component will return a confidence measure of the reliability of its prediction , c.f.( #AUTHOR_TAG ) .', 'The results from each component are evaluated to determine the final category of the word']",4
"['first feature represents the part of speech of the word.', 'We use an in-house statistical tagger ( based on( #AUTHOR_TAG ) ) to tag the text in which the unknown word occurs .', 'The tag set used is a simplified version of the tags used in the machinereadable version of the Oxford Advanced Learners Dictionary (OALD).', 'The tag set contains just one tag to identify nouns']","['first feature represents the part of speech of the word.', 'We use an in-house statistical tagger ( based on( #AUTHOR_TAG ) ) to tag the text in which the unknown word occurs .', 'The tag set used is a simplified version of the tags used in the machinereadable version of the Oxford Advanced Learners Dictionary (OALD).', 'The tag set contains just one tag to identify nouns']","['first feature represents the part of speech of the word.', 'We use an in-house statistical tagger ( based on( #AUTHOR_TAG ) ) to tag the text in which the unknown word occurs .', 'The tag set used is a simplified version of the tags used in the machinereadable version of the Oxford Advanced Learners Dictionary (OALD).', 'The tag set contains just one tag to identify nouns']","['first feature represents the part of speech of the word.', 'We use an in-house statistical tagger ( based on( #AUTHOR_TAG ) ) to tag the text in which the unknown word occurs .', 'The tag set used is a simplified version of the tags used in the machinereadable version of the Oxford Advanced Learners Dictionary (OALD).', 'The tag set contains just one tag to identify nouns']",5
"['that is more similar in goal to that outlined in this paper is Vosse( #AUTHOR_TAG ) .', 'Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names.', 'Capitalization is his sole means of identifying names.', 'However, capitalization information is not available in closed captions.', '']","['that is more similar in goal to that outlined in this paper is Vosse( #AUTHOR_TAG ) .', 'Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names.', 'Capitalization is his sole means of identifying names.', 'However, capitalization information is not available in closed captions.', '']","['that is more similar in goal to that outlined in this paper is Vosse( #AUTHOR_TAG ) .', 'Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names.', 'Capitalization is his sole means of identifying names.', 'However, capitalization information is not available in closed captions.', '']","['that is more similar in goal to that outlined in this paper is Vosse( #AUTHOR_TAG ) .', 'Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names.', 'Capitalization is his sole means of identifying names.', 'However, capitalization information is not available in closed captions.', '']",1
"['pus frequency :( #AUTHOR_TAG ) differentiates between misspellings and neologisms ( new words ) in terms of their frequency .', 'His algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms.', '']","['pus frequency :( #AUTHOR_TAG ) differentiates between misspellings and neologisms ( new words ) in terms of their frequency .', 'His algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms.', '']","['pus frequency :( #AUTHOR_TAG ) differentiates between misspellings and neologisms ( new words ) in terms of their frequency .', 'His algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms.', '']","['pus frequency :( #AUTHOR_TAG ) differentiates between misspellings and neologisms ( new words ) in terms of their frequency .', 'His algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms.', '']",5
"['situations.', 'Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance( #AUTHOR_TAG ) .', '']","['situations.', 'Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance( #AUTHOR_TAG ) .', '']","['atic situations.', 'Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance( #AUTHOR_TAG ) .', '']","['research reported here is the first that we know of to automatically analyze a corpus of logs from a spoken dialogue system for the purpose of learning to predict problematic situations.', 'Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance( #AUTHOR_TAG ) .', '']",2
"['percent-subdials).', 'The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors( #AUTHOR_TAG )']","['percent-subdials).', 'The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors( #AUTHOR_TAG )']","['running percentages (percent-reprompts, percentconfirms, percent-subdials).', 'The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors( #AUTHOR_TAG )']","['', 'The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors( #AUTHOR_TAG )']",4
"['', 'We use the same set of binary features as in previous work on this dataset( #AUTHOR_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) .', '']","['is a normalizer.', 'We use the same set of binary features as in previous work on this dataset( #AUTHOR_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) .', '']","['is a normalizer.', 'We use the same set of binary features as in previous work on this dataset( #AUTHOR_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) .', '']","['f (•) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and Z θ (x) def = y u(x, y) is a normalizer.', 'We use the same set of binary features as in previous work on this dataset( #AUTHOR_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) .', 'Specifically, let V = {v 1 , ..., v 17744 } be the set of word types with count ≥ 4 in the full 2000-document corpus.', '']",5
"['approach.', ""In future work we plan to experiment with richer representations , e.g. including long-range n-grams( Rosenfeld , 1996 ) , class n-grams( #AUTHOR_TAG ) , grammatical features( Amaya and Benedy , 2001 ) , etc '""]","['approach.', ""In future work we plan to experiment with richer representations , e.g. including long-range n-grams( Rosenfeld , 1996 ) , class n-grams( #AUTHOR_TAG ) , grammatical features( Amaya and Benedy , 2001 ) , etc '""]","['approach.', ""In future work we plan to experiment with richer representations , e.g. including long-range n-grams( Rosenfeld , 1996 ) , class n-grams( #AUTHOR_TAG ) , grammatical features( Amaya and Benedy , 2001 ) , etc '""]","['', ""In future work we plan to experiment with richer representations , e.g. including long-range n-grams( Rosenfeld , 1996 ) , class n-grams( #AUTHOR_TAG ) , grammatical features( Amaya and Benedy , 2001 ) , etc '""]",3
"['', 'Unfortunately , as shown in( #AUTHOR_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) .', '']","['', 'Unfortunately , as shown in( #AUTHOR_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) .', '']","['', 'Unfortunately , as shown in( #AUTHOR_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) .', '']","['', 'Unfortunately , as shown in( #AUTHOR_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) .', '']",4
"['anohara.', 'This code was extensively optimized to take advantage of the very sparse sentence representation described above.', 'As shown in( #AUTHOR_TAG ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences .', 'Therefore, we used a 3rd order polynomial kernel, which was found to give good results.', '']","['', 'The code for the classifier was generously provided by Daisuke Okanohara.', 'This code was extensively optimized to take advantage of the very sparse sentence representation described above.', 'As shown in( #AUTHOR_TAG ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences .', 'Therefore, we used a 3rd order polynomial kernel, which was found to give good results.', '']","['', 'The code for the classifier was generously provided by Daisuke Okanohara.', 'This code was extensively optimized to take advantage of the very sparse sentence representation described above.', 'As shown in( #AUTHOR_TAG ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences .', 'Therefore, we used a 3rd order polynomial kernel, which was found to give good results.', '']","['', 'The code for the classifier was generously provided by Daisuke Okanohara.', 'This code was extensively optimized to take advantage of the very sparse sentence representation described above.', 'As shown in( #AUTHOR_TAG ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences .', 'Therefore, we used a 3rd order polynomial kernel, which was found to give good results.', '']",4
"['eline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and#AUTHOR_TAG ] , which is still considered one of the best smoothing methods for n-gram language models .', '']","['eline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and#AUTHOR_TAG ] , which is still considered one of the best smoothing methods for n-gram language models .', '']","['eline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and#AUTHOR_TAG ] , which is still considered one of the best smoothing methods for n-gram language models .', '']","['eline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and#AUTHOR_TAG ] , which is still considered one of the best smoothing methods for n-gram language models .', '']",5
"['features can be easily obtained by modifying the TAT extraction algorithm described in( #AUTHOR_TAG ) .', '']","['features can be easily obtained by modifying the TAT extraction algorithm described in( #AUTHOR_TAG ) .', '']","['features can be easily obtained by modifying the TAT extraction algorithm described in( #AUTHOR_TAG ) .', '']","['features can be easily obtained by modifying the TAT extraction algorithm described in( #AUTHOR_TAG ) .', '']",2
"['We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking( #AUTHOR_TAG ; Huang , 2008 ) and history-based parsing ( Nivre and McDonald , 2008 ) .', 'We could also introduce']","['We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking( #AUTHOR_TAG ; Huang , 2008 ) and history-based parsing ( Nivre and McDonald , 2008 ) .', 'We could also introduce']","['We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking( #AUTHOR_TAG ; Huang , 2008 ) and history-based parsing ( Nivre and McDonald , 2008 ) .', 'We could also introduce']","['ief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking( #AUTHOR_TAG ; Huang , 2008 ) and history-based parsing ( Nivre and McDonald , 2008 ) .', '']",3
"['We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) and history-based parsing ( Nivre and McDonald , 2008 ) .', 'We could also introduce']","['We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) and history-based parsing ( Nivre and McDonald , 2008 ) .', 'We could also introduce']","['We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) and history-based parsing ( Nivre and McDonald , 2008 ) .', 'We could also introduce']","['ief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) and history-based parsing ( Nivre and McDonald , 2008 ) .', '']",3
"['We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking( Charniak and Johnson , 2005 ; Huang , 2008 ) and history-based parsing( #AUTHOR_TAG ) .', 'We could also']","['We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking( Charniak and Johnson , 2005 ; Huang , 2008 ) and history-based parsing( #AUTHOR_TAG ) .', 'We could also']","['We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking( Charniak and Johnson , 2005 ; Huang , 2008 ) and history-based parsing( #AUTHOR_TAG ) .', 'We could also']","['ief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking( Charniak and Johnson , 2005 ; Huang , 2008 ) and history-based parsing( #AUTHOR_TAG ) .', '']",3
"['', 'We could also introduce new variables , e.g. , nonterminal refinements( #AUTHOR_TAG ) , or']","['', 'We could also introduce new variables , e.g. , nonterminal refinements( #AUTHOR_TAG ) , or']","['', 'We could also introduce new variables , e.g. , nonterminal refinements( #AUTHOR_TAG ) , or']","['', 'We could also introduce new variables , e.g. , nonterminal refinements( #AUTHOR_TAG ) , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc.( Sleator and Temperley , 1993 ;  Buch-Kromann , 2006 )']",3
"['', 'Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system( Liu et al. , 2006 ; #AUTHOR_TAG ) , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses .', '']","['', 'Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system( Liu et al. , 2006 ; #AUTHOR_TAG ) , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses .', '']","['', 'Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system( Liu et al. , 2006 ; #AUTHOR_TAG ) , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses .', '']","['', 'Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system( Liu et al. , 2006 ; #AUTHOR_TAG ) , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses .', '']",2
"['', 'The first direct application of parse forest in translation is our previous work( #AUTHOR_TAG ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .', '']","['', 'The first direct application of parse forest in translation is our previous work( #AUTHOR_TAG ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .', '']","['is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models(Huang and Chiang, 2007) .', 'The first direct application of parse forest in translation is our previous work( #AUTHOR_TAG ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .', '']","['forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models(Huang and Chiang, 2007) .', 'The first direct application of parse forest in translation is our previous work( #AUTHOR_TAG ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .', '']",2
"['the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction( #AUTHOR_TAG ) , and Machine Translation ( Boas 2002 ) .', '']","['the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction( #AUTHOR_TAG ) , and Machine Translation ( Boas 2002 ) .', '']","['the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction( #AUTHOR_TAG ) , and Machine Translation ( Boas 2002 ) .', '']","['', 'Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction( #AUTHOR_TAG ) , and Machine Translation ( Boas 2002 ) .', '']",0
"['prove that our method is effective , we also make a comparison between the performances of our system and#AUTHOR_TAG ,Xue ( 2008 ) .', 'Xue (2008) is the best SRL system until now and it has the same data setting with ours.', 'The results are presented in Table 6']","['prove that our method is effective , we also make a comparison between the performances of our system and#AUTHOR_TAG ,Xue ( 2008 ) .', 'Xue (2008) is the best SRL system until now and it has the same data setting with ours.', 'The results are presented in Table 6']","['prove that our method is effective , we also make a comparison between the performances of our system and#AUTHOR_TAG ,Xue ( 2008 ) .', 'Xue (2008) is the best SRL system until now and it has the same data setting with ours.', 'The results are presented in Table 6']","['prove that our method is effective , we also make a comparison between the performances of our system and#AUTHOR_TAG ,Xue ( 2008 ) .', 'Xue (2008) is the best SRL system until now and it has the same data setting with ours.', '']",1
"['.', 'Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering( #AUTHOR_TAG ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) .', 'With the efforts of many researchers (Carreras and Màrque']","['etc.', 'Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering( #AUTHOR_TAG ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) .', 'With the efforts of many researchers (Carreras and Màrquez']","['', 'Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering( #AUTHOR_TAG ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) .', 'With the efforts of many researchers (Carreras and Màrque']","['', 'Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering( #AUTHOR_TAG ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) .', '']",0
"['.', 'The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese( #AUTHOR_TAG )']","['word.', 'The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese( #AUTHOR_TAG )']","['.', 'The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese( #AUTHOR_TAG )']","['', 'The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese( #AUTHOR_TAG )']",5
"['', '#AUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling .', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings']","['', '#AUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling .', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings']","['', '#AUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling .', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings']","['', '#AUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling .', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings']",0
"['', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank( #AUTHOR_TAG ) was built ,Xue and Palmer ( 2005 ) andXue ( 2008 ) have produced more complete and systematic research on Chinese SRL .', '']","['', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank( #AUTHOR_TAG ) was built ,Xue and Palmer ( 2005 ) andXue ( 2008 ) have produced more complete and systematic research on Chinese SRL .', '']","['', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank( #AUTHOR_TAG ) was built ,Xue and Palmer ( 2005 ) andXue ( 2008 ) have produced more complete and systematic research on Chinese SRL .', '']","['', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank( #AUTHOR_TAG ) was built ,Xue and Palmer ( 2005 ) andXue ( 2008 ) have produced more complete and systematic research on Chinese SRL .', '']",0
"['and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL( #AUTHOR_TAG , Xue 2008 ) reassured these findings']","['and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL( #AUTHOR_TAG , Xue 2008 ) reassured these findings']","['and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL( #AUTHOR_TAG , Xue 2008 ) reassured these findings']","['', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL( #AUTHOR_TAG , Xue 2008 ) reassured these findings']",4
"['has built a semantic role classifier exploiting the interdependence of semantic roles .', 'It has turned the single point classification problem into the sequence labeling problem with the introduction of semantic context features.', 'Se- mantic context features indicates the features ex- tracted from the arguments around the current one.', '']","['has built a semantic role classifier exploiting the interdependence of semantic roles .', 'It has turned the single point classification problem into the sequence labeling problem with the introduction of semantic context features.', 'Se- mantic context features indicates the features ex- tracted from the arguments around the current one.', '']","['has built a semantic role classifier exploiting the interdependence of semantic roles .', 'It has turned the single point classification problem into the sequence labeling problem with the introduction of semantic context features.', 'Se- mantic context features indicates the features ex- tracted from the arguments around the current one.', '']","['has built a semantic role classifier exploiting the interdependence of semantic roles .', 'It has turned the single point classification problem into the sequence labeling problem with the introduction of semantic context features.', 'Se- mantic context features indicates the features ex- tracted from the arguments around the current one.', '']",5
"['we make discriminations of arguments and adjuncts, the analysis is still coarse-grained.', '#AUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement .', '']","['we make discriminations of arguments and adjuncts, the analysis is still coarse-grained.', '#AUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement .', '']","['we make discriminations of arguments and adjuncts, the analysis is still coarse-grained.', '#AUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement .', '']","['we make discriminations of arguments and adjuncts, the analysis is still coarse-grained.', '#AUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement .', '']",1
"['ared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as#AUTHOR_TAG ,Xue and Palmer ( 2005 ) andXue ( 2008 ) .', '']","['ared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as#AUTHOR_TAG ,Xue and Palmer ( 2005 ) andXue ( 2008 ) .', '']","['ared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as#AUTHOR_TAG ,Xue and Palmer ( 2005 ) andXue ( 2008 ) .', '']","['ared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as#AUTHOR_TAG ,Xue and Palmer ( 2005 ) andXue ( 2008 ) .', '']",0
"['prove that our method is effective , we also make a comparison between the performances of our system andXue and Palmer ( 2005 ) ,#AUTHOR_TAG .', 'Xue (2008) is the best SRL system until now and it has the same data setting with ours.', 'The results are presented in Table 6']","['prove that our method is effective , we also make a comparison between the performances of our system andXue and Palmer ( 2005 ) ,#AUTHOR_TAG .', 'Xue (2008) is the best SRL system until now and it has the same data setting with ours.', 'The results are presented in Table 6']","['prove that our method is effective , we also make a comparison between the performances of our system andXue and Palmer ( 2005 ) ,#AUTHOR_TAG .', 'Xue (2008) is the best SRL system until now and it has the same data setting with ours.', 'The results are presented in Table 6']","['prove that our method is effective , we also make a comparison between the performances of our system andXue and Palmer ( 2005 ) ,#AUTHOR_TAG .', 'Xue (2008) is the best SRL system until now and it has the same data setting with ours.', '']",1
"['', 'The architectures of hierarchical semantic role classifiers can 2 Extra features e.g.', 'predicate may be still useful because that the information, provided by the high-level description of selfdescriptive features, e.g.', 'phrase type, are limited.', 'be found in figure 2 , which is similar with that in#AUTHOR_TAG']","['', 'The architectures of hierarchical semantic role classifiers can 2 Extra features e.g.', 'predicate may be still useful because that the information, provided by the high-level description of selfdescriptive features, e.g.', 'phrase type, are limited.', 'be found in figure 2 , which is similar with that in#AUTHOR_TAG']","['', 'The architectures of hierarchical semantic role classifiers can 2 Extra features e.g.', 'predicate may be still useful because that the information, provided by the high-level description of selfdescriptive features, e.g.', 'phrase type, are limited.', 'be found in figure 2 , which is similar with that in#AUTHOR_TAG']","['', 'The architectures of hierarchical semantic role classifiers can 2 Extra features e.g.', 'predicate may be still useful because that the information, provided by the high-level description of selfdescriptive features, e.g.', 'phrase type, are limited.', 'be found in figure 2 , which is similar with that in#AUTHOR_TAG']",1
"['and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from#AUTHOR_TAG']","['and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from#AUTHOR_TAG']","['and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from#AUTHOR_TAG']","[', subcat frame , phrase type , first word , last word , subcat frame + , predicate , path , head word and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from#AUTHOR_TAG']",5
"['arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation( #AUTHOR_TAG ) .', 'With the efforts of many researchers (Carreras and Màrquez 2004, Moschitti 2004,']","['arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation( #AUTHOR_TAG ) .', 'With the efforts of many researchers (Carreras and Màrquez 2004, Moschitti 2004,']","['the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation( #AUTHOR_TAG ) .', 'With the efforts of many researchers (Carreras and Màrquez 2004, Moschitti 2004,']","['', 'Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation( #AUTHOR_TAG ) .', 'With the efforts of many researchers (Carreras and Màrquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast']",0
"['', 'We use the same data setting with#AUTHOR_TAG , however a bit different fromXue and Palmer ( 2005 )']","['', 'We use the same data setting with#AUTHOR_TAG , however a bit different fromXue and Palmer ( 2005 )']","['', 'We use the same data setting with#AUTHOR_TAG , however a bit different fromXue and Palmer ( 2005 )']","['', 'We use the same data setting with#AUTHOR_TAG , however a bit different fromXue and Palmer ( 2005 )']",5
"['ared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such asSun and Jurafsky ( 2004 ) ,Xue and Palmer ( 2005 ) and#AUTHOR_TAG .', 'Sun and Jurafsky (2004)']","['ared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such asSun and Jurafsky ( 2004 ) ,Xue and Palmer ( 2005 ) and#AUTHOR_TAG .', 'Sun and Jurafsky (2004)']","['ared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such asSun and Jurafsky ( 2004 ) ,Xue and Palmer ( 2005 ) and#AUTHOR_TAG .', 'Sun and Jurafsky (2004)']","['ared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such asSun and Jurafsky ( 2004 ) ,Xue and Palmer ( 2005 ) and#AUTHOR_TAG .', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', '']",0
"['.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL ( Xue and Palmer 2005 ,#AUTHOR_TAG ) reassured these findings']","['classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL ( Xue and Palmer 2005 ,#AUTHOR_TAG ) reassured these findings']","['.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL ( Xue and Palmer 2005 ,#AUTHOR_TAG ) reassured these findings']","['', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL ( Xue and Palmer 2005 ,#AUTHOR_TAG ) reassured these findings']",0
"['', '#AUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', '']","['', '#AUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', '']","['', '#AUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', '']","['', '#AUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', '']",0
"['ared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such asSun and Jurafsky ( 2004 ) ,#AUTHOR_TAG andXue ( 2008 ) .', '']","['ared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such asSun and Jurafsky ( 2004 ) ,#AUTHOR_TAG andXue ( 2008 ) .', '']","['ared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such asSun and Jurafsky ( 2004 ) ,#AUTHOR_TAG andXue ( 2008 ) .', '']","['ared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such asSun and Jurafsky ( 2004 ) ,#AUTHOR_TAG andXue ( 2008 ) .', '']",0
"['', 'We use the same data setting withXue ( 2008 ) , however a bit different from#AUTHOR_TAG']","['', 'We use the same data setting withXue ( 2008 ) , however a bit different from#AUTHOR_TAG']","['', 'We use the same data setting withXue ( 2008 ) , however a bit different from#AUTHOR_TAG']","['', 'We use the same data setting withXue ( 2008 ) , however a bit different from#AUTHOR_TAG']",1
['candidate feature templates include : Voice from#AUTHOR_TAG'],['candidate feature templates include : Voice from#AUTHOR_TAG'],['candidate feature templates include : Voice from#AUTHOR_TAG'],['candidate feature templates include : Voice from#AUTHOR_TAG'],5
"['Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank( #AUTHOR_TAG ) .', 'It is constituted of two parts.', 'One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Treebank.', 'The other is a dictionary which lists the frames of all the labeled predicates.', 'Figure ']","['Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank( #AUTHOR_TAG ) .', 'It is constituted of two parts.', 'One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Treebank.', 'The other is a dictionary which lists the frames of all the labeled predicates.', 'Figure 1']","['Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank( #AUTHOR_TAG ) .', 'It is constituted of two parts.', 'One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Treebank.', 'The other is a dictionary which lists the frames of all the labeled predicates.', 'Figure 1']","['Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank( #AUTHOR_TAG ) .', 'It is constituted of two parts.', 'One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Treebank.', 'The other is a dictionary which lists the frames of all the labeled predicates.', '']",5
"['antic Role labeling ( SRL ) was first defined in#AUTHOR_TAG .', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag']","['antic Role labeling ( SRL ) was first defined in#AUTHOR_TAG .', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag']","['antic Role labeling ( SRL ) was first defined in#AUTHOR_TAG .', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag']","['antic Role labeling ( SRL ) was first defined in#AUTHOR_TAG .', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.', 'Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction (Surdeanu et al. 2003), and Machine Translation (Boas 2002).', '']",0
"['most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance( #AUTHOR_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .', '']","['most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance( #AUTHOR_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .', '']","['most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance( #AUTHOR_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .', '']","['most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance( #AUTHOR_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .', '']",0
"['most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance( Blume , 2005 ; #AUTHOR_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .', '']","['most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance( Blume , 2005 ; #AUTHOR_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .', '']","['most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance( Blume , 2005 ; #AUTHOR_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .', '']","['most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance( Blume , 2005 ; #AUTHOR_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .', '']",0
"['most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance( Blume , 2005 ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; #AUTHOR_TAG ) .', '']","['most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance( Blume , 2005 ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; #AUTHOR_TAG ) .', '']","['most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance( Blume , 2005 ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; #AUTHOR_TAG ) .', '']","['most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance( Blume , 2005 ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; #AUTHOR_TAG ) .', '']",0
"['', 'Other representations use the link structure( #AUTHOR_TAG ) or generate graph representations of the extracted features( Kalashnikov et al. , 2007 ) .', '']","['as the only feature(Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance(Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'Other representations use the link structure( #AUTHOR_TAG ) or generate graph representations of the extracted features( Kalashnikov et al. , 2007 ) .', '']","['', 'Other representations use the link structure( #AUTHOR_TAG ) or generate graph representations of the extracted features( Kalashnikov et al. , 2007 ) .', '']","['', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name(Bagga and Baldwin, 1998;Gooi and Allan, 2004) .', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature(Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance(Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'Other representations use the link structure( #AUTHOR_TAG ) or generate graph representations of the extracted features( Kalashnikov et al. , 2007 ) .', '']",0
"['', 'Some researchers( Cucerzan , 2007 ; #AUTHOR_TAG ) have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipedia']","['', 'Some researchers( Cucerzan , 2007 ; #AUTHOR_TAG ) have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipedia']","['', 'Some researchers( Cucerzan , 2007 ; #AUTHOR_TAG ) have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipedia']","['', 'Some researchers( Cucerzan , 2007 ; #AUTHOR_TAG ) have explored the use of Wikipedia information to improve the disambiguation process .', '']",0
"['.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own( Artiles et al. , 2005 ; #AUTHOR_TAG )']","['the WSD literature.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own( Artiles et al. , 2005 ; #AUTHOR_TAG )']","['the WSD literature.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own( Artiles et al. , 2005 ; #AUTHOR_TAG )']","['', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own( Artiles et al. , 2005 ; #AUTHOR_TAG )']",0
"['2The WePS-1 corpus includes data from the Web03 testbed( #AUTHOR_TAG ) which follows similar annotation guidelines , although the number of document per ambiguous name is more variable']","['2The WePS-1 corpus includes data from the Web03 testbed( #AUTHOR_TAG ) which follows similar annotation guidelines , although the number of document per ambiguous name is more variable']","['2The WePS-1 corpus includes data from the Web03 testbed( #AUTHOR_TAG ) which follows similar annotation guidelines , although the number of document per ambiguous name is more variable']","['2The WePS-1 corpus includes data from the Web03 testbed( #AUTHOR_TAG ) which follows similar annotation guidelines , although the number of document per ambiguous name is more variable']",5
"['different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name( Bagga and Baldwin , 1998 ; #AUTHOR_TAG ) .', '']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name( Bagga and Baldwin , 1998 ; #AUTHOR_TAG ) .', '']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name( Bagga and Baldwin , 1998 ; #AUTHOR_TAG ) .', '']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name( Bagga and Baldwin , 1998 ; #AUTHOR_TAG ) .', '']",0
"['.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own( #AUTHOR_TAG ; Artiles et al. , 2007 )']","['the WSD literature.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own( #AUTHOR_TAG ; Artiles et al. , 2007 )']","['the WSD literature.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own( #AUTHOR_TAG ; Artiles et al. , 2007 )']","['', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own( #AUTHOR_TAG ; Artiles et al. , 2007 )']",0
"['', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name(Bagga and Baldwin, 1998;Gooi and Allan, 2004) .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance( #AUTHOR_TAG ; Popescu and Magnini , 2007 ) - .', '']","['', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name(Bagga and Baldwin, 1998;Gooi and Allan, 2004) .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance( #AUTHOR_TAG ; Popescu and Magnini , 2007 ) - .', '']","['', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name(Bagga and Baldwin, 1998;Gooi and Allan, 2004) .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance( #AUTHOR_TAG ; Popescu and Magnini , 2007 ) - .', '']","['', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name(Bagga and Baldwin, 1998;Gooi and Allan, 2004) .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance( #AUTHOR_TAG ; Popescu and Magnini , 2007 ) - .', '']",0
"['', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature(Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance(Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'Other representations use the link structure( Malin , 2005 ) or generate graph representations of the extracted features( #AUTHOR_TAG ) .', '']","['', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature(Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance(Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'Other representations use the link structure( Malin , 2005 ) or generate graph representations of the extracted features( #AUTHOR_TAG ) .', '']","['', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature(Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance(Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'Other representations use the link structure( Malin , 2005 ) or generate graph representations of the extracted features( #AUTHOR_TAG ) .', '']","['', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name(Bagga and Baldwin, 1998;Gooi and Allan, 2004) .', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature(Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance(Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'Other representations use the link structure( Malin , 2005 ) or generate graph representations of the extracted features( #AUTHOR_TAG ) .', '']",0
"['study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names( #AUTHOR_TAG ) .', '']","['study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names( #AUTHOR_TAG ) .', '']","['study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names( #AUTHOR_TAG ) .', '']","['study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names( #AUTHOR_TAG ) .', '']",0
"['', '#AUTHOR_TAG']","['', '#AUTHOR_TAG']","['', '#AUTHOR_TAG']","['', '#AUTHOR_TAG compared the performace of NEs versus BoW features .', '']",0
"['different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name( #AUTHOR_TAG ; Gooi and Allan , 2004 ) .', '']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name( #AUTHOR_TAG ; Gooi and Allan , 2004 ) .', '']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name( #AUTHOR_TAG ; Gooi and Allan , 2004 ) .', '']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name( #AUTHOR_TAG ; Gooi and Allan , 2004 ) .', '']",0
"['AK 7 is a rule based English analyser that includes many functionalities (POS tagger, stemmer, chunker, Named Entity (NE) tagger, dependency analyser, parser, etc).', 'It provides a fine grained NE recognition covering 100 different NE types( #AUTHOR_TAG ) .', '']","['AK 7 is a rule based English analyser that includes many functionalities (POS tagger, stemmer, chunker, Named Entity (NE) tagger, dependency analyser, parser, etc).', 'It provides a fine grained NE recognition covering 100 different NE types( #AUTHOR_TAG ) .', '']","['AK 7 is a rule based English analyser that includes many functionalities (POS tagger, stemmer, chunker, Named Entity (NE) tagger, dependency analyser, parser, etc).', 'It provides a fine grained NE recognition covering 100 different NE types( #AUTHOR_TAG ) .', '']","['AK 7 is a rule based English analyser that includes many functionalities (POS tagger, stemmer, chunker, Named Entity (NE) tagger, dependency analyser, parser, etc).', 'It provides a fine grained NE recognition covering 100 different NE types( #AUTHOR_TAG ) .', 'Given the sparseness of most of these fine-grained NE types, we have merged them in coarser groups: event, facility, location, person, organisation, product, periodx, timex and numex']",5
"['study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task: 11-17% of the queries were composed of a person name with additional terms and 4% were identified as person names(Spink et al., 2004) .', 'According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people( #AUTHOR_TAG ) .', 'As the amount of information in the WWW grows, more of these people are mentioned in different web pages.', 'Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned']","['study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task: 11-17% of the queries were composed of a person name with additional terms and 4% were identified as person names(Spink et al., 2004) .', 'According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people( #AUTHOR_TAG ) .', 'As the amount of information in the WWW grows, more of these people are mentioned in different web pages.', 'Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned']","['study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task: 11-17% of the queries were composed of a person name with additional terms and 4% were identified as person names(Spink et al., 2004) .', 'According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people( #AUTHOR_TAG ) .', 'As the amount of information in the WWW grows, more of these people are mentioned in different web pages.', 'Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned']","['study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task: 11-17% of the queries were composed of a person name with additional terms and 4% were identified as person names(Spink et al., 2004) .', 'According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people( #AUTHOR_TAG ) .', 'As the amount of information in the WWW grows, more of these people are mentioned in different web pages.', 'Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned']",0
"['', 'Some researchers( #AUTHOR_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipedia']","['', 'Some researchers( #AUTHOR_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipedia']","['', 'Some researchers( #AUTHOR_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipedia']","['', 'Some researchers( #AUTHOR_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process .', '']",0
"['', 'The Web People Search task , as defined in the first WePS evaluation campaign( #AUTHOR_TAG ) , consists of grouping search results for a given name according to the different people that share it']","['', 'The Web People Search task , as defined in the first WePS evaluation campaign( #AUTHOR_TAG ) , consists of grouping search results for a given name according to the different people that share it']","['', 'The Web People Search task , as defined in the first WePS evaluation campaign( #AUTHOR_TAG ) , consists of grouping search results for a given name according to the different people that share it']","['', 'The Web People Search task , as defined in the first WePS evaluation campaign( #AUTHOR_TAG ) , consists of grouping search results for a given name according to the different people that share it']",0
"['have used the testbeds from WePS-1( #AUTHOR_TAG , 2007)2 and WePS-2(Artiles et al., 2009) evaluation campaigns 3']","['have used the testbeds from WePS-1( #AUTHOR_TAG , 2007)2 and WePS-2(Artiles et al., 2009) evaluation campaigns 3']","['have used the testbeds from WePS-1( #AUTHOR_TAG , 2007)2 and WePS-2(Artiles et al., 2009) evaluation campaigns 3']","['have used the testbeds from WePS-1( #AUTHOR_TAG , 2007)2 and WePS-2(Artiles et al., 2009) evaluation campaigns 3']",5
"['most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance( Blume , 2005 ; Chen and Martin , 2007 ; #AUTHOR_TAG ; Kalashnikov et al. , 2007 ) .', '']","['most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance( Blume , 2005 ; Chen and Martin , 2007 ; #AUTHOR_TAG ; Kalashnikov et al. , 2007 ) .', '']","['most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance( Blume , 2005 ; Chen and Martin , 2007 ; #AUTHOR_TAG ; Kalashnikov et al. , 2007 ) .', '']","['most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance( Blume , 2005 ; Chen and Martin , 2007 ; #AUTHOR_TAG ; Kalashnikov et al. , 2007 ) .', '']",0
"['disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD )( Agirre and Edmonds , 2006 ) and Cross-document Coreference ( CDC )( #AUTHOR_TAG ) .', 'Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the W']","['disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD )( Agirre and Edmonds , 2006 ) and Cross-document Coreference ( CDC )( #AUTHOR_TAG ) .', 'Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature.', 'It is only recently that the web name ambiguity has been approached as']","['disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD )( Agirre and Edmonds , 2006 ) and Cross-document Coreference ( CDC )( #AUTHOR_TAG ) .', 'Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature.', 'It is only recently that the web name ambiguity has been approached as']","['disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD )( Agirre and Edmonds , 2006 ) and Cross-document Coreference ( CDC )( #AUTHOR_TAG ) .', 'Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task -Web People Search -on its own(Artiles et al., 2005;Artiles et al., 2007)']",0
"['', 'In 2009 , the second WePS campaign showed similar trends regarding the use of NE features( #AUTHOR_TAG ) .', '']","['', 'In 2009 , the second WePS campaign showed similar trends regarding the use of NE features( #AUTHOR_TAG ) .', '']","['', 'In 2009 , the second WePS campaign showed similar trends regarding the use of NE features( #AUTHOR_TAG ) .', '']","['', 'In 2009 , the second WePS campaign showed similar trends regarding the use of NE features( #AUTHOR_TAG ) .', '']",0
"['', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name(Bagga and Baldwin, 1998;Gooi and Allan, 2004) .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature( #AUTHOR_TAG ) and sometimes in combination with others see for instance( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - .', '']","['', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name(Bagga and Baldwin, 1998;Gooi and Allan, 2004) .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature( #AUTHOR_TAG ) and sometimes in combination with others see for instance( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - .', '']","['', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name(Bagga and Baldwin, 1998;Gooi and Allan, 2004) .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature( #AUTHOR_TAG ) and sometimes in combination with others see for instance( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - .', '']","['', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name(Bagga and Baldwin, 1998;Gooi and Allan, 2004) .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature( #AUTHOR_TAG ) and sometimes in combination with others see for instance( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - .', '']",0
"['', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name(Bagga and Baldwin, 1998;Gooi and Allan, 2004) .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .', '']","['', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name(Bagga and Baldwin, 1998;Gooi and Allan, 2004) .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .', '']","['', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name(Bagga and Baldwin, 1998;Gooi and Allan, 2004) .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .', '']","['', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name(Bagga and Baldwin, 1998;Gooi and Allan, 2004) .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .', '']",0
"['our future plans include looking into optimal ways of acquiring SPs for verb classification.', 'Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.', 'Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach( Erk , 2007 ; #AUTHOR_TAG ) .', 'The number and type (']","['our future plans include looking into optimal ways of acquiring SPs for verb classification.', 'Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.', 'Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach( Erk , 2007 ; #AUTHOR_TAG ) .', 'The number and type (and']","['our future plans include looking into optimal ways of acquiring SPs for verb classification.', 'Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.', 'Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach( Erk , 2007 ; #AUTHOR_TAG ) .', 'The number and type (']","['addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification.', 'Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.', 'Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach( Erk , 2007 ; #AUTHOR_TAG ) .', '']",3
"['our future plans include looking into optimal ways of acquiring SPs for verb classification.', 'Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.', 'Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach( #AUTHOR_TAG ; Bergsma et al. , 2008 ) .', 'The number and type (']","['our future plans include looking into optimal ways of acquiring SPs for verb classification.', 'Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.', 'Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach( #AUTHOR_TAG ; Bergsma et al. , 2008 ) .', 'The number and type (and']","['our future plans include looking into optimal ways of acquiring SPs for verb classification.', 'Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.', 'Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach( #AUTHOR_TAG ; Bergsma et al. , 2008 ) .', 'The number and type (']","['addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification.', 'Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.', 'Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach( #AUTHOR_TAG ; Bergsma et al. , 2008 ) .', '']",3
"['jecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) for self training']","['conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) for self training']","['conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) for self training .', '']","['conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) for self training .', '']",3
"['jecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches( #AUTHOR_TAG ; Huang , 2008 ) for self training']","['conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches( #AUTHOR_TAG ; Huang , 2008 ) for self training']","['conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches( #AUTHOR_TAG ; Huang , 2008 ) for self training .', '']","['conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches( #AUTHOR_TAG ; Huang , 2008 ) for self training .', '']",3
"['.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches(Charniak and Johnson, 2005;Huang, 2008) for self training.', 'Self-training should also benefit other discriminatively trained parsers with latent annotations( #AUTHOR_TAG ) , although training would be much slower compared to using generative models , as in our case']","['', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches(Charniak and Johnson, 2005;Huang, 2008) for self training.', 'Self-training should also benefit other discriminatively trained parsers with latent annotations( #AUTHOR_TAG ) , although training would be much slower compared to using generative models , as in our case']","['.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches(Charniak and Johnson, 2005;Huang, 2008) for self training.', 'Self-training should also benefit other discriminatively trained parsers with latent annotations( #AUTHOR_TAG ) , although training would be much slower compared to using generative models , as in our case']","['conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches(Charniak and Johnson, 2005;Huang, 2008) for self training.', 'Self-training should also benefit other discriminatively trained parsers with latent annotations( #AUTHOR_TAG ) , although training would be much slower compared to using generative models , as in our case']",3
"['arding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features( Basili et al. , 2005 a ;Basili et al. , 2005 b ;Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees ,( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; #AUTHOR_TAG ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']","['arding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features( Basili et al. , 2005 a ;Basili et al. , 2005 b ;Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees ,( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; #AUTHOR_TAG ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']","['arding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features( Basili et al. , 2005 a ;Basili et al. , 2005 b ;Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees ,( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; #AUTHOR_TAG ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']","['arding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features( Basili et al. , 2005 a ;Basili et al. , 2005 b ;Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees ,( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; #AUTHOR_TAG ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']",3
"['arding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features( Basili et al. , 2005 a ;Basili et al. , 2005 b ;Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees ,( Giuglea and Moschitti , 2004 ; #AUTHOR_TAG ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']","['arding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features( Basili et al. , 2005 a ;Basili et al. , 2005 b ;Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees ,( Giuglea and Moschitti , 2004 ; #AUTHOR_TAG ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']","['arding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features( Basili et al. , 2005 a ;Basili et al. , 2005 b ;Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees ,( Giuglea and Moschitti , 2004 ; #AUTHOR_TAG ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']","['arding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features( Basili et al. , 2005 a ;Basili et al. , 2005 b ;Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees ,( Giuglea and Moschitti , 2004 ; #AUTHOR_TAG ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']",3
"['arding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features( Basili et al. , 2005 a ;Basili et al. , 2005 b ;Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees ,( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; #AUTHOR_TAG )']","['arding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features( Basili et al. , 2005 a ;Basili et al. , 2005 b ;Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees ,( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; #AUTHOR_TAG )']","['arding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features( Basili et al. , 2005 a ;Basili et al. , 2005 b ;Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees ,( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; #AUTHOR_TAG )']","['arding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features( Basili et al. , 2005 a ;Basili et al. , 2005 b ;Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees ,( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; #AUTHOR_TAG )']",3
"['arding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features( #AUTHOR_TAG a ;Basili et al. , 2005 b ;Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees ,( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']","['arding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features( #AUTHOR_TAG a ;Basili et al. , 2005 b ;Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees ,( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']","['arding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features( #AUTHOR_TAG a ;Basili et al. , 2005 b ;Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees ,( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']","['arding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features( #AUTHOR_TAG a ;Basili et al. , 2005 b ;Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees ,( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']",3
"['', 'When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique( #AUTHOR_TAG ) or a memory-efficient trie implementation based on a succinct data structure( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage']","['', 'When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique( #AUTHOR_TAG ) or a memory-efficient trie implementation based on a succinct data structure( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage']","['', 'When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique( #AUTHOR_TAG ) or a memory-efficient trie implementation based on a succinct data structure( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage']","['', 'When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique( #AUTHOR_TAG ) or a memory-efficient trie implementation based on a succinct data structure( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage']",3
"['reordering models we describe follow our previous work using function word models for translation( #AUTHOR_TAG ; Setiawan et al. , 2009 ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', '']","['reordering models we describe follow our previous work using function word models for translation( #AUTHOR_TAG ; Setiawan et al. , 2009 ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', '']","['reordering models we describe follow our previous work using function word models for translation( #AUTHOR_TAG ; Setiawan et al. , 2009 ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', '']","['reordering models we describe follow our previous work using function word models for translation( #AUTHOR_TAG ; Setiawan et al. , 2009 ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', '']",2
"['words.', 'With respect to the focus on function words , our reordering model is closely related to the UALIGN system( #AUTHOR_TAG ) .', 'However, UALIGN uses deep syntactic analysis and hand-crafted heuristics in its model']","['words.', 'With respect to the focus on function words , our reordering model is closely related to the UALIGN system( #AUTHOR_TAG ) .', 'However, UALIGN uses deep syntactic analysis and hand-crafted heuristics in its model']","['function words.', 'With respect to the focus on function words , our reordering model is closely related to the UALIGN system( #AUTHOR_TAG ) .', 'However, UALIGN uses deep syntactic analysis and hand-crafted heuristics in its model']","['', 'With respect to the focus on function words , our reordering model is closely related to the UALIGN system( #AUTHOR_TAG ) .', 'However, UALIGN uses deep syntactic analysis and hand-crafted heuristics in its model']",1
"['model o ( Li , S â\x86\x92 T ) , o ( Ri , S â\x86\x92 T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by#AUTHOR_TAG .', '']","['model o ( Li , S â\x86\x92 T ) , o ( Ri , S â\x86\x92 T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by#AUTHOR_TAG .', '']","['model o ( Li , S â\x86\x92 T ) , o ( Ri , S â\x86\x92 T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by#AUTHOR_TAG .', '']","['model o ( Li , S â\x86\x92 T ) , o ( Ri , S â\x86\x92 T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by#AUTHOR_TAG .', 'Formally, this model takes the form of probability distribution P ori (o(L i,S→T ), o(R i,S→T )|Y i,S→T ), which conditions the reordering on the lexical identity of the function word alignment (but independent of the lexical identity of its neighboring phrases).', '']",5
"['reordering models we describe follow our previous work using function word models for translation( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', '']","['reordering models we describe follow our previous work using function word models for translation( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', '']","['reordering models we describe follow our previous work using function word models for translation( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', '']","['reordering models we describe follow our previous work using function word models for translation( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', '']",2
"['model d ( FWi â\x88\x92 1 , S â\x86\x92 T ) , d ( FWi +1 , S â\x86\x92 T ) , i.e. whether Li , S â\x86\x92 T and Ri , S â\x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of#AUTHOR_TAG .', 'Taking d(F W i−1,S→T ) as a case in point, this model takes the']","['model d ( FWi â\x88\x92 1 , S â\x86\x92 T ) , d ( FWi +1 , S â\x86\x92 T ) , i.e. whether Li , S â\x86\x92 T and Ri , S â\x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of#AUTHOR_TAG .', 'Taking d(F W i−1,S→T ) as a case in point, this model takes the']","['model d ( FWi â\x88\x92 1 , S â\x86\x92 T ) , d ( FWi +1 , S â\x86\x92 T ) , i.e. whether Li , S â\x86\x92 T and Ri , S â\x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of#AUTHOR_TAG .', 'Taking d(F W i−1,S→T ) as a case in point, this model takes the']","['model d ( FWi â\x88\x92 1 , S â\x86\x92 T ) , d ( FWi +1 , S â\x86\x92 T ) , i.e. whether Li , S â\x86\x92 T and Ri , S â\x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of#AUTHOR_TAG .', 'Taking d(F W i−1,S→T ) as a case in point, this model takes the']",5
"['our previous work( #AUTHOR_TAG ) , we started an initial investigation on conversation entailment .', 'We have collected a dataset of 875 instances.', 'Each instance consists of a conversation segment and a hypothesis (as described in Section 1).', 'The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions.', 'We developed an approach that']","['our previous work( #AUTHOR_TAG ) , we started an initial investigation on conversation entailment .', 'We have collected a dataset of 875 instances.', 'Each instance consists of a conversation segment and a hypothesis (as described in Section 1).', 'The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions.', 'We developed an approach that']","['our previous work( #AUTHOR_TAG ) , we started an initial investigation on conversation entailment .', 'We have collected a dataset of 875 instances.', 'Each instance consists of a conversation segment and a hypothesis (as described in Section 1).', 'The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions.', 'We developed an approach that']","['our previous work( #AUTHOR_TAG ) , we started an initial investigation on conversation entailment .', 'We have collected a dataset of 875 instances.', 'Each instance consists of a conversation segment and a hypothesis (as described in Section 1).', 'The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions.', 'We developed an approach that is motivated by previous work on textual entailment.', '']",2
"['the implicit modeling of argument consistency , we follow the same approach as in our previous work( #AUTHOR_TAG ) and trained a logistic regression model to predict verb alignment based on the features in Table 1']","['the implicit modeling of argument consistency , we follow the same approach as in our previous work( #AUTHOR_TAG ) and trained a logistic regression model to predict verb alignment based on the features in Table 1']","['the implicit modeling of argument consistency , we follow the same approach as in our previous work( #AUTHOR_TAG ) and trained a logistic regression model to predict verb alignment based on the features in Table 1']","['the implicit modeling of argument consistency , we follow the same approach as in our previous work( #AUTHOR_TAG ) and trained a logistic regression model to predict verb alignment based on the features in Table 1']",2
"['that in our original work( #AUTHOR_TAG ) , only development data were used to show some initial observations.', 'Here we trained our mod- els on the development data and results shown are from the testing data']","['that in our original work( #AUTHOR_TAG ) , only development data were used to show some initial observations.', 'Here we trained our mod- els on the development data and results shown are from the testing data']","['that in our original work( #AUTHOR_TAG ) , only development data were used to show some initial observations.', 'Here we trained our mod- els on the development data and results shown are from the testing data']","['that in our original work( #AUTHOR_TAG ) , only development data were used to show some initial observations.', 'Here we trained our mod- els on the development data and results shown are from the testing data']",1
"['in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act.', 'This alignment is obtained by following the same set of rules learned from the development dataset as in( #AUTHOR_TAG )']","['in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act.', 'This alignment is obtained by following the same set of rules learned from the development dataset as in( #AUTHOR_TAG )']","['in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act.', 'This alignment is obtained by following the same set of rules learned from the development dataset as in( #AUTHOR_TAG )']","['', 'Figure 3 shows an example of alignment between the conversation terms and hypothesis terms in Example 2. Note that in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act.', 'This alignment is obtained by following the same set of rules learned from the development dataset as in( #AUTHOR_TAG )']",5
"['address this limitation , our previous work( #AUTHOR_TAG ) has initiated an investigation on the problem of conversation entailment .', 'The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H.', '']","['address this limitation , our previous work( #AUTHOR_TAG ) has initiated an investigation on the problem of conversation entailment .', 'The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H.', '']","['address this limitation , our previous work( #AUTHOR_TAG ) has initiated an investigation on the problem of conversation entailment .', 'The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H.', '']","['address this limitation , our previous work( #AUTHOR_TAG ) has initiated an investigation on the problem of conversation entailment .', 'The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H.', 'For instance, as in Example 1, the first hypothesis can be entailed from the conversation segment while the second hypothesis cannot.', '']",2
"['our previous work( #AUTHOR_TAG ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â\x88§ ... â\x88§ dm , and a hypothesis H represented by another set of clauses H = h1 â\x88§ ... â\x88§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows .', 'This is based']","['our previous work( #AUTHOR_TAG ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â\x88§ ... â\x88§ dm , and a hypothesis H represented by another set of clauses H = h1 â\x88§ ... â\x88§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows .', 'This is based']","['our previous work( #AUTHOR_TAG ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â\x88§ ... â\x88§ dm , and a hypothesis H represented by another set of clauses H = h1 â\x88§ ... â\x88§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows .', 'This is based']","['our previous work( #AUTHOR_TAG ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â\x88§ ... â\x88§ dm , and a hypothesis H represented by another set of clauses H = h1 â\x88§ ... â\x88§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows .', '']",2
"['we present the ""discrete"" structures followed by the kernel we used.', 'We use the structures previously used by#AUTHOR_TAG , and propose one new structure .', 'Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task.', '']","['we present the ""discrete"" structures followed by the kernel we used.', 'We use the structures previously used by#AUTHOR_TAG , and propose one new structure .', 'Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task.', '']","['we present the ""discrete"" structures followed by the kernel we used.', 'We use the structures previously used by#AUTHOR_TAG , and propose one new structure .', 'Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task.', '']","['', 'Now we present the ""discrete"" structures followed by the kernel we used.', 'We use the structures previously used by#AUTHOR_TAG , and propose one new structure .', 'Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task.', '']",5
"['structures perform the best.', 'This revalidates the observation of#AUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task .', '']","['structures perform the best.', 'This revalidates the observation of#AUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task .', '']","['based structures perform the best.', 'This revalidates the observation of#AUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task .', '']","['', 'This revalidates the observation of#AUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task .', '']",1
"['', 'Here , the PET and GR kernel perform similar : this is different from the results of( #AUTHOR_TAG ) where GR performed much worse than P']","['', 'Here , the PET and GR kernel perform similar : this is different from the results of( #AUTHOR_TAG ) where GR performed much worse than P']","['', 'Here , the PET and GR kernel perform similar : this is different from the results of( #AUTHOR_TAG ) where GR performed much worse than P']","['', 'Here , the PET and GR kernel perform similar : this is different from the results of( #AUTHOR_TAG ) where GR performed much worse than P']",1
"['results also confirm the insights gained by#AUTHOR_TAG , who observed that in crossdomain polarity analysis adding more training data is not always beneficial .', 'Apparently even the smallest training dataset (cameras) contain enough feature instances to learn a model which performs well on the testing data']","['results also confirm the insights gained by#AUTHOR_TAG , who observed that in crossdomain polarity analysis adding more training data is not always beneficial .', 'Apparently even the smallest training dataset (cameras) contain enough feature instances to learn a model which performs well on the testing data']","['results also confirm the insights gained by#AUTHOR_TAG , who observed that in crossdomain polarity analysis adding more training data is not always beneficial .', 'Apparently even the smallest training dataset (cameras) contain enough feature instances to learn a model which performs well on the testing data']","['results also confirm the insights gained by#AUTHOR_TAG , who observed that in crossdomain polarity analysis adding more training data is not always beneficial .', 'Apparently even the smallest training dataset (cameras) contain enough feature instances to learn a model which performs well on the testing data']",1
"['', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation( Blitzer et al. , 2007 ; #AUTHOR_TAG ) , perform in comparison to our approach .', '']","['', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation( Blitzer et al. , 2007 ; #AUTHOR_TAG ) , perform in comparison to our approach .', '']","['', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation( Blitzer et al. , 2007 ; #AUTHOR_TAG ) , perform in comparison to our approach .', '']","['', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation( Blitzer et al. , 2007 ; #AUTHOR_TAG ) , perform in comparison to our approach .', '']",3
"['', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .', '']","['', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .', '']","['', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .', '']","['', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .', '']",3
"['implementation of the transition-based dependency parsing frame- work( #AUTHOR_TAG ) using an arc-eager transi- tion strategy and are trained using the percep- tron algorithm as inZhang and Clark (2008) with a beam size of 8. Beams with varying sizes can be used to produce k-best lists.', '']","['implementation of the transition-based dependency parsing frame- work( #AUTHOR_TAG ) using an arc-eager transi- tion strategy and are trained using the percep- tron algorithm as inZhang and Clark (2008) with a beam size of 8. Beams with varying sizes can be used to produce k-best lists.', '']","['implementation of the transition-based dependency parsing frame- work( #AUTHOR_TAG ) using an arc-eager transi- tion strategy and are trained using the percep- tron algorithm as inZhang and Clark (2008) with a beam size of 8. Beams with varying sizes can be used to produce k-best lists.', '']","['implementation of the transition-based dependency parsing frame- work( #AUTHOR_TAG ) using an arc-eager transi- tion strategy and are trained using the percep- tron algorithm as inZhang and Clark (2008) with a beam size of 8. Beams with varying sizes can be used to produce k-best lists.', '']",5
"['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization( Ganchev et al. , 2010 ) and constraint driven learning( Chang et al. , 2007 ; #AUTHOR_TAG ) .', 'The work ofChang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', '']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization( Ganchev et al. , 2010 ) and constraint driven learning( Chang et al. , 2007 ; #AUTHOR_TAG ) .', 'The work ofChang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', '']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization( Ganchev et al. , 2010 ) and constraint driven learning( Chang et al. , 2007 ; #AUTHOR_TAG ) .', 'The work ofChang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', '']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization( Ganchev et al. , 2010 ) and constraint driven learning( Chang et al. , 2007 ; #AUTHOR_TAG ) .', 'The work ofChang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', '']",0
"['', 'We use the non-projective k-best MST algorithm to generate k-best lists( #AUTHOR_TAG ) , where k = 8 for the experiments in this paper .', '']","['', 'We use the non-projective k-best MST algorithm to generate k-best lists( #AUTHOR_TAG ) , where k = 8 for the experiments in this paper .', '']","['', 'We use the non-projective k-best MST algorithm to generate k-best lists( #AUTHOR_TAG ) , where k = 8 for the experiments in this paper .', '']","['', 'We use the non-projective k-best MST algorithm to generate k-best lists( #AUTHOR_TAG ) , where k = 8 for the experiments in this paper .', '']",5
"['application of the augmented-loss framework is to improve parser domain portability in the presence of partially labeled data.', 'Consider, for example, the case of questions.', '#AUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank .', 'Table']","['application of the augmented-loss framework is to improve parser domain portability in the presence of partially labeled data.', 'Consider, for example, the case of questions.', '#AUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank .', 'Table']","['application of the augmented-loss framework is to improve parser domain portability in the presence of partially labeled data.', 'Consider, for example, the case of questions.', '#AUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank .', 'Table']","['application of the augmented-loss framework is to improve parser domain portability in the presence of partially labeled data.', 'Consider, for example, the case of questions.', '#AUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank .', '']",1
['1Our rules are similar to those from#AUTHOR_TAG'],['1Our rules are similar to those from#AUTHOR_TAG'],['1Our rules are similar to those from#AUTHOR_TAG'],['1Our rules are similar to those from#AUTHOR_TAG'],1
"['accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering( Wang et al. , 2007 ) , sentiment analysis( #AUTHOR_TAG ) , MT reordering( Xu et al. , 2009 ) , and many other tasks .', '']","['accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering( Wang et al. , 2007 ) , sentiment analysis( #AUTHOR_TAG ) , MT reordering( Xu et al. , 2009 ) , and many other tasks .', '']","['accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering( Wang et al. , 2007 ) , sentiment analysis( #AUTHOR_TAG ) , MT reordering( Xu et al. , 2009 ) , and many other tasks .', '']","['accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering( Wang et al. , 2007 ) , sentiment analysis( #AUTHOR_TAG ) , MT reordering( Xu et al. , 2009 ) , and many other tasks .', '']",0
"['implementation of graph- based parsing algorithms with an arc-factored parameterization( #AUTHOR_TAG ) .', '']","['implementation of graph- based parsing algorithms with an arc-factored parameterization( #AUTHOR_TAG ) .', '']","['implementation of graph- based parsing algorithms with an arc-factored parameterization( #AUTHOR_TAG ) .', '']","['implementation of graph- based parsing algorithms with an arc-factored parameterization( #AUTHOR_TAG ) .', '']",5
"['method is called targeted self-training as it is similar in vein to self-training( #AUTHOR_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings']","['method is called targeted self-training as it is similar in vein to self-training( #AUTHOR_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings']","['to produce k-best lists.', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'The method is called targeted self-training as it is similar in vein to self-training( #AUTHOR_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings .', '']","['recent study by also investigates the task of training parsers to improve MT reordering.', 'In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'The method is called targeted self-training as it is similar in vein to self-training( #AUTHOR_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings .', '']",1
"['', 'Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality( #AUTHOR_TAG ) and is simpler to measure .', '']","['', 'Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality( #AUTHOR_TAG ) and is simpler to measure .', '']","['', 'Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality( #AUTHOR_TAG ) and is simpler to measure .', '']","['', 'Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality( #AUTHOR_TAG ) and is simpler to measure .', '']",4
"['accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering( Wang et al. , 2007 ) , sentiment analysis( Nakagawa et al. , 2010 ) , MT reordering( #AUTHOR_TAG ) , and many other tasks .', '']","['accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering( Wang et al. , 2007 ) , sentiment analysis( Nakagawa et al. , 2010 ) , MT reordering( #AUTHOR_TAG ) , and many other tasks .', '']","['accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering( Wang et al. , 2007 ) , sentiment analysis( Nakagawa et al. , 2010 ) , MT reordering( #AUTHOR_TAG ) , and many other tasks .', '']","['accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering( Wang et al. , 2007 ) , sentiment analysis( Nakagawa et al. , 2010 ) , MT reordering( #AUTHOR_TAG ) , and many other tasks .', '']",0
"['Transition-based: An implementation of the transition-based dependency parsing framework( Nivre , 2008 ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in#AUTHOR_TAG with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists.', '']","['Transition-based: An implementation of the transition-based dependency parsing framework( Nivre , 2008 ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in#AUTHOR_TAG with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists.', '']","['Transition-based: An implementation of the transition-based dependency parsing framework( Nivre , 2008 ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in#AUTHOR_TAG with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists.', '']","['Transition-based: An implementation of the transition-based dependency parsing framework( Nivre , 2008 ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in#AUTHOR_TAG with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists.', '']",5
"['MT reordering(Xu et al., 2009) , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data( Gildea , 2001 ; McClosky et al. , 2006 ; #AUTHOR_TAG ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There']","['MT reordering(Xu et al., 2009) , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data( Gildea , 2001 ; McClosky et al. , 2006 ; #AUTHOR_TAG ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There']","['MT reordering(Xu et al., 2009) , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data( Gildea , 2001 ; McClosky et al. , 2006 ; #AUTHOR_TAG ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There']","['accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering(Wang et al., 2007) , sentiment analysis(Nakagawa et al., 2010) , MT reordering(Xu et al., 2009) , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data( Gildea , 2001 ; McClosky et al. , 2006 ; #AUTHOR_TAG ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', '']",0
"['terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB )( #AUTHOR_TAG ) .', 'We also make use of the Brown corpus, and the Question Treebank (QTB)']","['terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB )( #AUTHOR_TAG ) .', 'We also make use of the Brown corpus, and the Question Treebank (QTB)']","['terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB )( #AUTHOR_TAG ) .', 'We also make use of the Brown corpus, and the Question Treebank (QTB)']","['terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB )( #AUTHOR_TAG ) .', 'We also make use of the Brown corpus, and the Question Treebank (QTB)']",5
"['MT reordering(Xu et al., 2009) , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data( Gildea , 2001 ; #AUTHOR_TAG ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There']","['MT reordering(Xu et al., 2009) , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data( Gildea , 2001 ; #AUTHOR_TAG ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There']","['MT reordering(Xu et al., 2009) , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data( Gildea , 2001 ; #AUTHOR_TAG ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There']","['accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering(Wang et al., 2007) , sentiment analysis(Nakagawa et al., 2010) , MT reordering(Xu et al., 2009) , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data( Gildea , 2001 ; #AUTHOR_TAG ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', '']",0
"['work that is most similar to ours is that of#AUTHOR_TAG , who introduced the Constraint Driven Learning algorithm ( CODL ) .', 'Their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled data (what we call extrinsic datasets).', 'For each unlabeled example, they use the current model along with their set of constraints to select a set of k automatically labeled examples which best meet the constraints.', 'These induced examples are then added to their training set and, after processing each unlabeled dataset, they perform full model optimization with the concatenation of training data and newly generated training items.', 'The augmented']","['work that is most similar to ours is that of#AUTHOR_TAG , who introduced the Constraint Driven Learning algorithm ( CODL ) .', 'Their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled data (what we call extrinsic datasets).', 'For each unlabeled example, they use the current model along with their set of constraints to select a set of k automatically labeled examples which best meet the constraints.', 'These induced examples are then added to their training set and, after processing each unlabeled dataset, they perform full model optimization with the concatenation of training data and newly generated training items.', 'The augmented-loss']","['work that is most similar to ours is that of#AUTHOR_TAG , who introduced the Constraint Driven Learning algorithm ( CODL ) .', 'Their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled data (what we call extrinsic datasets).', 'For each unlabeled example, they use the current model along with their set of constraints to select a set of k automatically labeled examples which best meet the constraints.', 'These induced examples are then added to their training set and, after processing each unlabeled dataset, they perform full model optimization with the concatenation of training data and newly generated training items.', 'The augmented-loss algorithm can be viewed']","['work that is most similar to ours is that of#AUTHOR_TAG , who introduced the Constraint Driven Learning algorithm ( CODL ) .', 'Their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled data (what we call extrinsic datasets).', 'For each unlabeled example, they use the current model along with their set of constraints to select a set of k automatically labeled examples which best meet the constraints.', 'These induced examples are then added to their training set and, after processing each unlabeled dataset, they perform full model optimization with the concatenation of training data and newly generated training items.', '']",1
"['', '#AUTHOR_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system .', '']","['evaluate the method on alternate extrinsic loss functions.', '#AUTHOR_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system .', '']","['evaluate the method on alternate extrinsic loss functions.', '#AUTHOR_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system .', '']","['', 'Furthermore, we also evaluate the method on alternate extrinsic loss functions.', '#AUTHOR_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system .', 'Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score.', 'In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score.', 'Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic (parsing) objectives']",1
"['recent study by#AUTHOR_TAG also investigates the task of training parsers to improve MT reordering .', 'In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'The method is called targeted self-training']","['recent study by#AUTHOR_TAG also investigates the task of training parsers to improve MT reordering .', 'In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'The method is called targeted self-training']","['recent study by#AUTHOR_TAG also investigates the task of training parsers to improve MT reordering .', 'In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'The method is called targeted self-training']","['recent study by#AUTHOR_TAG also investigates the task of training parsers to improve MT reordering .', 'In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', '']",1
"['', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction( #AUTHOR_TAG ) and textual entailment( Berant et al. , 2010 )']","['', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction( #AUTHOR_TAG ) and textual entailment( Berant et al. , 2010 )']","['', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction( #AUTHOR_TAG ) and textual entailment( Berant et al. , 2010 )']","['', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction( #AUTHOR_TAG ) and textual entailment( Berant et al. , 2010 )']",0
"['', 'Proof.', 'Identical to the standard perceptron proof , e.g. ,#AUTHOR_TAG , by inserting in loss-separability for normal separability']","['', 'Proof.', 'Identical to the standard perceptron proof , e.g. ,#AUTHOR_TAG , by inserting in loss-separability for normal separability']","['', 'Proof.', 'Identical to the standard perceptron proof , e.g. ,#AUTHOR_TAG , by inserting in loss-separability for normal separability']","['', 'Proof.', 'Identical to the standard perceptron proof , e.g. ,#AUTHOR_TAG , by inserting in loss-separability for normal separability']",0
"['', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies( #AUTHOR_TAG ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction(Yates and Etzioni, 2009) and textual entailment(Berant et al., 2010)']","['', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies( #AUTHOR_TAG ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction(Yates and Etzioni, 2009) and textual entailment(Berant et al., 2010)']","['', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies( #AUTHOR_TAG ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction(Yates and Etzioni, 2009) and textual entailment(Berant et al., 2010)']","['', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies( #AUTHOR_TAG ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction(Yates and Etzioni, 2009) and textual entailment(Berant et al., 2010)']",4
"['MT reordering(Xu et al., 2009) , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There']","['MT reordering(Xu et al., 2009) , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There']","['MT reordering(Xu et al., 2009) , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There']","['accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering(Wang et al., 2007) , sentiment analysis(Nakagawa et al., 2010) , MT reordering(Xu et al., 2009) , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', '']",0
"['obvious approach to this problem is to employ parser reranking( #AUTHOR_TAG ) .', 'In such a setting, an auxiliary reranker is added in a pipeline following the parser.', 'The standard setting involves training the base parser and applying it to a development set (this is often done in a cross-validated jack-knife training framework).', 'The reranker can then be trained to optimize for the downstream or extrinsic']","['obvious approach to this problem is to employ parser reranking( #AUTHOR_TAG ) .', 'In such a setting, an auxiliary reranker is added in a pipeline following the parser.', 'The standard setting involves training the base parser and applying it to a development set (this is often done in a cross-validated jack-knife training framework).', 'The reranker can then be trained to optimize for the downstream or extrinsic']","['obvious approach to this problem is to employ parser reranking( #AUTHOR_TAG ) .', 'In such a setting, an auxiliary reranker is added in a pipeline following the parser.', 'The standard setting involves training the base parser and applying it to a development set (this is often done in a cross-validated jack-knife training framework).', 'The reranker can then be trained to optimize for the downstream or extr']","['obvious approach to this problem is to employ parser reranking( #AUTHOR_TAG ) .', 'In such a setting, an auxiliary reranker is added in a pipeline following the parser.', 'The standard setting involves training the base parser and applying it to a development set (this is often done in a cross-validated jack-knife training framework).', 'The reranker can then be trained to optimize for the downstream or extrinsic objective.', '']",0
"['', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction( Yates and Etzioni , 2009 ) and textual entailment( #AUTHOR_TAG )']","['', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction( Yates and Etzioni , 2009 ) and textual entailment( #AUTHOR_TAG )']","['', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction( Yates and Etzioni , 2009 ) and textual entailment( #AUTHOR_TAG )']","['', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction( Yates and Etzioni , 2009 ) and textual entailment( #AUTHOR_TAG )']",0
"['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization( Ganchev et al. , 2010 ) and constraint driven learning( #AUTHOR_TAG ; Chang et al. , 2010 ) .', 'The work ofChang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', '']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization( Ganchev et al. , 2010 ) and constraint driven learning( #AUTHOR_TAG ; Chang et al. , 2010 ) .', 'The work ofChang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', '']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization( Ganchev et al. , 2010 ) and constraint driven learning( #AUTHOR_TAG ; Chang et al. , 2010 ) .', 'The work ofChang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', '']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization( Ganchev et al. , 2010 ) and constraint driven learning( #AUTHOR_TAG ; Chang et al. , 2010 ) .', 'The work ofChang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', '']",1
"['implementation of the transition-based dependency parsing framework( #AUTHOR_TAG ) using an arc-eager transition strategy and are trained using the perceptron algorithm as inZhang and Clark ( 2008 ) with a beam size of 8 .', 'Beams with varying sizes can be used to produce k-best lists.', '']","['implementation of the transition-based dependency parsing framework( #AUTHOR_TAG ) using an arc-eager transition strategy and are trained using the perceptron algorithm as inZhang and Clark ( 2008 ) with a beam size of 8 .', 'Beams with varying sizes can be used to produce k-best lists.', '']","['implementation of the transition-based dependency parsing framework( #AUTHOR_TAG ) using an arc-eager transition strategy and are trained using the perceptron algorithm as inZhang and Clark ( 2008 ) with a beam size of 8 .', 'Beams with varying sizes can be used to produce k-best lists.', '']","['implementation of the transition-based dependency parsing framework( #AUTHOR_TAG ) using an arc-eager transition strategy and are trained using the perceptron algorithm as inZhang and Clark ( 2008 ) with a beam size of 8 .', 'Beams with varying sizes can be used to produce k-best lists.', '']",5
"['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation( #AUTHOR_TAG ) , posterior regularization( Ganchev et al. , 2010 ) and constraint driven learning( Chang et al. , 2007 ; Chang et al. , 2010 ) .', 'The work ofChang et al. (2007) on constraint driven learning is perhaps the closest to our framework']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation( #AUTHOR_TAG ) , posterior regularization( Ganchev et al. , 2010 ) and constraint driven learning( Chang et al. , 2007 ; Chang et al. , 2010 ) .', 'The work ofChang et al. (2007) on constraint driven learning is perhaps the closest to our framework']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation( #AUTHOR_TAG ) , posterior regularization( Ganchev et al. , 2010 ) and constraint driven learning( Chang et al. , 2007 ; Chang et al. , 2010 ) .', 'The work ofChang et al. (2007) on constraint driven learning is perhaps the closest to our framework']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation( #AUTHOR_TAG ) , posterior regularization( Ganchev et al. , 2010 ) and constraint driven learning( Chang et al. , 2007 ; Chang et al. , 2010 ) .', 'The work ofChang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', '']",0
"['', 'For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS )( #AUTHOR_TAG )']","['', 'For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS )( #AUTHOR_TAG )']","['', 'For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS )( #AUTHOR_TAG )']","['', 'For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS )( #AUTHOR_TAG )']",5
"['MT reordering(Xu et al., 2009) , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #AUTHOR_TAG ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There']","['MT reordering(Xu et al., 2009) , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #AUTHOR_TAG ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There']","['MT reordering(Xu et al., 2009) , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #AUTHOR_TAG ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There']","['accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering(Wang et al., 2007) , sentiment analysis(Nakagawa et al., 2010) , MT reordering(Xu et al., 2009) , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #AUTHOR_TAG ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', '']",0
"['accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering( #AUTHOR_TAG ) , sentiment analysis( Nakagawa et al. , 2010 ) , MT reordering( Xu et al. , 2009 ) , and many other tasks .', '']","['accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering( #AUTHOR_TAG ) , sentiment analysis( Nakagawa et al. , 2010 ) , MT reordering( Xu et al. , 2009 ) , and many other tasks .', '']","['accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering( #AUTHOR_TAG ) , sentiment analysis( Nakagawa et al. , 2010 ) , MT reordering( Xu et al. , 2009 ) , and many other tasks .', '']","['accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering( #AUTHOR_TAG ) , sentiment analysis( Nakagawa et al. , 2010 ) , MT reordering( Xu et al. , 2009 ) , and many other tasks .', '']",0
['and data used in our experiments are based on the work of#AUTHOR_TAG'],['and data used in our experiments are based on the work of#AUTHOR_TAG'],['and data used in our experiments are based on the work of#AUTHOR_TAG'],['and data used in our experiments are based on the work of#AUTHOR_TAG'],5
"['this paper , inspired by KNN-SVM( #AUTHOR_TAG ) , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems .', '']","['this paper , inspired by KNN-SVM( #AUTHOR_TAG ) , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems .', '']","['this paper , inspired by KNN-SVM( #AUTHOR_TAG ) , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems .', '']","['this paper , inspired by KNN-SVM( #AUTHOR_TAG ) , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems .', '']",4
"['to the existence of L2 norm in objective function (5), the optimization algorithm MERT can not be applied for this question since the exact line search routine does not hold here.', 'Motivated by(#AUTHOR_TAG , 2003;Smith and Eisner, 2006) , we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW−WbII2+ A � j=1 Systems NIST02 NIST05 N']","['to the existence of L2 norm in objective function (5), the optimization algorithm MERT can not be applied for this question since the exact line search routine does not hold here.', 'Motivated by(#AUTHOR_TAG , 2003;Smith and Eisner, 2006) , we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW−WbII2+ A � j=1 Systems NIST02 NIST05']","['to the existence of L2 norm in objective function (5), the optimization algorithm MERT can not be applied for this question since the exact line search routine does not hold here.', 'Motivated by(#AUTHOR_TAG , 2003;Smith and Eisner, 2006) , we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW−WbII2+ A � j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison']",['(#AUTHOR_TAG'],4
"['introduced the log-linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem']","['introduced the log-linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem']","['introduced the log-linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem']","['introduced the log-linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem']",0
"['method resorts to some translation examples , which is similar as example-based translation or translation memory( #AUTHOR_TAG ; He et al. , 2010 ; Ma et al. , 2011 ) .', '']","['method resorts to some translation examples , which is similar as example-based translation or translation memory( #AUTHOR_TAG ; He et al. , 2010 ; Ma et al. , 2011 ) .', '']","['method resorts to some translation examples , which is similar as example-based translation or translation memory( #AUTHOR_TAG ; He et al. , 2010 ; Ma et al. , 2011 ) .', '']","['method resorts to some translation examples , which is similar as example-based translation or translation memory( #AUTHOR_TAG ; He et al. , 2010 ; Ma et al. , 2011 ) .', 'Instead of using translation examples to construct translation rules for enlarging the decoding space, we employed them to discriminatively learn local weights']",1
"['if we use LSH technique( #AUTHOR_TAG ) in retrieval process , the local method can be easily scaled to a larger training data']","['if we use LSH technique( #AUTHOR_TAG ) in retrieval process , the local method can be easily scaled to a larger training data']","['if we use LSH technique( #AUTHOR_TAG ) in retrieval process , the local method can be easily scaled to a larger training data']","['', 'Actually , if we use LSH technique( #AUTHOR_TAG ) in retrieval process , the local method can be easily scaled to a larger training data']",3
"['to learn weights for MT.', '(Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #AUTHOR_TAG ) employed an evaluation metric as a loss function and directly optimized it']","['to learn weights for MT.', '(Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #AUTHOR_TAG ) employed an evaluation metric as a loss function and directly optimized it.', '']","['to learn weights for MT.', '(Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #AUTHOR_TAG ) employed an evaluation metric as a loss function and directly optimized it']","['works have proposed discriminative tech- niques to train log-linear model for SMT.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '(Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #AUTHOR_TAG ) employed an evaluation metric as a loss function and directly optimized it.', '']",1
"['local training method( #AUTHOR_TAG ) is widely employed in computer vision( Zhang et al. , 2006 ; Cheng et al. , 2010 ) .', '']","['local training method( #AUTHOR_TAG ) is widely employed in computer vision( Zhang et al. , 2006 ; Cheng et al. , 2010 ) .', '']","['local training method( #AUTHOR_TAG ) is widely employed in computer vision( Zhang et al. , 2006 ; Cheng et al. , 2010 ) .', '']","['local training method( #AUTHOR_TAG ) is widely employed in computer vision( Zhang et al. , 2006 ; Cheng et al. , 2010 ) .', '']",0
"['. Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate( Och , 2003 ; #AUTHOR_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT )( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set']","['. Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate( Och , 2003 ; #AUTHOR_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT )( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', '']","['a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate( Och , 2003 ; #AUTHOR_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT )( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', '']","['', 'h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate( Och , 2003 ; #AUTHOR_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT )( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', '']",0
"['', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits( Stolcke , 2002 ) with modified Kneser-Ney smoothing( #AUTHOR_TAG ) .', '']","['', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits( Stolcke , 2002 ) with modified Kneser-Ney smoothing( #AUTHOR_TAG ) .', '']","['', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits( Stolcke , 2002 ) with modified Kneser-Ney smoothing( #AUTHOR_TAG ) .', '']","['', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits( Stolcke , 2002 ) with modified Kneser-Ney smoothing( #AUTHOR_TAG ) .', '']",5
"['', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits( #AUTHOR_TAG ) with modified Kneser-Ney smoothing( Chen and Goodman , 1998 ) .', '']","['', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits( #AUTHOR_TAG ) with modified Kneser-Ney smoothing( Chen and Goodman , 1998 ) .', '']","['', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits( #AUTHOR_TAG ) with modified Kneser-Ney smoothing( Chen and Goodman , 1998 ) .', '']","['', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits( #AUTHOR_TAG ) with modified Kneser-Ney smoothing( Chen and Goodman , 1998 ) .', '']",5
"['. Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #AUTHOR_TAG ) , margin( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT )( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set']","['. Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #AUTHOR_TAG ) , margin( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT )( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', '']","['a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #AUTHOR_TAG ) , margin( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT )( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', '']","['', 'h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #AUTHOR_TAG ) , margin( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT )( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', '']",0
"['ared with retraining mode, incremental training can improve the training efficiency.', 'In the field of machine learning research , incremental training has been employed in the work( #AUTHOR_TAG ; Shilton et al. , 2005 ) , but there is little work for tuning parameters of statistical machine translation .', 'The biggest difficulty lies in that the fea- ture vector of a given training example, i.e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable.', '']","['ared with retraining mode, incremental training can improve the training efficiency.', 'In the field of machine learning research , incremental training has been employed in the work( #AUTHOR_TAG ; Shilton et al. , 2005 ) , but there is little work for tuning parameters of statistical machine translation .', 'The biggest difficulty lies in that the fea- ture vector of a given training example, i.e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable.', '']","['ared with retraining mode, incremental training can improve the training efficiency.', 'In the field of machine learning research , incremental training has been employed in the work( #AUTHOR_TAG ; Shilton et al. , 2005 ) , but there is little work for tuning parameters of statistical machine translation .', 'The biggest difficulty lies in that the fea- ture vector of a given training example, i.e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable.', '']","['ared with retraining mode, incremental training can improve the training efficiency.', 'In the field of machine learning research , incremental training has been employed in the work( #AUTHOR_TAG ; Shilton et al. , 2005 ) , but there is little work for tuning parameters of statistical machine translation .', 'The biggest difficulty lies in that the fea- ture vector of a given training example, i.e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable.', '']",0
"['estimation to learn weights for MT.', '( Och , 2003 ; Moore and Quirk , 2008 ; #AUTHOR_TAG ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .', '']","['estimation to learn weights for MT.', '( Och , 2003 ; Moore and Quirk , 2008 ; #AUTHOR_TAG ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .', '']","['to learn weights for MT.', '( Och , 2003 ; Moore and Quirk , 2008 ; #AUTHOR_TAG ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .', '']","['works have proposed discriminative tech- niques to train log-linear model for SMT.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '( Och , 2003 ; Moore and Quirk , 2008 ; #AUTHOR_TAG ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .', '']",1
"['estimation to learn weights for MT.', '( Och , 2003 ; #AUTHOR_TAG ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .', '']","['estimation to learn weights for MT.', '( Och , 2003 ; #AUTHOR_TAG ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .', '']","['to learn weights for MT.', '( Och , 2003 ; #AUTHOR_TAG ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .', '']","['works have proposed discriminative tech- niques to train log-linear model for SMT.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '( Och , 2003 ; #AUTHOR_TAG ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .', '']",1
"['', 'The significance testing is performed by paired bootstrap re-sampling( #AUTHOR_TAG )']","['', 'The significance testing is performed by paired bootstrap re-sampling( #AUTHOR_TAG )']","['', 'The significance testing is performed by paired bootstrap re-sampling( #AUTHOR_TAG )']","['', 'The significance testing is performed by paired bootstrap re-sampling( #AUTHOR_TAG )']",5
"['b). The nonlinearly separable classification problem transformed from (a) via tuning as ranking(#AUTHOR_TAG , 2011).', 'Since score of e11 is greater than that of e12, (1, 0) corresponds to a possitive example denoted as ”•”, and (−1, 0) corresponds to a negative example denoted as ”*”.', 'Since the transformed classification']","['b). The nonlinearly separable classification problem transformed from (a) via tuning as ranking(#AUTHOR_TAG , 2011).', 'Since score of e11 is greater than that of e12, (1, 0) corresponds to a possitive example denoted as ”•”, and (−1, 0) corresponds to a negative example denoted as ”*”.', 'Since the transformed classification']","['b). The nonlinearly separable classification problem transformed from (a) via tuning as ranking(#AUTHOR_TAG , 2011).', 'Since score of e11 is greater than that of e12, (1, 0) corresponds to a possitive example denoted as ”•”, and (−1, 0) corresponds to a negative example denoted as ”*”.', 'Since the transformed classification problem']","['b). The nonlinearly separable classification problem transformed from (a) via tuning as ranking(#AUTHOR_TAG , 2011).', 'Since score of e11 is greater than that of e12, (1, 0) corresponds to a possitive example denoted as ”•”, and (−1, 0) corresponds to a negative example denoted as ”*”.', '']",0
"['works have proposed discriminative techniques to train log-linear model for SMT.', '( #AUTHOR_TAG ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT.', '']","['works have proposed discriminative techniques to train log-linear model for SMT.', '( #AUTHOR_TAG ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT.', '']","['works have proposed discriminative techniques to train log-linear model for SMT.', '( #AUTHOR_TAG ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT.', '']","['works have proposed discriminative techniques to train log-linear model for SMT.', '( #AUTHOR_TAG ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '']",1
"['metric we consider here is derived from an example-based machine translation.', 'To retrieve translation examples for a test sentence ,( #AUTHOR_TAG ) defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch Â¨ utze , 1999 ) as follows']","['metric we consider here is derived from an example-based machine translation.', 'To retrieve translation examples for a test sentence ,( #AUTHOR_TAG ) defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch Â¨ utze , 1999 ) as follows']","['metric we consider here is derived from an example-based machine translation.', 'To retrieve translation examples for a test sentence ,( #AUTHOR_TAG ) defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch Â¨ utze , 1999 ) as follows']","['metric we consider here is derived from an example-based machine translation.', 'To retrieve translation examples for a test sentence ,( #AUTHOR_TAG ) defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch Â¨ utze , 1999 ) as follows']",5
"['. Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate( #AUTHOR_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT )( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set']","['. Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate( #AUTHOR_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT )( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', '']","['a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate( #AUTHOR_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT )( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', '']","['', 'h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate( #AUTHOR_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT )( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', '']",0
"['', 'We employ the idea of ultraconservative update( #AUTHOR_TAG ; Crammer et al. , 2006 ) to propose two incremental methods for local training in Algorithm 2 as follows']","['', 'We employ the idea of ultraconservative update( #AUTHOR_TAG ; Crammer et al. , 2006 ) to propose two incremental methods for local training in Algorithm 2 as follows']","['', 'We employ the idea of ultraconservative update( #AUTHOR_TAG ; Crammer et al. , 2006 ) to propose two incremental methods for local training in Algorithm 2 as follows']","['', 'We employ the idea of ultraconservative update( #AUTHOR_TAG ; Crammer et al. , 2006 ) to propose two incremental methods for local training in Algorithm 2 as follows']",5
"['to learn weights for MT.', '( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .', '']","['to learn weights for MT.', '( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .', '']","['to learn weights for MT.', '( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .', '']","['works have proposed discriminative techniques to train log-linear model for SMT.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .', '']",1
"['use an in-house developed hierarchical phrase-based translation( #AUTHOR_TAG ) as our baseline system , and we denote it as In-Hiero .', 'To obtain satisfactory baseline performance, we tune In-Hiero system for 5 times using MERT, and then se-(Koehn et al., 2007)']","['use an in-house developed hierarchical phrase-based translation( #AUTHOR_TAG ) as our baseline system , and we denote it as In-Hiero .', 'To obtain satisfactory baseline performance, we tune In-Hiero system for 5 times using MERT, and then se-(Koehn et al., 2007)']","['use an in-house developed hierarchical phrase-based translation( #AUTHOR_TAG ) as our baseline system , and we denote it as In-Hiero .', 'To obtain satisfactory baseline performance, we tune In-Hiero system for 5 times using MERT, and then se-(Koehn et al., 2007) .', '']","['use an in-house developed hierarchical phrase-based translation( #AUTHOR_TAG ) as our baseline system , and we denote it as In-Hiero .', 'To obtain satisfactory baseline performance, we tune In-Hiero system for 5 times using MERT, and then se-(Koehn et al., 2007) .', '']",5
"['run GIZA + +( #AUTHOR_TAG ) on the training corpus in both directions( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair .', '']","['run GIZA + +( #AUTHOR_TAG ) on the training corpus in both directions( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair .', '']","['run GIZA + +( #AUTHOR_TAG ) on the training corpus in both directions( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair .', '']","['run GIZA + +( #AUTHOR_TAG ) on the training corpus in both directions( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair .', '']",5
"['weights for MT.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #AUTHOR_TAG ) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions']","['weights for MT.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #AUTHOR_TAG ) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions']","['weights for MT.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #AUTHOR_TAG ) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions']","['works have proposed discriminative techniques to train log-linear model for SMT.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #AUTHOR_TAG ) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions']",1
"['. Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT )( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set']","['. Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT )( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', '']","['a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT )( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', '']","['', 'h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT )( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', '']",0
"[', our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception.', 'Although evaluated on different data sets , this result is consistent with results from previous work( Gatt and Belz , 2008 ; #AUTHOR_TAG ) .', '']","[', our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception.', 'Although evaluated on different data sets , this result is consistent with results from previous work( Gatt and Belz , 2008 ; #AUTHOR_TAG ) .', '']","[', our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception.', 'Although evaluated on different data sets , this result is consistent with results from previous work( Gatt and Belz , 2008 ; #AUTHOR_TAG ) .', '']","[', our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception.', 'Although evaluated on different data sets , this result is consistent with results from previous work( Gatt and Belz , 2008 ; #AUTHOR_TAG ) .', '']",1
"['ed semantics provides a bridge to connect symbolic labels or words with lower level visual features(Harnad, 1990) .', 'Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG ) .', 'For']","['ed semantics provides a bridge to connect symbolic labels or words with lower level visual features(Harnad, 1990) .', 'Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG ) .', 'For']","['ed semantics provides a bridge to connect symbolic labels or words with lower level visual features(Harnad, 1990) .', 'Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG ) .', 'For']","['ed semantics provides a bridge to connect symbolic labels or words with lower level visual features(Harnad, 1990) .', 'Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG ) .', '']",2
"['this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work( #AUTHOR_TAG ) .', 'In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.', 'Since robots need to collaborate with human partners to']","['this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work( #AUTHOR_TAG ) .', 'In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.', 'Since robots need to collaborate with human partners to']","['this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work( #AUTHOR_TAG ) .', 'In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.', 'Since robots need to collaborate with human partners to']","['this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work( #AUTHOR_TAG ) .', 'In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.', 'Since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation (REG) becomes an equally important problem in situated dialogue.', '']",2
"['', 'In a similar vein ,#AUTHOR_TAG showed that a different feature-topic model improved predictions on a fill-in-the-blank task .', '']","['', 'In a similar vein ,#AUTHOR_TAG showed that a different feature-topic model improved predictions on a fill-in-the-blank task .', '']","['', 'In a similar vein ,#AUTHOR_TAG showed that a different feature-topic model improved predictions on a fill-in-the-blank task .', '']","['', 'In a similar vein ,#AUTHOR_TAG showed that a different feature-topic model improved predictions on a fill-in-the-blank task .', '']",0
"['approaches to multimodal research have succeeded by abstracting away raw perceptual in- formation and using high-level representations in- stead.', 'Some works abstract perception via the usage of symbolic logic representations( #AUTHOR_TAG ; Chen and Mooney , 2011 ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'Within the latter cat- egory, the two most common representations have been association norms, where subjects are given']","['approaches to multimodal research have succeeded by abstracting away raw perceptual in- formation and using high-level representations in- stead.', 'Some works abstract perception via the usage of symbolic logic representations( #AUTHOR_TAG ; Chen and Mooney , 2011 ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'Within the latter cat- egory, the two most common representations have been association norms, where subjects are given']","['approaches to multimodal research have succeeded by abstracting away raw perceptual in- formation and using high-level representations in- stead.', 'Some works abstract perception via the usage of symbolic logic representations( #AUTHOR_TAG ; Chen and Mooney , 2011 ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'Within the latter cat- egory, the two most common representations have been association norms, where subjects are given']","['approaches to multimodal research have succeeded by abstracting away raw perceptual in- formation and using high-level representations in- stead.', 'Some works abstract perception via the usage of symbolic logic representations( #AUTHOR_TAG ; Chen and Mooney , 2011 ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', '']",0
"['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models isFeng and Lapata (2010 b).', 'They use a Bag of Visual Words ( BoVW ) model( #AUTHOR_TAG ) to create a bimodal vocabulary describing documents .', 'The topic model using the bimodal voc']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models isFeng and Lapata (2010 b).', 'They use a Bag of Visual Words ( BoVW ) model( #AUTHOR_TAG ) to create a bimodal vocabulary describing documents .', 'The topic model using the bimodal vocabulary']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models isFeng and Lapata (2010 b).', 'They use a Bag of Visual Words ( BoVW ) model( #AUTHOR_TAG ) to create a bimodal vocabulary describing documents .', 'The topic model using the bimodal voc']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models isFeng and Lapata (2010 b).', 'They use a Bag of Visual Words ( BoVW ) model( #AUTHOR_TAG ) to create a bimodal vocabulary describing documents .', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '']",0
"['Silberer et al. ( 2013 ) show that visual attribute classifiers , which have been immensely successful in object recognition( #AUTHOR_TAG ) ,']","['Silberer et al. ( 2013 ) show that visual attribute classifiers , which have been immensely successful in object recognition( #AUTHOR_TAG ) ,']","['Silberer et al. ( 2013 ) show that visual attribute classifiers , which have been immensely successful in object recognition( #AUTHOR_TAG ) ,']","['', 'More recently ,Silberer et al. ( 2013 ) show that visual attribute classifiers , which have been immensely successful in object recognition( #AUTHOR_TAG ) , act as excellent substitutes for feature', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise(Mathe et al., 2008;Regneri et al., 2013)']",0
"['', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions(Chen and Mooney, 2011) or robot commands(Tellex et al., 2011;Matuszek et al., 2012) .', 'Some efforts have tackled tasks such as automatic image caption generation( Feng and Lapata , 2010 a ;Ordonez et al. , 2011 ) , text illustration( Joshi et al. , 2006 ) , or automatic location identification of Twitter users( Eisenstein et al. , 2010 ; #AUTHOR_TAG ; Roller et al. , 2012 )']","['', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions(Chen and Mooney, 2011) or robot commands(Tellex et al., 2011;Matuszek et al., 2012) .', 'Some efforts have tackled tasks such as automatic image caption generation( Feng and Lapata , 2010 a ;Ordonez et al. , 2011 ) , text illustration( Joshi et al. , 2006 ) , or automatic location identification of Twitter users( Eisenstein et al. , 2010 ; #AUTHOR_TAG ; Roller et al. , 2012 )']","['', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions(Chen and Mooney, 2011) or robot commands(Tellex et al., 2011;Matuszek et al., 2012) .', 'Some efforts have tackled tasks such as automatic image caption generation( Feng and Lapata , 2010 a ;Ordonez et al. , 2011 ) , text illustration( Joshi et al. , 2006 ) , or automatic location identification of Twitter users( Eisenstein et al. , 2010 ; #AUTHOR_TAG ; Roller et al. , 2012 )']","['', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions(Chen and Mooney, 2011) or robot commands(Tellex et al., 2011;Matuszek et al., 2012) .', 'Some efforts have tackled tasks such as automatic image caption generation( Feng and Lapata , 2010 a ;Ordonez et al. , 2011 ) , text illustration( Joshi et al. , 2006 ) , or automatic location identification of Twitter users( Eisenstein et al. , 2010 ; #AUTHOR_TAG ; Roller et al. , 2012 )']",0
"['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( #AUTHOR_TAG ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', '']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( #AUTHOR_TAG ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', '']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( #AUTHOR_TAG ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', '']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( #AUTHOR_TAG ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', '']",0
"['is, we simply take the original mLDA model of#AUTHOR_TAG (2009) and generalize it in the same way they generalize LDA.', '']","['is, we simply take the original mLDA model of#AUTHOR_TAG (2009) and generalize it in the same way they generalize LDA.', '']","['is, we simply take the original mLDA model of#AUTHOR_TAG (2009) and generalize it in the same way they generalize LDA.', '']","['is, we simply take the original mLDA model of#AUTHOR_TAG (2009) and generalize it in the same way they generalize LDA.', '']",2
"['language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing , where words and sentences are mapped to logical structure meaning( #AUTHOR_TAG ) .', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions(Chen and Mooney, 2011) or robot commands(Tellex et al., 2011;Matuszek et al., 2012) .', 'Some']","['language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing , where words and sentences are mapped to logical structure meaning( #AUTHOR_TAG ) .', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions(Chen and Mooney, 2011) or robot commands(Tellex et al., 2011;Matuszek et al., 2012) .', 'Some']","['language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing , where words and sentences are mapped to logical structure meaning( #AUTHOR_TAG ) .', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions(Chen and Mooney, 2011) or robot commands(Tellex et al., 2011;Matuszek et al., 2012) .', 'Some efforts']","['language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing , where words and sentences are mapped to logical structure meaning( #AUTHOR_TAG ) .', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions(Chen and Mooney, 2011) or robot commands(Tellex et al., 2011;Matuszek et al., 2012) .', '']",0
"['extend LDA to allow for the inference of document and topic distributions in a multimodal corpus .', 'In their model, a document consists of a set of (word, feature) pairs']","['extend LDA to allow for the inference of document and topic distributions in a multimodal corpus .', 'In their model, a document consists of a set of (word, feature) pairs,']","['extend LDA to allow for the inference of document and topic distributions in a multimodal corpus .', 'In their model, a document consists of a set of (word, feature) pairs']","['extend LDA to allow for the inference of document and topic distributions in a multimodal corpus .', 'In their model, a document consists of a set of (word, feature) pairs, 4 rather than just words, and documents are still modeled as mixtures of shared topics.', '']",0
"['s.', 'The keypoints are clustered into 5,000 visual codewords ( centroids ) using k-means clustering( #AUTHOR_TAG ) , and images are then quantized over the 5,000 codewords .', '']","['', 'The keypoints are clustered into 5,000 visual codewords ( centroids ) using k-means clustering( #AUTHOR_TAG ) , and images are then quantized over the 5,000 codewords .', '']","['', 'The keypoints are clustered into 5,000 visual codewords ( centroids ) using k-means clustering( #AUTHOR_TAG ) , and images are then quantized over the 5,000 codewords .', '']","['', 'The keypoints are clustered into 5,000 visual codewords ( centroids ) using k-means clustering( #AUTHOR_TAG ) , and images are then quantized over the 5,000 codewords .', '']",5
"['1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features( Andrews et al. , 2009 ; #AUTHOR_TAG )']","['1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features( Andrews et al. , 2009 ; #AUTHOR_TAG )']","['1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features( Andrews et al. , 2009 ; #AUTHOR_TAG )']","['1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features( Andrews et al. , 2009 ; #AUTHOR_TAG )']",1
"['', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise( Mathe et al. , 2008 ; #AUTHOR_TAG )']","['feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise( Mathe et al. , 2008 ; #AUTHOR_TAG )']","['feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise( Mathe et al. , 2008 ; #AUTHOR_TAG )']","['', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise( Mathe et al. , 2008 ; #AUTHOR_TAG )']",0
"['-level representations in- stead.', 'Some works abstract perception via the us- age of symbolic logic representations(Chen et al., 2010; Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013) , while others choose to employ concepts elicited from psycholin- guistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e.g. ,Nelson et al. ( 2004 ) ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept ( e.g. ,#AUTHOR_TAG )']","['high-level representations in- stead.', 'Some works abstract perception via the us- age of symbolic logic representations(Chen et al., 2010; Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013) , while others choose to employ concepts elicited from psycholin- guistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e.g. ,Nelson et al. ( 2004 ) ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept ( e.g. ,#AUTHOR_TAG )']","['approaches to multimodal research have succeeded by abstracting away raw perceptual in- formation and using high-level representations in- stead.', 'Some works abstract perception via the us- age of symbolic logic representations(Chen et al., 2010; Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013) , while others choose to employ concepts elicited from psycholin- guistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e.g. ,Nelson et al. ( 2004 ) ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept ( e.g. ,#AUTHOR_TAG )']","['approaches to multimodal research have succeeded by abstracting away raw perceptual in- formation and using high-level representations in- stead.', 'Some works abstract perception via the us- age of symbolic logic representations(Chen et al., 2010; Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013) , while others choose to employ concepts elicited from psycholin- guistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e.g. ,Nelson et al. ( 2004 ) ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept ( e.g. ,#AUTHOR_TAG )']",0
"['language grounding problem has come in many different flavors with just as many different ap- proaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning(Kate and Mooney, 2007) .', 'Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions( #AUTHOR_TAG ) or robot commands( Tellex et al. , 2011 ; Matuszek et al. , 2012 ) .', 'Some efforts have tackled tasks such']","['language grounding problem has come in many different flavors with just as many different ap- proaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning(Kate and Mooney, 2007) .', 'Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions( #AUTHOR_TAG ) or robot commands( Tellex et al. , 2011 ; Matuszek et al. , 2012 ) .', 'Some efforts have tackled tasks such']","['language grounding problem has come in many different flavors with just as many different ap- proaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning(Kate and Mooney, 2007) .', 'Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions( #AUTHOR_TAG ) or robot commands( Tellex et al. , 2011 ; Matuszek et al. , 2012 ) .', 'Some efforts have tackled tasks such']","['language grounding problem has come in many different flavors with just as many different ap- proaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning(Kate and Mooney, 2007) .', 'Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions( #AUTHOR_TAG ) or robot commands( Tellex et al. , 2011 ; Matuszek et al. , 2012 ) .', '']",0
"['', 'Some efforts have tackled tasks such as automatic image caption generation( Feng and Lapata , 2010 a ;#AUTHOR_TAG ) , text illustration( Joshi et al. , 2006 ) , or automatic location identification of Twitter users( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )']","['', 'Some efforts have tackled tasks such as automatic image caption generation( Feng and Lapata , 2010 a ;#AUTHOR_TAG ) , text illustration( Joshi et al. , 2006 ) , or automatic location identification of Twitter users( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )']","['', 'Some efforts have tackled tasks such as automatic image caption generation( Feng and Lapata , 2010 a ;#AUTHOR_TAG ) , text illustration( Joshi et al. , 2006 ) , or automatic location identification of Twitter users( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )']","['', 'Some efforts have tackled tasks such as automatic image caption generation( Feng and Lapata , 2010 a ;#AUTHOR_TAG ) , text illustration( Joshi et al. , 2006 ) , or automatic location identification of Twitter users( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )']",0
"['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;#AUTHOR_TAG ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;#AUTHOR_TAG ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;#AUTHOR_TAG ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;#AUTHOR_TAG ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words""(Bruni et al., 2012 b']",0
"['evaluate our algorithms, we first need to generate multimodal corpora for each of our non- textual modalities.', ""We use the same method as#AUTHOR_TAG for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a word-feature pair ."", 'Words without grounded features are all given the same placeholder feature,']","['evaluate our algorithms, we first need to generate multimodal corpora for each of our non- textual modalities.', ""We use the same method as#AUTHOR_TAG for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a word-feature pair ."", 'Words without grounded features are all given the same placeholder feature,']","['evaluate our algorithms, we first need to generate multimodal corpora for each of our non- textual modalities.', ""We use the same method as#AUTHOR_TAG for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a word-feature pair ."", 'Words without grounded features are all given the same placeholder feature,']","['order to evaluate our algorithms, we first need to generate multimodal corpora for each of our non- textual modalities.', ""We use the same method as#AUTHOR_TAG for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a word-feature pair ."", 'Words without grounded features are all given the same placeholder feature, also resulting in a word- feature pair.5', '']",5
"['language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr ,Von Ahn ( 2006 ) ) , computing power , improved computer vision models( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; #AUTHOR_TAG ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ;Tettamanti et al. , 2005 ;  Aziz-Zadeh et al. , 2006 )']","['language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr ,Von Ahn ( 2006 ) ) , computing power , improved computer vision models( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; #AUTHOR_TAG ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ;Tettamanti et al. , 2005 ;  Aziz-Zadeh et al. , 2006 )']","['language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr ,Von Ahn ( 2006 ) ) , computing power , improved computer vision models( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; #AUTHOR_TAG ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ;Tettamanti et al. , 2005 ;  Aziz-Zadeh et al. , 2006 )']","['language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr ,Von Ahn ( 2006 ) ) , computing power , improved computer vision models( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; #AUTHOR_TAG ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ;Tettamanti et al. , 2005 ;  Aziz-Zadeh et al. , 2006 )']",0
"['experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by#AUTHOR_TAG .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together(Andrews et al., 2009;Silberer and Lapata, 2012) .', 'It has also been shown to be useful']","['experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by#AUTHOR_TAG .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together(Andrews et al., 2009;Silberer and Lapata, 2012) .', 'It has also been shown to be useful']","['experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by#AUTHOR_TAG .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together(Andrews et al., 2009;Silberer and Lapata, 2012) .', 'It has also been shown to be useful']","['experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by#AUTHOR_TAG .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together(Andrews et al., 2009;Silberer and Lapata, 2012) .', '']",5
"['with them.', ""This seems to provide additional evidence of#AUTHOR_TAG b ) 's suggestion that something like a distributional hypothesis of images is plausible""]","['with them.', ""This seems to provide additional evidence of#AUTHOR_TAG b ) 's suggestion that something like a distributional hypothesis of images is plausible""]","['with them.', ""This seems to provide additional evidence of#AUTHOR_TAG b ) 's suggestion that something like a distributional hypothesis of images is plausible""]","['', ""This seems to provide additional evidence of#AUTHOR_TAG b ) 's suggestion that something like a distributional hypothesis of images is plausible""]",1
"['', 'It is frequently used in tasks like scene identification , and#AUTHOR_TAG shows that distance in GIST space correlates well with semantic distance in WordNet .', '']","['', 'It is frequently used in tasks like scene identification , and#AUTHOR_TAG shows that distance in GIST space correlates well with semantic distance in WordNet .', '']","['', 'It is frequently used in tasks like scene identification , and#AUTHOR_TAG shows that distance in GIST space correlates well with semantic distance in WordNet .', '']","['', 'It is frequently used in tasks like scene identification , and#AUTHOR_TAG shows that distance in GIST space correlates well with semantic distance in WordNet .', 'After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality']",4
"['', '#AUTHOR_TAG a ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation .', '']","['model(Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '#AUTHOR_TAG a ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation .', '']","['W) model(Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '#AUTHOR_TAG a ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation .', '']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models isFeng and Lapata (2010 b).', 'They use a Bag of Visual Words (BoVW) model(Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '#AUTHOR_TAG a ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation .', '']",0
"['', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions(Chen and Mooney, 2011) or robot commands(Tellex et al., 2011;Matuszek et al., 2012) .', 'Some efforts have tackled tasks such as automatic image caption generation( #AUTHOR_TAG a ;Ordonez et al. , 2011 ) , text illustration( Joshi et al. , 2006 ) , or automatic location identification of Twitter users( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )']","['', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions(Chen and Mooney, 2011) or robot commands(Tellex et al., 2011;Matuszek et al., 2012) .', 'Some efforts have tackled tasks such as automatic image caption generation( #AUTHOR_TAG a ;Ordonez et al. , 2011 ) , text illustration( Joshi et al. , 2006 ) , or automatic location identification of Twitter users( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )']","['', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions(Chen and Mooney, 2011) or robot commands(Tellex et al., 2011;Matuszek et al., 2012) .', 'Some efforts have tackled tasks such as automatic image caption generation( #AUTHOR_TAG a ;Ordonez et al. , 2011 ) , text illustration( Joshi et al. , 2006 ) , or automatic location identification of Twitter users( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )']","['', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions(Chen and Mooney, 2011) or robot commands(Tellex et al., 2011;Matuszek et al., 2012) .', 'Some efforts have tackled tasks such as automatic image caption generation( #AUTHOR_TAG a ;Ordonez et al. , 2011 ) , text illustration( Joshi et al. , 2006 ) , or automatic location identification of Twitter users( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )']",0
"['our Text modality , we use deWaC , a large German web corpus created by the WaCKy group( #AUTHOR_TAG ) containing approximately 1.7 B word tokens .', 'We filtered the corpus by: removing words with unprintable characters or encoding troubles; removing all stopwords; removing word types with a total frequency of less than 500; and removing documents with a length']","['our Text modality , we use deWaC , a large German web corpus created by the WaCKy group( #AUTHOR_TAG ) containing approximately 1.7 B word tokens .', 'We filtered the corpus by: removing words with unprintable characters or encoding troubles; removing all stopwords; removing word types with a total frequency of less than 500; and removing documents with a length']","['our Text modality , we use deWaC , a large German web corpus created by the WaCKy group( #AUTHOR_TAG ) containing approximately 1.7 B word tokens .', 'We filtered the corpus by: removing words with unprintable characters or encoding troubles; removing all stopwords; removing word types with a total frequency of less than 500; and removing documents with a length']","['our Text modality , we use deWaC , a large German web corpus created by the WaCKy group( #AUTHOR_TAG ) containing approximately 1.7 B word tokens .', 'We filtered the corpus by: removing words with unprintable characters or encoding troubles; removing all stopwords; removing word types with a total frequency of less than 500; and removing documents with a length shorter than 100.', 'The resulting corpus has 1,038,883 documents consisting of 75,678 word types and 466M word tokens']",5
"['Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples ,Rohrbach et al. ( 2010 ) andSocher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and#AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos']","['Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples ,Rohrbach et al. ( 2010 ) andSocher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and#AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos']","['Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples ,Rohrbach et al. ( 2010 ) andSocher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and#AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos']","['Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples ,Rohrbach et al. ( 2010 ) andSocher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and#AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos']",0
"['', '#AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity .', '']","['', '#AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity .', '']","['', '#AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity .', '']","['', '#AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity .', '']",0
"['', 'Following#AUTHOR_TAG , we measure association norm prediction as an average of percentile ranks .', 'For all possible pairs of words in our voc']","['', 'Following#AUTHOR_TAG , we measure association norm prediction as an average of percentile ranks .', 'For all possible pairs of words in our']","['', 'Following#AUTHOR_TAG , we measure association norm prediction as an average of percentile ranks .', 'For all possible pairs of words in our voc']","['', 'Following#AUTHOR_TAG , we measure association norm prediction as an average of percentile ranks .', 'For all possible pairs of words in our vocabulary, we compute the negative symmetric KL divergence between the two words.', '']",5
"['this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of#AUTHOR_TAG .', 'We found both fea- ture sets were directly compatible with multimodal LDA and provided significant gains in']","['this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of#AUTHOR_TAG .', 'We found both fea- ture sets were directly compatible with multimodal LDA and provided significant gains in']","['this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of#AUTHOR_TAG .', 'We found both fea- ture sets were directly compatible with multimodal LDA and provided significant gains in']","['this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of#AUTHOR_TAG .', 'We found both fea- ture sets were directly compatible with multimodal LDA and provided significant gains in their ability to predict association norms over traditional text-only LDA.', '']",5
"['also compute GIST vectors( #AUTHOR_TAG ) for every image using LearGIST( Douze et al. , 2009 ) .', 'Unlike SURF descriptors, GIST produces a single vector representation for an image']","['also compute GIST vectors( #AUTHOR_TAG ) for every image using LearGIST( Douze et al. , 2009 ) .', 'Unlike SURF descriptors, GIST produces a single vector representation for an image.', '']","['also compute GIST vectors( #AUTHOR_TAG ) for every image using LearGIST( Douze et al. , 2009 ) .', 'Unlike SURF descriptors, GIST produces a single vector representation for an image.', '']","['also compute GIST vectors( #AUTHOR_TAG ) for every image using LearGIST( Douze et al. , 2009 ) .', 'Unlike SURF descriptors, GIST produces a single vector representation for an image.', '']",5
"['', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions(Chen and Mooney, 2011) or robot commands(Tellex et al., 2011;Matuszek et al., 2012) .', 'Some efforts have tackled tasks such as automatic image caption generation( Feng and Lapata , 2010 a ;Ordonez et al. , 2011 ) , text illustration( Joshi et al. , 2006 ) , or automatic location identification of Twitter users( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #AUTHOR_TAG )']","['', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions(Chen and Mooney, 2011) or robot commands(Tellex et al., 2011;Matuszek et al., 2012) .', 'Some efforts have tackled tasks such as automatic image caption generation( Feng and Lapata , 2010 a ;Ordonez et al. , 2011 ) , text illustration( Joshi et al. , 2006 ) , or automatic location identification of Twitter users( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #AUTHOR_TAG )']","['', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions(Chen and Mooney, 2011) or robot commands(Tellex et al., 2011;Matuszek et al., 2012) .', 'Some efforts have tackled tasks such as automatic image caption generation( Feng and Lapata , 2010 a ;Ordonez et al. , 2011 ) , text illustration( Joshi et al. , 2006 ) , or automatic location identification of Twitter users( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #AUTHOR_TAG )']","['', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions(Chen and Mooney, 2011) or robot commands(Tellex et al., 2011;Matuszek et al., 2012) .', 'Some efforts have tackled tasks such as automatic image caption generation( Feng and Lapata , 2010 a ;Ordonez et al. , 2011 ) , text illustration( Joshi et al. , 2006 ) , or automatic location identification of Twitter users( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #AUTHOR_TAG )']",0
"['the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g.,Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005) ).', '#AUTHOR_TAG helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis( Deerwester et al. , 1990 ) in the prediction of association norms .', '']","['the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g.,Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005) ).', '#AUTHOR_TAG helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis( Deerwester et al. , 1990 ) in the prediction of association norms .', '']","['the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g.,Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005) ).', '#AUTHOR_TAG helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis( Deerwester et al. , 1990 ) in the prediction of association norms .', '']","['', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g.,Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005) ).', '#AUTHOR_TAG helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis( Deerwester et al. , 1990 ) in the prediction of association norms .', '']",0
"['Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples ,Rohrbach et al. ( 2010 ) and#AUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , andMotwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']","['Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples ,Rohrbach et al. ( 2010 ) and#AUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , andMotwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']","['Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples ,Rohrbach et al. ( 2010 ) and#AUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , andMotwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']","['Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples ,Rohrbach et al. ( 2010 ) and#AUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , andMotwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']",0
"['erNetle (""little ImageNet"" in Swabian German) is our new data set of German noun-to-ImageNet synset mappings.', 'ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets( #AUTHOR_TAG ) .', 'Multiple synsets exist for each meaning of a word.', 'For example, Im-ageNet contains two different synsets for the word mouse: one contains images of the animal, while the other contains images of the computer peripheral.', 'This BilderNetle data set provides mappings from German noun']","['erNetle (""little ImageNet"" in Swabian German) is our new data set of German noun-to-ImageNet synset mappings.', 'ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets( #AUTHOR_TAG ) .', 'Multiple synsets exist for each meaning of a word.', 'For example, Im-ageNet contains two different synsets for the word mouse: one contains images of the animal, while the other contains images of the computer peripheral.', 'This BilderNetle data set provides mappings from German noun']","['erNetle (""little ImageNet"" in Swabian German) is our new data set of German noun-to-ImageNet synset mappings.', 'ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets( #AUTHOR_TAG ) .', 'Multiple synsets exist for each meaning of a word.', 'For example, Im-ageNet contains two different synsets for the word mouse: one contains images of the animal, while the other contains images of the computer peripheral.', 'This BilderNetle data set provides mappings from German noun types']","['erNetle (""little ImageNet"" in Swabian German) is our new data set of German noun-to-ImageNet synset mappings.', 'ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets( #AUTHOR_TAG ) .', 'Multiple synsets exist for each meaning of a word.', 'For example, Im-ageNet contains two different synsets for the word mouse: one contains images of the animal, while the other contains images of the computer peripheral.', 'This BilderNetle data set provides mappings from German noun types to images of the nouns via ImageNet']",5
"['', '#AUTHOR_TAG introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA-based model and others on association norm prediction , held out feature prediction , and word similarity']","['', '#AUTHOR_TAG introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA-based model and others on association norm prediction , held out feature prediction , and word similarity']","['', '#AUTHOR_TAG introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA-based model and others on association norm prediction , held out feature prediction , and word similarity']","['', '#AUTHOR_TAG introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA-based model and others on association norm prediction , held out feature prediction , and word similarity']",0
"['recently ,#AUTHOR_TAG show that visual attribute classifiers , which have been immensely successful in object recognition( Farhadi et al. , 2009 ) , act as excellent substitutes for feature', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise(Mathe et al., 2008;Regneri et al., 2013)']","['recently ,#AUTHOR_TAG show that visual attribute classifiers , which have been immensely successful in object recognition( Farhadi et al. , 2009 ) , act as excellent substitutes for feature', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise(Mathe et al., 2008;Regneri et al., 2013)']","['recently ,#AUTHOR_TAG show that visual attribute classifiers , which have been immensely successful in object recognition( Farhadi et al. , 2009 ) , act as excellent substitutes for feature', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise(Mathe et al., 2008;Regneri et al., 2013)']","['', 'More recently ,#AUTHOR_TAG show that visual attribute classifiers , which have been immensely successful in object recognition( Farhadi et al. , 2009 ) , act as excellent substitutes for feature', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise(Mathe et al., 2008;Regneri et al., 2013)']",0
"['.', 'The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from#AUTHOR_TAG']","['time.', 'The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from#AUTHOR_TAG']","['.', 'The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from#AUTHOR_TAG']","['', 'The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from#AUTHOR_TAG']",5
"['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition,']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition,']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition,']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', '']",0
"['language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr ,Von Ahn ( 2006 ) ) , computing power , improved computer vision models( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ;Tettamanti et al. , 2005 ;  Aziz-Zadeh et al. , 2006 )']","['language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr ,Von Ahn ( 2006 ) ) , computing power , improved computer vision models( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ;Tettamanti et al. , 2005 ;  Aziz-Zadeh et al. , 2006 )']","['language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr ,Von Ahn ( 2006 ) ) , computing power , improved computer vision models( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ;Tettamanti et al. , 2005 ;  Aziz-Zadeh et al. , 2006 )']","['language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr ,Von Ahn ( 2006 ) ) , computing power , improved computer vision models( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ;Tettamanti et al. , 2005 ;  Aziz-Zadeh et al. , 2006 )']",0
"['experiments are based on the multimodal extension of Latent Dirichlet Allocation developed byAndrews et al. (2009) .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together( Andrews et al. , 2009 ; #AUTHOR_TAG ) .', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers(Silberer et al., 2013) .', '']","['experiments are based on the multimodal extension of Latent Dirichlet Allocation developed byAndrews et al. (2009) .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together( Andrews et al. , 2009 ; #AUTHOR_TAG ) .', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers(Silberer et al., 2013) .', '']","['experiments are based on the multimodal extension of Latent Dirichlet Allocation developed byAndrews et al. (2009) .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together( Andrews et al. , 2009 ; #AUTHOR_TAG ) .', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers(Silberer et al., 2013) .', '']","['experiments are based on the multimodal extension of Latent Dirichlet Allocation developed byAndrews et al. (2009) .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together( Andrews et al. , 2009 ; #AUTHOR_TAG ) .', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers(Silberer et al., 2013) .', '']",0
"['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', '']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', '']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', '']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', '']",0
"['', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise( #AUTHOR_TAG ; Regneri et al. , 2013 )']","['feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise( #AUTHOR_TAG ; Regneri et al. , 2013 )']","['feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise( #AUTHOR_TAG ; Regneri et al. , 2013 )']","['', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise( #AUTHOR_TAG ; Regneri et al. , 2013 )']",0
"['Norms ( AN ) is a collection of association norms collected by Schulte im#AUTHOR_TAG .', 'In association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.', 'With enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.', '']","['Norms ( AN ) is a collection of association norms collected by Schulte im#AUTHOR_TAG .', 'In association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.', 'With enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.', '']","['Norms ( AN ) is a collection of association norms collected by Schulte im#AUTHOR_TAG .', 'In association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.', 'With enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.', '']","['Norms ( AN ) is a collection of association norms collected by Schulte im#AUTHOR_TAG .', 'In association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.', 'With enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.', '']",5
"['ent Dirichlet Allocation( #AUTHOR_TAG ) , or LDA , is an unsupervised Bayesian probabilistic model of text documents .', 'It assumes that all documents are probabilistically generated from a shared set of K common topics, where each topic is a multinomial distribution over the vocabulary (notated as β), and documents are modeled as mixtures of these shared topics (notated as θ).', 'LDA assumes every document in the cor']","['ent Dirichlet Allocation( #AUTHOR_TAG ) , or LDA , is an unsupervised Bayesian probabilistic model of text documents .', 'It assumes that all documents are probabilistically generated from a shared set of K common topics, where each topic is a multinomial distribution over the vocabulary (notated as β), and documents are modeled as mixtures of these shared topics (notated as θ).', 'LDA assumes every document in the corpus is generated']","['ent Dirichlet Allocation( #AUTHOR_TAG ) , or LDA , is an unsupervised Bayesian probabilistic model of text documents .', 'It assumes that all documents are probabilistically generated from a shared set of K common topics, where each topic is a multinomial distribution over the vocabulary (notated as β), and documents are modeled as mixtures of these shared topics (notated as θ).', 'LDA assumes every document in the corpus is generated using']","['ent Dirichlet Allocation( #AUTHOR_TAG ) , or LDA , is an unsupervised Bayesian probabilistic model of text documents .', 'It assumes that all documents are probabilistically generated from a shared set of K common topics, where each topic is a multinomial distribution over the vocabulary (notated as β), and documents are modeled as mixtures of these shared topics (notated as θ).', '']",0
"['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #AUTHOR_TAG ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', '']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #AUTHOR_TAG ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', '']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #AUTHOR_TAG ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', '']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #AUTHOR_TAG ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g.,Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005) ).', '']",0
"['Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples ,#AUTHOR_TAG andSocher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , andMotwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']","['Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples ,#AUTHOR_TAG andSocher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , andMotwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']","['Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples ,#AUTHOR_TAG andSocher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , andMotwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']","['Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples ,#AUTHOR_TAG andSocher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , andMotwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']",0
"['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations( Chen et al. , 2010 ; #AUTHOR_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', '']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations( Chen et al. , 2010 ; #AUTHOR_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', '']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations( Chen et al. , 2010 ; #AUTHOR_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', '']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations( Chen et al. , 2010 ; #AUTHOR_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g.,Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005) ).', '']",0
"['', '#AUTHOR_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks .', '']","['outperformed Latent Semantic Analysis(Deerwester et al., 1990) in the prediction of association norms.', '#AUTHOR_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks .', '']","['performed Latent Semantic Analysis(Deerwester et al., 1990) in the prediction of association norms.', '#AUTHOR_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks .', '']","['', '#AUTHOR_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks .', '']",0
"['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;#AUTHOR_TAG ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition,']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;#AUTHOR_TAG ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition,']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;#AUTHOR_TAG ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition,']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;#AUTHOR_TAG ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', '']",0
"['Allocation developed byAndrews et al. (2009) .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together(Andrews et al., 2009;Silberer and Lapata, 2012) .', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers( #AUTHOR_TAG ) .', 'These multimodal LDA models (hereafter, mLDA) have']","['experiments are based on the multimodal extension of Latent Dirichlet Allocation developed byAndrews et al. (2009) .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together(Andrews et al., 2009;Silberer and Lapata, 2012) .', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers( #AUTHOR_TAG ) .', 'These multimodal LDA models (hereafter, mLDA) have']","['experiments are based on the multimodal extension of Latent Dirichlet Allocation developed byAndrews et al. (2009) .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together(Andrews et al., 2009;Silberer and Lapata, 2012) .', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers( #AUTHOR_TAG ) .', 'These multimodal LDA models (hereafter, mLDA)']","['experiments are based on the multimodal extension of Latent Dirichlet Allocation developed byAndrews et al. (2009) .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together(Andrews et al., 2009;Silberer and Lapata, 2012) .', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers( #AUTHOR_TAG ) .', 'These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks(Andrews et al., 2009) .', '']",0
"['(Tellex et al., 2011;Matuszek et al., 2012) .', 'Some efforts have tackled tasks such as automatic image caption generation( Feng and Lapata , 2010 a ;Ordonez et al. , 2011 ) , text illustration( #AUTHOR_TAG ) , or automatic location identification of Twitter users( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )']","['(Tellex et al., 2011;Matuszek et al., 2012) .', 'Some efforts have tackled tasks such as automatic image caption generation( Feng and Lapata , 2010 a ;Ordonez et al. , 2011 ) , text illustration( #AUTHOR_TAG ) , or automatic location identification of Twitter users( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )']","['(Tellex et al., 2011;Matuszek et al., 2012) .', 'Some efforts have tackled tasks such as automatic image caption generation( Feng and Lapata , 2010 a ;Ordonez et al. , 2011 ) , text illustration( #AUTHOR_TAG ) , or automatic location identification of Twitter users( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )']","['', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions(Chen and Mooney, 2011) or robot commands(Tellex et al., 2011;Matuszek et al., 2012) .', 'Some efforts have tackled tasks such as automatic image caption generation( Feng and Lapata , 2010 a ;Ordonez et al. , 2011 ) , text illustration( #AUTHOR_TAG ) , or automatic location identification of Twitter users( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )']",0
"['language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning(Kate and Mooney, 2007) .', 'Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions( Chen and Mooney , 2011 ) or robot commands( Tellex et al. , 2011 ; #AUTHOR_TAG ) .', 'Some efforts have tackled tasks such as automatic image caption generation(Feng and Lapata, 2010 a;']","['language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning(Kate and Mooney, 2007) .', 'Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions( Chen and Mooney , 2011 ) or robot commands( Tellex et al. , 2011 ; #AUTHOR_TAG ) .', 'Some efforts have tackled tasks such as automatic image caption generation(Feng and Lapata, 2010 a;Ordonez et al., 2011)']","['language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning(Kate and Mooney, 2007) .', 'Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions( Chen and Mooney , 2011 ) or robot commands( Tellex et al. , 2011 ; #AUTHOR_TAG ) .', 'Some efforts have tackled tasks such as automatic image caption generation(Feng and Lapata, 2010 a;']","['language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning(Kate and Mooney, 2007) .', 'Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions( Chen and Mooney , 2011 ) or robot commands( Tellex et al. , 2011 ; #AUTHOR_TAG ) .', 'Some efforts have tackled tasks such as automatic image caption generation(Feng and Lapata, 2010 a;Ordonez et al., 2011) , text illustration(Joshi et al., 2006) , or automatic location identification of Twitter users(Eisenstein et al., 2010;Wing and Baldridge, 2011;Roller et al., 2012)']",0
"['language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr ,Von Ahn ( 2006 ) ) , computing power , improved computer vision models( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ;Tettamanti et al. , 2005 ; #AUTHOR_TAG )']","['language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr ,Von Ahn ( 2006 ) ) , computing power , improved computer vision models( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ;Tettamanti et al. , 2005 ; #AUTHOR_TAG )']","['language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr ,Von Ahn ( 2006 ) ) , computing power , improved computer vision models( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ;Tettamanti et al. , 2005 ; #AUTHOR_TAG )']","['language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr ,Von Ahn ( 2006 ) ) , computing power , improved computer vision models( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ;Tettamanti et al. , 2005 ; #AUTHOR_TAG )']",0
"['solve these scaling issues , we implement Online Variational Bayesian Inference( Hoffman et al. , 2010 ; #AUTHOR_TAG ) for our models .', '']","['solve these scaling issues , we implement Online Variational Bayesian Inference( Hoffman et al. , 2010 ; #AUTHOR_TAG ) for our models .', '']","['solve these scaling issues , we implement Online Variational Bayesian Inference( Hoffman et al. , 2010 ; #AUTHOR_TAG ) for our models .', '']","['solve these scaling issues , we implement Online Variational Bayesian Inference( Hoffman et al. , 2010 ; #AUTHOR_TAG ) for our models .', '']",5
"['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAG a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition,']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAG a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition,']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAG a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition,']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAG a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', '']",0
"['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition,']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition,']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition,']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ;Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012 a ;Bruni et al. , 2012 b ;Silberer et al. , 2013 ) .', '']",0
"['1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features( #AUTHOR_TAG ; Silberer and Lapata , 2012 )']","['1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features( #AUTHOR_TAG ; Silberer and Lapata , 2012 )']","['1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features( #AUTHOR_TAG ; Silberer and Lapata , 2012 )']","['1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features( #AUTHOR_TAG ; Silberer and Lapata , 2012 )']",1
"['language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr ,Von Ahn ( 2006 ) ) , computing power , improved computer vision models( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ;Tettamanti et al. , 2005 ;  Aziz-Zadeh et al. , 2006 )']","['language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr ,Von Ahn ( 2006 ) ) , computing power , improved computer vision models( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ;Tettamanti et al. , 2005 ;  Aziz-Zadeh et al. , 2006 )']","['language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr ,Von Ahn ( 2006 ) ) , computing power , improved computer vision models( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ;Tettamanti et al. , 2005 ;  Aziz-Zadeh et al. , 2006 )']","['language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr ,Von Ahn ( 2006 ) ) , computing power , improved computer vision models( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ;Tettamanti et al. , 2005 ;  Aziz-Zadeh et al. , 2006 )']",0
"['language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr ,Von Ahn ( 2006 ) ) , computing power , improved computer vision models( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ;Tettamanti et al. , 2005 ;  Aziz-Zadeh et al. , 2006 )']","['language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr ,Von Ahn ( 2006 ) ) , computing power , improved computer vision models( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ;Tettamanti et al. , 2005 ;  Aziz-Zadeh et al. , 2006 )']","['language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr ,Von Ahn ( 2006 ) ) , computing power , improved computer vision models( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ;Tettamanti et al. , 2005 ;  Aziz-Zadeh et al. , 2006 )']","['language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr ,Von Ahn ( 2006 ) ) , computing power , improved computer vision models( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ;Tettamanti et al. , 2005 ;  Aziz-Zadeh et al. , 2006 )']",0
"['', 'Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis( #AUTHOR_TAG ) in the prediction of association norms .', '']","['', 'Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis( #AUTHOR_TAG ) in the prediction of association norms .', '']","['', 'Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis( #AUTHOR_TAG ) in the prediction of association norms .', '']","['', 'Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis( #AUTHOR_TAG ) in the prediction of association norms .', '']",0
"['solve these scaling issues , we implement Online Variational Bayesian Inference( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .', '']","['solve these scaling issues , we implement Online Variational Bayesian Inference( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .', '']","['solve these scaling issues , we implement Online Variational Bayesian Inference( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .', '']","['solve these scaling issues , we implement Online Variational Bayesian Inference( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .', '']",5
"['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is#AUTHOR_TAG b ) .', 'They use a Bag of Visual Words (BoVW) model(Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is#AUTHOR_TAG b ) .', 'They use a Bag of Visual Words (BoVW) model(Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is#AUTHOR_TAG b ) .', 'They use a Bag of Visual Words (BoVW) model(Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is#AUTHOR_TAG b ) .', 'They use a Bag of Visual Words (BoVW) model(Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '']",0
"['model we rely on was originally developed byAndrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .', '']","['model we rely on was originally developed byAndrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .', '']","['', 'The model we rely on was originally developed byAndrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .', '']","['', 'The model we rely on was originally developed byAndrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .', '']",2
['choice is motivated by an observation we made previously( #AUTHOR_TAG'],['choice is motivated by an observation we made previously( #AUTHOR_TAG'],"['', 'This choice is motivated by an observation we made previously( #AUTHOR_TAG']",['( #AUTHOR_TAG'],2
"['-semantic features.', 'While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence.', 'Following our previous work on stance classification( #AUTHOR_TAG c ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR( Das et al. , 2010 ) .', '']","['-semantic features.', 'While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence.', 'Following our previous work on stance classification( #AUTHOR_TAG c ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR( Das et al. , 2010 ) .', '']","['-semantic features.', 'While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence.', 'Following our previous work on stance classification( #AUTHOR_TAG c ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR( Das et al. , 2010 ) .', '']","['-semantic features.', 'While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence.', 'Following our previous work on stance classification( #AUTHOR_TAG c ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR( Das et al. , 2010 ) .', '']",2
"['', 'Our experimental design with professional bilingual translators follows our previous work#AUTHOR_TAG a ) comparing scratch translation to post-edit .', '']","['', 'Our experimental design with professional bilingual translators follows our previous work#AUTHOR_TAG a ) comparing scratch translation to post-edit .', '']","['', 'Our experimental design with professional bilingual translators follows our previous work#AUTHOR_TAG a ) comparing scratch translation to post-edit .', '']","['', 'Our experimental design with professional bilingual translators follows our previous work#AUTHOR_TAG a ) comparing scratch translation to post-edit .', '']",2
"['', 'This is in line with our previous findings from( #AUTHOR_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators']","['', 'This is in line with our previous findings from( #AUTHOR_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators']","['', 'This is in line with our previous findings from( #AUTHOR_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators .', '']","['', 'This is in line with our previous findings from( #AUTHOR_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators .', '']",1
"['', 'We follow our previous work( #AUTHOR_TAG b ) and restrict bridging to non-coreferential cases .', 'We']","['', 'We follow our previous work( #AUTHOR_TAG b ) and restrict bridging to non-coreferential cases .', 'We']","['', 'We follow our previous work( #AUTHOR_TAG b ) and restrict bridging to non-coreferential cases .', 'We']","['', 'We follow our previous work( #AUTHOR_TAG b ) and restrict bridging to non-coreferential cases .', '']",2
"['We augment mlSystem ruleFeats with more features from our previous work( Markert et al. , 2012 ; #AUTHOR_TAG a ;Hou et al. , 2013 b ) on bridging anaphora recognition and antecedent selection .', '']","['We augment mlSystem ruleFeats with more features from our previous work( Markert et al. , 2012 ; #AUTHOR_TAG a ;Hou et al. , 2013 b ) on bridging anaphora recognition and antecedent selection .', '']","['We augment mlSystem ruleFeats with more features from our previous work( Markert et al. , 2012 ; #AUTHOR_TAG a ;Hou et al. , 2013 b ) on bridging anaphora recognition and antecedent selection .', '']","['', 'mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work( Markert et al. , 2012 ; #AUTHOR_TAG a ;Hou et al. , 2013 b ) on bridging anaphora recognition and antecedent selection .', '']",2
"['', 'in history-based models( #AUTHOR_TAG ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i .', 'This allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions']","['', 'in history-based models( #AUTHOR_TAG ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i .', 'This allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions']","['', 'in history-based models( #AUTHOR_TAG ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i .', 'This allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions']","['', 'in history-based models( #AUTHOR_TAG ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i .', 'This allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions']",5
"['most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length.', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG )']","['most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length.', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG )']","['most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length.', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG )']","['most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length.', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG )']",1
['7A11 our results are computed with the evalb program following the now-standard criteria in( #AUTHOR_TAG )'],['7A11 our results are computed with the evalb program following the now-standard criteria in( #AUTHOR_TAG )'],['7A11 our results are computed with the evalb program following the now-standard criteria in( #AUTHOR_TAG )'],['7A11 our results are computed with the evalb program following the now-standard criteria in( #AUTHOR_TAG )'],5
"['statistical parsers( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2001 ) are based on a history-based probability model( #AUTHOR_TAG ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .', 'A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', '']","['statistical parsers( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2001 ) are based on a history-based probability model( #AUTHOR_TAG ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .', 'A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', '']","['statistical parsers( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2001 ) are based on a history-based probability model( #AUTHOR_TAG ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .', 'A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', '']","['statistical parsers( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2001 ) are based on a history-based probability model( #AUTHOR_TAG ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .', 'A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', '']",0
"['most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length.', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2000 )']","['most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length.', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2000 )']","['most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length.', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2000 )']","['most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length.', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2000 )']",1
"[""the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features( #AUTHOR_TAG )""]","[""the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features( #AUTHOR_TAG )""]","[""the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features( #AUTHOR_TAG )""]","['', ""Collins and Duffy ( 2002 ) define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features( #AUTHOR_TAG )""]",0
"['any).', 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial( #AUTHOR_TAG ) , as has conditioning on the left-corner child( Roark and Johnson , 1999 ) .', '']","['any).', 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial( #AUTHOR_TAG ) , as has conditioning on the left-corner child( Roark and Johnson , 1999 ) .', '']","['any).', 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial( #AUTHOR_TAG ) , as has conditioning on the left-corner child( Roark and Johnson , 1999 ) .', '']","['', 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial( #AUTHOR_TAG ) , as has conditioning on the left-corner child( Roark and Johnson , 1999 ) .', '']",0
"[', but then efficiency becomes a problem.', ""#AUTHOR_TAG define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features( Collins , 2000 )""]","['sets, but then efficiency becomes a problem.', ""#AUTHOR_TAG define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features( Collins , 2000 )""]","['but then efficiency becomes a problem.', ""#AUTHOR_TAG define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features( Collins , 2000 )""]","['', 'feature sets, but then efficiency becomes a problem.', ""#AUTHOR_TAG define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features( Collins , 2000 )""]",0
"['', 'We used a publicly available tagger( #AUTHOR_TAG ) to tag the words and then used these in the input to the system']","['', 'We used a publicly available tagger( #AUTHOR_TAG ) to tag the words and then used these in the input to the system']","['', 'We used a publicly available tagger( #AUTHOR_TAG ) to tag the words and then used these in the input to the system']","['', 'We used a publicly available tagger( #AUTHOR_TAG ) to tag the words and then used these in the input to the system']",5
"['bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ; Collins , 2000 ; Bod , 2001 ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model(Collins, 2000) has only 6']","['bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ; Collins , 2000 ; Bod , 2001 ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model(Collins, 2000) has only 6%']","['bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ; Collins , 2000 ; Bod , 2001 ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model(Collins, 2000) has only 6% less precision error and']","['bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ; Collins , 2000 ; Bod , 2001 ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model(Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.', '']",1
"['statistical parsers( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2001 ) are based on a history-based probability model( Black et al. , 1993 ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .', 'A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'Previous approaches have used a hand-crafted finite set of features to']","['statistical parsers( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2001 ) are based on a history-based probability model( Black et al. , 1993 ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .', 'A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'Previous approaches have used a hand-crafted finite set of features to']","['statistical parsers( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2001 ) are based on a history-based probability model( Black et al. , 1993 ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .', 'A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'Previous approaches have used a hand-crafted finite set of features to']","['statistical parsers( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2001 ) are based on a history-based probability model( Black et al. , 1993 ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .', 'A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history(Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001) .', '']",0
"['', 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial( Johnson , 1998 ) , as has conditioning on the left-corner child( #AUTHOR_TAG ) .', '']","['', 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial( Johnson , 1998 ) , as has conditioning on the left-corner child( #AUTHOR_TAG ) .', '']","['', 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial( Johnson , 1998 ) , as has conditioning on the left-corner child( #AUTHOR_TAG ) .', '']","['', 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial( Johnson , 1998 ) , as has conditioning on the left-corner child( #AUTHOR_TAG ) .', 'Because these inputs include the history features of both the leftcorner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i -1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features.', '']",0
"['bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model(Collins, 2000) has only 6']","['bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model(Collins, 2000) has only 6%']","['bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model(Collins, 2000) has only 6% less precision error and']","['bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model(Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.', '']",1
"['bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model(Collins, 2000) has only 6']","['bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model(Collins, 2000) has only 6%']","['bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model(Collins, 2000) has only 6% less precision error and']","['bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model(Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.', '']",1
"['this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'The method is a form of multi-layered artificial neural network called Simple Synchrony Networks(Lane and Henderson, 2001;Henderson, 2000) .', 'The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in(Ratnaparkhi, 1999) .', 'Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers( #AUTHOR_TAG ) .', 'The difference from previous approaches is in the nature of the input to the log-linear model.', '']","['this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'The method is a form of multi-layered artificial neural network called Simple Synchrony Networks(Lane and Henderson, 2001;Henderson, 2000) .', 'The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in(Ratnaparkhi, 1999) .', 'Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers( #AUTHOR_TAG ) .', 'The difference from previous approaches is in the nature of the input to the log-linear model.', '']","['this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'The method is a form of multi-layered artificial neural network called Simple Synchrony Networks(Lane and Henderson, 2001;Henderson, 2000) .', 'The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in(Ratnaparkhi, 1999) .', 'Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers( #AUTHOR_TAG ) .', 'The difference from previous approaches is in the nature of the input to the log-linear model.', '']","['this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'The method is a form of multi-layered artificial neural network called Simple Synchrony Networks(Lane and Henderson, 2001;Henderson, 2000) .', 'The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in(Ratnaparkhi, 1999) .', 'Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers( #AUTHOR_TAG ) .', 'The difference from previous approaches is in the nature of the input to the log-linear model.', '']",1
"['', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'This is roughly an 11 % relative reduction in error rate over#AUTHOR_TAG and Bods PCFG-reduction reported in Table 1 .', '']","['', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'This is roughly an 11 % relative reduction in error rate over#AUTHOR_TAG and Bods PCFG-reduction reported in Table 1 .', '']","['', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'This is roughly an 11 % relative reduction in error rate over#AUTHOR_TAG and Bods PCFG-reduction reported in Table 1 .', '']","['', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'This is roughly an 11 % relative reduction in error rate over#AUTHOR_TAG and Bods PCFG-reduction reported in Table 1 .', '']",1
"['#AUTHOR_TAG showed how the perceptron algorithm can', ""be used to efficiently compute the best parse with DOP1 's sub"", 'trees , reporting a 5.1 % relative reduction in error rate over the model inBonnema et al. (1999) on the WSJ .']","['#AUTHOR_TAG showed how the perceptron algorithm can', ""be used to efficiently compute the best parse with DOP1 's sub"", 'trees , reporting a 5.1 % relative reduction in error rate over the model inBonnema et al. (1999) on the WSJ .']","['#AUTHOR_TAG showed how the perceptron algorithm can', ""be used to efficiently compute the best parse with DOP1 's sub"", 'trees , reporting a 5.1 % relative reduction in error rate over the model inBonnema et al. (1999) on the WSJ']",['#AUTHOR_TAG'],0
"[', 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable toCharniak ( 2000 ) andCollins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable to#AUTHOR_TAG""]","[', 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable toCharniak ( 2000 ) andCollins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable to#AUTHOR_TAG""]","[', 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable toCharniak ( 2000 ) andCollins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable to#AUTHOR_TAG""]","[""paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) andBod's (2001) estimators on the WSJ."", 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', 'Bod (2001 Bod ( , 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable toCharniak ( 2000 ) andCollins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable to#AUTHOR_TAG""]",0
"['', 'Compared to the reranking technique in#AUTHOR_TAG , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction .', 'While SL-DOP and LS-DOP have been compared before']","['Table 1.', 'Compared to the reranking technique in#AUTHOR_TAG , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction .', 'While SL-DOP and LS-DOP have been compared before']","['Table 1.', 'Compared to the reranking technique in#AUTHOR_TAG , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction .', 'While SL-DOP and LS-DOP have been compared before']","['', 'Compared to the reranking technique in#AUTHOR_TAG , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction .', 'While SL-DOP and LS-DOP have been compared before']",1
"[', 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable toCharniak ( 2000 ) and#AUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable toCollins ( 1996 )""]","[', 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable toCharniak ( 2000 ) and#AUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable toCollins ( 1996 )""]","[', 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable toCharniak ( 2000 ) and#AUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable toCollins ( 1996 )""]","[""paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) andBod's (2001) estimators on the WSJ."", 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', 'Bod (2001 Bod ( , 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable toCharniak ( 2000 ) and#AUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable toCollins ( 1996 )""]",0
"[""DOP models , such as inBod ( 1993 ) ,Goodman ( 1996 ) ,#AUTHOR_TAG ,Sima'an ( 2000 ) andCollins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."", 'most probable) tree as a candidate for the best tree of a sentence.', 'We will refer to these models as Likelihood-DOP models, but']","[""DOP models , such as inBod ( 1993 ) ,Goodman ( 1996 ) ,#AUTHOR_TAG ,Sima'an ( 2000 ) andCollins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."", 'most probable) tree as a candidate for the best tree of a sentence.', 'We will refer to these models as Likelihood-DOP models, but']","[""DOP models , such as inBod ( 1993 ) ,Goodman ( 1996 ) ,#AUTHOR_TAG ,Sima'an ( 2000 ) andCollins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."", 'most probable) tree as a candidate for the best tree of a sentence.', 'We will refer to these models as Likelihood-DOP models, but']","[""DOP models , such as inBod ( 1993 ) ,Goodman ( 1996 ) ,#AUTHOR_TAG ,Sima'an ( 2000 ) andCollins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."", 'most probable) tree as a candidate for the best tree of a sentence.', '']",0
"['', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', 'Collins 1996 , Charniak 1997 , Collins 1999 and#AUTHOR_TAG ) .', '']","['', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', 'Collins 1996 , Charniak 1997 , Collins 1999 and#AUTHOR_TAG ) .', '']","['', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', 'Collins 1996 , Charniak 1997 , Collins 1999 and#AUTHOR_TAG ) .', '']","['', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', 'Collins 1996 , Charniak 1997 , Collins 1999 and#AUTHOR_TAG ) .', '']",1
"['our experiments we used the standard division of the WSJ( #AUTHOR_TAG ) , with sections 2 through 21 for training ( approx .', '40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set.', 'As usual, all trees were stripped off their semantic tags, co-reference information and quotation marks.', '']","['our experiments we used the standard division of the WSJ( #AUTHOR_TAG ) , with sections 2 through 21 for training ( approx .', '40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set.', 'As usual, all trees were stripped off their semantic tags, co-reference information and quotation marks.', '']","['our experiments we used the standard division of the WSJ( #AUTHOR_TAG ) , with sections 2 through 21 for training ( approx .', '40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set.', 'As usual, all trees were stripped off their semantic tags, co-reference information and quotation marks.', '']","['our experiments we used the standard division of the WSJ( #AUTHOR_TAG ) , with sections 2 through 21 for training ( approx .', '40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set.', 'As usual, all trees were stripped off their semantic tags, co-reference information and quotation marks.', '']",5
"['', 'as Bod01 and Bon99.', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', '#AUTHOR_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) .', '(1996).', '']","['', 'as Bod01 and Bon99.', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', '#AUTHOR_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) .', '(1996).', '']","['', 'as Bod01 and Bon99.', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', '#AUTHOR_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) .', '(1996).', '']","['', 'Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp.', 'as Bod01 and Bon99.', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', '#AUTHOR_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) .', '(1996).', '']",1
"['2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to#AUTHOR_TAG andCollins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable toCollins ( 1996 )""]","['2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to#AUTHOR_TAG andCollins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable toCollins ( 1996 )""]","['2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to#AUTHOR_TAG andCollins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable toCollins ( 1996 )""]","['', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to#AUTHOR_TAG andCollins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable toCollins ( 1996 )""]",0
"['1998).', ""And#AUTHOR_TAG argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out inCollins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed inBod ( 1992 )""]","['1998).', ""And#AUTHOR_TAG argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out inCollins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed inBod ( 1992 )""]","['', ""And#AUTHOR_TAG argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out inCollins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed inBod ( 1992 )""]","['', ""And#AUTHOR_TAG argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out inCollins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed inBod ( 1992 )""]",0
"[""DOP models , such as inBod ( 1993 ) ,Goodman ( 1996 ) ,Bonnema et al. ( 1997 ) ,Sima'an ( 2000 ) and#AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence ."", 'We will refer to these models as Likelihood- DOP models, but']","[""DOP models , such as inBod ( 1993 ) ,Goodman ( 1996 ) ,Bonnema et al. ( 1997 ) ,Sima'an ( 2000 ) and#AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence ."", 'We will refer to these models as Likelihood- DOP models, but']","[""DOP models , such as inBod ( 1993 ) ,Goodman ( 1996 ) ,Bonnema et al. ( 1997 ) ,Sima'an ( 2000 ) and#AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence ."", 'We will refer to these models as Likelihood- DOP models, but']","[""DOP models , such as inBod ( 1993 ) ,Goodman ( 1996 ) ,Bonnema et al. ( 1997 ) ,Sima'an ( 2000 ) and#AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence ."", 'We will refer to these models as Likelihood- DOP models, but in this paper we will specifically mean by ""Likelihood-DOP"" the PCFG-reduction ofBod (2001) given in Section 2.2']",0
"[""DOP models , such as inBod ( 1993 ) ,#AUTHOR_TAG ,Bonnema et al. ( 1997 ) ,Sima'an ( 2000 ) andCollins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."", 'most probable) tree as a candidate for the best tree of a sentence.', 'We will refer to these models as Likelihood-DOP models, but']","[""DOP models , such as inBod ( 1993 ) ,#AUTHOR_TAG ,Bonnema et al. ( 1997 ) ,Sima'an ( 2000 ) andCollins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."", 'most probable) tree as a candidate for the best tree of a sentence.', 'We will refer to these models as Likelihood-DOP models, but']","[""DOP models , such as inBod ( 1993 ) ,#AUTHOR_TAG ,Bonnema et al. ( 1997 ) ,Sima'an ( 2000 ) andCollins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."", 'most probable) tree as a candidate for the best tree of a sentence.', 'We will refer to these models as Likelihood-DOP models, but']","[""DOP models , such as inBod ( 1993 ) ,#AUTHOR_TAG ,Bonnema et al. ( 1997 ) ,Sima'an ( 2000 ) andCollins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."", 'most probable) tree as a candidate for the best tree of a sentence.', 'We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by ""Likelihood-DOP"" the PCFG-reduction ofBod (2001) given in Section 2.2']",0
"['DOP models , such as inBod ( 1993 ) ,Goodman ( 1996 ) ,Bonnema et al. ( 1997 ) ,#AUTHOR_TAG andCollins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .', 'We will refer to these models as Likelihood-DOP models, but']","['DOP models , such as inBod ( 1993 ) ,Goodman ( 1996 ) ,Bonnema et al. ( 1997 ) ,#AUTHOR_TAG andCollins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .', 'We will refer to these models as Likelihood-DOP models, but']","['DOP models , such as inBod ( 1993 ) ,Goodman ( 1996 ) ,Bonnema et al. ( 1997 ) ,#AUTHOR_TAG andCollins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .', 'We will refer to these models as Likelihood-DOP models, but']","['DOP models , such as inBod ( 1993 ) ,Goodman ( 1996 ) ,Bonnema et al. ( 1997 ) ,#AUTHOR_TAG andCollins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .', '']",0
"['DOP1 considers counts of subtrees of a wide range of sizes in computing the probability of a tree: everything from counts of single-level rules to counts of entire trees.', 'A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account.', 'Fortunately , there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities , as shown by#AUTHOR_TAG , 2002 ) .', 'Here we will only sketch this PCFG-reduction, which is heavily based onGoodman (2002)']","['DOP1 considers counts of subtrees of a wide range of sizes in computing the probability of a tree: everything from counts of single-level rules to counts of entire trees.', 'A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account.', 'Fortunately , there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities , as shown by#AUTHOR_TAG , 2002 ) .', 'Here we will only sketch this PCFG-reduction, which is heavily based onGoodman (2002)']","['DOP1 considers counts of subtrees of a wide range of sizes in computing the probability of a tree: everything from counts of single-level rules to counts of entire trees.', 'A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account.', 'Fortunately , there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities , as shown by#AUTHOR_TAG , 2002 ) .', 'Here we will only sketch this PCFG-reduction, which is heavily based onGoodman (2002)']","['DOP1 considers counts of subtrees of a wide range of sizes in computing the probability of a tree: everything from counts of single-level rules to counts of entire trees.', 'A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account.', 'Fortunately , there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities , as shown by#AUTHOR_TAG , 2002 ) .', 'Here we will only sketch this PCFG-reduction, which is heavily based onGoodman (2002)']",0
"['on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar.', 'This approach has now gained wide usage , as exemplified by the work of#AUTHOR_TAG , 1999 ) ,Charniak ( 1996  , 1997 ) ,Johnson ( 1998 ) ,Chiang ( 2000 ) , and many others']","['egner 1992; Pereira and Schabes 1992).', 'The DOP model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar.', 'This approach has now gained wide usage , as exemplified by the work of#AUTHOR_TAG , 1999 ) ,Charniak ( 1996  , 1997 ) ,Johnson ( 1998 ) ,Chiang ( 2000 ) , and many others']","['egner 1992; Pereira and Schabes 1992).', 'The DOP model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar.', 'This approach has now gained wide usage , as exemplified by the work of#AUTHOR_TAG , 1999 ) ,Charniak ( 1996  , 1997 ) ,Johnson ( 1998 ) ,Chiang ( 2000 ) , and many others']","['egner 1992; Pereira and Schabes 1992).', 'The DOP model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar.', 'This approach has now gained wide usage , as exemplified by the work of#AUTHOR_TAG , 1999 ) ,Charniak ( 1996  , 1997 ) ,Johnson ( 1998 ) ,Chiang ( 2000 ) , and many others']",4
"[').', 'The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ;#AUTHOR_TAG ; Goodman 1998 ) .', '']","['', 'The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ;#AUTHOR_TAG ; Goodman 1998 ) .', '']","['', 'The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ;#AUTHOR_TAG ; Goodman 1998 ) .', '']","['', 'The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ;#AUTHOR_TAG ; Goodman 1998 ) .', '']",0
"['e.g.', 'Collins 1999;Charniak 2000;Goodman 1998).', ""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in#AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed inBod ( 1992 )""]","['(e.g.', 'Collins 1999;Charniak 2000;Goodman 1998).', ""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in#AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed inBod ( 1992 )""]","['e.g.', 'Collins 1999;Charniak 2000;Goodman 1998).', ""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in#AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed inBod ( 1992 )""]","['', 'Collins 1999;Charniak 2000;Goodman 1998).', ""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in#AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed inBod ( 1992 )""]",4
"['man then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.', 'And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1( #AUTHOR_TAG ) .', ""Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability."", 'This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the']","['man then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.', 'And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1( #AUTHOR_TAG ) .', ""Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability."", 'This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the']","['man then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.', 'And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1( #AUTHOR_TAG ) .', ""Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability."", 'This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in']","['man then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.', 'And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1( #AUTHOR_TAG ) .', ""Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability."", 'This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the PCFG.', '']",0
"['', ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", '#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .', '']","['', ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", '#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .', '']","['', ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", '#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .', '']","['', ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", '#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .', '']",0
"['', 'Our Moses systems use default settings.', 'The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit( #AUTHOR_TAG ) .', 'We run MERT separately for each system.', '']","['', 'Our Moses systems use default settings.', 'The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit( #AUTHOR_TAG ) .', 'We run MERT separately for each system.', '']","['', 'Our Moses systems use default settings.', 'The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit( #AUTHOR_TAG ) .', 'We run MERT separately for each system.', 'The recaser used is the']","['', 'Our Moses systems use default settings.', 'The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit( #AUTHOR_TAG ) .', 'We run MERT separately for each system.', '']",5
"['translation, compound parts have to be resynthesized into compounds before inflection.', 'Two decisions have to be taken: i) where to merge and ii) how to merge.', 'Following the work of#AUTHOR_TAG , we implement a linear-chain CRF merging system using the following features : stemmed ( separated ) surface form , part-of-speech14 and frequencies from the training corpus for bigrams/merging of word and word +1 , word as true prefix , word +1 as true suffix , plus frequency comparisons of these .', 'The CRF is trained on the split monolingual data.', 'It']","['translation, compound parts have to be resynthesized into compounds before inflection.', 'Two decisions have to be taken: i) where to merge and ii) how to merge.', 'Following the work of#AUTHOR_TAG , we implement a linear-chain CRF merging system using the following features : stemmed ( separated ) surface form , part-of-speech14 and frequencies from the training corpus for bigrams/merging of word and word +1 , word as true prefix , word +1 as true suffix , plus frequency comparisons of these .', 'The CRF is trained on the split monolingual data.', 'It']","['translation, compound parts have to be resynthesized into compounds before inflection.', 'Two decisions have to be taken: i) where to merge and ii) how to merge.', 'Following the work of#AUTHOR_TAG , we implement a linear-chain CRF merging system using the following features : stemmed ( separated ) surface form , part-of-speech14 and frequencies from the training corpus for bigrams/merging of word and word +1 , word as true prefix , word +1 as true suffix , plus frequency comparisons of these .', 'The CRF is trained on the split monolingual data.', 'It']","['translation, compound parts have to be resynthesized into compounds before inflection.', 'Two decisions have to be taken: i) where to merge and ii) how to merge.', 'Following the work of#AUTHOR_TAG , we implement a linear-chain CRF merging system using the following features : stemmed ( separated ) surface form , part-of-speech14 and frequencies from the training corpus for bigrams/merging of word and word +1 , word as true prefix , word +1 as true suffix , plus frequency comparisons of these .', 'The CRF is trained on the split monolingual data.', '']",5
"['compound splitting , we follow#AUTHOR_TAG , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies .', 'Other approaches use less deep linguistic resources (e.g., POS-tags Stymne ( 2008) ) or are (almost) knowledge-free (e.g.,Koehn and Knight (2003) ).', '']","['compound splitting , we follow#AUTHOR_TAG , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies .', 'Other approaches use less deep linguistic resources (e.g., POS-tags Stymne ( 2008) ) or are (almost) knowledge-free (e.g.,Koehn and Knight (2003) ).', '']","['compound splitting , we follow#AUTHOR_TAG , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies .', 'Other approaches use less deep linguistic resources (e.g., POS-tags Stymne ( 2008) ) or are (almost) knowledge-free (e.g.,Koehn and Knight (2003) ).', '']","['compound splitting , we follow#AUTHOR_TAG , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies .', 'Other approaches use less deep linguistic resources (e.g., POS-tags Stymne ( 2008) ) or are (almost) knowledge-free (e.g.,Koehn and Knight (2003) ).', '']",5
"['use an additional target factor to obtain the coarse POS for each stem, applying a 7-gram POS model.', '#AUTHOR_TAG showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models']","['use an additional target factor to obtain the coarse POS for each stem, applying a 7-gram POS model.', '#AUTHOR_TAG showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models']","['use an additional target factor to obtain the coarse POS for each stem, applying a 7-gram POS model.', '#AUTHOR_TAG showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models']","['use an additional target factor to obtain the coarse POS for each stem, applying a 7-gram POS model.', '#AUTHOR_TAG showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models']",0
"['', 'We follow#AUTHOR_TAG , for compound merging .', 'We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merg- ing) on one of our two test sets']","['', 'We follow#AUTHOR_TAG , for compound merging .', 'We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merg- ing) on one of our two test sets']","['', 'We follow#AUTHOR_TAG , for compound merging .', 'We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merg- ing) on one of our two test sets']","['', 'We follow#AUTHOR_TAG , for compound merging .', 'We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merg- ing) on one of our two test sets']",5
"['by simply building an SMT system for translating from stems to inflected forms.', '#AUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) .', '']","['by simply building an SMT system for translating from stems to inflected forms.', '#AUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) .', '']","['to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms.', '#AUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) .', '']","['', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms.', '#AUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) .', '']",1
"['frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , POS-tags Stymne ( 2008 ) ) or are ( almost ) knowledge-free ( e.g. ,#AUTHOR_TAG ) .', 'Compound merging is less']","['frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , POS-tags Stymne ( 2008 ) ) or are ( almost ) knowledge-free ( e.g. ,#AUTHOR_TAG ) .', 'Compound merging is less']","['the geometric mean of word part frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , POS-tags Stymne ( 2008 ) ) or are ( almost ) knowledge-free ( e.g. ,#AUTHOR_TAG ) .', 'Compound merging is less well']","['compound splitting, we followFritzinger and Fraser (2010) , using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , POS-tags Stymne ( 2008 ) ) or are ( almost ) knowledge-free ( e.g. ,#AUTHOR_TAG ) .', 'Compound merging is less well studied.', '']",1
"['', '#AUTHOR_TAG used unification in an SMT system to model some of the agreement phenomena that we model.', 'Our CR']","['ineffective on large data sets.', '#AUTHOR_TAG used unification in an SMT system to model some of the agreement phenomena that we model.', 'Our CRF']","['ineffective on large data sets.', '#AUTHOR_TAG used unification in an SMT system to model some of the agreement phenomena that we model.', 'Our CR']","['', 'Both efforts were ineffective on large data sets.', '#AUTHOR_TAG used unification in an SMT system to model some of the agreement phenomena that we model.', '']",1
"['key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German( Schmid et al. , 2004 ) and the BitPar parser , which is a state-of-the-art parser of German( #AUTHOR_TAG )']","['key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German( Schmid et al. , 2004 ) and the BitPar parser , which is a state-of-the-art parser of German( #AUTHOR_TAG )']","['key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German( Schmid et al. , 2004 ) and the BitPar parser , which is a state-of-the-art parser of German( #AUTHOR_TAG )']","['key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German( Schmid et al. , 2004 ) and the BitPar parser , which is a state-of-the-art parser of German( #AUTHOR_TAG )']",5
"['prepare the training data by splitting compounds in two steps , following the technique of#AUTHOR_TAG .', 'First, possible split points are extracted using SMOR, and second, the best split points are selected using the geometric mean of word part frequencies.', 'Training']","['prepare the training data by splitting compounds in two steps , following the technique of#AUTHOR_TAG .', 'First, possible split points are extracted using SMOR, and second, the best split points are selected using the geometric mean of word part frequencies.', 'Training']","['prepare the training data by splitting compounds in two steps , following the technique of#AUTHOR_TAG .', 'First, possible split points are extracted using SMOR, and second, the best split points are selected using the geometric mean of word part frequencies.', 'Training data']","['prepare the training data by splitting compounds in two steps , following the technique of#AUTHOR_TAG .', 'First, possible split points are extracted using SMOR, and second, the best split points are selected using the geometric mean of word part frequencies.', '']",5
"['improves, the performance of linguistic-feature-based approaches will increase.', 'Virpioja et al. ( 2007 ) ,Badr et al. ( 2008 ) ,Luong et al. ( 2010 ) ,#AUTHOR_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation .', '']","['improves, the performance of linguistic-feature-based approaches will increase.', 'Virpioja et al. ( 2007 ) ,Badr et al. ( 2008 ) ,Luong et al. ( 2010 ) ,#AUTHOR_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation .', 'However, this does not deal directly with']","['improves, the performance of linguistic-feature-based approaches will increase.', 'Virpioja et al. ( 2007 ) ,Badr et al. ( 2008 ) ,Luong et al. ( 2010 ) ,#AUTHOR_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation .', '']","['', 'As parsing performance improves, the performance of linguistic-feature-based approaches will increase.', 'Virpioja et al. ( 2007 ) ,Badr et al. ( 2008 ) ,Luong et al. ( 2010 ) ,#AUTHOR_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation .', '']",1
"['', 'We use more complex context features.', '#AUTHOR_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms .', '']","['', 'We use more complex context features.', '#AUTHOR_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms .', '']","['', 'We use more complex context features.', '#AUTHOR_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms .', '']","['', 'We use more complex context features.', '#AUTHOR_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms .', 'Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system).', '']",1
"['that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', '#AUTHOR_TAG introduced factored SMT .', 'We use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms.', '']","['that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', '#AUTHOR_TAG introduced factored SMT .', 'We use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms.', '']","['that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', '#AUTHOR_TAG introduced factored SMT .', 'We use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms.', '']","['', 'The only work that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', '#AUTHOR_TAG introduced factored SMT .', 'We use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms.', '']",1
"['previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those ofAvramidis and Koehn ( 2008 ) ,#AUTHOR_TAG and others .', 'Toutanova et.', ""al.'s work showed that it is""]","['previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those ofAvramidis and Koehn ( 2008 ) ,#AUTHOR_TAG and others .', 'Toutanova et.', ""al.'s work showed that it is""]","['previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those ofAvramidis and Koehn ( 2008 ) ,#AUTHOR_TAG and others .', 'Toutanova et.', ""al.'s work showed that it is most""]","['previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those ofAvramidis and Koehn ( 2008 ) ,#AUTHOR_TAG and others .', 'Toutanova et.', '']",1
"['previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of#AUTHOR_TAG ,Yeniterzi and Oflazer ( 2010 ) and others .', 'Toutanova et.', ""al.'s work showed that it is""]","['previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of#AUTHOR_TAG ,Yeniterzi and Oflazer ( 2010 ) and others .', 'Toutanova et.', ""al.'s work showed that it is""]","['previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of#AUTHOR_TAG ,Yeniterzi and Oflazer ( 2010 ) and others .', 'Toutanova et.', ""al.'s work showed that it is most""]","['previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of#AUTHOR_TAG ,Yeniterzi and Oflazer ( 2010 ) and others .', 'Toutanova et.', '']",1
"['word part frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , POS-tags#AUTHOR_TAG ) or are ( almost ) knowledge-free ( e.g. ,Koehn and Knight ( 2003 ) ) .', '']","['word part frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , POS-tags#AUTHOR_TAG ) or are ( almost ) knowledge-free ( e.g. ,Koehn and Knight ( 2003 ) ) .', '']","['word part frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , POS-tags#AUTHOR_TAG ) or are ( almost ) knowledge-free ( e.g. ,Koehn and Knight ( 2003 ) ) .', '']","['', 'Other approaches use less deep linguistic resources ( e.g. , POS-tags#AUTHOR_TAG ) or are ( almost ) knowledge-free ( e.g. ,Koehn and Knight ( 2003 ) ) .', '']",1
"['approach to extract and classify social events builds on our previous work( #AUTHOR_TAG ) , which in turn builds on work from the relation extraction community( Nguyen et al. , 2009 ) .', 'Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper.', '']","['approach to extract and classify social events builds on our previous work( #AUTHOR_TAG ) , which in turn builds on work from the relation extraction community( Nguyen et al. , 2009 ) .', 'Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper.', '']","['approach to extract and classify social events builds on our previous work( #AUTHOR_TAG ) , which in turn builds on work from the relation extraction community( Nguyen et al. , 2009 ) .', 'Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper.', '']","['approach to extract and classify social events builds on our previous work( #AUTHOR_TAG ) , which in turn builds on work from the relation extraction community( Nguyen et al. , 2009 ) .', 'Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper.', '']",2
"['2This view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in( #AUTHOR_TAG ) .', ""Typed feature structures as normal form ir~'~Eterms are merely syntactic objects""]","['2This view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in( #AUTHOR_TAG ) .', ""Typed feature structures as normal form ir~'~Eterms are merely syntactic objects""]","['2This view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in( #AUTHOR_TAG ) .', ""Typed feature structures as normal form ir~'~Eterms are merely syntactic objects""]","['2This view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in( #AUTHOR_TAG ) .', ""Typed feature structures as normal form ir~'~Eterms are merely syntactic objects""]",0
['A more detailed discussion of various aspects of the proposed parser can be found in( #AUTHOR_TAG )'],['A more detailed discussion of various aspects of the proposed parser can be found in( #AUTHOR_TAG )'],['A more detailed discussion of various aspects of the proposed parser can be found in( #AUTHOR_TAG )'],['A more detailed discussion of various aspects of the proposed parser can be found in( #AUTHOR_TAG )'],0
"['ining control strategies depends on a way to differentiate between types of constraints.', ""Proceedings of EACL '99 example , the ALE parser( #AUTHOR_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown ."", '']","['ining control strategies depends on a way to differentiate between types of constraints.', ""Proceedings of EACL '99 example , the ALE parser( #AUTHOR_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown ."", '']","['ining control strategies depends on a way to differentiate between types of constraints.', ""Proceedings of EACL '99 example , the ALE parser( #AUTHOR_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown ."", '']","['ining control strategies depends on a way to differentiate between types of constraints.', ""Proceedings of EACL '99 example , the ALE parser( #AUTHOR_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown ."", '']",0
"[').', 'As shown in( #AUTHOR_TAG ) â\x80¢ The presented research was carried out at the University of Tubingen , Germany , as part of the Sonderforschungsbereich 340']","['among others, (Ramakrishnan et al. 1992).', 'As shown in( #AUTHOR_TAG ) â\x80¢ The presented research was carried out at the University of Tubingen , Germany , as part of the Sonderforschungsbereich 340']","[').', 'As shown in( #AUTHOR_TAG ) â\x80¢ The presented research was carried out at the University of Tubingen , Germany , as part of the Sonderforschungsbereich 340']","['', 'See, among others, (Ramakrishnan et al. 1992).', 'As shown in( #AUTHOR_TAG ) â\x80¢ The presented research was carried out at the University of Tubingen , Germany , as part of the Sonderforschungsbereich 340']",0
"['ed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ;#AUTHOR_TAG )', '3(Meurers and Minnen, 1997) propose a compilation of lexical rules into T~r/: definite clauses 2This view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in(Carpenter, 1992) .', ""Typed feature structures as normal form ir~'~E terms are merely syntactic objects""]","['ed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ;#AUTHOR_TAG )', '3(Meurers and Minnen, 1997) propose a compilation of lexical rules into T~r/: definite clauses 2This view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in(Carpenter, 1992) .', ""Typed feature structures as normal form ir~'~E terms are merely syntactic objects""]","['ed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ;#AUTHOR_TAG )', '3(Meurers and Minnen, 1997) propose a compilation of lexical rules into T~r/: definite clauses 2This view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in(Carpenter, 1992) .', ""Typed feature structures as normal form ir~'~E terms are merely syntactic objects""]","['ed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ;#AUTHOR_TAG )', '3(Meurers and Minnen, 1997) propose a compilation of lexical rules into T~r/: definite clauses 2This view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in(Carpenter, 1992) .', ""Typed feature structures as normal form ir~'~E terms are merely syntactic objects""]",0
"['ed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar( #AUTHOR_TAG ) .', '3( Meurers and Minnen , 1997 ) propose a compilation of lexical rules into']","['ed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar( #AUTHOR_TAG ) .', '3( Meurers and Minnen , 1997 ) propose a compilation of lexical rules into']","['ed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar( #AUTHOR_TAG ) .', '3( Meurers and Minnen , 1997 ) propose a compilation of lexical rules into']","['ed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar( #AUTHOR_TAG ) .', '']",0
"['is a compilation technique originally developed for goal-directed bottom-up processing of logic programs.', 'See , among others ,( #AUTHOR_TAG ) .', '']","['is a compilation technique originally developed for goal-directed bottom-up processing of logic programs.', 'See , among others ,( #AUTHOR_TAG ) .', '']","['is a compilation technique originally developed for goal-directed bottom-up processing of logic programs.', 'See , among others ,( #AUTHOR_TAG ) .', '']","['is a compilation technique originally developed for goal-directed bottom-up processing of logic programs.', 'See , among others ,( #AUTHOR_TAG ) .', '']",0
"['have to refrain from an example.', 'The ConTroll grammar development system as described in( #AUTHOR_TAG b ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars']","['have to refrain from an example.', 'The ConTroll grammar development system as described in( #AUTHOR_TAG b ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars']","['we have to refrain from an example.', 'The ConTroll grammar development system as described in( #AUTHOR_TAG b ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars']","['', 'Meurers, 1997 b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', '4 Because of space limitations we have to refrain from an example.', 'The ConTroll grammar development system as described in( #AUTHOR_TAG b ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars']",0
"['outining appears under many different guises, like for example, suspension, residuation, (goal) freezing, and blocking.', 'See also( #AUTHOR_TAG ; Naish , 1986 )']","['outining appears under many different guises, like for example, suspension, residuation, (goal) freezing, and blocking.', 'See also( #AUTHOR_TAG ; Naish , 1986 )']","['outining appears under many different guises, like for example, suspension, residuation, (goal) freezing, and blocking.', 'See also( #AUTHOR_TAG ; Naish , 1986 )']","['outining appears under many different guises, like for example, suspension, residuation, (goal) freezing, and blocking.', 'See also( #AUTHOR_TAG ; Naish , 1986 )']",0
"['', ""In contrast to Johnson and DSrre's deduction system, though, the selective magic parsing approach combines top-down and bottom-up control strategies."", 'As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of( #AUTHOR_TAG ) .', 'Unlike the ALE parser, though, the selective magic parser does not presuppose a phrase structure backbone']","['', ""In contrast to Johnson and DSrre's deduction system, though, the selective magic parsing approach combines top-down and bottom-up control strategies."", 'As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of( #AUTHOR_TAG ) .', 'Unlike the ALE parser, though, the selective magic parser does not presuppose a phrase structure backbone']","['', ""In contrast to Johnson and DSrre's deduction system, though, the selective magic parsing approach combines top-down and bottom-up control strategies."", 'As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of( #AUTHOR_TAG ) .', 'Unlike the ALE parser, though, the selective magic parser does not presuppose a phrase structure backbone']","['', ""In contrast to Johnson and DSrre's deduction system, though, the selective magic parsing approach combines top-down and bottom-up control strategies."", 'As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of( #AUTHOR_TAG ) .', 'Unlike the ALE parser, though, the selective magic parser does not presuppose a phrase structure backbone and is more flexible as to which sub-computations are tabled/filtered.', '']",1
"['', 'Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ;Pollard and Sag , 1994 ) as discussed in( #AUTHOR_TAG a ) and( Meurers and Minnen , 1997 ) .', '']","['', 'Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ;Pollard and Sag , 1994 ) as discussed in( #AUTHOR_TAG a ) and( Meurers and Minnen , 1997 ) .', '']","['', 'Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ;Pollard and Sag , 1994 ) as discussed in( #AUTHOR_TAG a ) and( Meurers and Minnen , 1997 ) .', '']","['', 'In this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (Tgv£:;GStz, 1995) .', 'Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ;Pollard and Sag , 1994 ) as discussed in( #AUTHOR_TAG a ) and( Meurers and Minnen , 1997 ) .', '']",2
"['See( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .', 'append ([~,[~,[~).', 'Meurers, 1997 b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', '']","['See( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .', 'append ([~,[~,[~).', 'Meurers, 1997 b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', '']","['See( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .', 'append ([~,[~,[~).', 'Meurers, 1997 b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', '']","['See( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .', 'append ([~,[~,[~).', 'Meurers, 1997 b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', '']",0
"['headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving so-called head-switching.', 'This contrasts with one of the traditional approaches ( e.g. ,#AUTHOR_TAG ; Watanabe 1995 ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language']","['headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving so-called head-switching.', 'This contrasts with one of the traditional approaches ( e.g. ,#AUTHOR_TAG ; Watanabe 1995 ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language']","[', headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving so-called head-switching.', 'This contrasts with one of the traditional approaches ( e.g. ,#AUTHOR_TAG ; Watanabe 1995 ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language']","['', 'For example, headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving so-called head-switching.', 'This contrasts with one of the traditional approaches ( e.g. ,#AUTHOR_TAG ; Watanabe 1995 ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language']",1
"['the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers( #AUTHOR_TAG ) and automatically trained transducers ( Alshawi and Douglas 2000 ) with a larger range of positions']","['the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers( #AUTHOR_TAG ) and automatically trained transducers ( Alshawi and Douglas 2000 ) with a larger range of positions']","['the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers( #AUTHOR_TAG ) and automatically trained transducers ( Alshawi and Douglas 2000 ) with a larger range of positions']","['the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers( #AUTHOR_TAG ) and automatically trained transducers ( Alshawi and Douglas 2000 ) with a larger range of positions']",5
"['the same time , we believe our method has advantages over the approach developed initially at IBM( #AUTHOR_TAG ; Brown et al. 1993 ) for training translation systems automatically .', 'One advantage is that our method attempts to model the natural decomposition of sentences into phrases.', '']","['the same time , we believe our method has advantages over the approach developed initially at IBM( #AUTHOR_TAG ; Brown et al. 1993 ) for training translation systems automatically .', 'One advantage is that our method attempts to model the natural decomposition of sentences into phrases.', '']","['the same time , we believe our method has advantages over the approach developed initially at IBM( #AUTHOR_TAG ; Brown et al. 1993 ) for training translation systems automatically .', 'One advantage is that our method attempts to model the natural decomposition of sentences into phrases.', '']","['the same time , we believe our method has advantages over the approach developed initially at IBM( #AUTHOR_TAG ; Brown et al. 1993 ) for training translation systems automatically .', 'One advantage is that our method attempts to model the natural decomposition of sentences into phrases.', '']",1
"[""9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner ''( #AUTHOR_TAG , 203 ) ."", ""Their planner uses plan structures similar to IGEN's, except that the plan operators they use are generally instantiations of rhetorical relations drawn from Rhetorical Structure""]","[""9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner ''( #AUTHOR_TAG , 203 ) ."", ""Their planner uses plan structures similar to IGEN's, except that the plan operators they use are generally instantiations of rhetorical relations drawn from Rhetorical Structure""]","[""Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner ''( #AUTHOR_TAG , 203 ) ."", ""Their planner uses plan structures similar to IGEN's, except that the plan operators they use are generally instantiations of rhetorical relations drawn from Rhetorical Structure Theory""]","['', ""1 Â° The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner ''( #AUTHOR_TAG , 203 ) ."", ""Their planner uses plan structures similar to IGEN's, except that the plan operators they use are generally instantiations of rhetorical relations drawn from Rhetorical Structure Theory (Mann and Thompson 1987)."", '']",0
"['', 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( Meteer 1994 ; Panaget 1994 ;#AUTHOR_TAG ) .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994']","['', 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( Meteer 1994 ; Panaget 1994 ;#AUTHOR_TAG ) .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994']","['', 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( Meteer 1994 ; Panaget 1994 ;#AUTHOR_TAG ) .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994']","['', 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( Meteer 1994 ; Panaget 1994 ;#AUTHOR_TAG ) .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994']",0
"['opposite approach is to simply ignore the limitations of a modular design and proceed as if there need be no interactions between the components.', 'Whatever problems result will be handled as best they can, on a case-by-case basis.', 'This approach is the one taken (implicitly or explicitly) in the majority of generators.', 'In fact , Reiter has even argued in favor of this approach , claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis )( #AUTHOR_TAG ) .', 'While this certainly has appeal as a design methodology, it seems reckless to assume that problems will never appear.', 'Certainly an approach to generation that does handle these interactions would be an improvement']","['opposite approach is to simply ignore the limitations of a modular design and proceed as if there need be no interactions between the components.', 'Whatever problems result will be handled as best they can, on a case-by-case basis.', 'This approach is the one taken (implicitly or explicitly) in the majority of generators.', 'In fact , Reiter has even argued in favor of this approach , claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis )( #AUTHOR_TAG ) .', 'While this certainly has appeal as a design methodology, it seems reckless to assume that problems will never appear.', 'Certainly an approach to generation that does handle these interactions would be an improvement,']","['opposite approach is to simply ignore the limitations of a modular design and proceed as if there need be no interactions between the components.', 'Whatever problems result will be handled as best they can, on a case-by-case basis.', 'This approach is the one taken (implicitly or explicitly) in the majority of generators.', 'In fact , Reiter has even argued in favor of this approach , claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis )( #AUTHOR_TAG ) .', 'While this certainly has appeal as a design methodology, it seems reckless to assume that problems will never appear.', 'Certainly an approach to generation that does handle these interactions would be an improvement,']","['opposite approach is to simply ignore the limitations of a modular design and proceed as if there need be no interactions between the components.', 'Whatever problems result will be handled as best they can, on a case-by-case basis.', 'This approach is the one taken (implicitly or explicitly) in the majority of generators.', 'In fact , Reiter has even argued in favor of this approach , claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis )( #AUTHOR_TAG ) .', 'While this certainly has appeal as a design methodology, it seems reckless to assume that problems will never appear.', '']",0
"['', 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component( #AUTHOR_TAG ; Panaget 1994 ; Wanner 1994 ) .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994']","['', 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component( #AUTHOR_TAG ; Panaget 1994 ; Wanner 1994 ) .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994']","['', 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component( #AUTHOR_TAG ; Panaget 1994 ; Wanner 1994 ) .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994']","['', 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component( #AUTHOR_TAG ; Panaget 1994 ; Wanner 1994 ) .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994']",0
"['', 'Something like this approach is in fact used in some systems ( e.g. , Elhadad and Robin 1992 ; PenMan 1989 ;#AUTHOR_TAG a )']","['', 'Something like this approach is in fact used in some systems ( e.g. , Elhadad and Robin 1992 ; PenMan 1989 ;#AUTHOR_TAG a )']","['', 'Something like this approach is in fact used in some systems ( e.g. , Elhadad and Robin 1992 ; PenMan 1989 ;#AUTHOR_TAG a )']","['', 'Something like this approach is in fact used in some systems ( e.g. , Elhadad and Robin 1992 ; PenMan 1989 ;#AUTHOR_TAG a )']",0
"['have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner( #AUTHOR_TAG ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c )""]","['have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner( #AUTHOR_TAG ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c )""]","['have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner( #AUTHOR_TAG ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c )""]","['have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner( #AUTHOR_TAG ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) ."", '']",0
"['have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ;#AUTHOR_TAG ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy ""]","['have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ;#AUTHOR_TAG ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a ,""]","['have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ;#AUTHOR_TAG ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , ""]","['have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ;#AUTHOR_TAG ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) ."", '']",0
"['possible response would be to abandon the separation; the generator could be a single component that handles all of the work.', 'This approach has occasionally been taken , as inKantrowitz and Bates ( 1992 ) andDanlos ( 1987 ) and , at least implicitly , in#AUTHOR_TAG andDelin et al. ( 1994 ) ;']","['possible response would be to abandon the separation; the generator could be a single component that handles all of the work.', 'This approach has occasionally been taken , as inKantrowitz and Bates ( 1992 ) andDanlos ( 1987 ) and , at least implicitly , in#AUTHOR_TAG andDelin et al. ( 1994 ) ;']","['possible response would be to abandon the separation; the generator could be a single component that handles all of the work.', 'This approach has occasionally been taken , as inKantrowitz and Bates ( 1992 ) andDanlos ( 1987 ) and , at least implicitly , in#AUTHOR_TAG andDelin et al. ( 1994 ) ;']","['possible response would be to abandon the separation; the generator could be a single component that handles all of the work.', 'This approach has occasionally been taken , as inKantrowitz and Bates ( 1992 ) andDanlos ( 1987 ) and , at least implicitly , in#AUTHOR_TAG andDelin et al. ( 1994 ) ; however , under this approach , all of the flexibility and simplicity of modular design is lost']",0
"[""89 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning( #AUTHOR_TAG a , 1988c ) ."", '']","[""; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning( #AUTHOR_TAG a , 1988c ) ."", '']","[""( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning( #AUTHOR_TAG a , 1988c ) ."", '']","['', ""These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning( #AUTHOR_TAG a , 1988c ) ."", '']",0
"['ovy has described another text planner that builds similar plans( #AUTHOR_TAG b ) .', '']","['ovy has described another text planner that builds similar plans( #AUTHOR_TAG b ) .', '']","['ovy has described another text planner that builds similar plans( #AUTHOR_TAG b ) .', '']","['ovy has described another text planner that builds similar plans( #AUTHOR_TAG b ) .', '']",0
"['', 'McDonald has even argued for extending the model to a large number of components (McDonald 1988), and several systems have indeed added an additional component between the planner and the linguistic component (Meteer 1994;Panaget 1994;Wanner 1994).', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation( #AUTHOR_TAG )']","['', 'McDonald has even argued for extending the model to a large number of components (McDonald 1988), and several systems have indeed added an additional component between the planner and the linguistic component (Meteer 1994;Panaget 1994;Wanner 1994).', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation( #AUTHOR_TAG )']","['', 'McDonald has even argued for extending the model to a large number of components (McDonald 1988), and several systems have indeed added an additional component between the planner and the linguistic component (Meteer 1994;Panaget 1994;Wanner 1994).', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation( #AUTHOR_TAG )']","['', 'McDonald has even argued for extending the model to a large number of components (McDonald 1988), and several systems have indeed added an additional component between the planner and the linguistic component (Meteer 1994;Panaget 1994;Wanner 1994).', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation( #AUTHOR_TAG )']",0
"['""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ;#AUTHOR_TAG a ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .', '']","['""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ;#AUTHOR_TAG a ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .', '']","['""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ;#AUTHOR_TAG a ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .', '']","['', 'The names given to the components vary ; they have been called ""strategic"" and ""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ;#AUTHOR_TAG a ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .', '']",0
"['veys and articles on the topic includeLamarche and Retord ( 1996 ) , deGroote and Retord ( 1996 ) , and#AUTHOR_TAG .', '']","['veys and articles on the topic includeLamarche and Retord ( 1996 ) , deGroote and Retord ( 1996 ) , and#AUTHOR_TAG .', '']","['veys and articles on the topic includeLamarche and Retord ( 1996 ) , deGroote and Retord ( 1996 ) , and#AUTHOR_TAG .', '']","['veys and articles on the topic includeLamarche and Retord ( 1996 ) , deGroote and Retord ( 1996 ) , and#AUTHOR_TAG .', 'Still, at the risk of proceeding at a slightly slower pace, we aim nonetheless to include here enough details to make the present paper self-contained']",0
"['', 'An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in#AUTHOR_TAG but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction']","['', 'An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in#AUTHOR_TAG but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction']","['', 'An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in#AUTHOR_TAG but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction']","['', 'An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in#AUTHOR_TAG but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction']",0
"['approach to this problem consists in defining , within the Cut-free atomic-id space , normal form derivations in which the succession of rule application is regulated( #AUTHOR_TAG , Hepple 1990 , Hendriks 1993 ) .', 'Each sequent has a distinguished category formula (underlined) on which rule applications are keyed: In the regulated calculus there is no spurious ambiguity, and provided there is no explicit or implicit antecedent product, i.e., provided .L is not needed, F ~ A is a theorem of the']","['approach to this problem consists in defining , within the Cut-free atomic-id space , normal form derivations in which the succession of rule application is regulated( #AUTHOR_TAG , Hepple 1990 , Hendriks 1993 ) .', 'Each sequent has a distinguished category formula (underlined) on which rule applications are keyed: In the regulated calculus there is no spurious ambiguity, and provided there is no explicit or implicit antecedent product, i.e., provided .L is not needed, F ~ A is a theorem of the']","['approach to this problem consists in defining , within the Cut-free atomic-id space , normal form derivations in which the succession of rule application is regulated( #AUTHOR_TAG , Hepple 1990 , Hendriks 1993 ) .', 'Each sequent has a distinguished category formula (underlined) on which rule applications are keyed: In the regulated calculus there is no spurious ambiguity, and provided there is no explicit or implicit antecedent product, i.e., provided .L is not needed, F ~ A is a theorem of the Lambek calculus iff F ~ A is a theorem of the regulated calculus.', '']","['approach to this problem consists in defining , within the Cut-free atomic-id space , normal form derivations in which the succession of rule application is regulated( #AUTHOR_TAG , Hepple 1990 , Hendriks 1993 ) .', 'Each sequent has a distinguished category formula (underlined) on which rule applications are keyed: In the regulated calculus there is no spurious ambiguity, and provided there is no explicit or implicit antecedent product, i.e., provided .L is not needed, F ~ A is a theorem of the Lambek calculus iff F ~ A is a theorem of the regulated calculus.', '']",0
"['classification using words is based on the observation that different DAs use distinctive word strings.', 'It is known that certain cue words and phrases( #AUTHOR_TAG ) can serve as explicit indicators of discourse structure .', 'Similarly']","['classification using words is based on the observation that different DAs use distinctive word strings.', 'It is known that certain cue words and phrases( #AUTHOR_TAG ) can serve as explicit indicators of discourse structure .', 'Similarly,']","['classification using words is based on the observation that different DAs use distinctive word strings.', 'It is known that certain cue words and phrases( #AUTHOR_TAG ) can serve as explicit indicators of discourse structure .', 'Similarly']","['classification using words is based on the observation that different DAs use distinctive word strings.', 'It is known that certain cue words and phrases( #AUTHOR_TAG ) can serve as explicit indicators of discourse structure .', '']",4
"['many other classifier architectures are applicable to the tasks discussed, in particular to DA classification.', 'A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and Vijay-Shanker ( 1998 ) is transformation-based learning( #AUTHOR_TAG ) .', '']","['many other classifier architectures are applicable to the tasks discussed, in particular to DA classification.', 'A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and Vijay-Shanker ( 1998 ) is transformation-based learning( #AUTHOR_TAG ) .', '']","['many other classifier architectures are applicable to the tasks discussed, in particular to DA classification.', 'A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and Vijay-Shanker ( 1998 ) is transformation-based learning( #AUTHOR_TAG ) .', '']","['', 'However, many other classifier architectures are applicable to the tasks discussed, in particular to DA classification.', 'A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and Vijay-Shanker ( 1998 ) is transformation-based learning( #AUTHOR_TAG ) .', '']",1
"['combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and Mercer 1983 ) and tagging( #AUTHOR_TAG ) .', 'It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995).', '']","['combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and Mercer 1983 ) and tagging( #AUTHOR_TAG ) .', 'It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995).', '']","['combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and Mercer 1983 ) and tagging( #AUTHOR_TAG ) .', 'It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995).', '']","['combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and Mercer 1983 ) and tagging( #AUTHOR_TAG ) .', 'It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995).', '']",1
"['combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988).', 'It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct( #AUTHOR_TAG ) .', '']","['combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988).', 'It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct( #AUTHOR_TAG ) .', '']","['combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988).', 'It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct( #AUTHOR_TAG ) .', '']","['combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988).', 'It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct( #AUTHOR_TAG ) .', '']",0
"[""equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in#AUTHOR_TAG ."", 'It will identify a pronoun with any term of type e elsewhere in the QLF, relying on the binding conditions to prevent impossible associations']","[""equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in#AUTHOR_TAG ."", 'It will identify a pronoun with any term of type e elsewhere in the QLF, relying on the binding conditions to prevent impossible associations']","[""equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in#AUTHOR_TAG ."", 'It will identify a pronoun with any term of type e elsewhere in the QLF, relying on the binding conditions to prevent impossible associations']","[""equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in#AUTHOR_TAG ."", 'It will identify a pronoun with any term of type e elsewhere in the QLF, relying on the binding conditions to prevent impossible associations']",1
"['etc.).', 'These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs [#AUTHOR_TAG ] , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing )']","['etc.).', 'These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs [#AUTHOR_TAG ] , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing )']","[', etc.).', 'These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs [#AUTHOR_TAG ] , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing )']","['is required is that QLFs are, as here, expressed in a typed higher-order logic, augmented with constructs representing the interpretation of context-dependent elements (pronouns, ellipsis, focus, etc.).', 'These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs [#AUTHOR_TAG ] , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing ) .', '']",0
"['', 'In the CLE-QLF approach, as rationally reconstructed by#AUTHOR_TAG andCrouch and Putman ( 1994 ) , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.', 'Just as here, these QLFs repre- sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context']","['', 'In the CLE-QLF approach, as rationally reconstructed by#AUTHOR_TAG andCrouch and Putman ( 1994 ) , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.', 'Just as here, these QLFs repre- sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context']","['', 'In the CLE-QLF approach, as rationally reconstructed by#AUTHOR_TAG andCrouch and Putman ( 1994 ) , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.', 'Just as here, these QLFs repre- sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context']","['', 'In the CLE-QLF approach, as rationally reconstructed by#AUTHOR_TAG andCrouch and Putman ( 1994 ) , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.', 'Just as here, these QLFs repre- sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context']",1
"['third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by#AUTHOR_TAG .', 'This describes the semantics of QLFs via a supervaluation over the semantics of the RQLFs that they subsume.', 'Although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many QLFs subsumed RQLFs that are not actually permitted by the resolution']","['third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by#AUTHOR_TAG .', 'This describes the semantics of QLFs via a supervaluation over the semantics of the RQLFs that they subsume.', 'Although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many QLFs subsumed RQLFs that are not actually permitted by the resolution rules: for example, those that can only arise via a violation of scoping or binding constraints.', '']","['third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by#AUTHOR_TAG .', 'This describes the semantics of QLFs via a supervaluation over the semantics of the RQLFs that they subsume.', 'Although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many QLFs subsumed RQLFs that are not actually permitted by the resolution rules: for example, those that can only arise via a violation of scoping or binding constraints.', '']","['third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by#AUTHOR_TAG .', 'This describes the semantics of QLFs via a supervaluation over the semantics of the RQLFs that they subsume.', 'Although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many QLFs subsumed RQLFs that are not actually permitted by the resolution rules: for example, those that can only arise via a violation of scoping or binding constraints.', '']",1
"['then go on to compare the current approach with that of some other theories with similar aims : the `` standard version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by#AUTHOR_TAG andCrouch and Pulman ( 1994 ) ; underspecified Discourse Representation Theory ( Reyle 1993 ) ; and the `` glue language approach']","['then go on to compare the current approach with that of some other theories with similar aims : the `` standard version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by#AUTHOR_TAG andCrouch and Pulman ( 1994 ) ; underspecified Discourse Representation Theory ( Reyle 1993 ) ; and the `` glue language approach']","['then go on to compare the current approach with that of some other theories with similar aims : the `` standard version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by#AUTHOR_TAG andCrouch and Pulman ( 1994 ) ; underspecified Discourse Representation Theory ( Reyle 1993 ) ; and the `` glue language approach']","['then go on to compare the current approach with that of some other theories with similar aims : the `` standard version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by#AUTHOR_TAG andCrouch and Pulman ( 1994 ) ; underspecified Discourse Representation Theory ( Reyle 1993 ) ; and the `` glue language approach ofDalrymple et al. ( 1996 )']",1
"[""starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in#AUTHOR_TAG , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) ."", '']","[""starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in#AUTHOR_TAG , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) ."", '']","[""starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in#AUTHOR_TAG , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) ."", '']","[""starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in#AUTHOR_TAG , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) ."", '']",1
"['assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any( #AUTHOR_TAG ) , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity']","['assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any( #AUTHOR_TAG ) , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity']","['assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any( #AUTHOR_TAG ) , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity']","['assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any( #AUTHOR_TAG ) , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity']",0
"['', ""The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure( #AUTHOR_TAG ) , with the resolution process as described here ."", '']","['', ""The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure( #AUTHOR_TAG ) , with the resolution process as described here ."", '']","['', ""The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure( #AUTHOR_TAG ) , with the resolution process as described here ."", '']","['are several stategies that might be pursued.', 'One is to adopt Pinkal\'s ""radical underspecification"" approach (Pinkal 1995) and use underspecified representations for all types of ambiguity, even syntactic ambiguity.', ""The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure( #AUTHOR_TAG ) , with the resolution process as described here ."", '']",3
"['ifier scope using the conditional equivalence mechanism.', 'The version proposed here combines a basic insight fromLewin ( 1990 ) with higher-order unification to give an analysis that has a strong resemblance to that proposed in#AUTHOR_TAG , 1991 ) , with some differences that are commented on below .', '']","['quantifier scope using the conditional equivalence mechanism.', 'The version proposed here combines a basic insight fromLewin ( 1990 ) with higher-order unification to give an analysis that has a strong resemblance to that proposed in#AUTHOR_TAG , 1991 ) , with some differences that are commented on below .', '']","['ifier scope using the conditional equivalence mechanism.', 'The version proposed here combines a basic insight fromLewin ( 1990 ) with higher-order unification to give an analysis that has a strong resemblance to that proposed in#AUTHOR_TAG , 1991 ) , with some differences that are commented on below .', '']","['can implement a deductive theory of quantifier scope using the conditional equivalence mechanism.', 'The version proposed here combines a basic insight fromLewin ( 1990 ) with higher-order unification to give an analysis that has a strong resemblance to that proposed in#AUTHOR_TAG , 1991 ) , with some differences that are commented on below .', '']",1
"['is interesting to compare this analysis with that described in Dalrymple , Shieber , andPereira ( 1991 ) and#AUTHOR_TAG , 1991 ) .', '']","['is interesting to compare this analysis with that described in Dalrymple , Shieber , andPereira ( 1991 ) and#AUTHOR_TAG , 1991 ) .', '']","['is interesting to compare this analysis with that described in Dalrymple , Shieber , andPereira ( 1991 ) and#AUTHOR_TAG , 1991 ) .', '']","['is interesting to compare this analysis with that described in Dalrymple , Shieber , andPereira ( 1991 ) and#AUTHOR_TAG , 1991 ) .', '']",1
"['the available five relative scopings of the quantifiers are produced( #AUTHOR_TAG , 47 ) , but without the need for a free variable constraint -- the HOU algorithm will not produce any solutions in which a previously bound variable becomes free ;']","['the available five relative scopings of the quantifiers are produced( #AUTHOR_TAG , 47 ) , but without the need for a free variable constraint -- the HOU algorithm will not produce any solutions in which a previously bound variable becomes free ;']","['the available five relative scopings of the quantifiers are produced( #AUTHOR_TAG , 47 ) , but without the need for a free variable constraint -- the HOU algorithm will not produce any solutions in which a previously bound variable becomes free ;']",['( #AUTHOR_TAG'],0
"['ing a calculus for reasoning with QLFs is too large a task to be undertaken here.', 'But the general outlines are reasonably clear , and we can adapt some of the UDRS( #AUTHOR_TAG ) work to our own framework .', '']","['ing a calculus for reasoning with QLFs is too large a task to be undertaken here.', 'But the general outlines are reasonably clear , and we can adapt some of the UDRS( #AUTHOR_TAG ) work to our own framework .', '']","['ing a calculus for reasoning with QLFs is too large a task to be undertaken here.', 'But the general outlines are reasonably clear , and we can adapt some of the UDRS( #AUTHOR_TAG ) work to our own framework .', '']","['ing a calculus for reasoning with QLFs is too large a task to be undertaken here.', 'But the general outlines are reasonably clear , and we can adapt some of the UDRS( #AUTHOR_TAG ) work to our own framework .', '']",5
"['present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise']","['present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise']","['present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise']","['present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise']",0
"['; Williams , Harvey , and Preston 1996 ;#AUTHOR_TAG ; Mitkov 1996 , 1998b )']","['; Williams , Harvey , and Preston 1996 ;#AUTHOR_TAG ; Mitkov 1996 , 1998b )']","['; Williams , Harvey , and Preston 1996 ;#AUTHOR_TAG ; Mitkov 1996 , 1998b )']",['#AUTHOR_TAG'],0
"['the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ;#AUTHOR_TAG ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , ']","['the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ;#AUTHOR_TAG ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a ,']","['the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ;#AUTHOR_TAG ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , ']",['#AUTHOR_TAG'],0
"[""etreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( Hobbs 1978 ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach( #AUTHOR_TAG ) ."", '']","[""etreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( Hobbs 1978 ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach( #AUTHOR_TAG ) ."", '']","[""etreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( Hobbs 1978 ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach( #AUTHOR_TAG ) ."", '']","[""etreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( Hobbs 1978 ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach( #AUTHOR_TAG ) ."", '']",0
"['; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution( #AUTHOR_TAG a , 2001b ) .', '']","['; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution( #AUTHOR_TAG a , 2001b ) .', '']","['; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution( #AUTHOR_TAG a , 2001b ) .', '']",['( #AUTHOR_TAG'],0
"['istic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ;#AUTHOR_TAG ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov ']","['linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ;#AUTHOR_TAG ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov']","['or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ;#AUTHOR_TAG ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , ']","['', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ;#AUTHOR_TAG ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b )']",0
"['the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ;#AUTHOR_TAG ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a ,']","['the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ;#AUTHOR_TAG ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a ,']","['the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ;#AUTHOR_TAG ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a ,']",['#AUTHOR_TAG'],0
"['', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ;#AUTHOR_TAG ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in']","['', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ;#AUTHOR_TAG ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in']","['', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ;#AUTHOR_TAG ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in']",['#AUTHOR_TAG'],0
"['; Mitkov and Barbu 2000 ;#AUTHOR_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '']","['; Mitkov and Barbu 2000 ;#AUTHOR_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '']","['; Mitkov and Barbu 2000 ;#AUTHOR_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '']","['', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ;#AUTHOR_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '']",0
"['istic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ;#AUTHOR_TAG ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ;']","['linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ;#AUTHOR_TAG ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ;']","['or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ;#AUTHOR_TAG ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ;']","['', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ;#AUTHOR_TAG ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b )']",0
"['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ;#AUTHOR_TAG ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ;#AUTHOR_TAG ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ;#AUTHOR_TAG ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', '']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ;#AUTHOR_TAG ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', '']",0
"['( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ;#AUTHOR_TAG ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '']","['( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ;#AUTHOR_TAG ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '']","['has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ;#AUTHOR_TAG ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '']","['', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ;#AUTHOR_TAG ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '']",0
"['last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field( #AUTHOR_TAG a ) .', '']","['last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field( #AUTHOR_TAG a ) .', '']","['last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field( #AUTHOR_TAG a ) .', '']","['last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field( #AUTHOR_TAG a ) .', 'A fundamental question that needs further investigation is how far the performance of anaphora resolution algorithms can go and what the limitations of knowledge-poor methods are.', '']",3
"['ering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ;#AUTHOR_TAG ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .', '']","['the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ;#AUTHOR_TAG ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .', '']","['ering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ;#AUTHOR_TAG ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .', '']","['', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ;#AUTHOR_TAG ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .', '']",0
"['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; Rich and LuperFoy 1988 ;#AUTHOR_TAG ) , which was difficult both to represent and to process , and which required considerable human input .', '']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; Rich and LuperFoy 1988 ;#AUTHOR_TAG ) , which was difficult both to represent and to process , and which required considerable human input .', '']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; Rich and LuperFoy 1988 ;#AUTHOR_TAG ) , which was difficult both to represent and to process , and which required considerable human input .', '']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; Rich and LuperFoy 1988 ;#AUTHOR_TAG ) , which was difficult both to represent and to process , and which required considerable human input .', '']",0
"['the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in#AUTHOR_TAG ,Gaizauskas and Humphreys ( 1996 ) , andKameyama ( 1997 ) .', 'The last decade of the 2']","['the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in#AUTHOR_TAG ,Gaizauskas and Humphreys ( 1996 ) , andKameyama ( 1997 ) .', 'The last decade of the 20th century saw a number of anaphora resolution projects for']","['the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in#AUTHOR_TAG ,Gaizauskas and Humphreys ( 1996 ) , andKameyama ( 1997 ) .', 'The last decade of the 20th century saw a number of anaphora resolution projects for']","['the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in#AUTHOR_TAG ,Gaizauskas and Humphreys ( 1996 ) , andKameyama ( 1997 ) .', '']",0
"['', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ;#AUTHOR_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '']","['', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ;#AUTHOR_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '']","['', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ;#AUTHOR_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '']","['', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ;#AUTHOR_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '']",0
"[', Harvey , and Preston 1996 ; Baldwin 1997 ;#AUTHOR_TAG , 1998b )']","[', Harvey , and Preston 1996 ; Baldwin 1997 ;#AUTHOR_TAG , 1998b )']","[', Harvey , and Preston 1996 ; Baldwin 1997 ;#AUTHOR_TAG , 1998b )']",['#AUTHOR_TAG'],0
"[""etreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) ."", '']","[""etreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) ."", '']","[""etreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) ."", '']","[""etreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) ."", '']",0
"['; Mitkov 1999 ;#AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998']","['; Mitkov 1999 ;#AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998']","['; Mitkov 1999 ;#AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998']",['#AUTHOR_TAG'],0
"['the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described inBaldwin et al. ( 1995 ) ,Gaizauskas and Humphreys ( 1996 ) , and#AUTHOR_TAG .', 'The last decade of the 2']","['the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described inBaldwin et al. ( 1995 ) ,Gaizauskas and Humphreys ( 1996 ) , and#AUTHOR_TAG .', 'The last decade of the 20th century saw a number of anaphora resolution projects for']","['the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described inBaldwin et al. ( 1995 ) ,Gaizauskas and Humphreys ( 1996 ) , and#AUTHOR_TAG .', 'The last decade of the 20th century saw a number of anaphora resolution projects for']","['the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described inBaldwin et al. ( 1995 ) ,Gaizauskas and Humphreys ( 1996 ) , and#AUTHOR_TAG .', '']",0
"['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', '']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', '']",0
"['per names are the main concern of the named-entity recognition subtask( #AUTHOR_TAG 1998) of information extraction.', 'The main objective of this subtask is the identification of proper names and also their classification into semantic categories (person, organization, location, etc.).1', 'There the']","['per names are the main concern of the named-entity recognition subtask( #AUTHOR_TAG 1998) of information extraction.', 'The main objective of this subtask is the identification of proper names and also their classification into semantic categories (person, organization, location, etc.).1', 'There the disambiguation of the']","['per names are the main concern of the named-entity recognition subtask( #AUTHOR_TAG 1998) of information extraction.', 'The main objective of this subtask is the identification of proper names and also their classification into semantic categories (person, organization, location, etc.).1', 'There the disambiguation of']","['per names are the main concern of the named-entity recognition subtask( #AUTHOR_TAG 1998) of information extraction.', 'The main objective of this subtask is the identification of proper names and also their classification into semantic categories (person, organization, location, etc.).1', '']",0
"['', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [#AUTHOR_TAG ] , Brill 's [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", '']","['', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [#AUTHOR_TAG ] , Brill 's [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", '']","['', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [#AUTHOR_TAG ] , Brill 's [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", '']","['', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [#AUTHOR_TAG ] , Brill 's [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", '']",1
"['entire document.', 'This is implemented as a cascade of simple strategies , which were briefly described in#AUTHOR_TAG']","['entire document.', 'This is implemented as a cascade of simple strategies , which were briefly described in#AUTHOR_TAG']","['the entire document.', 'This is implemented as a cascade of simple strategies , which were briefly described in#AUTHOR_TAG']","['', 'This is implemented as a cascade of simple strategies , which were briefly described in#AUTHOR_TAG']",5
"['are two corpora normally used for evaluation in a number of text-processing tasks : the Brown corpus( #AUTHOR_TAG ) and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank ( Marcus , Marcinkiewicz , and Santorini 1993 ) .', 'The Brown corpus represents general English.', 'It contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech.', 'The Brown corpus is rich in out-of-vocabulary (unknown) words, spelling errors, and ungrammatical sentences with complex internal structure.', 'Altogether there are about 500 documents in the Brown']","['are two corpora normally used for evaluation in a number of text-processing tasks : the Brown corpus( #AUTHOR_TAG ) and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank ( Marcus , Marcinkiewicz , and Santorini 1993 ) .', 'The Brown corpus represents general English.', 'It contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech.', 'The Brown corpus is rich in out-of-vocabulary (unknown) words, spelling errors, and ungrammatical sentences with complex internal structure.', 'Altogether there are about 500 documents in the Brown']","['are two corpora normally used for evaluation in a number of text-processing tasks : the Brown corpus( #AUTHOR_TAG ) and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank ( Marcus , Marcinkiewicz , and Santorini 1993 ) .', 'The Brown corpus represents general English.', 'It contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech.', 'The Brown corpus is rich in out-of-vocabulary (unknown) words, spelling errors, and ungrammatical sentences with complex internal structure.', 'Altogether there are about 500 documents in the Brown corpus, with']","['are two corpora normally used for evaluation in a number of text-processing tasks : the Brown corpus( #AUTHOR_TAG ) and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank ( Marcus , Marcinkiewicz , and Santorini 1993 ) .', 'The Brown corpus represents general English.', 'It contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech.', 'The Brown corpus is rich in out-of-vocabulary (unknown) words, spelling errors, and ungrammatical sentences with complex internal structure.', 'Altogether there are about 500 documents in the Brown corpus, with an average length of 2,300 word tokens']",5
"['', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition( #AUTHOR_TAG ) .', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", '']","['', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition( #AUTHOR_TAG ) .', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", '']","['', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition( #AUTHOR_TAG ) .', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", '']","['', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition( #AUTHOR_TAG ) .', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", '']",0
"['.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers( #AUTHOR_TAG ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Ma- chine learning systems treat the SBD task as a']","['language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers( #AUTHOR_TAG ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Ma- chine learning systems treat the SBD task as a']","['', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers( #AUTHOR_TAG ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Ma- chine learning systems treat the SBD task as']","['atically trainable software is generally seen as a way of producing sys- tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers( #AUTHOR_TAG ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', '']",0
"['.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks( #AUTHOR_TAG ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Ma- chine learning systems treat the SBD task as a']","['language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks( #AUTHOR_TAG ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Ma- chine learning systems treat the SBD task as a']","['', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks( #AUTHOR_TAG ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Ma- chine learning systems treat the SBD task as']","['atically trainable software is generally seen as a way of producing sys- tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks( #AUTHOR_TAG ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', '']",0
"[""since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [#AUTHOR_TAG a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", '']","[""since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [#AUTHOR_TAG a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", '']","[""since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [#AUTHOR_TAG a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", '']","['', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [#AUTHOR_TAG a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", '']",1
"['1995a ] , and MaxEnt [#AUTHOR_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .', '']","['1995a ] , and MaxEnt [#AUTHOR_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .', '']","["", Brill 's [ Brill 1995a ] , and MaxEnt [#AUTHOR_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", '']",['#AUTHOR_TAG'],1
"['C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system( #AUTHOR_TAG ) with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate .', '']","['C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system( #AUTHOR_TAG ) with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate .', '']","['C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system( #AUTHOR_TAG ) with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate .', '']","['C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system( #AUTHOR_TAG ) with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate .', '']",1
"['-consuming enterprise.', 'For instance , the Alembic workbench( #AUTHOR_TAG ) contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex .', '']","['to develop a rule-based system with good performance is quite a labor-consuming enterprise.', 'For instance , the Alembic workbench( #AUTHOR_TAG ) contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex .', '']","['to develop a rule-based system with good performance is quite a labor-consuming enterprise.', 'For instance , the Alembic workbench( #AUTHOR_TAG ) contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex .', '']","['', 'To put together a few rules is fast and easy, but to develop a rule-based system with good performance is quite a labor-consuming enterprise.', 'For instance , the Alembic workbench( #AUTHOR_TAG ) contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex .', '']",0
"['error rate).', 'On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in#AUTHOR_TAG ( 0.44 % vs. 0.5 % error rate ) .', '']","['error rate).', 'On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in#AUTHOR_TAG ( 0.44 % vs. 0.5 % error rate ) .', '']","['% error rate).', 'On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in#AUTHOR_TAG ( 0.44 % vs. 0.5 % error rate ) .', '']","['', 'On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in#AUTHOR_TAG ( 0.44 % vs. 0.5 % error rate ) .', 'Although these error rates seem to be very small, they are quite significant.', '']",1
"['', ""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill 's [#AUTHOR_TAG b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", '']","['', ""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill 's [#AUTHOR_TAG b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", '']","['', ""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill 's [#AUTHOR_TAG b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", '']","['', ""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill 's [#AUTHOR_TAG b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", '']",1
"['', 'In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in#AUTHOR_TAG .', '']","['', 'In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in#AUTHOR_TAG .', '']","['', 'In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in#AUTHOR_TAG .', '']","['', 'In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in#AUTHOR_TAG .', '']",1
"['', 'As#AUTHOR_TAG rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not .', 'Estimates from the Brown Corpus can be misleading.', ""For example, the capitalized word 'Acts' is""]","['', 'As#AUTHOR_TAG rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not .', 'Estimates from the Brown Corpus can be misleading.', ""For example, the capitalized word 'Acts' is""]","['', 'As#AUTHOR_TAG rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not .', 'Estimates from the Brown Corpus can be misleading.', ""For example, the capitalized word 'Acts'""]","['', 'As#AUTHOR_TAG rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not .', 'Estimates from the Brown Corpus can be misleading.', ""For example, the capitalized word 'Acts' is found twice in the Brown Corpus, both times as a proper noun (in a title)."", '']",1
"['', ""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [#AUTHOR_TAG ] or Brill 's [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", '']","['', ""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [#AUTHOR_TAG ] or Brill 's [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", '']","['', ""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [#AUTHOR_TAG ] or Brill 's [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", '']","['', ""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [#AUTHOR_TAG ] or Brill 's [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", '']",1
"['to deal with the case normalization issue.', 'Before using the DCA method , we applied a Russian morphological processor( #AUTHOR_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. .', '']","['to deal with the case normalization issue.', 'Before using the DCA method , we applied a Russian morphological processor( #AUTHOR_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. .', '']","['to deal with the case normalization issue.', 'Before using the DCA method , we applied a Russian morphological processor( #AUTHOR_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. .', '']","['', 'Before using the DCA method , we applied a Russian morphological processor( #AUTHOR_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. .', '']",5
"['we use this assumption with caution and first apply strategies that rely not just on', 'single words but on words together with their local contexts (n-grams). This is similar to ""one sense per collocation"" idea of#AUTHOR_TAG', '']","['however, we use this assumption with caution and first apply strategies that rely not just on', 'single words but on words together with their local contexts (n-grams). This is similar to ""one sense per collocation"" idea of#AUTHOR_TAG', '']","['we use this assumption with caution and first apply strategies that rely not just on', 'single words but on words together with their local contexts (n-grams). This is similar to ""one sense per collocation"" idea of#AUTHOR_TAG', '']","['', 'single words but on words together with their local contexts (n-grams). This is similar to ""one sense per collocation"" idea of#AUTHOR_TAG', '']",1
"[', but no supplementary information such as POS or morphological information is required to be present in this list.', 'A variety of such lists for many languages are already available ( e.g. ,#AUTHOR_TAG ) .', 'Words in such lists are usually supplemented with morphological and POS information (which is not required by our method']","['language, but no supplementary information such as POS or morphological information is required to be present in this list.', 'A variety of such lists for many languages are already available ( e.g. ,#AUTHOR_TAG ) .', 'Words in such lists are usually supplemented with morphological and POS information (which is not required by our method).', '']","[', but no supplementary information such as POS or morphological information is required to be present in this list.', 'A variety of such lists for many languages are already available ( e.g. ,#AUTHOR_TAG ) .', 'Words in such lists are usually supplemented with morphological and POS information (which is not required by our method']","['', 'This list includes common words for a given language, but no supplementary information such as POS or morphological information is required to be present in this list.', 'A variety of such lists for many languages are already available ( e.g. ,#AUTHOR_TAG ) .', 'Words in such lists are usually supplemented with morphological and POS information (which is not required by our method).', '']",0
"['its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before( #AUTHOR_TAG : 0.28 % vs. 0.20 % error rate ) .', '']","['its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before( #AUTHOR_TAG : 0.28 % vs. 0.20 % error rate ) .', '']","['its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before( #AUTHOR_TAG : 0.28 % vs. 0.20 % error rate ) .', '']","['its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before( #AUTHOR_TAG : 0.28 % vs. 0.20 % error rate ) .', '']",1
"['the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name.', 'Named-entity recognition systems usually use sets of complex hand-crafted rules that employ a gazetteer and a local context (Krupa and Hausman 1998).', 'In some systems such dependencies are learned from labeled examples( #AUTHOR_TAG ) .', 'The']","['the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name.', 'Named-entity recognition systems usually use sets of complex hand-crafted rules that employ a gazetteer and a local context (Krupa and Hausman 1998).', 'In some systems such dependencies are learned from labeled examples( #AUTHOR_TAG ) .', 'The']","['the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name.', 'Named-entity recognition systems usually use sets of complex hand-crafted rules that employ a gazetteer and a local context (Krupa and Hausman 1998).', 'In some systems such dependencies are learned from labeled examples( #AUTHOR_TAG ) .', 'The advantage']","['the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name.', 'Named-entity recognition systems usually use sets of complex hand-crafted rules that employ a gazetteer and a local context (Krupa and Hausman 1998).', 'In some systems such dependencies are learned from labeled examples( #AUTHOR_TAG ) .', '']",0
"['', 'For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 )( #AUTHOR_TAG ) .', 'We used these texts because the approach described in this article was initially designed to be part of a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed for MUC-7.', '']","['', 'For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 )( #AUTHOR_TAG ) .', 'We used these texts because the approach described in this article was initially designed to be part of a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed for MUC-7.', '']","['', 'For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 )( #AUTHOR_TAG ) .', 'We used these texts because the approach described in this article was initially designed to be part of a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed for MUC-7.', '']","['abbreviation (as opposed to regular word) These four lists can be acquired completely automatically from raw (unlabeled) texts.', 'For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 )( #AUTHOR_TAG ) .', 'We used these texts because the approach described in this article was initially designed to be part of a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed for MUC-7.', '']",5
"['system (Aberdeen et al. 1995): a 0.5% error rate.', 'The best performance on the Brown corpus , a 0.2 % error rate , was reported by#AUTHOR_TAG , who trained a decision tree classifier on a 25-million-word corpus .', '']","['system (Aberdeen et al. 1995): a 0.5% error rate.', 'The best performance on the Brown corpus , a 0.2 % error rate , was reported by#AUTHOR_TAG , who trained a decision tree classifier on a 25-million-word corpus .', '']","['C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate.', 'The best performance on the Brown corpus , a 0.2 % error rate , was reported by#AUTHOR_TAG , who trained a decision tree classifier on a 25-million-word corpus .', '']","['C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate.', 'The best performance on the Brown corpus , a 0.2 % error rate , was reported by#AUTHOR_TAG , who trained a decision tree classifier on a 25-million-word corpus .', '']",1
"['much information has been published on abbreviation identification.', 'One of the better-known approaches is described in#AUTHOR_TAG , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing .', 'This is similar to']","['much information has been published on abbreviation identification.', 'One of the better-known approaches is described in#AUTHOR_TAG , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing .', 'This is similar to']","['much information has been published on abbreviation identification.', 'One of the better-known approaches is described in#AUTHOR_TAG , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing .', 'This is similar to']","['much information has been published on abbreviation identification.', 'One of the better-known approaches is described in#AUTHOR_TAG , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing .', '']",0
"['', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks , including word sense disambiguation( #AUTHOR_TAG ) and named-entity recognition ( Cucerzan and Yarowsky 1999 ) .', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", 'In capitalized-word disambiguation,']","['', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks , including word sense disambiguation( #AUTHOR_TAG ) and named-entity recognition ( Cucerzan and Yarowsky 1999 ) .', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", 'In capitalized-word disambiguation,']","['', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks , including word sense disambiguation( #AUTHOR_TAG ) and named-entity recognition ( Cucerzan and Yarowsky 1999 ) .', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", 'In capitalized-word disambiguation,']","['', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks , including word sense disambiguation( #AUTHOR_TAG ) and named-entity recognition ( Cucerzan and Yarowsky 1999 ) .', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", 'In capitalized-word disambiguation,']",0
"[').', 'A detailed introduction to the SBD problem can be found in#AUTHOR_TAG']","['', 'A detailed introduction to the SBD problem can be found in#AUTHOR_TAG']","[').', 'A detailed introduction to the SBD problem can be found in#AUTHOR_TAG']","['', 'A detailed introduction to the SBD problem can be found in#AUTHOR_TAG']",0
"['very short documents of one to three sentences also present a difficulty for our approach.', 'This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in#AUTHOR_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage']","['very short documents of one to three sentences also present a difficulty for our approach.', 'This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in#AUTHOR_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage']","['very short documents of one to three sentences also present a difficulty for our approach.', 'This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in#AUTHOR_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage']","['', 'We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in#AUTHOR_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage']",1
"['', '#AUTHOR_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names .', 'They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.', 'The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.', 'It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens ']","['', '#AUTHOR_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names .', 'They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.', 'The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.', 'It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens']","['', '#AUTHOR_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names .', 'They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.', 'The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.', 'It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Mo']","['', '#AUTHOR_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names .', 'They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.', 'The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.', 'It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998).', '']",5
"['.', 'Unlike other POS taggers , this POS tagger( #AUTHOR_TAG ) was also trained to disambiguate sentence boundaries']","['', 'Unlike other POS taggers , this POS tagger( #AUTHOR_TAG ) was also trained to disambiguate sentence boundaries']","['', 'Unlike other POS taggers , this POS tagger( #AUTHOR_TAG ) was also trained to disambiguate sentence boundaries']","['', 'Unlike other POS taggers , this POS tagger( #AUTHOR_TAG ) was also trained to disambiguate sentence boundaries']",5
"['C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system( #AUTHOR_TAG ) : a 0.5 % error rate .', '']","['C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system( #AUTHOR_TAG ) : a 0.5 % error rate .', '']","['C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system( #AUTHOR_TAG ) : a 0.5 % error rate .', '']","['C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system( #AUTHOR_TAG ) : a 0.5 % error rate .', '']",1
"['in Section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'This is where robust syntactic systems like SATZ( #AUTHOR_TAG ) or the POS tagger reported inMikheev ( 2000 ) , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage']","['in Section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'This is where robust syntactic systems like SATZ( #AUTHOR_TAG ) or the POS tagger reported inMikheev ( 2000 ) , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage']","['in Section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'This is where robust syntactic systems like SATZ( #AUTHOR_TAG ) or the POS tagger reported inMikheev ( 2000 ) , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage']","['', 'We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'This is where robust syntactic systems like SATZ( #AUTHOR_TAG ) or the POS tagger reported inMikheev ( 2000 ) , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage']",1
"['', ""#AUTHOR_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last"", '']","['', ""#AUTHOR_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last"", '']","['', ""#AUTHOR_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last"", '']","['', ""#AUTHOR_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last"", '']",2
"['description of the EAGLE workbench for linguistic engineering( #AUTHOR_TAG ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document .', 'This heuristic also employs a database of bigrams and unigrams of lower-cased and capitalized words found in unambiguous positions.', 'It is quite similar to our method for capitalized-word disambiguation.', '']","['description of the EAGLE workbench for linguistic engineering( #AUTHOR_TAG ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document .', 'This heuristic also employs a database of bigrams and unigrams of lower-cased and capitalized words found in unambiguous positions.', 'It is quite similar to our method for capitalized-word disambiguation.', '']","['description of the EAGLE workbench for linguistic engineering( #AUTHOR_TAG ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document .', 'This heuristic also employs a database of bigrams and unigrams of lower-cased and capitalized words found in unambiguous positions.', 'It is quite similar to our method for capitalized-word disambiguation.', '']","['description of the EAGLE workbench for linguistic engineering( #AUTHOR_TAG ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document .', 'This heuristic also employs a database of bigrams and unigrams of lower-cased and capitalized words found in unambiguous positions.', 'It is quite similar to our method for capitalized-word disambiguation.', '']",1
"[', the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling( #AUTHOR_TAG ) .', 'Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.', '']","[', the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling( #AUTHOR_TAG ) .', 'Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.', '']","[', the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling( #AUTHOR_TAG ) .', 'Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.', '']","['', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling( #AUTHOR_TAG ) .', 'Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.', '']",0
"['', 'The disambiguation of capitalized words in ambiguous positions leads to the identification of proper names (or their derivatives), and in this article we will use these two terms and the term case normalization interchangeably.', ""#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''"", 'Obviously these differences']","['', 'The disambiguation of capitalized words in ambiguous positions leads to the identification of proper names (or their derivatives), and in this article we will use these two terms and the term case normalization interchangeably.', ""#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''"", 'Obviously these differences']","['', 'The disambiguation of capitalized words in ambiguous positions leads to the identification of proper names (or their derivatives), and in this article we will use these two terms and the term case normalization interchangeably.', ""#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''"", 'Obviously these differences']","['', 'The disambiguation of capitalized words in ambiguous positions leads to the identification of proper names (or their derivatives), and in this article we will use these two terms and the term case normalization interchangeably.', ""#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''"", 'Obviously these differences arise because some capitalized words stand for proper names (such as Continental, the name of an airline) and some do not']",0
"['', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .', 'This method first applies an ""abbreviation recognizer"" that generates a set of ""candidate abbreviations"" for a document.', 'Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK).', 'The abbreviation recognizer for these purposes is allowed to overgenerate significantly.', '']","['', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .', 'This method first applies an ""abbreviation recognizer"" that generates a set of ""candidate abbreviations"" for a document.', 'Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK).', 'The abbreviation recognizer for these purposes is allowed to overgenerate significantly.', '']","['', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .', 'This method first applies an ""abbreviation recognizer"" that generates a set of ""candidate abbreviations"" for a document.', 'Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK).', 'The abbreviation recognizer for these purposes is allowed to overgenerate significantly.', '']","['', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .', 'This method first applies an ""abbreviation recognizer"" that generates a set of ""candidate abbreviations"" for a document.', 'Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK).', 'The abbreviation recognizer for these purposes is allowed to overgenerate significantly.', 'There is no harm (apart from the performance issues) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained.', '']",0
"['#AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .', 'This is a']","['#AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .', 'This is a']","['#AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .', 'This is']","['', 'Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance.', 'For instance ,#AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .', '']",1
"['', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ;#AUTHOR_TAG ) .']","['', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ;#AUTHOR_TAG ) .7']","['', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ;#AUTHOR_TAG ) .']","['', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ;#AUTHOR_TAG ) .7 As an example , consider the translation into French of the house collapsed']",0
"['', 'Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed( #AUTHOR_TAG :460 ff .']","['', 'Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed( #AUTHOR_TAG :460 ff .']","['', 'Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed( #AUTHOR_TAG :460 ff .']","['', 'Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed( #AUTHOR_TAG :460 ff . )', 'However, only two of these criteria are explicit in the coding system']",3
"['a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information( #AUTHOR_TAG ) .', '']","['a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information( #AUTHOR_TAG ) .', '']","['a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information( #AUTHOR_TAG ) .', '']","['', 'In addition , a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information( #AUTHOR_TAG ) .', 'Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample']",0
"['tested the classification of verbs into semantic types using a verb list of 139 pre-classified items drawn from the lists published in#AUTHOR_TAG andStockwell et al. ( 1973 ) .', 'Figure 16 gives the number of verbs classified under each']","['tested the classification of verbs into semantic types using a verb list of 139 pre-classified items drawn from the lists published in#AUTHOR_TAG andStockwell et al. ( 1973 ) .', 'Figure 16 gives the number of verbs classified under each']","['tested the classification of verbs into semantic types using a verb list of 139 pre-classified items drawn from the lists published in#AUTHOR_TAG andStockwell et al. ( 1973 ) .', 'Figure 16 gives the number of verbs classified under each category']","['tested the classification of verbs into semantic types using a verb list of 139 pre-classified items drawn from the lists published in#AUTHOR_TAG andStockwell et al. ( 1973 ) .', 'Figure 16 gives the number of verbs classified under each category by these authors and the number successfully classified into the same categories by the system']",5
"['', 'As#AUTHOR_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.', 'The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation.', '']","['', 'As#AUTHOR_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.', 'The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation.', '']","['', 'As#AUTHOR_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.', 'The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation.', '']","['', 'As#AUTHOR_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.', 'The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation.', '']",0
"['research described below is taking place in the context of three collaborative projects( #AUTHOR_TAG ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', '']","['research described below is taking place in the context of three collaborative projects( #AUTHOR_TAG ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', '']","['', 'The research described below is taking place in the context of three collaborative projects( #AUTHOR_TAG ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', '']","['', 'The research described below is taking place in the context of three collaborative projects( #AUTHOR_TAG ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', '']",0
"['isting text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators (e.g.', 'Many investigators ( e.g.#AUTHOR_TAG ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', 'And']","['isting text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators (e.g.', 'Many investigators ( e.g.#AUTHOR_TAG ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', 'And']","['isting text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators (e.g.', 'Many investigators ( e.g.#AUTHOR_TAG ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', 'And']","['isting text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators (e.g.', 'Many investigators ( e.g.#AUTHOR_TAG ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', '']",4
"['previous work ( Bachenko et al. 1986 ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer( #AUTHOR_TAG ) .', 'The system generated phrase boundaries using information derived from the syntactic structure of a sentence.', '']","['previous work ( Bachenko et al. 1986 ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer( #AUTHOR_TAG ) .', 'The system generated phrase boundaries using information derived from the syntactic structure of a sentence.', '']","['previous work ( Bachenko et al. 1986 ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer( #AUTHOR_TAG ) .', 'The system generated phrase boundaries using information derived from the syntactic structure of a sentence.', '']","['previous work ( Bachenko et al. 1986 ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer( #AUTHOR_TAG ) .', 'The system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', '']",0
"['investigators (e.g.', 'Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ;#AUTHOR_TAG ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', '']","['investigators (e.g.', 'Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ;#AUTHOR_TAG ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', '']","['', 'Many investigators (e.g.', 'Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ;#AUTHOR_TAG ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', '']","['isting text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators (e.g.', 'Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ;#AUTHOR_TAG ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', '']",4
"['have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer( #AUTHOR_TAG ) .', '']","['have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer( #AUTHOR_TAG ) .', '']","['have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer( #AUTHOR_TAG ) .', '']","['have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer( #AUTHOR_TAG ) .', '']",5
"['see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', '#AUTHOR_TAG andLitman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing .', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing']","['(see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', '#AUTHOR_TAG andLitman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing .', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing']","['see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', '#AUTHOR_TAG andLitman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing .', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing']","['', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', '#AUTHOR_TAG andLitman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing .', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing']",0
"['', 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'Sentences like 12 , from#AUTHOR_TAG , are frequently cited .', '(Square brackets mark off the NP constituents that contain embed- ded sentences']","[""this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'Sentences like 12 , from#AUTHOR_TAG , are frequently cited .', '(Square brackets mark off the NP constituents that contain embed- ded sentences']","['', 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'Sentences like 12 , from#AUTHOR_TAG , are frequently cited .', '(Square brackets mark off the NP constituents that contain embed- ded sentences']","['', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'Sentences like 12 , from#AUTHOR_TAG , are frequently cited .', '(Square brackets mark off the NP constituents that contain embed- ded sentences']",0
"['previous work( #AUTHOR_TAG ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( Olive and Liberman 1985 ) .', 'The system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'This paper re']","['previous work( #AUTHOR_TAG ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( Olive and Liberman 1985 ) .', 'The system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'This paper']","['previous work( #AUTHOR_TAG ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( Olive and Liberman 1985 ) .', 'The system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'This paper re']","['previous work( #AUTHOR_TAG ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( Olive and Liberman 1985 ) .', 'The system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', '']",2
"['', '#AUTHOR_TAG proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model .', 'He']","['rules outside the grammar proper.', '#AUTHOR_TAG proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model .', 'He']","['special rules outside the grammar proper.', '#AUTHOR_TAG proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model .', 'He']","['', 'Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', '#AUTHOR_TAG proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model .', 'He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology']",0
"['psycholinguistic studies ofMartin ( 1970 ) ,Allen ( 1975 ) ,Hillinger et al. ( 1976 ) ,Grosjean et al. ( 1979 ) ,Dommergues and Grosjean ( 1983 ) , and#AUTHOR_TAG , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .', '']","['psycholinguistic studies ofMartin ( 1970 ) ,Allen ( 1975 ) ,Hillinger et al. ( 1976 ) ,Grosjean et al. ( 1979 ) ,Dommergues and Grosjean ( 1983 ) , and#AUTHOR_TAG , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .', '']","['psycholinguistic studies ofMartin ( 1970 ) ,Allen ( 1975 ) ,Hillinger et al. ( 1976 ) ,Grosjean et al. ( 1979 ) ,Dommergues and Grosjean ( 1983 ) , and#AUTHOR_TAG , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .', '']","['psycholinguistic studies ofMartin ( 1970 ) ,Allen ( 1975 ) ,Hillinger et al. ( 1976 ) ,Grosjean et al. ( 1979 ) ,Dommergues and Grosjean ( 1983 ) , and#AUTHOR_TAG , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .', '']",0
"['3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in#AUTHOR_TAG , which ""are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option"" (p. 251) .', 'This approach to prosodic phrase boundary determination brings us']","['', '3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in#AUTHOR_TAG , which ""are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option"" (p. 251) .', 'This approach to prosodic phrase boundary determination brings us']","['3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in#AUTHOR_TAG , which ""are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option"" (p. 251) .', 'This approach to prosodic phrase boundary determination brings us']","['', '3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in#AUTHOR_TAG , which ""are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option"" (p. 251) .', 'This approach to prosodic phrase boundary determination brings us closer to a framework in which phonological, syntactic, and discourse features all contribute to prosodic phrasing']",1
"['for such mismatches, ""readjustment rules"" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'The result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In#AUTHOR_TAG , this flattening process is not part of the grammar .', '']","['for such mismatches, ""readjustment rules"" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'The result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In#AUTHOR_TAG , this flattening process is not part of the grammar .', '']","['for such mismatches, ""readjustment rules"" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'The result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In#AUTHOR_TAG , this flattening process is not part of the grammar .', '']","['', 'Sentences like 12, fromChomsky (1965) To account for such mismatches, ""readjustment rules"" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'The result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In#AUTHOR_TAG , this flattening process is not part of the grammar .', 'Rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12]"" (p.', '372).', '']",0
"['G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'An alternative representation based on#AUTHOR_TAG is presented inSelkirk ( 1984 ) , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree .', '']","['G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'An alternative representation based on#AUTHOR_TAG is presented inSelkirk ( 1984 ) , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree .', '']","['G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'An alternative representation based on#AUTHOR_TAG is presented inSelkirk ( 1984 ) , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree .', '']","['G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'An alternative representation based on#AUTHOR_TAG is presented inSelkirk ( 1984 ) , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree .', '']",1
"['syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', '#AUTHOR_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct .', '']","['syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', '#AUTHOR_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct .', '']","['syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', '#AUTHOR_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct .', '']","['syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', '#AUTHOR_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct .', '']",0
"['psycholinguistic studies ofMartin ( 1970 ) ,#AUTHOR_TAG ,Hillinger et al. ( 1976 ) ,Grosjean et al. ( 1979 ) ,Dommergues and Grosjean ( 1983 ) , andGee and Grosjean ( 1983 ) , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .', '']","['psycholinguistic studies ofMartin ( 1970 ) ,#AUTHOR_TAG ,Hillinger et al. ( 1976 ) ,Grosjean et al. ( 1979 ) ,Dommergues and Grosjean ( 1983 ) , andGee and Grosjean ( 1983 ) , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .', '']","['psycholinguistic studies ofMartin ( 1970 ) ,#AUTHOR_TAG ,Hillinger et al. ( 1976 ) ,Grosjean et al. ( 1979 ) ,Dommergues and Grosjean ( 1983 ) , andGee and Grosjean ( 1983 ) , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .', '']","['psycholinguistic studies ofMartin ( 1970 ) ,#AUTHOR_TAG ,Hillinger et al. ( 1976 ) ,Grosjean et al. ( 1979 ) ,Dommergues and Grosjean ( 1983 ) , andGee and Grosjean ( 1983 ) , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .', '']",0
"['', 'Many investigators (e.g.', 'Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ;#AUTHOR_TAG ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', 'And']","['', 'Many investigators (e.g.', 'Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ;#AUTHOR_TAG ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', 'And']","['', 'Many investigators (e.g.', 'Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ;#AUTHOR_TAG ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', 'And']","['isting text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators (e.g.', 'Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ;#AUTHOR_TAG ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', '']",4
"['', 'Although a grid may be more descriptively suitable for some aspects of prosody ( for example ,#AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing']","['', 'Although a grid may be more descriptively suitable for some aspects of prosody ( for example ,#AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing']","['', 'Although a grid may be more descriptively suitable for some aspects of prosody ( for example ,#AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing']","['', 'Although a grid may be more descriptively suitable for some aspects of prosody ( for example ,#AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing']",1
"['', 'Previous versions of our work , as described in#AUTHOR_TAG also assume that phrasing is dependent on predicate-argument structure .', 'The problem here is that the phrasing in observed data often ignores the argument status of constituents.', '']","['', 'Previous versions of our work , as described in#AUTHOR_TAG also assume that phrasing is dependent on predicate-argument structure .', 'The problem here is that the phrasing in observed data often ignores the argument status of constituents.', '']","['', 'Previous versions of our work , as described in#AUTHOR_TAG also assume that phrasing is dependent on predicate-argument structure .', 'The problem here is that the phrasing in observed data often ignores the argument status of constituents.', '']","['', 'Previous versions of our work , as described in#AUTHOR_TAG also assume that phrasing is dependent on predicate-argument structure .', 'The problem here is that the phrasing in observed data often ignores the argument status of constituents.', '']",2
"['', 'This observation has led some researchers , e.g. ,#AUTHOR_TAG , to claim a direct mapping between the syntactic phrase and the prosodic phrase .', '']","['', 'This observation has led some researchers , e.g. ,#AUTHOR_TAG , to claim a direct mapping between the syntactic phrase and the prosodic phrase .', '']","['', 'This observation has led some researchers , e.g. ,#AUTHOR_TAG , to claim a direct mapping between the syntactic phrase and the prosodic phrase .', '']","['', 'This observation has led some researchers , e.g. ,#AUTHOR_TAG , to claim a direct mapping between the syntactic phrase and the prosodic phrase .', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", '']",0
['4 The most explicit version of this approach is the analysis presented in#AUTHOR_TAG ( henceforth G&G )'],['.4 The most explicit version of this approach is the analysis presented in#AUTHOR_TAG ( henceforth G&G )'],['4 The most explicit version of this approach is the analysis presented in#AUTHOR_TAG ( henceforth G&G )'],['#AUTHOR_TAG'],1
"['', 'The relation between discourse and prosodic phrasing has been examined in some detail by#AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3']","['', 'The relation between discourse and prosodic phrasing has been examined in some detail by#AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'To our']","['', 'The relation between discourse and prosodic phrasing has been examined in some detail by#AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'To our']","['', 'The relation between discourse and prosodic phrasing has been examined in some detail by#AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', '']",0
"['rules for phonological word formation are adopted , for the most part , from G & G ,#AUTHOR_TAG , and the account of monosyllabic destressing inSelkirk ( 1984 ) .', 'Thus in our analysis, rules of phonological word formation apply to the non-null terminal nodes in a syntax tree.', 'If the terminal is a content word']","['rules for phonological word formation are adopted , for the most part , from G & G ,#AUTHOR_TAG , and the account of monosyllabic destressing inSelkirk ( 1984 ) .', 'Thus in our analysis, rules of phonological word formation apply to the non-null terminal nodes in a syntax tree.', 'If the terminal is a content word,']","['rules for phonological word formation are adopted , for the most part , from G & G ,#AUTHOR_TAG , and the account of monosyllabic destressing inSelkirk ( 1984 ) .', 'Thus in our analysis, rules of phonological word formation apply to the non-null terminal nodes in a syntax tree.', 'If the terminal is']","['rules for phonological word formation are adopted , for the most part , from G & G ,#AUTHOR_TAG , and the account of monosyllabic destressing inSelkirk ( 1984 ) .', 'Thus in our analysis, rules of phonological word formation apply to the non-null terminal nodes in a syntax tree.', '']",5
"['the cooperative principle of#AUTHOR_TAG , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ;']","['the cooperative principle of#AUTHOR_TAG , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ;']","['the cooperative principle of#AUTHOR_TAG , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ;']","['', 'Secondly , the cooperative principle of#AUTHOR_TAG , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader .', '']",4
"['this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others.', 'We are going to make such a comparison with the theories proposed by J.#AUTHOR_TAG , 1982 ) that represent a more computationally oriented approach to coherence , and those of T.A. van Dijk and W.Kintch ( 1983 ) , who are more interested in addressing psychological and cognitive aspects of discourse coherence .', 'The quoted works seem to be good representatives for each of the directions; they also point to related literature']","['this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others.', 'We are going to make such a comparison with the theories proposed by J.#AUTHOR_TAG , 1982 ) that represent a more computationally oriented approach to coherence , and those of T.A. van Dijk and W.Kintch ( 1983 ) , who are more interested in addressing psychological and cognitive aspects of discourse coherence .', 'The quoted works seem to be good representatives for each of the directions; they also point to related literature']","['this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others.', 'We are going to make such a comparison with the theories proposed by J.#AUTHOR_TAG , 1982 ) that represent a more computationally oriented approach to coherence , and those of T.A. van Dijk and W.Kintch ( 1983 ) , who are more interested in addressing psychological and cognitive aspects of discourse coherence .', 'The quoted works seem to be good representatives for each of the directions; they also point to related literature']","['this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others.', 'We are going to make such a comparison with the theories proposed by J.#AUTHOR_TAG , 1982 ) that represent a more computationally oriented approach to coherence , and those of T.A. van Dijk and W.Kintch ( 1983 ) , who are more interested in addressing psychological and cognitive aspects of discourse coherence .', 'The quoted works seem to be good representatives for each of the directions; they also point to related literature']",1
"['techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure as- sumption"" (ibid.), ""domain circumscription"" (cf.#AUTHOR_TAG) , and their kin.', 'Similarly']","['techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure as- sumption"" (ibid.), ""domain circumscription"" (cf.#AUTHOR_TAG) , and their kin.', 'Similarly, the notion of R+ M-abduction is']","['techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure as- sumption"" (ibid.), ""domain circumscription"" (cf.#AUTHOR_TAG) , and their kin.', 'Similarly']","['techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure as- sumption"" (ibid.), ""domain circumscription"" (cf.#AUTHOR_TAG) , and their kin.', '']",1
['.#AUTHOR_TAG discussed sentences of the form * This is a chair but you can sit on it'],['.#AUTHOR_TAG discussed sentences of the form * This is a chair but you can sit on it'],['.#AUTHOR_TAG discussed sentences of the form * This is a chair but you can sit on it'],['.#AUTHOR_TAG discussed sentences of the form * This is a chair but you can sit on it'],0
"['', 'Opposition (called ""adversative"" or ""contrary-to-expectation"" by Halliday and Hasan 1976;cf. also#AUTHOR_TAG , p. 672 )']","['', 'Opposition (called ""adversative"" or ""contrary-to-expectation"" by Halliday and Hasan 1976;cf. also#AUTHOR_TAG , p. 672 )']","['have three subfunctions: . .', 'Opposition (called ""adversative"" or ""contrary-to-expectation"" by Halliday and Hasan 1976;cf. also#AUTHOR_TAG , p. 672 )']","['', 'Opposition (called ""adversative"" or ""contrary-to-expectation"" by Halliday and Hasan 1976;cf. also#AUTHOR_TAG , p. 672 )']",0
"['there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 ,#AUTHOR_TAG , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure']","['there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 ,#AUTHOR_TAG , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure']","['there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 ,#AUTHOR_TAG , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', '']","['there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 ,#AUTHOR_TAG , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', '']",1
"[""collections of words ( e.g. , `` colorless green ideas ... '' ) , while before the advent of Chomskyan formalisms , a sentence was defined as the smallest meaningful collection of words ;#AUTHOR_TAG , p. 546 ) gives 10 definitions of a sentence""]","[""collections of words ( e.g. , `` colorless green ideas ... '' ) , while before the advent of Chomskyan formalisms , a sentence was defined as the smallest meaningful collection of words ;#AUTHOR_TAG , p. 546 ) gives 10 definitions of a sentence""]","[""able collections of words ( e.g. , `` colorless green ideas ... '' ) , while before the advent of Chomskyan formalisms , a sentence was defined as the smallest meaningful collection of words ;#AUTHOR_TAG , p. 546 ) gives 10 definitions of a sentence""]","['', ""This problem may be similar to the situation in which current formal grammars allow nonsensical but parsable collections of words ( e.g. , `` colorless green ideas ... '' ) , while before the advent of Chomskyan formalisms , a sentence was defined as the smallest meaningful collection of words ;#AUTHOR_TAG , p. 546 ) gives 10 definitions of a sentence""]",0
"['', 'This Principle of Finitism is also assumed by#AUTHOR_TAG ,Jackendoff ( 1983 ) ,Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .', '']","['the logic.', 'This Principle of Finitism is also assumed by#AUTHOR_TAG ,Jackendoff ( 1983 ) ,Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .', '']","['describing the logic.', 'This Principle of Finitism is also assumed by#AUTHOR_TAG ,Jackendoff ( 1983 ) ,Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .', '']","['', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by#AUTHOR_TAG ,Jackendoff ( 1983 ) ,Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .', '']",1
"['', ""But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS 's ) of#AUTHOR_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora ."", '']","['', ""But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS 's ) of#AUTHOR_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora ."", '']","['', ""But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS 's ) of#AUTHOR_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora ."", '']","['', ""But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS 's ) of#AUTHOR_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora ."", '']",1
"['have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus ( Grosz 1977 , 1978 ; Sidner 1983 ) or quantifier scoping( #AUTHOR_TAG ) must play a role , too .', '']","['have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus ( Grosz 1977 , 1978 ; Sidner 1983 ) or quantifier scoping( #AUTHOR_TAG ) must play a role , too .', '']","['have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus ( Grosz 1977 , 1978 ; Sidner 1983 ) or quantifier scoping( #AUTHOR_TAG ) must play a role , too .', '']","['have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus ( Grosz 1977 , 1978 ; Sidner 1983 ) or quantifier scoping( #AUTHOR_TAG ) must play a role , too .', 'Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']",0
"['to#AUTHOR_TAG , p. 67 ) , these two sentences are incoherent .', 'However, the same fragment, augmented with the third sentence Mary told him yesterday that the French spinach crop failed and Turkey is the only country... (ibid.)', 'suddenly (for Hobbs) becomes coherent.', 'It seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change; after all, the first two sentences didn']","['to#AUTHOR_TAG , p. 67 ) , these two sentences are incoherent .', 'However, the same fragment, augmented with the third sentence Mary told him yesterday that the French spinach crop failed and Turkey is the only country... (ibid.)', 'suddenly (for Hobbs) becomes coherent.', 'It seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change; after all, the first two sentences']","['to#AUTHOR_TAG , p. 67 ) , these two sentences are incoherent .', 'However, the same fragment, augmented with the third sentence Mary told him yesterday that the French spinach crop failed and Turkey is the only country... (ibid.)', 'suddenly (for Hobbs) becomes coherent.', 'It seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change; after all, the first two sentences didn']","['to#AUTHOR_TAG , p. 67 ) , these two sentences are incoherent .', 'However, the same fragment, augmented with the third sentence Mary told him yesterday that the French spinach crop failed and Turkey is the only country... (ibid.)', 'suddenly (for Hobbs) becomes coherent.', ""It seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change; after all, the first two sentences didn't change when the third one was added."", '']",1
"['many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.', 'For instance , relating ""they"" to ""apples"" in the sentence ( cfXXX#AUTHOR_TAG p. 195 ; Zadrozny 1987a ) : We bought the boys apples because they were so']","['many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.', 'For instance , relating ""they"" to ""apples"" in the sentence ( cfXXX#AUTHOR_TAG p. 195 ; Zadrozny 1987a ) : We bought the boys apples because they were so']","[', many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.', 'For instance , relating ""they"" to ""apples"" in the sentence ( cfXXX#AUTHOR_TAG p. 195 ; Zadrozny 1987a ) : We bought the boys apples because they were so']","['', 'It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.', 'For instance , relating ""they"" to ""apples"" in the sentence ( cfXXX#AUTHOR_TAG p. 195 ; Zadrozny 1987a ) : We bought the boys apples because they were so']",0
"['adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found inBlack and Bower ( 1979 ) and#AUTHOR_TAG']","['adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found inBlack and Bower ( 1979 ) and#AUTHOR_TAG']","['(2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found inBlack and Bower ( 1979 ) and#AUTHOR_TAG']","['', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found inBlack and Bower ( 1979 ) and#AUTHOR_TAG']",0
"['furthermore , our reading of the analysis of five paragraphs by#AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph .', '']","['furthermore , our reading of the analysis of five paragraphs by#AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph .', '']","['furthermore , our reading of the analysis of five paragraphs by#AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph .', '']","['', 'Finally , it has been shown byGroesser ( 1981 ) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by#AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph .', '']",4
"['last point may be seen better if we look at some differences between our system and KRYPTON , which also distinguishes between an object theory and background knowledge ( cfXXX#AUTHOR_TAG ) .', 'Brachman et al. 1985']","['last point may be seen better if we look at some differences between our system and KRYPTON , which also distinguishes between an object theory and background knowledge ( cfXXX#AUTHOR_TAG ) .', 'Brachman et al. 1985).', ""KRYPTON's A-box, encoding the object theory as a set of assertions, uses standard first order logic;""]","['last point may be seen better if we look at some differences between our system and KRYPTON , which also distinguishes between an object theory and background knowledge ( cfXXX#AUTHOR_TAG ) .', 'Brachman et al. 1985']","['last point may be seen better if we look at some differences between our system and KRYPTON , which also distinguishes between an object theory and background knowledge ( cfXXX#AUTHOR_TAG ) .', 'Brachman et al. 1985).', '']",1
"['.', ""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of#AUTHOR_TAG , the `` diagnosis from first principles '' ofReiter ( 1987 ) , `` explainability '' ofPoole ( 1988 ) , and the subset principle ofBerwick ( 1986 )""]","['kin.', ""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of#AUTHOR_TAG , the `` diagnosis from first principles '' ofReiter ( 1987 ) , `` explainability '' ofPoole ( 1988 ) , and the subset principle ofBerwick ( 1986 )""]","['.', ""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of#AUTHOR_TAG , the `` diagnosis from first principles '' ofReiter ( 1987 ) , `` explainability '' ofPoole ( 1988 ) , and the subset principle ofBerwick ( 1986 ) ."", '']","['techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure assumption"" (ibid.),', '""domain circumscription"" (cf.', 'Etherington and Mercer 1987), and their kin.', ""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of#AUTHOR_TAG , the `` diagnosis from first principles '' ofReiter ( 1987 ) , `` explainability '' ofPoole ( 1988 ) , and the subset principle ofBerwick ( 1986 ) ."", '']",1
"['', 'The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ;#AUTHOR_TAG ) to see what a formal interpretation of events in time might look like .', '']","['', 'The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ;#AUTHOR_TAG ) to see what a formal interpretation of events in time might look like .', '']","['', 'The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ;#AUTHOR_TAG ) to see what a formal interpretation of events in time might look like .', '']","['', 'The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ;#AUTHOR_TAG ) to see what a formal interpretation of events in time might look like .', '']",0
"['', 'According to#AUTHOR_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases .', 'Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types.', 'The three types are procedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation).', 'For each type, its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central,']","['', 'According to#AUTHOR_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases .', 'Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types.', 'The three types are procedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation).', 'For each type, its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central,']","['', 'According to#AUTHOR_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases .', 'Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types.', 'The three types are procedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation).', 'For each type, its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central,']","['', 'According to#AUTHOR_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases .', 'Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types.', 'The three types are procedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation).', 'For each type, its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined).', '']",0
"['.', ""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' ofReggia ( 1985 ) , the `` diagnosis from first principles '' of#AUTHOR_TAG , `` explainability '' ofPoole ( 1988 ) , and the subset principle ofBerwick ( 1986 ) ."", '']","['kin.', ""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' ofReggia ( 1985 ) , the `` diagnosis from first principles '' of#AUTHOR_TAG , `` explainability '' ofPoole ( 1988 ) , and the subset principle ofBerwick ( 1986 ) ."", '']","['.', ""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' ofReggia ( 1985 ) , the `` diagnosis from first principles '' of#AUTHOR_TAG , `` explainability '' ofPoole ( 1988 ) , and the subset principle ofBerwick ( 1986 ) ."", '']","['', '""domain circumscription"" (cf.', 'Etherington and Mercer 1987), and their kin.', ""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' ofReggia ( 1985 ) , the `` diagnosis from first principles '' of#AUTHOR_TAG , `` explainability '' ofPoole ( 1988 ) , and the subset principle ofBerwick ( 1986 ) ."", '']",1
"['in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX Levesque 1984 ;#AUTHOR_TAG ; Patel-Schneider 1985 ) .', 'Levesque 1984;Frisch 1987;Patel-Schneider 1985).', '']","['in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX Levesque 1984 ;#AUTHOR_TAG ; Patel-Schneider 1985 ) .', 'Levesque 1984;Frisch 1987;Patel-Schneider 1985).', '']","['in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX Levesque 1984 ;#AUTHOR_TAG ; Patel-Schneider 1985 ) .', 'Levesque 1984;Frisch 1987;Patel-Schneider 1985).', '']","['in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX Levesque 1984 ;#AUTHOR_TAG ; Patel-Schneider 1985 ) .', 'Levesque 1984;Frisch 1987;Patel-Schneider 1985).', ""But we won't pursue this topic further here""]",1
"['explicitly stated otherwise, we assume that formulas are expressed in a certain (formal) language L without equality; the extension L(=) of L is going to be used only in Section 5 for dealing with noun phrase references.', ""This means that natural language expressions such as `` A is B , '' `` A is the same as B , '' etc. are not directly represented by logical equality ; similarly , `` not '' is often not treated as logical negation ; cfXXX#AUTHOR_TAG ."", 'Hintikka (1985)']","['explicitly stated otherwise, we assume that formulas are expressed in a certain (formal) language L without equality; the extension L(=) of L is going to be used only in Section 5 for dealing with noun phrase references.', ""This means that natural language expressions such as `` A is B , '' `` A is the same as B , '' etc. are not directly represented by logical equality ; similarly , `` not '' is often not treated as logical negation ; cfXXX#AUTHOR_TAG ."", 'Hintikka (1985)']","['explicitly stated otherwise, we assume that formulas are expressed in a certain (formal) language L without equality; the extension L(=) of L is going to be used only in Section 5 for dealing with noun phrase references.', ""This means that natural language expressions such as `` A is B , '' `` A is the same as B , '' etc. are not directly represented by logical equality ; similarly , `` not '' is often not treated as logical negation ; cfXXX#AUTHOR_TAG ."", 'Hintikka (1985)']","['explicitly stated otherwise, we assume that formulas are expressed in a certain (formal) language L without equality; the extension L(=) of L is going to be used only in Section 5 for dealing with noun phrase references.', ""This means that natural language expressions such as `` A is B , '' `` A is the same as B , '' etc. are not directly represented by logical equality ; similarly , `` not '' is often not treated as logical negation ; cfXXX#AUTHOR_TAG ."", 'Hintikka (1985)']",1
"[': The notions of strong provability and strong R + M-abduction can be in- troduced by replacing ""there exists"" by ""all"" in the above definitions (cf.#AUTHOR_TAG b ).', 'We will have, however, no need for ""strong"" notions in this paper']","[': The notions of strong provability and strong R + M-abduction can be in- troduced by replacing ""there exists"" by ""all"" in the above definitions (cf.#AUTHOR_TAG b ).', 'We will have, however, no need for ""strong"" notions in this paper.', '']","[': The notions of strong provability and strong R + M-abduction can be in- troduced by replacing ""there exists"" by ""all"" in the above definitions (cf.#AUTHOR_TAG b ).', 'We will have, however, no need for ""strong"" notions in this paper.', '']","[': The notions of strong provability and strong R + M-abduction can be in- troduced by replacing ""there exists"" by ""all"" in the above definitions (cf.#AUTHOR_TAG b ).', 'We will have, however, no need for ""strong"" notions in this paper.', '']",1
"['ing selectional restrictions ( semantic feature information ,#AUTHOR_TAG ) does not solve the problem , because isolated features offer only part of the background knowledge necessary for reference disambiguation .', '']","['ing selectional restrictions ( semantic feature information ,#AUTHOR_TAG ) does not solve the problem , because isolated features offer only part of the background knowledge necessary for reference disambiguation .', '']","['ing selectional restrictions ( semantic feature information ,#AUTHOR_TAG ) does not solve the problem , because isolated features offer only part of the background knowledge necessary for reference disambiguation .', '']","['ing selectional restrictions ( semantic feature information ,#AUTHOR_TAG ) does not solve the problem , because isolated features offer only part of the background knowledge necessary for reference disambiguation .', '']",0
"['', 'The reader may consult recent papers on this subject ( e.g.#AUTHOR_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like .', '']","['', 'The reader may consult recent papers on this subject ( e.g.#AUTHOR_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like .', '']","['', 'The reader may consult recent papers on this subject ( e.g.#AUTHOR_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like .', '']","['', 'The reader may consult recent papers on this subject ( e.g.#AUTHOR_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like .', '']",0
"['it is the ""highest"" path, fint is the most plausible (relative to R) interpretation of the words that appear in the sentence.', 'Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX#AUTHOR_TAG a , 1987b ) .', 'Zadrozny 1987aZadrozny , 1987 b.', '']","['it is the ""highest"" path, fint is the most plausible (relative to R) interpretation of the words that appear in the sentence.', 'Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX#AUTHOR_TAG a , 1987b ) .', 'Zadrozny 1987aZadrozny , 1987 b.', '']","['it is the ""highest"" path, fint is the most plausible (relative to R) interpretation of the words that appear in the sentence.', 'Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX#AUTHOR_TAG a , 1987b ) .', 'Zadrozny 1987aZadrozny , 1987 b.', '']","['it is the ""highest"" path, fint is the most plausible (relative to R) interpretation of the words that appear in the sentence.', 'Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX#AUTHOR_TAG a , 1987b ) .', 'Zadrozny 1987aZadrozny , 1987 b.', '']",0
"['there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 ,#AUTHOR_TAG , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure']","['there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 ,#AUTHOR_TAG , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure']","['there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 ,#AUTHOR_TAG , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', '']","['there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 ,#AUTHOR_TAG , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', '']",1
"['it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX#AUTHOR_TAG ) .', 'Mycielski 1981']","['it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX#AUTHOR_TAG ) .', 'Mycielski 1981']","['it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX#AUTHOR_TAG ) .', 'Mycielski 1981']","['', 'As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX#AUTHOR_TAG ) .', 'Mycielski 1981']",0
"['have shown elsewhere( #AUTHOR_TAG ; Zadrozny 1987a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory']","['have shown elsewhere( #AUTHOR_TAG ; Zadrozny 1987a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory']","['have shown elsewhere( #AUTHOR_TAG ; Zadrozny 1987a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory']","['have shown elsewhere( #AUTHOR_TAG ; Zadrozny 1987a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory']",2
"['necessity of this kind of merging of arguments has been recognized before :#AUTHOR_TAG call it abductive unification/matching ,Hobbs ( 1978  , 1979 ) refers to such operations using the terms knitting or petty conversational implicature']","['necessity of this kind of merging of arguments has been recognized before :#AUTHOR_TAG call it abductive unification/matching ,Hobbs ( 1978  , 1979 ) refers to such operations using the terms knitting or petty conversational implicature']","['necessity of this kind of merging of arguments has been recognized before :#AUTHOR_TAG call it abductive unification/matching ,Hobbs ( 1978  , 1979 ) refers to such operations using the terms knitting or petty conversational implicature']","['necessity of this kind of merging of arguments has been recognized before :#AUTHOR_TAG call it abductive unification/matching ,Hobbs ( 1978  , 1979 ) refers to such operations using the terms knitting or petty conversational implicature']",0
"['', 'The logical notation of#AUTHOR_TAG is more sophisticated , and may be considered another possibility .', '']","['', 'The logical notation of#AUTHOR_TAG is more sophisticated , and may be considered another possibility .', '']","['', 'The logical notation of#AUTHOR_TAG is more sophisticated , and may be considered another possibility .', '']","['', 'The logical notation of#AUTHOR_TAG is more sophisticated , and may be considered another possibility .', '']",1
"['', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) ,#AUTHOR_TAG ,Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .', '']","['', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) ,#AUTHOR_TAG ,Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .', '']","['', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) ,#AUTHOR_TAG ,Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .', '']","['', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) ,#AUTHOR_TAG ,Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .', '']",1
"['`` the linguistic-logical notions of consequent and presupposition#AUTHOR_TAG :112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger']","['`` the linguistic-logical notions of consequent and presupposition#AUTHOR_TAG :112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger']","['`` the linguistic-logical notions of consequent and presupposition#AUTHOR_TAG :112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger']","['', 'He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition#AUTHOR_TAG :112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) .', 'These chunks are sometimes called ""episodes,"" and sometimes ""paragraphs.""', '']",0
"['', 'We investigate here only the ""grounding"" in logical theories.', 'However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX#AUTHOR_TAG ) .', '']","['', 'We investigate here only the ""grounding"" in logical theories.', 'However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX#AUTHOR_TAG ) .', '']","['', 'We investigate here only the ""grounding"" in logical theories.', 'However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX#AUTHOR_TAG ) .', 'Woods 1987']","['', 'We investigate here only the ""grounding"" in logical theories.', 'However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX#AUTHOR_TAG ) .', 'Woods 1987']",0
"['.', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", ""#AUTHOR_TAG , p. 14 ) writes `` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys . ''"", 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', '']","['possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", ""#AUTHOR_TAG , p. 14 ) writes `` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys . ''"", 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', '']","['', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", ""#AUTHOR_TAG , p. 14 ) writes `` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys . ''"", 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', '']","['', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", ""#AUTHOR_TAG , p. 14 ) writes `` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys . ''"", 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', '']",1
"['example of psycholinguistically oriented research work can be found in#AUTHOR_TAG .', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholinguistic studies that confirm the validity of paragraph units can be found inBlack and Bower (1979) andHaberlandt et al. (1980)']","['example of psycholinguistically oriented research work can be found in#AUTHOR_TAG .', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholinguistic studies that confirm the validity of paragraph units can be found inBlack and Bower (1979) andHaberlandt et al. (1980)']","['example of psycholinguistically oriented research work can be found in#AUTHOR_TAG .', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholinguistic studies that confirm the validity of paragraph units can be found inBlack and Bower (1979) andHaberlandt et al. (1980)']","['example of psycholinguistically oriented research work can be found in#AUTHOR_TAG .', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholinguistic studies that confirm the validity of paragraph units can be found inBlack and Bower (1979) andHaberlandt et al. (1980)']",0
"['and thesauri.', '(Other reference works could be treated as additional sources of world knowledge.)', 'This type of consultation uses existing natural language texts as a referential level for processing purposes.', 'It is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit.', ""#AUTHOR_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , ' which could be invoked to generate inferences ... '' ."", '']","['and thesauri.', '(Other reference works could be treated as additional sources of world knowledge.)', 'This type of consultation uses existing natural language texts as a referential level for processing purposes.', 'It is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit.', ""#AUTHOR_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , ' which could be invoked to generate inferences ... '' ."", '']","['and thesauri.', '(Other reference works could be treated as additional sources of world knowledge.)', 'This type of consultation uses existing natural language texts as a referential level for processing purposes.', 'It is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit.', ""#AUTHOR_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , ' which could be invoked to generate inferences ... '' ."", '']","['demonstrates that information needed to identify and resolve anaphoric references can be found, to an interesting extent, in standard dictionaries and thesauri.', '(Other reference works could be treated as additional sources of world knowledge.)', 'This type of consultation uses existing natural language texts as a referential level for processing purposes.', 'It is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit.', ""#AUTHOR_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , ' which could be invoked to generate inferences ... '' ."", '']",0
"['6.1.1 Was the Use of a Gricean Maxim Necessary?', 'Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only ""petty conversational implicature""( #AUTHOR_TAG ) , or the metarules of Section 5.2?', 'It seems to us that the answer is no']","['6.1.1 Was the Use of a Gricean Maxim Necessary?', 'Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only ""petty conversational implicature""( #AUTHOR_TAG ) , or the metarules of Section 5.2?', 'It seems to us that the answer is no']","['6.1.1 Was the Use of a Gricean Maxim Necessary?', 'Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only ""petty conversational implicature""( #AUTHOR_TAG ) , or the metarules of Section 5.2?', 'It seems to us that the answer is no']","['6.1.1 Was the Use of a Gricean Maxim Necessary?', 'Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only ""petty conversational implicature""( #AUTHOR_TAG ) , or the metarules of Section 5.2?', 'It seems to us that the answer is no']",0
"['adopt the three-level semantics as a formal tool for the analysis of paragraphs.', 'This semantics was constructed( #AUTHOR_TAG a , 1987b ) as a formal framework for default and commonsense reasoning .', 'It should not come as a surprise that we can now use this apparatus for text/discourse analysis']","['adopt the three-level semantics as a formal tool for the analysis of paragraphs.', 'This semantics was constructed( #AUTHOR_TAG a , 1987b ) as a formal framework for default and commonsense reasoning .', 'It should not come as a surprise that we can now use this apparatus for text/discourse analysis;']","['adopt the three-level semantics as a formal tool for the analysis of paragraphs.', 'This semantics was constructed( #AUTHOR_TAG a , 1987b ) as a formal framework for default and commonsense reasoning .', 'It should not come as a surprise that we can now use this apparatus for text/discourse analysis;']","['adopt the three-level semantics as a formal tool for the analysis of paragraphs.', 'This semantics was constructed( #AUTHOR_TAG a , 1987b ) as a formal framework for default and commonsense reasoning .', '']",5
"['the notion of R + M-abduction is spiritually related to the ""abductive inference"" ofReggia (1985) , the ""diagnosis from first principles"" ofReiter (1987) , ""explainability"" ofPoole (1988) , and the subset principle of#AUTHOR_TAG .', '']","['the notion of R + M-abduction is spiritually related to the ""abductive inference"" ofReggia (1985) , the ""diagnosis from first principles"" ofReiter (1987) , ""explainability"" ofPoole (1988) , and the subset principle of#AUTHOR_TAG .', '']","['the notion of R + M-abduction is spiritually related to the ""abductive inference"" ofReggia (1985) , the ""diagnosis from first principles"" ofReiter (1987) , ""explainability"" ofPoole (1988) , and the subset principle of#AUTHOR_TAG .', '']","['', 'Similarly, the notion of R + M-abduction is spiritually related to the ""abductive inference"" ofReggia (1985) , the ""diagnosis from first principles"" ofReiter (1987) , ""explainability"" ofPoole (1988) , and the subset principle of#AUTHOR_TAG .', '']",1
"['there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , Longacre 1979 ,#AUTHOR_TAG ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', '']","['there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , Longacre 1979 ,#AUTHOR_TAG ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', '']","['there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , Longacre 1979 ,#AUTHOR_TAG ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', '']","['there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , Longacre 1979 ,#AUTHOR_TAG ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', '']",1
"['have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus ( Grosz 1977 , 1978 ;#AUTHOR_TAG ) or quantifier scoping ( Webber 1983 ) must play a role , too .', '']","['have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus ( Grosz 1977 , 1978 ;#AUTHOR_TAG ) or quantifier scoping ( Webber 1983 ) must play a role , too .', '']","['have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus ( Grosz 1977 , 1978 ;#AUTHOR_TAG ) or quantifier scoping ( Webber 1983 ) must play a role , too .', '']","['have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus ( Grosz 1977 , 1978 ;#AUTHOR_TAG ) or quantifier scoping ( Webber 1983 ) must play a role , too .', 'Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']",0
"['referential structures we are going to use are collections of logical theories, but the concept of reference is more general.', 'Some of the intuitions we associate with this notion have been very well expressed by#AUTHOR_TAG , pp. 7-8 ) : ... Semantics is constrained by our models of ourselves and our worlds .', 'We have models of']","['referential structures we are going to use are collections of logical theories, but the concept of reference is more general.', 'Some of the intuitions we associate with this notion have been very well expressed by#AUTHOR_TAG , pp. 7-8 ) : ... Semantics is constrained by our models of ourselves and our worlds .', 'We have models of']","['referential structures we are going to use are collections of logical theories, but the concept of reference is more general.', 'Some of the intuitions we associate with this notion have been very well expressed by#AUTHOR_TAG , pp. 7-8 ) : ... Semantics is constrained by our models of ourselves and our worlds .', 'We have models of']","['referential structures we are going to use are collections of logical theories, but the concept of reference is more general.', 'Some of the intuitions we associate with this notion have been very well expressed by#AUTHOR_TAG , pp. 7-8 ) : ... Semantics is constrained by our models of ourselves and our worlds .', '']",0
['version of marker passing ( Hirst 1987 ;#AUTHOR_TAG )'],['version of marker passing ( Hirst 1987 ;#AUTHOR_TAG )'],['version of marker passing ( Hirst 1987 ;#AUTHOR_TAG )'],"['idea of using preferences among theories is new, hence it was described in more detail.', ""`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( Hirst 1987 ;#AUTHOR_TAG ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text""]",1
"['in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX#AUTHOR_TAG ; Frisch 1987 ; Patel-Schneider 1985 ) .', 'Levesque 1984;Frisch 1987;Patel-Schneider 1985).', '']","['in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX#AUTHOR_TAG ; Frisch 1987 ; Patel-Schneider 1985 ) .', 'Levesque 1984;Frisch 1987;Patel-Schneider 1985).', '']","['in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX#AUTHOR_TAG ; Frisch 1987 ; Patel-Schneider 1985 ) .', 'Levesque 1984;Frisch 1987;Patel-Schneider 1985).', '']","['in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX#AUTHOR_TAG ; Frisch 1987 ; Patel-Schneider 1985 ) .', 'Levesque 1984;Frisch 1987;Patel-Schneider 1985).', ""But we won't pursue this topic further here""]",1
"['have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus( #AUTHOR_TAG , 1978 ; Sidner 1983 ) or quantifier scoping ( Webber 1983 ) must play a role , too']","['have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus( #AUTHOR_TAG , 1978 ; Sidner 1983 ) or quantifier scoping ( Webber 1983 ) must play a role , too']","['have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus( #AUTHOR_TAG , 1978 ; Sidner 1983 ) or quantifier scoping ( Webber 1983 ) must play a role , too .', '']","['have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus( #AUTHOR_TAG , 1978 ; Sidner 1983 ) or quantifier scoping ( Webber 1983 ) must play a role , too .', '']",0
"['', 'This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose( #AUTHOR_TAG , p. 329 )']","['', 'This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose( #AUTHOR_TAG , p. 329 )']","['', 'This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose( #AUTHOR_TAG , p. 329 )']","['', 'This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose( #AUTHOR_TAG , p. 329 )']",4
"['can also hope for some fine-tuning of the notion of topic, which would prevent many offensive examples.', 'This approach is taken in computational syntactic grammars (e.g.', '#AUTHOR_TAG ) ; the number of unlikely parses is severely reduced whenever possible , but no attempt is made to define only the so-called grammatical strings of a language']","['can also hope for some fine-tuning of the notion of topic, which would prevent many offensive examples.', 'This approach is taken in computational syntactic grammars (e.g.', '#AUTHOR_TAG ) ; the number of unlikely parses is severely reduced whenever possible , but no attempt is made to define only the so-called grammatical strings of a language']","['can also hope for some fine-tuning of the notion of topic, which would prevent many offensive examples.', 'This approach is taken in computational syntactic grammars (e.g.', '#AUTHOR_TAG ) ; the number of unlikely parses is severely reduced whenever possible , but no attempt is made to define only the so-called grammatical strings of a language']","['can also hope for some fine-tuning of the notion of topic, which would prevent many offensive examples.', 'This approach is taken in computational syntactic grammars (e.g.', '#AUTHOR_TAG ) ; the number of unlikely parses is severely reduced whenever possible , but no attempt is made to define only the so-called grammatical strings of a language']",0
"['idea of using preferences among theories is new, hence it was described in more detail.', ""`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing( #AUTHOR_TAG ; Charniak 1983""]","['idea of using preferences among theories is new, hence it was described in more detail.', ""`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing( #AUTHOR_TAG ; Charniak 1983""]","['idea of using preferences among theories is new, hence it was described in more detail.', ""`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing( #AUTHOR_TAG ; Charniak 1983""]",['( #AUTHOR_TAG'],1
"['a rule.', 'We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by#AUTHOR_TAG']","['a rule.', 'We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by#AUTHOR_TAG']","['such a rule.', 'We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by#AUTHOR_TAG']","['', 'Rather, we stress the possibility that one can axiomatize and productively use such a rule.', 'We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by#AUTHOR_TAG']",2
"['', 'Opposition ( called ""adversative"" or ""contrary-to-expectation"" by#AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 )']","['', 'Opposition ( called ""adversative"" or ""contrary-to-expectation"" by#AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 )']","[', ""but"" does not behave quite like the other two--semantically, ""but"" signals a contradiction, and in this role it seems to have three subfunctions: . .', 'Opposition ( called ""adversative"" or ""contrary-to-expectation"" by#AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 )']","['ives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '""And,"" ""or,"" and ""but"" are the three main coordinating connectives in English.', 'However, ""but"" does not behave quite like the other two--semantically, ""but"" signals a contradiction, and in this role it seems to have three subfunctions: . .', 'Opposition ( called ""adversative"" or ""contrary-to-expectation"" by#AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 )']",0
"['there are other discussions of the paragraph as a central element of discourse (e.g.', '#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure']","['there are other discussions of the paragraph as a central element of discourse (e.g.', '#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure']","['there are other discussions of the paragraph as a central element of discourse (e.g.', '#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', '']","['there are other discussions of the paragraph as a central element of discourse (e.g.', '#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', '']",1
"['', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) ,Jackendoff ( 1983 ) ,#AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .', '']","['', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) ,Jackendoff ( 1983 ) ,#AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .', '']","['', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) ,Jackendoff ( 1983 ) ,#AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .', '']","['', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) ,Jackendoff ( 1983 ) ,#AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .', '']",1
"['verbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in#AUTHOR_TAG andHaberlandt et al. ( 1980 )']","['adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in#AUTHOR_TAG andHaberlandt et al. ( 1980 )']","['(2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in#AUTHOR_TAG andHaberlandt et al. ( 1980 )']","['', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in#AUTHOR_TAG andHaberlandt et al. ( 1980 )']",0
"['have shown elsewhere ( Jensen and Binot 1988 ;#AUTHOR_TAG a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory']","['have shown elsewhere ( Jensen and Binot 1988 ;#AUTHOR_TAG a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory']","['have shown elsewhere ( Jensen and Binot 1988 ;#AUTHOR_TAG a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory']","['have shown elsewhere ( Jensen and Binot 1988 ;#AUTHOR_TAG a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory']",2
"['', 'The partial theories pick up from the referential level the most obvious or the most important information about a formula.', 'This immediate information may be insufficient to decide the truth of certain predicates.', 'It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX#AUTHOR_TAG b ) .', 'Zadrozny 1987b']","['', 'The partial theories pick up from the referential level the most obvious or the most important information about a formula.', 'This immediate information may be insufficient to decide the truth of certain predicates.', 'It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX#AUTHOR_TAG b ) .', 'Zadrozny 1987b']","['', 'The partial theories pick up from the referential level the most obvious or the most important information about a formula.', 'This immediate information may be insufficient to decide the truth of certain predicates.', 'It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX#AUTHOR_TAG b ) .', 'Zadrozny 1987b']","['', 'The partial theories pick up from the referential level the most obvious or the most important information about a formula.', 'This immediate information may be insufficient to decide the truth of certain predicates.', 'It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX#AUTHOR_TAG b ) .', 'Zadrozny 1987b']",1
"['ing selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""Later ,#AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base""]","['ing selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""Later ,#AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base""]","['ing selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""Later ,#AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base""]","['ing selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""Later ,#AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base""]",0
"['necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching ,#AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature']","['necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching ,#AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature']","['necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching ,#AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature']","['necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching ,#AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature']",0
"['we may need also a quotation operator ;#AUTHOR_TAG describes how first order logic can be augmented with such an operator .', '']","['we may need also a quotation operator ;#AUTHOR_TAG describes how first order logic can be augmented with such an operator .', '']","['we may need also a quotation operator ;#AUTHOR_TAG describes how first order logic can be augmented with such an operator .', '']","['', 'Since sentences can refer to events described by other sentences , we may need also a quotation operator ;#AUTHOR_TAG describes how first order logic can be augmented with such an operator .', '']",0
"['paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings.', ""For instance ,#AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's""]","['paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings.', ""For instance ,#AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's""]","['paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings.', ""For instance ,#AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's""]","['paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings.', ""For instance ,#AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''"", 'The pairing of these two sentences may be said to create a small paragraph.', '']",4
"['', 'Representative systems are described inBoisen et al. ( 1989 ) ,De Mattia and Giachin ( 1989 ) ,Niedermair ( 1989 ) ,#AUTHOR_TAG , andYoung ( 1989 )']","['', 'Representative systems are described inBoisen et al. ( 1989 ) ,De Mattia and Giachin ( 1989 ) ,Niedermair ( 1989 ) ,#AUTHOR_TAG , andYoung ( 1989 )']","['the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described inBoisen et al. ( 1989 ) ,De Mattia and Giachin ( 1989 ) ,Niedermair ( 1989 ) ,#AUTHOR_TAG , andYoung ( 1989 )']","['', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described inBoisen et al. ( 1989 ) ,De Mattia and Giachin ( 1989 ) ,Niedermair ( 1989 ) ,#AUTHOR_TAG , andYoung ( 1989 )']",0
['formula for the test set perplexity( #AUTHOR_TAG ) is :1'],['formula for the test set perplexity( #AUTHOR_TAG ) is :1'],['formula for the test set perplexity( #AUTHOR_TAG ) is :1'],['formula for the test set perplexity( #AUTHOR_TAG ) is :1'],0
"['', 'The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end( #AUTHOR_TAG ) .', 'The']","['', 'The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end( #AUTHOR_TAG ) .', 'The']","['', 'The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end( #AUTHOR_TAG ) .', '']","['', 'The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end( #AUTHOR_TAG ) .', '']",5
"['subject.', 'Subsequent processing by the natural language and response generation components was done automatically by the computer( #AUTHOR_TAG )']","['subject.', 'Subsequent processing by the natural language and response generation components was done automatically by the computer( #AUTHOR_TAG )']","['the subject.', 'Subsequent processing by the natural language and response generation components was done automatically by the computer( #AUTHOR_TAG )']","['', 'Instead, an experimenter in a separate room typed in the utterances as spoken by the subject.', 'Subsequent processing by the natural language and response generation components was done automatically by the computer( #AUTHOR_TAG )']",5
[''],[''],[''],[''],0
"['', 'This approach resembles the work byGrishman et al. ( 1986 ) and#AUTHOR_TAG on selectional restrictions .', 'The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', '']","['', 'This approach resembles the work byGrishman et al. ( 1986 ) and#AUTHOR_TAG on selectional restrictions .', 'The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', '']","['', 'This approach resembles the work byGrishman et al. ( 1986 ) and#AUTHOR_TAG on selectional restrictions .', 'The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', '']","['', 'This approach resembles the work byGrishman et al. ( 1986 ) and#AUTHOR_TAG on selectional restrictions .', 'The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', '']",1
"['', 'The search algorithm is the standard Viterbi search( #AUTHOR_TAG ) , except that the match involves a network-to-network alignment problem rather than sequence-to-sequence']","['', 'The search algorithm is the standard Viterbi search( #AUTHOR_TAG ) , except that the match involves a network-to-network alignment problem rather than sequence-to-sequence']","['', 'The search algorithm is the standard Viterbi search( #AUTHOR_TAG ) , except that the match involves a network-to-network alignment problem rather than sequence-to-sequence']","['', 'The search algorithm is the standard Viterbi search( #AUTHOR_TAG ) , except that the match involves a network-to-network alignment problem rather than sequence-to-sequence']",5
['antic filters can also be used to prevent multiple versions of the same case frame( #AUTHOR_TAG ) showing up as complements'],['antic filters can also be used to prevent multiple versions of the same case frame( #AUTHOR_TAG ) showing up as complements'],"['antic filters can also be used to prevent multiple versions of the same case frame( #AUTHOR_TAG ) showing up as complements .', '']","['antic filters can also be used to prevent multiple versions of the same case frame( #AUTHOR_TAG ) showing up as complements .', '']",5
"['in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in#AUTHOR_TAG ,De Mattia and Giachin ( 1989 ) ,Niedermair ( 1989 ) ,Niemann ( 1990 ) , andYoung ( 1989 )']","['in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in#AUTHOR_TAG ,De Mattia and Giachin ( 1989 ) ,Niedermair ( 1989 ) ,Niemann ( 1990 ) , andYoung ( 1989 )']","['in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in#AUTHOR_TAG ,De Mattia and Giachin ( 1989 ) ,Niedermair ( 1989 ) ,Niemann ( 1990 ) , andYoung ( 1989 )']","['', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in#AUTHOR_TAG ,De Mattia and Giachin ( 1989 ) ,Niedermair ( 1989 ) ,Niemann ( 1990 ) , andYoung ( 1989 )']",0
"[', each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance( #AUTHOR_TAG ) ."", '']","['search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance( #AUTHOR_TAG ) ."", '']","[', each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance( #AUTHOR_TAG ) ."", '']","['as in the loosely coupled system the parser acts as a filter only on completed candidate solutions (Zue et al. 1991), the tightly coupled system allows the parser to discard partial theories that have no way of continuing.', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance( #AUTHOR_TAG ) ."", '']",5
"['date, four distinct domain-specific versions of TINA have been implemented.', 'The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database( #AUTHOR_TAG ) .', '']","['date, four distinct domain-specific versions of TINA have been implemented.', 'The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database( #AUTHOR_TAG ) .', '']","['date, four distinct domain-specific versions of TINA have been implemented.', 'The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database( #AUTHOR_TAG ) .', '']","['date, four distinct domain-specific versions of TINA have been implemented.', 'The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database( #AUTHOR_TAG ) .', '']",5
"['actly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'However , the method we are currently using in the ATIS domain( #AUTHOR_TAG ) represents our most promising approach to this problem .', 'We have decided to limit semantic frame types to']","['actly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'However , the method we are currently using in the ATIS domain( #AUTHOR_TAG ) represents our most promising approach to this problem .', 'We have decided to limit semantic frame types to']","['actly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'However , the method we are currently using in the ATIS domain( #AUTHOR_TAG ) represents our most promising approach to this problem .', 'We have decided to limit semantic frame types to']","['actly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'However , the method we are currently using in the ATIS domain( #AUTHOR_TAG ) represents our most promising approach to this problem .', 'We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects).', '']",3
"['vicinity of MIT and Harvard University.', 'The second one, ATIS( #AUTHOR_TAG et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights.', '']","['vicinity of MIT and Harvard University.', 'The second one, ATIS( #AUTHOR_TAG et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights.', '']","['the vicinity of MIT and Harvard University.', 'The second one, ATIS( #AUTHOR_TAG et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights.', '']","['', 'The second one, ATIS( #AUTHOR_TAG et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights.', '']",5
"['', 'This approach resembles the work by#AUTHOR_TAG andHirschman et al. ( 1975 ) on selectional restrictions .', 'The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', '']","['', 'This approach resembles the work by#AUTHOR_TAG andHirschman et al. ( 1975 ) on selectional restrictions .', 'The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', '']","['', 'This approach resembles the work by#AUTHOR_TAG andHirschman et al. ( 1975 ) on selectional restrictions .', 'The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', '']","['', 'This approach resembles the work by#AUTHOR_TAG andHirschman et al. ( 1975 ) on selectional restrictions .', 'The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', '']",1
"['as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an N-gram back-off model( #AUTHOR_TAG )']","['as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an N-gram back-off model( #AUTHOR_TAG )']","['as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an N-gram back-off model( #AUTHOR_TAG )']","['', 'Unless it is desired to intentionally filter these out as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an N-gram back-off model( #AUTHOR_TAG )']",0
"['', 'To a first approximation , a CURRENT-FOCUS reaches only nodes that are c-commanded( #AUTHOR_TAG ) by its generator .', '']","['', 'To a first approximation , a CURRENT-FOCUS reaches only nodes that are c-commanded( #AUTHOR_TAG ) by its generator .', '']","['', 'To a first approximation , a CURRENT-FOCUS reaches only nodes that are c-commanded( #AUTHOR_TAG ) by its generator .', '']","['', 'To a first approximation , a CURRENT-FOCUS reaches only nodes that are c-commanded( #AUTHOR_TAG ) by its generator .', '']",0
"['_currently have two application domains that can carry on a spoken dialog with a user.', 'One , the VOYAGER domain( #AUTHOR_TAG ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University .', '']","['_currently have two application domains that can carry on a spoken dialog with a user.', 'One , the VOYAGER domain( #AUTHOR_TAG ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University .', '']","['_currently have two application domains that can carry on a spoken dialog with a user.', 'One , the VOYAGER domain( #AUTHOR_TAG ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University .', '']","['_currently have two application domains that can carry on a spoken dialog with a user.', 'One , the VOYAGER domain( #AUTHOR_TAG ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University .', '']",5
"[""example used to illustrate the power of ATNs( #AUTHOR_TAG ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator""]","[""example used to illustrate the power of ATNs( #AUTHOR_TAG ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator""]","[""example used to illustrate the power of ATNs( #AUTHOR_TAG ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator ."", '']","[""example used to illustrate the power of ATNs( #AUTHOR_TAG ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator ."", '']",1
"['.', 'Representative systems are described inBoisen et al. ( 1989 ) ,De Mattia and Giachin ( 1989 ) ,#AUTHOR_TAG ,Niemann ( 1990 ) , andYoung ( 1989 )']","['the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described inBoisen et al. ( 1989 ) ,De Mattia and Giachin ( 1989 ) ,#AUTHOR_TAG ,Niemann ( 1990 ) , andYoung ( 1989 )']","['the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described inBoisen et al. ( 1989 ) ,De Mattia and Giachin ( 1989 ) ,#AUTHOR_TAG ,Niemann ( 1990 ) , andYoung ( 1989 )']","['', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described inBoisen et al. ( 1989 ) ,De Mattia and Giachin ( 1989 ) ,#AUTHOR_TAG ,Niemann ( 1990 ) , andYoung ( 1989 )']",0
"['', 'The second version ( RM ) concerns the Resource Management task( #AUTHOR_TAG ) that has been popular within the DARPA community in recent years .', 'The third version (']","['', 'The second version ( RM ) concerns the Resource Management task( #AUTHOR_TAG ) that has been popular within the DARPA community in recent years .', 'The third version']","['', 'The second version ( RM ) concerns the Resource Management task( #AUTHOR_TAG ) that has been popular within the DARPA community in recent years .', 'The third version (']","['', 'The second version ( RM ) concerns the Resource Management task( #AUTHOR_TAG ) that has been popular within the DARPA community in recent years .', '']",5
"['', 'For the A * algorithm( #AUTHOR_TAG ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion .', '']","['', 'For the A * algorithm( #AUTHOR_TAG ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion .', '']","['', 'For the A * algorithm( #AUTHOR_TAG ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion .', '']","['modification of this scheme is necessary when the input stream is not deterministic.', 'For the A * algorithm( #AUTHOR_TAG ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion .', 'Unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied.', '']",5
"['', 'The recognizer for these systems is the SUMMIT system( #AUTHOR_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing .', 'The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'The search algorithm is the standard Viterbi search (Viterbi 196']","['', 'The recognizer for these systems is the SUMMIT system( #AUTHOR_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing .', 'The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'The search algorithm is the standard Viterbi search (Viterbi 1967),']","['', 'The recognizer for these systems is the SUMMIT system( #AUTHOR_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing .', 'The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'The search algorithm is the standard Viterbi search (Viterbi 196']","['', 'The recognizer for these systems is the SUMMIT system( #AUTHOR_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing .', 'The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', '']",5
"['', 'The gap mechanism resembles the Hold register idea of ATNs( #AUTHOR_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( Bresnan 1982 , p. 235 ff']","['', 'The gap mechanism resembles the Hold register idea of ATNs( #AUTHOR_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( Bresnan 1982 , p. 235 ff']","['', 'The gap mechanism resembles the Hold register idea of ATNs( #AUTHOR_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( Bresnan 1982 , p. 235 ff . ),']","['', 'The gap mechanism resembles the Hold register idea of ATNs( #AUTHOR_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( Bresnan 1982 , p. 235 ff . ), but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent nodes']",1
"['the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described inBoisen et al. ( 1989 ) ,De Mattia and Giachin ( 1989 ) ,Niedermair ( 1989 ) ,Niemann ( 1990 ) , and#AUTHOR_TAG']","['the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described inBoisen et al. ( 1989 ) ,De Mattia and Giachin ( 1989 ) ,Niedermair ( 1989 ) ,Niemann ( 1990 ) , and#AUTHOR_TAG']","['the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described inBoisen et al. ( 1989 ) ,De Mattia and Giachin ( 1989 ) ,Niedermair ( 1989 ) ,Niemann ( 1990 ) , and#AUTHOR_TAG']","['', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described inBoisen et al. ( 1989 ) ,De Mattia and Giachin ( 1989 ) ,Niedermair ( 1989 ) ,Niemann ( 1990 ) , and#AUTHOR_TAG']",0
"['', ""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart""]","['', ""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart""]","['', ""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm (""]","['', ""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) ."", '']",5
"['as in the loosely coupled system the parser acts as a filter only on completed candidate solutions( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', '']","['as in the loosely coupled system the parser acts as a filter only on completed candidate solutions( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', '']","['as in the loosely coupled system the parser acts as a filter only on completed candidate solutions( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', '']","['as in the loosely coupled system the parser acts as a filter only on completed candidate solutions( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', '']",5
"['thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'The formalization of DLRs provided by#AUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1 .', 'This description can then be given the standard set-theoretical interpretation ofKing (1989 , 199']","['thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'The formalization of DLRs provided by#AUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1 .', 'This description can then be given the standard set-theoretical interpretation ofKing (1989 , 1994']","['thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'The formalization of DLRs provided by#AUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1 .', 'This description can then be given the standard set-theoretical interpretation ofKing (1989 , 199']","['thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'The formalization of DLRs provided by#AUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1 .', 'This description can then be given the standard set-theoretical interpretation ofKing (1989 , 1994']",0
"['', 'The reader is referred to#AUTHOR_TAG for a more detailed discussion of our use of constraint propagation']","['', 'The reader is referred to#AUTHOR_TAG for a more detailed discussion of our use of constraint propagation']","['', 'The reader is referred to#AUTHOR_TAG for a more detailed discussion of our use of constraint propagation']","['', 'The reader is referred to#AUTHOR_TAG for a more detailed discussion of our use of constraint propagation']",0
"['common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( Copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules( #AUTHOR_TAG , 31 ) .', '']","['common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( Copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules( #AUTHOR_TAG , 31 ) .', '']","['common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( Copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules( #AUTHOR_TAG , 31 ) .', '']","['common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( Copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules( #AUTHOR_TAG , 31 ) .', '']",1
"['way these predicates interconnect is represented in Figure 19.', '27#AUTHOR_TAG argue that semi-productivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry .', '']","['way these predicates interconnect is represented in Figure 19.', '27#AUTHOR_TAG argue that semi-productivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry .', '']","['way these predicates interconnect is represented in Figure 19.', '27#AUTHOR_TAG argue that semi-productivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry .', '']","['way these predicates interconnect is represented in Figure 19.', '27#AUTHOR_TAG argue that semi-productivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry .', '']",0
"['9) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Extraction Lexical Rule( #AUTHOR_TAG ) to operate on those raised elements.', 'Also an analysis treating adjunct extraction']","['that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Extraction Lexical Rule( #AUTHOR_TAG ) to operate on those raised elements.', 'Also an analysis treating adjunct extraction']","['that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Extraction Lexical Rule( #AUTHOR_TAG ) to operate on those raised elements.', 'Also an analysis treating adjunct extraction']","['', '/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Extraction Lexical Rule( #AUTHOR_TAG ) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon']",0
"['the DLR formalization therefore requires all words feeding lexical rules to be grammatical with respect to the theory.', 'Since lexical rules are expressed in the theory just like any other part of the theory, they are represented in the same way, as unary immediate dominance schemata.', 'This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system( #AUTHOR_TAG ) .', '']","['the DLR formalization therefore requires all words feeding lexical rules to be grammatical with respect to the theory.', 'Since lexical rules are expressed in the theory just like any other part of the theory, they are represented in the same way, as unary immediate dominance schemata.', 'This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system( #AUTHOR_TAG ) .', '']","['the DLR formalization therefore requires all words feeding lexical rules to be grammatical with respect to the theory.', 'Since lexical rules are expressed in the theory just like any other part of the theory, they are represented in the same way, as unary immediate dominance schemata.', 'This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system( #AUTHOR_TAG ) .', '']","['', 'Contrary to the MLR setup, the DLR formalization therefore requires all words feeding lexical rules to be grammatical with respect to the theory.', 'Since lexical rules are expressed in the theory just like any other part of the theory, they are represented in the same way, as unary immediate dominance schemata.', 'This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system( #AUTHOR_TAG ) .', '']",0
"['This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body( #AUTHOR_TAG ) .', '']","['This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body( #AUTHOR_TAG ) .', '']","['9 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body( #AUTHOR_TAG ) .', '']","['', 'As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body( #AUTHOR_TAG ) .', '']",1
"['In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( Maxwell and Kaplan 1989 ; Eisele and Dorre 1990 ;#AUTHOR_TAG ) can be used to circumvent constraint propagation .', '']","['In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( Maxwell and Kaplan 1989 ; Eisele and Dorre 1990 ;#AUTHOR_TAG ) can be used to circumvent constraint propagation .', '']","['In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( Maxwell and Kaplan 1989 ; Eisele and Dorre 1990 ;#AUTHOR_TAG ) can be used to circumvent constraint propagation .', '']","['32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( Maxwell and Kaplan 1989 ; Eisele and Dorre 1990 ;#AUTHOR_TAG ) can be used to circumvent constraint propagation .', '']",0
"['2 In order to focus on the computational aspects of the covariation approach , in this paper we will not go into a discussion of the full lexical rule specification language introduced in#AUTHOR_TAG .', 'The reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 A']","['In order to focus on the computational aspects of the covariation approach , in this paper we will not go into a discussion of the full lexical rule specification language introduced in#AUTHOR_TAG .', 'The reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 A']","['2 In order to focus on the computational aspects of the covariation approach , in this paper we will not go into a discussion of the full lexical rule specification language introduced in#AUTHOR_TAG .', 'The reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 A']","['', '12 In order to focus on the computational aspects of the covariation approach , in this paper we will not go into a discussion of the full lexical rule specification language introduced in#AUTHOR_TAG .', 'The reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 A more detailed presentation can be found in Minnen (in preparation).', '']",0
"['including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).', '6 The Partial-VP Topicalization Lexical Rule proposed by#AUTHOR_TAG , 10 ) is a linguistic example .', 'The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement']","['including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).', '6 The Partial-VP Topicalization Lexical Rule proposed by#AUTHOR_TAG , 10 ) is a linguistic example .', 'The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement']","['a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).', '6 The Partial-VP Topicalization Lexical Rule proposed by#AUTHOR_TAG , 10 ) is a linguistic example .', 'The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement']","['', '5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).', '6 The Partial-VP Topicalization Lexical Rule proposed by#AUTHOR_TAG , 10 ) is a linguistic example .', 'The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement']",0
"['', 'To avoid confusion, we will only use the terminology introduced in the text.', '4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ;#AUTHOR_TAG ) .', '5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).', '6']","['', 'To avoid confusion, we will only use the terminology introduced in the text.', '4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ;#AUTHOR_TAG ) .', '5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).', '6']","['', 'To avoid confusion, we will only use the terminology introduced in the text.', '4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ;#AUTHOR_TAG ) .', '5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).', '']","['', 'To avoid confusion, we will only use the terminology introduced in the text.', '4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ;#AUTHOR_TAG ) .', '5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).', '']",0
"['a lexical rule.', 'Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up( #AUTHOR_TAG )']","['a lexical rule.', 'Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up( #AUTHOR_TAG )']","['a lexical rule.', 'Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up( #AUTHOR_TAG )']","['', 'Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up( #AUTHOR_TAG )']",0
"['', 'As shown in#AUTHOR_TAG this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries']","['', 'As shown in#AUTHOR_TAG this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries']","['', 'As shown in#AUTHOR_TAG this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries']","['', 'As shown in#AUTHOR_TAG this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries']",4
"['elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques( #AUTHOR_TAG ) .', '']","['elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques( #AUTHOR_TAG ) .', '']","['elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques( #AUTHOR_TAG ) .', '']","['elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques( #AUTHOR_TAG ) .', '']",5
"['information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993; Oliva 1994; Frank 1994;#AUTHOR_TAG ; Sanfilippo 1995).', 'The lex']","['information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993; Oliva 1994; Frank 1994;#AUTHOR_TAG ; Sanfilippo 1995).', 'The lexical']","['ical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993; Oliva 1994; Frank 1994;#AUTHOR_TAG ; Sanfilippo 1995).', 'The lexical entries are only partially']","['ical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993; Oliva 1994; Frank 1994;#AUTHOR_TAG ; Sanfilippo 1995).', '']",1
"['', 'A similar method is included in PATR-II( #AUTHOR_TAG ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; Emele 1994 ) .', '']","['', 'A similar method is included in PATR-II( #AUTHOR_TAG ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; Emele 1994 ) .', '']","['', 'A similar method is included in PATR-II( #AUTHOR_TAG ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; Emele 1994 ) .', '']","['', 'A similar method is included in PATR-II( #AUTHOR_TAG ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; Emele 1994 ) .', '']",1
"['be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ;#AUTHOR_TAG ) .', '']","['be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ;#AUTHOR_TAG ) .', '']","['can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ;#AUTHOR_TAG ) .', '']","['', 'A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ;#AUTHOR_TAG ) .', '']",1
"['the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; Calcagno 1995 ;#AUTHOR_TAG ) and the .', 'lexical rules (DLRs; Meurers 1995).', '']","['the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; Calcagno 1995 ;#AUTHOR_TAG ) and the .', 'lexical rules (DLRs; Meurers 1995).', '']","['the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; Calcagno 1995 ;#AUTHOR_TAG ) and the .', 'lexical rules (DLRs; Meurers 1995).', '']","['the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; Calcagno 1995 ;#AUTHOR_TAG ) and the .', 'lexical rules (DLRs; Meurers 1995).', '']",0
"['computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and#AUTHOR_TAG ; Gotz and Meurers 1997a ) .', '']","['computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and#AUTHOR_TAG ; Gotz and Meurers 1997a ) .', '']","['computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and#AUTHOR_TAG ; Gotz and Meurers 1997a ) .', '']","['computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and#AUTHOR_TAG ; Gotz and Meurers 1997a ) .', '']",5
"['the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ;#AUTHOR_TAG ; Calcagno and Pollard 1995 ) and the .', 'lexical rules (DLRs; Meurers 1995).', '']","['the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ;#AUTHOR_TAG ; Calcagno and Pollard 1995 ) and the .', 'lexical rules (DLRs; Meurers 1995).', '']","['the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ;#AUTHOR_TAG ; Calcagno and Pollard 1995 ) and the .', 'lexical rules (DLRs; Meurers 1995).', '']","['the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ;#AUTHOR_TAG ; Calcagno and Pollard 1995 ) and the .', 'lexical rules (DLRs; Meurers 1995).', '']",0
"['1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement( #AUTHOR_TAG ) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and']","['1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement( #AUTHOR_TAG ) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and']","['html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement( #AUTHOR_TAG ) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and']","['', '/b4home.html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement( #AUTHOR_TAG ) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon']",0
"['common computational treatment of lexical rules adopted , for example , in the ALE system( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time .', 'While this provides a front-end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation.', '']","['common computational treatment of lexical rules adopted , for example , in the ALE system( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time .', 'While this provides a front-end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation.', '']","['common computational treatment of lexical rules adopted , for example , in the ALE system( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time .', 'While this provides a front-end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation.', '']","['common computational treatment of lexical rules adopted , for example , in the ALE system( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time .', 'While this provides a front-end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation.', '']",1
"['logic that provides the formal architecture required byPollard and Sag ( 1994 ) was defined by#AUTHOR_TAG , 1994 ) .', 'The formal language of King allows the expression of grammatical principles using type assignments to refer to the type of an object and path equalities to require the (token) identity of objects']","['logic that provides the formal architecture required byPollard and Sag ( 1994 ) was defined by#AUTHOR_TAG , 1994 ) .', 'The formal language of King allows the expression of grammatical principles using type assignments to refer to the type of an object and path equalities to require the (token) identity of objects.', '']","['logic that provides the formal architecture required byPollard and Sag ( 1994 ) was defined by#AUTHOR_TAG , 1994 ) .', 'The formal language of King allows the expression of grammatical principles using type assignments to refer to the type of an object and path equalities to require the (token) identity of objects']","['logic that provides the formal architecture required byPollard and Sag ( 1994 ) was defined by#AUTHOR_TAG , 1994 ) .', 'The formal language of King allows the expression of grammatical principles using type assignments to refer to the type of an object and path equalities to require the (token) identity of objects.', '']",0
"['', 'The unfolding transformation is also referred to as partial execution , for example , by#AUTHOR_TAG .', '']","['', 'The unfolding transformation is also referred to as partial execution , for example , by#AUTHOR_TAG .', '']","['', 'The unfolding transformation is also referred to as partial execution , for example , by#AUTHOR_TAG .', '']","['elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( Tamaki and Sato 1984 ) .29', 'The unfolding transformation is also referred to as partial execution , for example , by#AUTHOR_TAG .', 'Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time.', '']",0
"['be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system( #AUTHOR_TAG ; Emele 1994 ) .', '']","['be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system( #AUTHOR_TAG ; Emele 1994 ) .', '']","['can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system( #AUTHOR_TAG ; Emele 1994 ) .', '']","['', 'A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system( #AUTHOR_TAG ; Emele 1994 ) .', '']",1
"['thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'The formalization of DLRs provided byMeurers (1995) defines a formal lexical rule specification language and provides a semantics for that language in two steps: A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1.', ""This description can then be given the standard set-theoretical interpretation of#AUTHOR_TAG , 1994 ) . '"", '11 10 Note']","['thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'The formalization of DLRs provided byMeurers (1995) defines a formal lexical rule specification language and provides a semantics for that language in two steps: A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1.', ""This description can then be given the standard set-theoretical interpretation of#AUTHOR_TAG , 1994 ) . '"", '11 10 Note']","['thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'The formalization of DLRs provided byMeurers (1995) defines a formal lexical rule specification language and provides a semantics for that language in two steps: A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1.', ""This description can then be given the standard set-theoretical interpretation of#AUTHOR_TAG , 1994 ) . '"", '11 10 Note']","['thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'The formalization of DLRs provided byMeurers (1995) defines a formal lexical rule specification language and provides a semantics for that language in two steps: A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1.', ""This description can then be given the standard set-theoretical interpretation of#AUTHOR_TAG , 1994 ) . '"", '']",0
"['ed as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ;#AUTHOR_TAG ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are']","['as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ;#AUTHOR_TAG ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are']","['ed as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ;#AUTHOR_TAG ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries']","['ical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ;#AUTHOR_TAG ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .', '']",1
['11#AUTHOR_TAG proposes to unify these two steps by including an update operator in the description language'],['11#AUTHOR_TAG proposes to unify these two steps by including an update operator in the description language'],['11#AUTHOR_TAG proposes to unify these two steps by including an update operator in the description language'],['11#AUTHOR_TAG proposes to unify these two steps by including an update operator in the description language'],0
"['32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints( #AUTHOR_TAG ; Eisele and Dorre 1990 ; Griffith 1996 ) can be used to circumvent constraint propagation .', 'Encoding the disj']","['32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints( #AUTHOR_TAG ; Eisele and Dorre 1990 ; Griffith 1996 ) can be used to circumvent constraint propagation .', 'Encoding the disjunctive']","['32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints( #AUTHOR_TAG ; Eisele and Dorre 1990 ; Griffith 1996 ) can be used to circumvent constraint propagation .', 'Encoding the disjunctive possibilities']","['32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints( #AUTHOR_TAG ; Eisele and Dorre 1990 ; Griffith 1996 ) can be used to circumvent constraint propagation .', '']",0
"['powerful mechanism of lexical rules( #AUTHOR_TAG ) has been used in many natural language processing systems .', 'In this section we briefly discuss some of the more prominent approaches and compare them with the treatment proposed in this paper']","['powerful mechanism of lexical rules( #AUTHOR_TAG ) has been used in many natural language processing systems .', 'In this section we briefly discuss some of the more prominent approaches and compare them with the treatment proposed in this paper']","['powerful mechanism of lexical rules( #AUTHOR_TAG ) has been used in many natural language processing systems .', 'In this section we briefly discuss some of the more prominent approaches and compare them with the treatment proposed in this paper']","['powerful mechanism of lexical rules( #AUTHOR_TAG ) has been used in many natural language processing systems .', 'In this section we briefly discuss some of the more prominent approaches and compare them with the treatment proposed in this paper']",0
"['', 'This idea of preserving properties can be considered an instance of the well-known frame problem in AT( #AUTHOR_TAG ) , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule .', 'Not having to']","['', 'This idea of preserving properties can be considered an instance of the well-known frame problem in AT( #AUTHOR_TAG ) , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule .', 'Not having to']","['', 'This idea of preserving properties can be considered an instance of the well-known frame problem in AT( #AUTHOR_TAG ) , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule .', 'Not having to']","['', 'This idea of preserving properties can be considered an instance of the well-known frame problem in AT( #AUTHOR_TAG ) , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule .', '']",1
"['', 'Using an accumulator passing technique( #AUTHOR_TAG ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules .', '']","['', 'Using an accumulator passing technique( #AUTHOR_TAG ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules .', '']","['', 'Using an accumulator passing technique( #AUTHOR_TAG ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules .', '']","['', 'Each transition in the automaton is translated into a definite relation in which the corresponding lexical rule predicate is called, and each final state is encoded by a unit clause.', 'Using an accumulator passing technique( #AUTHOR_TAG ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules .', '']",5
"['15#AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention .', 'We here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form (G6tz 1994).', 'Computationally, a subsumption test could equally well be used in our compiler']","['We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15#AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention .', 'We here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form (G6tz 1994).', 'Computationally, a subsumption test could equally well be used in our compiler']","['15#AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention .', 'We here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form (G6tz 1994).', 'Computationally, a subsumption test could equally well be used in our compiler']","['', '14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15#AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention .', 'We here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form (G6tz 1994).', 'Computationally, a subsumption test could equally well be used in our compiler']",0
"['ical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992;#AUTHOR_TAG ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only']","['ical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992;#AUTHOR_TAG ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only']","['ical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992;#AUTHOR_TAG ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially']","['ical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992;#AUTHOR_TAG ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .', '']",1
"['information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; Oliva 1994 ;#AUTHOR_TAG ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially']","['information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; Oliva 1994 ;#AUTHOR_TAG ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially']","['ical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; Oliva 1994 ;#AUTHOR_TAG ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially']","['ical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; Oliva 1994 ;#AUTHOR_TAG ; Opalka 1995 ; Sanfilippo 1995 ) .', '']",1
"['', 'To avoid confusion, we will only use the terminology introduced in the text.', '4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and#AUTHOR_TAG ; Gerdemann 1995 ) .', '5']","['', 'To avoid confusion, we will only use the terminology introduced in the text.', '4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and#AUTHOR_TAG ; Gerdemann 1995 ) .', '5']","['', 'To avoid confusion, we will only use the terminology introduced in the text.', '4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and#AUTHOR_TAG ; Gerdemann 1995 ) .', '']","['', 'To avoid confusion, we will only use the terminology introduced in the text.', '4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and#AUTHOR_TAG ; Gerdemann 1995 ) .', '']",0
"['9 ) that also use lexical rules such as the Complement Extraction Lexical Rule ( Pollard and Sag 1994 ) or the Complement Cliticization Lexical Rule( #AUTHOR_TAG ) to operate on those raised elements .', 'Also an analysis treating adjunct extraction via lexical']","[') that also use lexical rules such as the Complement Extraction Lexical Rule ( Pollard and Sag 1994 ) or the Complement Cliticization Lexical Rule( #AUTHOR_TAG ) to operate on those raised elements .', 'Also an analysis treating adjunct extraction via lexical']","[') that also use lexical rules such as the Complement Extraction Lexical Rule ( Pollard and Sag 1994 ) or the Complement Cliticization Lexical Rule( #AUTHOR_TAG ) to operate on those raised elements .', 'Also an analysis treating adjunct extraction via lexical rules']",['( #AUTHOR_TAG )'],0
"['A linguistic example based on the signature given by#AUTHOR_TAG would be a lexical rule deriving predicative signs from nonpredicative ones, i.e., changing the PRD value of substantive signs from - to +, much like the lexical rule for NPs given byPollard and Sag (1994, p. 360 , fn. 20).', '']","['16 A linguistic example based on the signature given by#AUTHOR_TAG would be a lexical rule deriving predicative signs from nonpredicative ones, i.e., changing the PRD value of substantive signs from - to +, much like the lexical rule for NPs given byPollard and Sag (1994, p. 360 , fn. 20).', '']","['16 A linguistic example based on the signature given by#AUTHOR_TAG would be a lexical rule deriving predicative signs from nonpredicative ones, i.e., changing the PRD value of substantive signs from - to +, much like the lexical rule for NPs given byPollard and Sag (1994, p. 360 , fn. 20).', '']","['16 A linguistic example based on the signature given by#AUTHOR_TAG would be a lexical rule deriving predicative signs from nonpredicative ones, i.e., changing the PRD value of substantive signs from - to +, much like the lexical rule for NPs given byPollard and Sag (1994, p. 360 , fn. 20).', '']",0
"['', 'The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and#AUTHOR_TAG , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program']","['', 'The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and#AUTHOR_TAG , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program']","['', 'The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and#AUTHOR_TAG , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program']","['', 'The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and#AUTHOR_TAG , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program']",2
"['ical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification( #AUTHOR_TAG ; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']","['ical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification( #AUTHOR_TAG ; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']","['ical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification( #AUTHOR_TAG ; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']","['ical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification( #AUTHOR_TAG ; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
"['information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ;#AUTHOR_TAG ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']","['information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ;#AUTHOR_TAG ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']","['ical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ;#AUTHOR_TAG ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']","['ical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ;#AUTHOR_TAG ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
"['This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule( #AUTHOR_TAG ) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules']","['This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule( #AUTHOR_TAG ) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules']","['html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule( #AUTHOR_TAG ) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules']","['', '/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule( #AUTHOR_TAG ) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon']",0
"['on the research results reported in#AUTHOR_TAG , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG .', '']","['on the research results reported in#AUTHOR_TAG , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG .', '']","['on the research results reported in#AUTHOR_TAG , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG .', '']","['on the research results reported in#AUTHOR_TAG , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG .', '']",4
"['common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .', '']","['common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .', '']","['common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .', '']","['common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .', '']",1
"['a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'In the above example, this would result in two lexical rules: one for words with tl as their c value and one for those with t2 as their c value.', 'In the latter case, we can also take care of transferring the value of z.', 'However , as discussed by#AUTHOR_TAG , creating several instances of lexical rules can be avoided .', '']","['a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'In the above example, this would result in two lexical rules: one for words with tl as their c value and one for those with t2 as their c value.', 'In the latter case, we can also take care of transferring the value of z.', 'However , as discussed by#AUTHOR_TAG , creating several instances of lexical rules can be avoided .', '']","['applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'In the above example, this would result in two lexical rules: one for words with tl as their c value and one for those with t2 as their c value.', 'In the latter case, we can also take care of transferring the value of z.', 'However , as discussed by#AUTHOR_TAG , creating several instances of lexical rules can be avoided .', '']","['ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'In the above example, this would result in two lexical rules: one for words with tl as their c value and one for those with t2 as their c value.', 'In the latter case, we can also take care of transferring the value of z.', 'However , as discussed by#AUTHOR_TAG , creating several instances of lexical rules can be avoided .', '']",4
"['example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented byPollard and Sag ( 1987  , 215 ) in terms of the setup of#AUTHOR_TAG , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 °']","['example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented byPollard and Sag ( 1987  , 215 ) in terms of the setup of#AUTHOR_TAG , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 °']","['example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented byPollard and Sag ( 1987  , 215 ) in terms of the setup of#AUTHOR_TAG , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 °']","['', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented byPollard and Sag ( 1987  , 215 ) in terms of the setup of#AUTHOR_TAG , ch .', '']",0
"['ical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ;#AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).', 'The lexical entries are']","['ical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ;#AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).', 'The lexical entries are']","['ical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ;#AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).', 'The lexical entries']","['ical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ;#AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).', '']",1
"[', for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by#AUTHOR_TAG , 215 ) in terms of the setup ofPollard and Sag ( 1994  , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form']","[', for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by#AUTHOR_TAG , 215 ) in terms of the setup ofPollard and Sag ( 1994  , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2']","[', for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by#AUTHOR_TAG , 215 ) in terms of the setup ofPollard and Sag ( 1994  , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2']","['', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by#AUTHOR_TAG , 215 ) in terms of the setup ofPollard and Sag ( 1994  , ch .', '']",1
"['setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ;']","['setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ;']","['the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ;']","['the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ;']",0
"['', '#AUTHOR_TAG present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge .', 'N']","['', '#AUTHOR_TAG present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge .', 'NER']","['', '#AUTHOR_TAG present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge .', 'N']","['', '#AUTHOR_TAG present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge .', '']",0
"['', 'For this mention-pair coreference model Ï\x86 ( u , v ) , we use the same set of features used in#AUTHOR_TAG']","['co-referred.', 'For this mention-pair coreference model Ï\x86 ( u , v ) , we use the same set of features used in#AUTHOR_TAG']","['', 'For this mention-pair coreference model Ï\x86 ( u , v ) , we use the same set of features used in#AUTHOR_TAG']","['', 'For this mention-pair coreference model Ï\x86 ( u , v ) , we use the same set of features used in#AUTHOR_TAG']",5
"['the mention heads are sufficient to support the decisions: ""they"" refers to ""companies"", and ""They"" refers to ""manufacturers"".', 'In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads( #AUTHOR_TAG )']","['the mention heads are sufficient to support the decisions: ""they"" refers to ""companies"", and ""They"" refers to ""manufacturers"".', 'In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads( #AUTHOR_TAG )']","['the mention heads are sufficient to support the decisions: ""they"" refers to ""companies"", and ""They"" refers to ""manufacturers"".', 'In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads( #AUTHOR_TAG )']","['', 'In both cases, the mention heads are sufficient to support the decisions: ""they"" refers to ""companies"", and ""They"" refers to ""manufacturers"".', 'In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads( #AUTHOR_TAG )']",0
"['', 'For Berkeley system , we use the reported results from#AUTHOR_TAG']","['', 'For Berkeley system , we use the reported results from#AUTHOR_TAG']","['', 'For Berkeley system , we use the reported results from#AUTHOR_TAG']","['', 'For Berkeley system , we use the reported results from#AUTHOR_TAG']",1
"['We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system( Lee et al. , 2011 ) , Berkeley system( #AUTHOR_TAG ) and HOTCoref system ( Bj Â¨ orkelund andKuhn , 2014 ) .', '']","['We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system( Lee et al. , 2011 ) , Berkeley system( #AUTHOR_TAG ) and HOTCoref system ( Bj Â¨ orkelund andKuhn , 2014 ) .', '']","['We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system( Lee et al. , 2011 ) , Berkeley system( #AUTHOR_TAG ) and HOTCoref system ( Bj Â¨ orkelund andKuhn , 2014 ) .', '']","['', 'Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system( Lee et al. , 2011 ) , Berkeley system( #AUTHOR_TAG ) and HOTCoref system ( Bj Â¨ orkelund andKuhn , 2014 ) .', '']",1
"['ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents( Culotta et al. , 2007 ; #AUTHOR_TAG ) .', '']","['ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents( Culotta et al. , 2007 ; #AUTHOR_TAG ) .', '']","['ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents( Culotta et al. , 2007 ; #AUTHOR_TAG ) .', 'The OntoNotes-5.0 dataset,']","['ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents( Culotta et al. , 2007 ; #AUTHOR_TAG ) .', '']",5
"['%.', 'We then use Illinois Chunker( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ Collins head rules( #AUTHOR_TAG ) to identify their heads .', 'When these extracted heads do not overlap with gold mention heads, we treat them as negative examples']","['', 'We then use Illinois Chunker( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ Collins head rules( #AUTHOR_TAG ) to identify their heads .', 'When these extracted heads do not overlap with gold mention heads, we treat them as negative examples']","['%.', 'We then use Illinois Chunker( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ Collins head rules( #AUTHOR_TAG ) to identify their heads .', 'When these extracted heads do not overlap with gold mention heads, we treat them as negative examples']","['', '3), we train on a set of candidates with precision larger than 50%.', 'We then use Illinois Chunker( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ Collins head rules( #AUTHOR_TAG ) to identify their heads .', 'When these extracted heads do not overlap with gold mention heads, we treat them as negative examples']",5
"['ention detection is rarely studied as a stand-alone research problem (Recasens et al. (2013) is one key exception).', 'Most coreference resolution work simply mentions it in passing as a module in the pipelined system( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ;  Bj Â¨ orkelund andKuhn , 2014 ) .', '']","['ention detection is rarely studied as a stand-alone research problem (Recasens et al. (2013) is one key exception).', 'Most coreference resolution work simply mentions it in passing as a module in the pipelined system( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ;  Bj Â¨ orkelund andKuhn , 2014 ) .', '']","['ention detection is rarely studied as a stand-alone research problem (Recasens et al. (2013) is one key exception).', 'Most coreference resolution work simply mentions it in passing as a module in the pipelined system( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ;  Bj Â¨ orkelund andKuhn , 2014 ) .', '']","['ention detection is rarely studied as a stand-alone research problem (Recasens et al. (2013) is one key exception).', 'Most coreference resolution work simply mentions it in passing as a module in the pipelined system( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ;  Bj Â¨ orkelund andKuhn , 2014 ) .', '']",0
"['on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in#AUTHOR_TAG .', 'The BILOUrepresentation suggests learning classifiers that identify the Beginning, Inside and Last tokens of multi-token chunks as well as Unit-length']","['on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in#AUTHOR_TAG .', 'The BILOUrepresentation suggests learning classifiers that identify the Beginning, Inside and Last tokens of multi-token chunks as well as Unit-length']","['on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in#AUTHOR_TAG .', 'The BILOUrepresentation suggests learning classifiers that identify the Beginning, Inside and Last tokens of multi-token chunks as well as Unit-']","['on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in#AUTHOR_TAG .', 'The BILOUrepresentation suggests learning classifiers that identify the Beginning, Inside and Last tokens of multi-token chunks as well as Unit-length chunks.', '']",4
"['OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task( #AUTHOR_TAG ) , contains 3,145 annotated documents .', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', '']","['OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task( #AUTHOR_TAG ) , contains 3,145 annotated documents .', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', '']","['', 'The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task( #AUTHOR_TAG ) , contains 3,145 annotated documents .', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', '']","['', 'The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task( #AUTHOR_TAG ) , contains 3,145 annotated documents .', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', '']",5
"['', 'Developed Systems Our developed system is built on the work by#AUTHOR_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 .', '']","['', 'Developed Systems Our developed system is built on the work by#AUTHOR_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 .', '']","['', 'Developed Systems Our developed system is built on the work by#AUTHOR_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 .', '']","['', 'Developed Systems Our developed system is built on the work by#AUTHOR_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 .', '']",5
"['', 'In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in#AUTHOR_TAG in our experiments']","['', 'In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in#AUTHOR_TAG in our experiments']","['', 'In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in#AUTHOR_TAG in our experiments']","['', 'In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in#AUTHOR_TAG in our experiments']",5
"['ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents( #AUTHOR_TAG ; Bengtson and Roth , 2008 ) .', '']","['ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents( #AUTHOR_TAG ; Bengtson and Roth , 2008 ) .', '']","['ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents( #AUTHOR_TAG ; Bengtson and Roth , 2008 ) .', 'The OntoNotes-5.0 dataset,']","['ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents( #AUTHOR_TAG ; Bengtson and Roth , 2008 ) .', '']",5
"['details can be found in#AUTHOR_TAG et al. (2013).', 'The difference here is that we also consider the validity of mention heads using �(u),�(m']","['details can be found in#AUTHOR_TAG et al. (2013).', 'The difference here is that we also consider the validity of mention heads using �(u),�(m']","['details can be found in#AUTHOR_TAG et al. (2013).', 'The difference here is that we also consider the validity of mention heads using �(u),�(m']","['details can be found in#AUTHOR_TAG et al. (2013).', 'The difference here is that we also consider the validity of mention heads using �(u),�(m']",0
"['recent works suggest studying coreference jointly with other tasks.', 'Lee et al. ( 2012 ) model entity coreference and event coreference jointly ;#AUTHOR_TAG consider joint coreference and entity-linking .', '']","['recent works suggest studying coreference jointly with other tasks.', 'Lee et al. ( 2012 ) model entity coreference and event coreference jointly ;#AUTHOR_TAG consider joint coreference and entity-linking .', '']","['recent works suggest studying coreference jointly with other tasks.', 'Lee et al. ( 2012 ) model entity coreference and event coreference jointly ;#AUTHOR_TAG consider joint coreference and entity-linking .', '']","['recent works suggest studying coreference jointly with other tasks.', 'Lee et al. ( 2012 ) model entity coreference and event coreference jointly ;#AUTHOR_TAG consider joint coreference and entity-linking .', '']",0
"[', we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules( #AUTHOR_TAG ) with gold constituency parsing information and gold named entity information .', '']","[', we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules( #AUTHOR_TAG ) with gold constituency parsing information and gold named entity information .', '']","[', we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules( #AUTHOR_TAG ) with gold constituency parsing information and gold named entity information .', '']","['', 'Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules( #AUTHOR_TAG ) with gold constituency parsing information and gold named entity information .', '']",5
"['section describes our joint coreference resolution and mention head detection framework.', 'Our work is inspired by the latent left-linking model in#AUTHOR_TAG and the ILP formulation fromChang et al. ( 2011 ) .', '']","['section describes our joint coreference resolution and mention head detection framework.', 'Our work is inspired by the latent left-linking model in#AUTHOR_TAG and the ILP formulation fromChang et al. ( 2011 ) .', '']","['section describes our joint coreference resolution and mention head detection framework.', 'Our work is inspired by the latent left-linking model in#AUTHOR_TAG and the ILP formulation fromChang et al. ( 2011 ) .', '']","['section describes our joint coreference resolution and mention head detection framework.', 'Our work is inspired by the latent left-linking model in#AUTHOR_TAG and the ILP formulation fromChang et al. ( 2011 ) .', '']",5
"['present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0( #AUTHOR_TAG ) .', '(Hovy et al., 2006) .', '']","['present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0( #AUTHOR_TAG ) .', '(Hovy et al., 2006) .', '']","['present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0( #AUTHOR_TAG ) .', '(Hovy et al., 2006) .', '']","['present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0( #AUTHOR_TAG ) .', '(Hovy et al., 2006) .', '']",5
"['we extend the category generator of our previous work, which we will call P CAT .', 'We can define PCAT using a probabilistic grammar( #AUTHOR_TAG ) .', 'The grammar may first generate a start or end category ( S , E ) with probability p se or a special tokendeletion category ( D ; explained in §5) with probability p del , or a standard CCG category C']","['we extend the category generator of our previous work, which we will call P CAT .', 'We can define PCAT using a probabilistic grammar( #AUTHOR_TAG ) .', 'The grammar may first generate a start or end category ( S , E ) with probability p se or a special tokendeletion category ( D ; explained in §5) with probability p del , or a standard CCG category C']","['we extend the category generator of our previous work, which we will call P CAT .', 'We can define PCAT using a probabilistic grammar( #AUTHOR_TAG ) .', 'The grammar may first generate a start or end category ( S , E ) with probability p se or a special tokendeletion category ( D ; explained in §5) with probability p del , or a standard CCG category C']","['', 'We can define PCAT using a probabilistic grammar( #AUTHOR_TAG ) .', 'The grammar may first generate a start or end category ( S , E ) with probability p se or a special tokendeletion category ( D ; explained in §5) with probability p del , or a standard CCG category C']",0
"[""'s CCM is an unlabeled bracketing model that generates the span of part-of-speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non-constituent ) ."", 'They found that modeling constituent context aids in parser learning because it is able to capture the observation that the same contexts tend to appear repeatedly in a corpus, even with different constituents.', '']","[""'s CCM is an unlabeled bracketing model that generates the span of part-of-speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non-constituent ) ."", 'They found that modeling constituent context aids in parser learning because it is able to capture the observation that the same contexts tend to appear repeatedly in a corpus, even with different constituents.', '']","[""'s CCM is an unlabeled bracketing model that generates the span of part-of-speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non-constituent ) ."", 'They found that modeling constituent context aids in parser learning because it is able to capture the observation that the same contexts tend to appear repeatedly in a corpus, even with different constituents.', '']","[""'s CCM is an unlabeled bracketing model that generates the span of part-of-speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non-constituent ) ."", 'They found that modeling constituent context aids in parser learning because it is able to capture the observation that the same contexts tend to appear repeatedly in a corpus, even with different constituents.', '']",0
"['', 'In order to estimate the parameters of our model , we develop a blocked sampler based on that of#AUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities .', '']","['', 'In order to estimate the parameters of our model , we develop a blocked sampler based on that of#AUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities .', '']","['', 'In order to estimate the parameters of our model , we develop a blocked sampler based on that of#AUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities .', '']","['', 'In order to estimate the parameters of our model , we develop a blocked sampler based on that of#AUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities .', '']",5
"['SCM extends.', 'We evaluated on the English CCGBank( Hockenmaier and Steedman , 2007 ) , which is a transformation of the Penn Treebank( #AUTHOR_TAG ) ; the']","['SCM extends.', 'We evaluated on the English CCGBank( Hockenmaier and Steedman , 2007 ) , which is a transformation of the Penn Treebank( #AUTHOR_TAG ) ; the']","['SCM extends.', 'We evaluated on the English CCGBank( Hockenmaier and Steedman , 2007 ) , which is a transformation of the Penn Treebank( #AUTHOR_TAG ) ;']",['( #AUTHOR_TAG )'],5
"[""right-side context of a non-terminal category -- the probability of generating a category to the right of the current constituent 's category -- corresponds directly to the category transitions used for the HMM supertagger of#AUTHOR_TAG ."", ""Thus, the right-side context prior mean θ RCTX-0 t can be biased in exactly the same way as the HMM supertagger's transitions: toward context supertags that connect to the constituent label""]","[""right-side context of a non-terminal category -- the probability of generating a category to the right of the current constituent 's category -- corresponds directly to the category transitions used for the HMM supertagger of#AUTHOR_TAG ."", ""Thus, the right-side context prior mean θ RCTX-0 t can be biased in exactly the same way as the HMM supertagger's transitions: toward context supertags that connect to the constituent label""]","[""right-side context of a non-terminal category -- the probability of generating a category to the right of the current constituent 's category -- corresponds directly to the category transitions used for the HMM supertagger of#AUTHOR_TAG ."", ""Thus, the right-side context prior mean θ RCTX-0 t can be biased in exactly the same way as the HMM supertagger's transitions: toward context supertags that connect to the constituent label""]","[""right-side context of a non-terminal category -- the probability of generating a category to the right of the current constituent 's category -- corresponds directly to the category transitions used for the HMM supertagger of#AUTHOR_TAG ."", ""Thus, the right-side context prior mean θ RCTX-0 t can be biased in exactly the same way as the HMM supertagger's transitions: toward context supertags that connect to the constituent label""]",1
"['a test set.', 'We use the same splits as#AUTHOR_TAG .', 'Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\\X)/X rather than introducing special conjunction rules.', '']","['a test set.', 'We use the same splits as#AUTHOR_TAG .', 'Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\\X)/X rather than introducing special conjunction rules.', '']","['a test set.', 'We use the same splits as#AUTHOR_TAG .', 'Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\\X)/X rather than introducing special conjunction rules.', '']","['corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set.', 'We use the same splits as#AUTHOR_TAG .', 'Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\\X)/X rather than introducing special conjunction rules.', '']",5
"['', 'This is similar to the ""deletion"" strategy employed by#AUTHOR_TAG , but we do it directly in the grammar .', 'We add un']","['', 'This is similar to the ""deletion"" strategy employed by#AUTHOR_TAG , but we do it directly in the grammar .', 'We add unary']","['', 'This is similar to the ""deletion"" strategy employed by#AUTHOR_TAG , but we do it directly in the grammar .', 'We add un']","['', 'This is similar to the ""deletion"" strategy employed by#AUTHOR_TAG , but we do it directly in the grammar .', 'We add unary rules of the form D →u for every potential supertag u in the tree.', '']",1
"['', 'We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger( #AUTHOR_TAG )']","['', 'We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger( #AUTHOR_TAG )']","['', 'We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger( #AUTHOR_TAG )']","['', 'We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger( #AUTHOR_TAG )']",2
"['employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus( #AUTHOR_TAG ; Garrette et al. , 2015 ) .']","['employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus( #AUTHOR_TAG ; Garrette et al. , 2015 ) .4', 'This procedure attempts to automatically estimate the frequency of each word/tag combination by dividing the number of raw-corpus occurrences of each word in the dictionary evenly across all of its associated tags.', 'These']","['employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus( #AUTHOR_TAG ; Garrette et al. , 2015 ) .4', 'This procedure attempts to automatically estimate the frequency of each word/tag combination by dividing the number of raw-corpus occurrences of each word in the dictionary evenly across all of its associated tags.', 'These counts are then combined with estimates of the �openness� of each tag']","['employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus( #AUTHOR_TAG ; Garrette et al. , 2015 ) .4', 'This procedure attempts to automatically estimate the frequency of each word/tag combination by dividing the number of raw-corpus occurrences of each word in the dictionary evenly across all of its associated tags.', '']",5
"['CCM, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid.', 'Since we are not generating from the model , this does not introduce difficulties( #AUTHOR_TAG )']","['CCM, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid.', 'Since we are not generating from the model , this does not introduce difficulties( #AUTHOR_TAG )']","['CCM, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid.', 'Since we are not generating from the model , this does not introduce difficulties( #AUTHOR_TAG )']","['CCM, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid.', 'Since we are not generating from the model , this does not introduce difficulties( #AUTHOR_TAG )']",4
"['.', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X â\x86\x92 X X of#AUTHOR_TAG']","['binary grammar rules.', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X â\x86\x92 X X of#AUTHOR_TAG']","['binary grammar rules.', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X â\x86\x92 X X of#AUTHOR_TAG']","['', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X â\x86\x92 X X of#AUTHOR_TAG']",5
"['sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed byGoodman ( 1998 ) and used by#AUTHOR_TAG that samples entire parse trees .', '']","['sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed byGoodman ( 1998 ) and used by#AUTHOR_TAG that samples entire parse trees .', '']","['sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed byGoodman ( 1998 ) and used by#AUTHOR_TAG that samples entire parse trees .', '']","['sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed byGoodman ( 1998 ) and used by#AUTHOR_TAG that samples entire parse trees .', '']",5
"['Gibbs sampling to find an approximate solution.', 'Our strategy is based on the approach presented by#AUTHOR_TAG .', 'At a high level, we alternate between resampling model parameters (_ROOT, _BIN, _ ,_ ,_,_ ,']","['Gibbs sampling to find an approximate solution.', 'Our strategy is based on the approach presented by#AUTHOR_TAG .', 'At a high level, we alternate between resampling model parameters (_ROOT, _BIN, _ ,_ ,_,_ ,_']","['Gibbs sampling to find an approximate solution.', 'Our strategy is based on the approach presented by#AUTHOR_TAG .', 'At a high level, we alternate between resampling model parameters (_ROOT, _BIN, _ ,_ ,_,_ ,']","['', 'Our strategy is based on the approach presented by#AUTHOR_TAG .', 'At a high level, we alternate between resampling model parameters (_ROOT, _BIN, _ ,_ ,_,_ ,_ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences.', '']",5
"['important example is the constituentcontext model ( CCM ) of#AUTHOR_TAG , which was specifically designed to capture the linguistic observation made byRadford ( 1988 ) that there are regularities to the contexts in which constituents appear']","['important example is the constituentcontext model ( CCM ) of#AUTHOR_TAG , which was specifically designed to capture the linguistic observation made byRadford ( 1988 ) that there are regularities to the contexts in which constituents appear']","['important example is the constituentcontext model ( CCM ) of#AUTHOR_TAG , which was specifically designed to capture the linguistic observation made byRadford ( 1988 ) that there are regularities to the contexts in which constituents appear .', 'This phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts.', '']","['important example is the constituentcontext model ( CCM ) of#AUTHOR_TAG , which was specifically designed to capture the linguistic observation made byRadford ( 1988 ) that there are regularities to the contexts in which constituents appear .', 'This phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts.', 'For example, the part-of-speech (POS) sequence ADJ NOUN frequently occurs between the tags DET and VERB.', 'This DET-VERB context also frequently applies to the single-word sequence NOUN and to ADJ ADJ NOUN.', '']",0
"['', 'We follow#AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .', '']","['', 'We follow#AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .', '']","['', 'We follow#AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .', '']","['', 'We follow#AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .', '']",5
"['.', 'Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by#AUTHOR_TAG .', 'The motivation for the features was to capture declaratively decisions made by the randomized SPG.', '']","['', 'Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by#AUTHOR_TAG .', 'The motivation for the features was to capture declaratively decisions made by the randomized SPG.', '']","['', 'Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by#AUTHOR_TAG .', 'The motivation for the features was to capture declaratively decisions made by the randomized SPG.', '']","['', 'Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by#AUTHOR_TAG .', 'The motivation for the features was to capture declaratively decisions made by the randomized SPG.', '']",1
"['vious work in sentence planning in the natural language generation ( NLG ) community uses hand-written rules to approximate the distribution of linguistic phenomena in a corpus ( see( #AUTHOR_TAG ) for a recent example with further references ) .', 'This approach is difficult to scale due to the nonrobustness of rules and unexpected interactions(Hovy and Wanner, 1996) , and it is difficult to develop new applications quickly']","['vious work in sentence planning in the natural language generation ( NLG ) community uses hand-written rules to approximate the distribution of linguistic phenomena in a corpus ( see( #AUTHOR_TAG ) for a recent example with further references ) .', 'This approach is difficult to scale due to the nonrobustness of rules and unexpected interactions(Hovy and Wanner, 1996) , and it is difficult to develop new applications quickly.', '']","['vious work in sentence planning in the natural language generation ( NLG ) community uses hand-written rules to approximate the distribution of linguistic phenomena in a corpus ( see( #AUTHOR_TAG ) for a recent example with further references ) .', 'This approach is difficult to scale due to the nonrobustness of rules and unexpected interactions(Hovy and Wanner, 1996) , and it is difficult to develop new applications quickly']","['vious work in sentence planning in the natural language generation ( NLG ) community uses hand-written rules to approximate the distribution of linguistic phenomena in a corpus ( see( #AUTHOR_TAG ) for a recent example with further references ) .', 'This approach is difficult to scale due to the nonrobustness of rules and unexpected interactions(Hovy and Wanner, 1996) , and it is difficult to develop new applications quickly.', '']",0
"['Our primary contribution is', 'a method for training the SPR. The SPR uses rules automatically learned from training data , using techniques similar to( #AUTHOR_TAG ; Freund et al. , 1998 )']","['Our primary contribution is', 'a method for training the SPR. The SPR uses rules automatically learned from training data , using techniques similar to( #AUTHOR_TAG ; Freund et al. , 1998 )']","['Our primary contribution is', 'a method for training the SPR. The SPR uses rules automatically learned from training data , using techniques similar to( #AUTHOR_TAG ; Freund et al. , 1998 )']","['', 'a method for training the SPR. The SPR uses rules automatically learned from training data , using techniques similar to( #AUTHOR_TAG ; Freund et al. , 1998 )']",1
"['', 'These operations are not domain-specific and are similar to those of previous aggregation components( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 ) , although the various MERGE operations are , to our knowledge , novel in this form']","['', 'These operations are not domain-specific and are similar to those of previous aggregation components( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 ) , although the various MERGE operations are , to our knowledge , novel in this form']","['', 'These operations are not domain-specific and are similar to those of previous aggregation components( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 ) , although the various MERGE operations are , to our knowledge , novel in this form']","['', 'These operations are not domain-specific and are similar to those of previous aggregation components( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 ) , although the various MERGE operations are , to our knowledge , novel in this form']",0
"['sp-tree is inspired by(Lavoie and Rambow, 1998) .', 'The representations used byDanlos ( 2000 ) ,Gardent and Webber ( 1998 ) , or#AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes']","['sp-tree is inspired by(Lavoie and Rambow, 1998) .', 'The representations used byDanlos ( 2000 ) ,Gardent and Webber ( 1998 ) , or#AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes']","['is inspired by(Lavoie and Rambow, 1998) .', 'The representations used byDanlos ( 2000 ) ,Gardent and Webber ( 1998 ) , or#AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .', '']","['', 'The representations used byDanlos ( 2000 ) ,Gardent and Webber ( 1998 ) , or#AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .', '']",0
"['2The algorithm was implemented by the the authors , following the description in#AUTHOR_TAG']","['2The algorithm was implemented by the the authors , following the description in#AUTHOR_TAG']","['2The algorithm was implemented by the the authors , following the description in#AUTHOR_TAG']","['2The algorithm was implemented by the the authors , following the description in#AUTHOR_TAG']",5
"['paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation-based learner , and the ICA system( #AUTHOR_TAG )']","['paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation-based learner , and the ICA system( #AUTHOR_TAG )']","['paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation-based learner , and the ICA system( #AUTHOR_TAG )']","['paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation-based learner , and the ICA system( #AUTHOR_TAG )']",1
"['data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by#AUTHOR_TAG']","['data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by#AUTHOR_TAG']","['data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by#AUTHOR_TAG']","['data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by#AUTHOR_TAG']",5
"[""\x80¢ The regular TBL , as described in section 2 ; â\x80¢ An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ; â\x80¢ The FastTBL algorithm ; â\x80¢ The ICA algorithm( #AUTHOR_TAG )""]","[""\x80¢ The regular TBL , as described in section 2 ; â\x80¢ An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ; â\x80¢ The FastTBL algorithm ; â\x80¢ The ICA algorithm( #AUTHOR_TAG )""]","[""\x80¢ The regular TBL , as described in section 2 ; â\x80¢ An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ; â\x80¢ The FastTBL algorithm ; â\x80¢ The ICA algorithm( #AUTHOR_TAG )""]","[""\x80¢ The regular TBL , as described in section 2 ; â\x80¢ An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ; â\x80¢ The FastTBL algorithm ; â\x80¢ The ICA algorithm( #AUTHOR_TAG )""]",1
['ICA system( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance'],['ICA system( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance'],['ICA system( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance'],['ICA system( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance'],0
"['the tree-cut technique described above , our previous work( #AUTHOR_TAG ) extracted systematic polysemy from WordNet .', '']","['the tree-cut technique described above , our previous work( #AUTHOR_TAG ) extracted systematic polysemy from WordNet .', '']","['the tree-cut technique described above , our previous work( #AUTHOR_TAG ) extracted systematic polysemy from WordNet .', '']","['the tree-cut technique described above , our previous work( #AUTHOR_TAG ) extracted systematic polysemy from WordNet .', '']",2
"['', 'In our previous work( #AUTHOR_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability .', '']","['', 'In our previous work( #AUTHOR_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability .', '']","['', 'In our previous work( #AUTHOR_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability .', '']","['', 'In our previous work( #AUTHOR_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability .', 'In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy.', '']",2
"['order to obtain semantic representations of each word , we apply our previous strategy( #AUTHOR_TAG ) .', '']","['order to obtain semantic representations of each word , we apply our previous strategy( #AUTHOR_TAG ) .', '']","['order to obtain semantic representations of each word , we apply our previous strategy( #AUTHOR_TAG ) .', '']","['order to obtain semantic representations of each word , we apply our previous strategy( #AUTHOR_TAG ) .', '']",2
"['', 'Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011 a ;#AUTHOR_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges( Schuller et al. , 2010 ; Schuller et al. , 2009 b ) and made freely']","['', 'Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011 a ;#AUTHOR_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges( Schuller et al. , 2010 ; Schuller et al. , 2009 b ) and made freely']","['', 'Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011 a ;#AUTHOR_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges( Schuller et al. , 2010 ; Schuller et al. , 2009 b ) and made freely']","['', 'Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011 a ;#AUTHOR_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges( Schuller et al. , 2010 ; Schuller et al. , 2009 b ) and made freely available in the openSMILE Toolkit( Florian et al. , 2010 ) .', '']",2
"['ite state transducers , which can be learned from bilingual corpora , have been proposed for automatic translation( Amengual et al. , 2000 ) , as have been bilingual stochastic grammars( #AUTHOR_TAG )']","['ite state transducers , which can be learned from bilingual corpora , have been proposed for automatic translation( Amengual et al. , 2000 ) , as have been bilingual stochastic grammars( #AUTHOR_TAG )']","['ite state transducers , which can be learned from bilingual corpora , have been proposed for automatic translation( Amengual et al. , 2000 ) , as have been bilingual stochastic grammars( #AUTHOR_TAG )']","['ite state transducers , which can be learned from bilingual corpora , have been proposed for automatic translation( Amengual et al. , 2000 ) , as have been bilingual stochastic grammars( #AUTHOR_TAG )']",0
['also shows the structural identity to bilingual grammars as used in( #AUTHOR_TAG )'],['also shows the structural identity to bilingual grammars as used in( #AUTHOR_TAG )'],['also shows the structural identity to bilingual grammars as used in( #AUTHOR_TAG )'],['also shows the structural identity to bilingual grammars as used in( #AUTHOR_TAG )'],5
['can be shown( #AUTHOR_TAG ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events'],['can be shown( #AUTHOR_TAG ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events'],['can be shown( #AUTHOR_TAG ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events'],['can be shown( #AUTHOR_TAG ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events'],4
['describe an efficient algorithm for accomplishing this in which approximations to Pst ( TIS ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t'],['describe an efficient algorithm for accomplishing this in which approximations to Pst ( TIS ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t'],['describe an efficient algorithm for accomplishing this in which approximations to Pst ( TIS ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t'],['describe an efficient algorithm for accomplishing this in which approximations to Pst ( TIS ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t'],0
['statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling( #AUTHOR_TAG )'],['statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling( #AUTHOR_TAG )'],['statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling( #AUTHOR_TAG )'],['statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling( #AUTHOR_TAG )'],5
"['( #AUTHOR_TAG ) show , lexical information improves on NP and VP chunking as well']","['( #AUTHOR_TAG ) show , lexical information improves on NP and VP chunking as well']","['( #AUTHOR_TAG ) show , lexical information improves on NP and VP chunking as well']","['( #AUTHOR_TAG ) show , lexical information improves on NP and VP chunking as well']",3
"['system was trained on the Penn Treebank( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by#AUTHOR_TAG ,Collins ( 1997 ) , andRatnaparkhi ( 1997 ) , and became a common testbed']","['system was trained on the Penn Treebank( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by#AUTHOR_TAG ,Collins ( 1997 ) , andRatnaparkhi ( 1997 ) , and became a common testbed']","['system was trained on the Penn Treebank( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by#AUTHOR_TAG ,Collins ( 1997 ) , andRatnaparkhi ( 1997 ) , and became a common testbed']","['system was trained on the Penn Treebank( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by#AUTHOR_TAG ,Collins ( 1997 ) , andRatnaparkhi ( 1997 ) , and became a common testbed']",1
"['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. ,Bod ( 1992 ) ,Magerman ( 1995 ) ,#AUTHOR_TAG ,Ratnaparkhi ( 1997 ) , andSekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. ,Bod ( 1992 ) ,Magerman ( 1995 ) ,#AUTHOR_TAG ,Ratnaparkhi ( 1997 ) , andSekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. ,Bod ( 1992 ) ,Magerman ( 1995 ) ,#AUTHOR_TAG ,Ratnaparkhi ( 1997 ) , andSekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. ,Bod ( 1992 ) ,Magerman ( 1995 ) ,#AUTHOR_TAG ,Ratnaparkhi ( 1997 ) , andSekine ( 1998 ) )']",0
"['is not aimed at handling dependencies , which require heavy use of lexical information( #AUTHOR_TAG , for PP attachment )']","['is not aimed at handling dependencies , which require heavy use of lexical information( #AUTHOR_TAG , for PP attachment )']","['is not aimed at handling dependencies , which require heavy use of lexical information( #AUTHOR_TAG , for PP attachment )']","['is not aimed at handling dependencies , which require heavy use of lexical information( #AUTHOR_TAG , for PP attachment )']",1
['approach for partial parsing was presented by#AUTHOR_TAG'],['approach for partial parsing was presented by#AUTHOR_TAG'],['approach for partial parsing was presented by#AUTHOR_TAG'],['approach for partial parsing was presented by#AUTHOR_TAG'],0
"['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. ,Bod ( 1992 ) ,Magerman ( 1995 ) ,Collins ( 1997 ) ,#AUTHOR_TAG , andSekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. ,Bod ( 1992 ) ,Magerman ( 1995 ) ,Collins ( 1997 ) ,#AUTHOR_TAG , andSekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. ,Bod ( 1992 ) ,Magerman ( 1995 ) ,Collins ( 1997 ) ,#AUTHOR_TAG , andSekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. ,Bod ( 1992 ) ,Magerman ( 1995 ) ,Collins ( 1997 ) ,#AUTHOR_TAG , andSekine ( 1998 ) )']",0
"['results are lower than those of full parsers , e.g. ,#AUTHOR_TAG as might be expected since much less structural data , and no lexical data are being used']","['results are lower than those of full parsers , e.g. ,#AUTHOR_TAG as might be expected since much less structural data , and no lexical data are being used']","['results are lower than those of full parsers , e.g. ,#AUTHOR_TAG as might be expected since much less structural data , and no lexical data are being used']","['results are lower than those of full parsers , e.g. ,#AUTHOR_TAG as might be expected since much less structural data , and no lexical data are being used']",1
"['a similar vain toSkut and Brants ( 1998 ) and#AUTHOR_TAG , the method extends an existing flat shallow-parsing method to handle composite structures']","['a similar vain toSkut and Brants ( 1998 ) and#AUTHOR_TAG , the method extends an existing flat shallow-parsing method to handle composite structures']","['a similar vain toSkut and Brants ( 1998 ) and#AUTHOR_TAG , the method extends an existing flat shallow-parsing method to handle composite structures']","['a similar vain toSkut and Brants ( 1998 ) and#AUTHOR_TAG , the method extends an existing flat shallow-parsing method to handle composite structures']",3
"['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. ,Bod ( 1992 ) ,#AUTHOR_TAG ,Collins ( 1997 ) ,Ratnaparkhi ( 1997 ) , andSekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. ,Bod ( 1992 ) ,#AUTHOR_TAG ,Collins ( 1997 ) ,Ratnaparkhi ( 1997 ) , andSekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. ,Bod ( 1992 ) ,#AUTHOR_TAG ,Collins ( 1997 ) ,Ratnaparkhi ( 1997 ) , andSekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. ,Bod ( 1992 ) ,#AUTHOR_TAG ,Collins ( 1997 ) ,Ratnaparkhi ( 1997 ) , andSekine ( 1998 ) )']",0
"['system was trained on the Penn Treebank( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used byMagerman ( 1995 ) ,Collins ( 1997 ) , and#AUTHOR_TAG , and became a common testbed']","['system was trained on the Penn Treebank( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used byMagerman ( 1995 ) ,Collins ( 1997 ) , and#AUTHOR_TAG , and became a common testbed']","['system was trained on the Penn Treebank( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used byMagerman ( 1995 ) ,Collins ( 1997 ) , and#AUTHOR_TAG , and became a common testbed']","['system was trained on the Penn Treebank( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used byMagerman ( 1995 ) ,Collins ( 1997 ) , and#AUTHOR_TAG , and became a common testbed']",1
"['system was trained on the Penn Treebank( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used byMagerman ( 1995 ) ,#AUTHOR_TAG , andRatnaparkhi ( 1997 ) , and became a common testbed']","['system was trained on the Penn Treebank( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used byMagerman ( 1995 ) ,#AUTHOR_TAG , andRatnaparkhi ( 1997 ) , and became a common testbed']","['system was trained on the Penn Treebank( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used byMagerman ( 1995 ) ,#AUTHOR_TAG , andRatnaparkhi ( 1997 ) , and became a common testbed']","['system was trained on the Penn Treebank( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used byMagerman ( 1995 ) ,#AUTHOR_TAG , andRatnaparkhi ( 1997 ) , and became a common testbed']",1
"['approach to partial parsing was presented by#AUTHOR_TAG , who extended a shallow-parsing technique to partial parsing']","['approach to partial parsing was presented by#AUTHOR_TAG , who extended a shallow-parsing technique to partial parsing']","['approach to partial parsing was presented by#AUTHOR_TAG , who extended a shallow-parsing technique to partial parsing']","['approach to partial parsing was presented by#AUTHOR_TAG , who extended a shallow-parsing technique to partial parsing']",0
"['a similar vain to#AUTHOR_TAG andBuchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures']","['a similar vain to#AUTHOR_TAG andBuchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures']","['a similar vain to#AUTHOR_TAG andBuchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures']","['a similar vain to#AUTHOR_TAG andBuchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures']",3
"['', 'To quantify the relative strengths of these transitive inferences ,#AUTHOR_TAG propose to assign a weight to each link .', 'Say the order a, b occurs m times and the pair {a, b} occurs n times in total.', 'Then the weight of the pair a → b is']","['', 'To quantify the relative strengths of these transitive inferences ,#AUTHOR_TAG propose to assign a weight to each link .', 'Say the order a, b occurs m times and the pair {a, b} occurs n times in total.', 'Then the weight of the pair a → b is']","['', 'To quantify the relative strengths of these transitive inferences ,#AUTHOR_TAG propose to assign a weight to each link .', 'Say the order a, b occurs m times and the pair {a, b} occurs n times in total.', 'Then the weight of the pair a → b is']","['uitively, the evidence for the first order is quite a bit stronger than the evidence for the second.', 'The first ordered pairs are more frequent, as are the individual adjectives involved.', 'To quantify the relative strengths of these transitive inferences ,#AUTHOR_TAG propose to assign a weight to each link .', 'Say the order a, b occurs m times and the pair {a, b} occurs n times in total.', 'Then the weight of the pair a → b is']",0
"['distributional clustering techniques ( Sch Â¨ utze , 1992 ;#AUTHOR_TAG ) could be applied to extract semantic classes from the corpus itself .', 'Since the constraints on adjective ordering in English depend largely on semantic classes, the addition of semantic information to the model ought to improve the results']","['distributional clustering techniques ( Sch Â¨ utze , 1992 ;#AUTHOR_TAG ) could be applied to extract semantic classes from the corpus itself .', 'Since the constraints on adjective ordering in English depend largely on semantic classes, the addition of semantic information to the model ought to improve the results']","['distributional clustering techniques ( Sch Â¨ utze , 1992 ;#AUTHOR_TAG ) could be applied to extract semantic classes from the corpus itself .', 'Since the constraints on adjective ordering in English depend largely on semantic classes, the addition of semantic information to the model ought to improve the results']","['', 'More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ;#AUTHOR_TAG ) could be applied to extract semantic classes from the corpus itself .', 'Since the constraints on adjective ordering in English depend largely on semantic classes, the addition of semantic information to the model ought to improve the results']",3
"['simplest strategy for ordering adjectives is what#AUTHOR_TAG call the direct evidence method .', '']","['simplest strategy for ordering adjectives is what#AUTHOR_TAG call the direct evidence method .', '']","['simplest strategy for ordering adjectives is what#AUTHOR_TAG call the direct evidence method .', '']","['simplest strategy for ordering adjectives is what#AUTHOR_TAG call the direct evidence method .', 'To order the pair {a, b}, count how many times the ordered sequences a, b and b, a appear in the training data and output the pair in the order which occurred more often']",0
"['be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature(Dietterich, 1997) .', 'In particular , boosting( Schapire , 1999 ; #AUTHOR_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly']","['be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature(Dietterich, 1997) .', 'In particular , boosting( Schapire , 1999 ; #AUTHOR_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly']","['worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature(Dietterich, 1997) .', 'In particular , boosting( Schapire , 1999 ; #AUTHOR_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly']","['', 'It would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature(Dietterich, 1997) .', 'In particular , boosting( Schapire , 1999 ; #AUTHOR_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly']",3
"['', '#AUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation .', '']","['', '#AUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation .', '']","['', '#AUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation .', '']","['', '#AUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation .', '']",0
"['problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""One approach to this more general problem , taken by the ` Nitrogen ' generator( #AUTHOR_TAG a ;Langkilde and Knight , 1998 b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model ."", 'Langkilde and']","['problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""One approach to this more general problem , taken by the ` Nitrogen ' generator( #AUTHOR_TAG a ;Langkilde and Knight , 1998 b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model ."", 'Langkilde and']","['problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""One approach to this more general problem , taken by the ` Nitrogen ' generator( #AUTHOR_TAG a ;Langkilde and Knight , 1998 b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model ."", 'Langkilde and']","['problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""One approach to this more general problem , taken by the ` Nitrogen ' generator( #AUTHOR_TAG a ;Langkilde and Knight , 1998 b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model ."", '']",5
"['availability of toolkits for this weighted case(Mohri et al., 1998; vanNoord and Gerdemann, 2001) promises to unify much of statistical NLP.', ""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition( Pereira and Riley , 1997 ) and machine translation( #AUTHOR_TAG ) ."", '']","['availability of toolkits for this weighted case(Mohri et al., 1998; vanNoord and Gerdemann, 2001) promises to unify much of statistical NLP.', ""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition( Pereira and Riley , 1997 ) and machine translation( #AUTHOR_TAG ) ."", '']","['availability of toolkits for this weighted case(Mohri et al., 1998; vanNoord and Gerdemann, 2001) promises to unify much of statistical NLP.', ""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition( Pereira and Riley , 1997 ) and machine translation( #AUTHOR_TAG ) ."", '']","['availability of toolkits for this weighted case(Mohri et al., 1998; vanNoord and Gerdemann, 2001) promises to unify much of statistical NLP.', ""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition( Pereira and Riley , 1997 ) and machine translation( #AUTHOR_TAG ) ."", '']",0
"['log(strength) values are called weights, but this paper uses ""weight"" to mean something else.', 'fÎ¸ on demand( #AUTHOR_TAG ) can pay off here , since only part of fÎ¸ may be needed subsequently .']","['log(strength) values are called weights, but this paper uses ""weight"" to mean something else.', 'fÎ¸ on demand( #AUTHOR_TAG ) can pay off here , since only part of fÎ¸ may be needed subsequently .']","['log(strength) values are called weights, but this paper uses ""weight"" to mean something else.', 'fÎ¸ on demand( #AUTHOR_TAG ) can pay off here , since only part of fÎ¸ may be needed subsequently .']","['10 Traditionally log(strength) values are called weights, but this paper uses ""weight"" to mean something else.', 'fÎ¸ on demand( #AUTHOR_TAG ) can pay off here , since only part of fÎ¸ may be needed subsequently .']",0
"['', '#AUTHOR_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) .', '']","['', '#AUTHOR_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) .', '']","['', '#AUTHOR_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) .', '']","['', '#AUTHOR_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) .', '']",5
"['of ⊕ and ⊗ operations.', 'For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm( #AUTHOR_TAG ) .', '']","['of ⊕ and ⊗ operations.', 'For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm( #AUTHOR_TAG ) .', '']","['of ⊕ and ⊗ operations.', 'For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm( #AUTHOR_TAG ) .', '']","['In many cases of interest, T i is an acyclic graph. 20', ""hen Tarjan's method computes w 0j for each j in topologically sorted order, thereby finding t i in a linear number of ⊕ and ⊗ operations."", 'For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm( #AUTHOR_TAG ) .', '']",0
"['availability of toolkits for this weighted case( #AUTHOR_TAG ; vanNoord and Gerdemann , 2001 ) promises to unify much of statistical NLP .', '']","['availability of toolkits for this weighted case( #AUTHOR_TAG ; vanNoord and Gerdemann , 2001 ) promises to unify much of statistical NLP .', '']","['availability of toolkits for this weighted case( #AUTHOR_TAG ; vanNoord and Gerdemann , 2001 ) promises to unify much of statistical NLP .', '']","['availability of toolkits for this weighted case( #AUTHOR_TAG ; vanNoord and Gerdemann , 2001 ) promises to unify much of statistical NLP .', '']",0
"['outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'For example , the forward-backward algorithm( Baum , 1972 ) trains only Hidden Markov Models , while( #AUTHOR_TAG ) trains only stochastic edit distance']","['outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'For example , the forward-backward algorithm( Baum , 1972 ) trains only Hidden Markov Models , while( #AUTHOR_TAG ) trains only stochastic edit distance']","['these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'For example , the forward-backward algorithm( Baum , 1972 ) trains only Hidden Markov Models , while( #AUTHOR_TAG ) trains only stochastic edit distance']","[', there is a stumbling block: Where do the weights come from?', 'After all, statistical models require supervised or unsupervised training.', 'Currently, finite-state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'For example , the forward-backward algorithm( Baum , 1972 ) trains only Hidden Markov Models , while( #AUTHOR_TAG ) trains only stochastic edit distance']",0
"['and backward probabilities , p0j and pkn , can be computed using single-source algebraic path for the simpler semiring ( R , + , x , â\x88\x97 ) -- or equivalently , by solving a sparse linear system of equations over R , a much-studied problem at O ( n ) space , O ( nm ) time , and faster approximations( #AUTHOR_TAG )']","['and backward probabilities , p0j and pkn , can be computed using single-source algebraic path for the simpler semiring ( R , + , x , â\x88\x97 ) -- or equivalently , by solving a sparse linear system of equations over R , a much-studied problem at O ( n ) space , O ( nm ) time , and faster approximations( #AUTHOR_TAG )']","['and backward probabilities , p0j and pkn , can be computed using single-source algebraic path for the simpler semiring ( R , + , x , â\x88\x97 ) -- or equivalently , by solving a sparse linear system of equations over R , a much-studied problem at O ( n ) space , O ( nm ) time , and faster approximations( #AUTHOR_TAG )']","['', 'The forward and backward probabilities , p0j and pkn , can be computed using single-source algebraic path for the simpler semiring ( R , + , x , â\x88\x97 ) -- or equivalently , by solving a sparse linear system of equations over R , a much-studied problem at O ( n ) space , O ( nm ) time , and faster approximations( #AUTHOR_TAG )']",0
"['4To prove ( 1 ) â\x87\x92 ( 3 ) , express f as an FST and apply the well-known Kleene-Sch Â¨ utzenberger construction( #AUTHOR_TAG ) , taking care to write each regexp in the construction as a constant times a probabilistic regexp .', 'A']","['4To prove ( 1 ) â\x87\x92 ( 3 ) , express f as an FST and apply the well-known Kleene-Sch Â¨ utzenberger construction( #AUTHOR_TAG ) , taking care to write each regexp in the construction as a constant times a probabilistic regexp .', 'A']","['4To prove ( 1 ) â\x87\x92 ( 3 ) , express f as an FST and apply the well-known Kleene-Sch Â¨ utzenberger construction( #AUTHOR_TAG ) , taking care to write each regexp in the construction as a constant times a probabilistic regexp .', '']","['4To prove ( 1 ) â\x87\x92 ( 3 ) , express f as an FST and apply the well-known Kleene-Sch Â¨ utzenberger construction( #AUTHOR_TAG ) , taking care to write each regexp in the construction as a constant times a probabilistic regexp .', '']",5
"['and subtraction are also possible: −(p, v) = (−p, −v) and (p, v) −1 = (p −1 , −p −1 vp −1 ).', 'Division is commonly used in defining f θ (for normalization). 19', 'Multiple edges from j to k are summed into a single edge.', '(Mohri, 2002) .', 'Efficient hardware implementation is also possible via chip-level parallelism( #AUTHOR_TAG )']","['and subtraction are also possible: −(p, v) = (−p, −v) and (p, v) −1 = (p −1 , −p −1 vp −1 ).', 'Division is commonly used in defining f θ (for normalization). 19', 'Multiple edges from j to k are summed into a single edge.', '(Mohri, 2002) .', 'Efficient hardware implementation is also possible via chip-level parallelism( #AUTHOR_TAG )']","['and subtraction are also possible: −(p, v) = (−p, −v) and (p, v) −1 = (p −1 , −p −1 vp −1 ).', 'Division is commonly used in defining f θ (for normalization). 19', 'Multiple edges from j to k are summed into a single edge.', '(Mohri, 2002) .', 'Efficient hardware implementation is also possible via chip-level parallelism( #AUTHOR_TAG )']","['and subtraction are also possible: −(p, v) = (−p, −v) and (p, v) −1 = (p −1 , −p −1 vp −1 ).', 'Division is commonly used in defining f θ (for normalization). 19', 'Multiple edges from j to k are summed into a single edge.', '(Mohri, 2002) .', 'Efficient hardware implementation is also possible via chip-level parallelism( #AUTHOR_TAG )']",3
"['', 'Per-state joint normalization( #AUTHOR_TAG b , Â§ 8.2 ) is similar but drops the dependence on a .', '']","['', 'Per-state joint normalization( #AUTHOR_TAG b , Â§ 8.2 ) is similar but drops the dependence on a .', '']","['', 'Per-state joint normalization( #AUTHOR_TAG b , Â§ 8.2 ) is similar but drops the dependence on a .', '']","['', 'Per-state joint normalization( #AUTHOR_TAG b , Â§ 8.2 ) is similar but drops the dependence on a .', '']",1
"['EM algorithm( #AUTHOR_TAG ) can maximize these functions .', '']","['EM algorithm( #AUTHOR_TAG ) can maximize these functions .', '']","['EM algorithm( #AUTHOR_TAG ) can maximize these functions .', '']","['EM algorithm( #AUTHOR_TAG ) can maximize these functions .', 'Roughly, the E step guesses hidden information: if (x i , y i ) was generated from the current f θ , which FST paths stand a chance of having been the path used?', '']",5
"['defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules( Mohri and Sproat , 1996 ) , ( 3 ) by compilation of decision trees( #AUTHOR_TAG ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and vanNoord , 1999 ) ,5 ( 5 ) by conditionalization of a']","['defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules( Mohri and Sproat , 1996 ) , ( 3 ) by compilation of decision trees( #AUTHOR_TAG ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and vanNoord , 1999 ) ,5 ( 5 ) by conditionalization of a']","['defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules( Mohri and Sproat , 1996 ) , ( 3 ) by compilation of decision trees( #AUTHOR_TAG ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and vanNoord , 1999 ) ,5 ( 5 ) by conditionalization of']","['defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules( Mohri and Sproat , 1996 ) , ( 3 ) by compilation of decision trees( #AUTHOR_TAG ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and vanNoord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below']",0
"['for some important remarks on efficiency : â\x80¢ Computing ti is an instance of the well-known algebraic path problem( #AUTHOR_TAG ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and un- weighted']","['for some important remarks on efficiency : â\x80¢ Computing ti is an instance of the well-known algebraic path problem( #AUTHOR_TAG ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and un- weighted).', '']","['for some important remarks on efficiency : â\x80¢ Computing ti is an instance of the well-known algebraic path problem( #AUTHOR_TAG ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and un- weighted).', 'It is wasteful to']","['for some important remarks on efficiency : â\x80¢ Computing ti is an instance of the well-known algebraic path problem( #AUTHOR_TAG ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and un- weighted).', '']",0
"[':e −→, and a:ae −→ share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; Eisner , 2001 b ;#AUTHOR_TAG )']","[':e −→, and a:ae −→ share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; Eisner , 2001 b ;#AUTHOR_TAG )']","[':e −→, and a:ae −→ share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; Eisner , 2001 b ;#AUTHOR_TAG )']","[':e −→, and a:ae −→ share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; Eisner , 2001 b ;#AUTHOR_TAG )']",0
"['imum-posterior estimation tries to maximize P (θ) • i f θ (x i , y i ) where P (θ) is a prior probability.', 'In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting( #AUTHOR_TAG )']","['imum-posterior estimation tries to maximize P (θ) • i f θ (x i , y i ) where P (θ) is a prior probability.', 'In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting( #AUTHOR_TAG )']","['imum-posterior estimation tries to maximize P (θ) • i f θ (x i , y i ) where P (θ) is a prior probability.', 'In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting( #AUTHOR_TAG )']","['imum-posterior estimation tries to maximize P (θ) • i f θ (x i , y i ) where P (θ) is a prior probability.', 'In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting( #AUTHOR_TAG )']",5
"['�_ may be given as regular sets in which input and output were observed to fall.', 'For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX#AUTHOR_TAG , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .1']","['�_ may be given as regular sets in which input and output were observed to fall.', 'For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX#AUTHOR_TAG , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .1']","['observed (partly supervised training): thus xi _ __, yi _ �_ may be given as regular sets in which input and output were observed to fall.', 'For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX#AUTHOR_TAG , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .1']","['', 'Samples need not be fully observed (partly supervised training): thus xi _ __, yi _ �_ may be given as regular sets in which input and output were observed to fall.', 'For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX#AUTHOR_TAG , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .1']",0
"['availability of toolkits for this weighted case(Mohri et al., 1998; vanNoord and Gerdemann, 2001) promises to unify much of statistical NLP.', ""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition( #AUTHOR_TAG ) and machine translation ( Knight and Al-Onaizan , 1998 ) ."", '']","['availability of toolkits for this weighted case(Mohri et al., 1998; vanNoord and Gerdemann, 2001) promises to unify much of statistical NLP.', ""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition( #AUTHOR_TAG ) and machine translation ( Knight and Al-Onaizan , 1998 ) ."", '']","['availability of toolkits for this weighted case(Mohri et al., 1998; vanNoord and Gerdemann, 2001) promises to unify much of statistical NLP.', ""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition( #AUTHOR_TAG ) and machine translation ( Knight and Al-Onaizan , 1998 ) ."", '']","['availability of toolkits for this weighted case(Mohri et al., 1998; vanNoord and Gerdemann, 2001) promises to unify much of statistical NLP.', ""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition( #AUTHOR_TAG ) and machine translation ( Knight and Al-Onaizan , 1998 ) ."", '']",0
"['since then the real work is done by an c-closure step( #AUTHOR_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version']","['since then the real work is done by an c-closure step( #AUTHOR_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version']","['since then the real work is done by an c-closure step( #AUTHOR_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version .', '']",['( #AUTHOR_TAG )'],0
"['An easy approach is to normalize the options at each state to make the FST Markovian.', 'Unfortunately, the result may differ for equivalent FSTs that express the same weighted relation.', 'Undesirable consequences of this fact have been termed ""label bias""( #AUTHOR_TAG ) .', '']","['An easy approach is to normalize the options at each state to make the FST Markovian.', 'Unfortunately, the result may differ for equivalent FSTs that express the same weighted relation.', 'Undesirable consequences of this fact have been termed ""label bias""( #AUTHOR_TAG ) .', '']","['An easy approach is to normalize the options at each state to make the FST Markovian.', 'Unfortunately, the result may differ for equivalent FSTs that express the same weighted relation.', 'Undesirable consequences of this fact have been termed ""label bias""( #AUTHOR_TAG ) .', '']","['An easy approach is to normalize the options at each state to make the FST Markovian.', 'Unfortunately, the result may differ for equivalent FSTs that express the same weighted relation.', 'Undesirable consequences of this fact have been termed ""label bias""( #AUTHOR_TAG ) .', 'Also, in the conditional case such per-state normalization is only correct if all states accept all input suffixes (since ""dead ends"" leak probability mass). 8', '']",0
"['brief version of this work, with some additional material, first appeared as( #AUTHOR_TAG a ) .', 'A leisurely journal-length version with more details has been prepared and is available']","['brief version of this work, with some additional material, first appeared as( #AUTHOR_TAG a ) .', 'A leisurely journal-length version with more details has been prepared and is available']","['brief version of this work, with some additional material, first appeared as( #AUTHOR_TAG a ) .', 'A leisurely journal-length version with more details has been prepared and is available']","['brief version of this work, with some additional material, first appeared as( #AUTHOR_TAG a ) .', 'A leisurely journal-length version with more details has been prepared and is available']",2
"['', 'Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into : /2.7 −→ arcs).', 'A more subtle example is weighted FSAs that approximate PCFGs( Nederhof , 2000 ; #AUTHOR_TAG ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .', ""These are parameterized by the PCFG's parameters, but add or remove strings of the PCFG to leave an improper probability distribution""]","['', 'Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into : /2.7 −→ arcs).', 'A more subtle example is weighted FSAs that approximate PCFGs( Nederhof , 2000 ; #AUTHOR_TAG ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .', ""These are parameterized by the PCFG's parameters, but add or remove strings of the PCFG to leave an improper probability distribution""]","['', 'Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into : /2.7 −→ arcs).', 'A more subtle example is weighted FSAs that approximate PCFGs( Nederhof , 2000 ; #AUTHOR_TAG ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .', ""These are parameterized by the PCFG's parameters, but add or remove strings of the PCFG to leave an improper probability distribution""]","['', 'Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into : /2.7 −→ arcs).', 'A more subtle example is weighted FSAs that approximate PCFGs( Nederhof , 2000 ; #AUTHOR_TAG ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .', ""These are parameterized by the PCFG's parameters, but add or remove strings of the PCFG to leave an improper probability distribution""]",0
"['defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules( #AUTHOR_TAG ) , ( 3 ) by compilation of decision trees( Sproat and Riley , 1996 ) , ( 4 ) as a relation that']","['defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules( #AUTHOR_TAG ) , ( 3 ) by compilation of decision trees( Sproat and Riley , 1996 ) , ( 4 ) as a relation that']","['defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules( #AUTHOR_TAG ) , ( 3 ) by compilation of decision trees( Sproat and Riley , 1996 ) , ( 4 ) as a relation that']","['defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules( #AUTHOR_TAG ) , ( 3 ) by compilation of decision trees( Sproat and Riley , 1996 ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and vanNoord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below']",0
"['globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases( #AUTHOR_TAG ; Eisner , 2001 b ;Lafferty et al. , 2001 )']","['globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases( #AUTHOR_TAG ; Eisner , 2001 b ;Lafferty et al. , 2001 )']","['globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases( #AUTHOR_TAG ; Eisner , 2001 b ;Lafferty et al. , 2001 )']","[':e −→, and a:ae −→ share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases( #AUTHOR_TAG ; Eisner , 2001 b ;Lafferty et al. , 2001 )']",0
"['', 'Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into : /2.7 −→ arcs).', 'A more subtle example is weighted FSAs that approximate PCFGs( #AUTHOR_TAG ; Mohri and Nederhof , 2001 ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .', ""These are parameterized by the PCFG's parameters, but add or remove strings of the PCFG to leave an improper probability distribution""]","['', 'Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into : /2.7 −→ arcs).', 'A more subtle example is weighted FSAs that approximate PCFGs( #AUTHOR_TAG ; Mohri and Nederhof , 2001 ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .', ""These are parameterized by the PCFG's parameters, but add or remove strings of the PCFG to leave an improper probability distribution""]","['', 'Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into : /2.7 −→ arcs).', 'A more subtle example is weighted FSAs that approximate PCFGs( #AUTHOR_TAG ; Mohri and Nederhof , 2001 ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .', ""These are parameterized by the PCFG's parameters, but add or remove strings of the PCFG to leave an improper probability distribution""]","['', 'Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into : /2.7 −→ arcs).', 'A more subtle example is weighted FSAs that approximate PCFGs( #AUTHOR_TAG ; Mohri and Nederhof , 2001 ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .', ""These are parameterized by the PCFG's parameters, but add or remove strings of the PCFG to leave an improper probability distribution""]",0
"[', there is a stumbling block: Where do the weights come from?', 'After all, statistical models require supervised or unsupervised training.', 'Currently, finite-state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'For example , the forward-backward algorithm( #AUTHOR_TAG ) trains only Hidden Markov Models , while( Ristad and Yianilos , 1996 ) trains only stochastic edit distance']","[', there is a stumbling block: Where do the weights come from?', 'After all, statistical models require supervised or unsupervised training.', 'Currently, finite-state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'For example , the forward-backward algorithm( #AUTHOR_TAG ) trains only Hidden Markov Models , while( Ristad and Yianilos , 1996 ) trains only stochastic edit distance']","[', there is a stumbling block: Where do the weights come from?', 'After all, statistical models require supervised or unsupervised training.', 'Currently, finite-state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'For example , the forward-backward algorithm( #AUTHOR_TAG ) trains only Hidden Markov Models , while( Ristad and Yianilos , 1996 ) trains only stochastic edit distance']","[', there is a stumbling block: Where do the weights come from?', 'After all, statistical models require supervised or unsupervised training.', 'Currently, finite-state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'For example , the forward-backward algorithm( #AUTHOR_TAG ) trains only Hidden Markov Models , while( Ristad and Yianilos , 1996 ) trains only stochastic edit distance']",0
"['globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases( McCallum et al. , 2000 ; #AUTHOR_TAG b ;Lafferty et al. , 2001 )']","['globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases( McCallum et al. , 2000 ; #AUTHOR_TAG b ;Lafferty et al. , 2001 )']","['globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases( McCallum et al. , 2000 ; #AUTHOR_TAG b ;Lafferty et al. , 2001 )']","[':e −→, and a:ae −→ share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases( McCallum et al. , 2000 ; #AUTHOR_TAG b ;Lafferty et al. , 2001 )']",0
"['', 'The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della#AUTHOR_TAG ; Chen and', 'For globally normalized, joint models, the predicted vector is ec f (Σ * , ∆ * ).', 'If the log-linear probabilities are condition']","['', 'The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della#AUTHOR_TAG ; Chen and', 'For globally normalized, joint models, the predicted vector is ec f (Σ * , ∆ * ).', 'If the log-linear probabilities are conditioned']","['', 'The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della#AUTHOR_TAG ; Chen and', 'For globally normalized, joint models, the predicted vector is ec f (Σ * , ∆ * ).', 'If the log-linear probabilities are conditioned']","['', 'The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della#AUTHOR_TAG ; Chen and', 'For globally normalized, joint models, the predicted vector is ec f (Σ * , ∆ * ).', '']",5
"['central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .', 'The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (λ); reading another b (ν); transducing b to p rather than q (µ); starting to transduce p to rather than x (']","['central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .', 'The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (λ); reading another b (ν); transducing b to p rather than q (µ); starting to transduce p to rather than x']","['central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .', 'The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (λ); reading another b (ν); transducing b to p rather than q (µ); starting to transduce p to rather than x (']","['central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .', 'The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (λ); reading another b (ν); transducing b to p rather than q (µ); starting to transduce p to rather than x (ρ). 4 To prove (1)⇒(3), express f as an FST and apply the well-known Kleene-Schützenberger construction(Berstel and Reutenauer, 1988) , taking care to write each regexp in the construction as a constant times a probabilistic regexp.', '']",0
"['system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq#AUTHOR_TAG , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) [ 14 ] , and', 'Additionally, several model organism databases or nomenclature databases were']","['system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq#AUTHOR_TAG , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) [ 14 ] , and', 'Additionally, several model organism databases or nomenclature databases were']","['system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq#AUTHOR_TAG , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) [ 14 ] , and', 'Additionally, several model organism databases or nomenclature databases were']","['system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq#AUTHOR_TAG , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) [ 14 ] , and', 'Additionally, several model organism databases or nomenclature databases were used.', '']",5
"[', fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD )#AUTHOR_TAG , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] ,']","[', fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD )#AUTHOR_TAG , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] ,']","['MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD )#AUTHOR_TAG , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database']",['#AUTHOR_TAG'],5
"['molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD )#AUTHOR_TAG , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD )']","['molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD )#AUTHOR_TAG , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD )']","['molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD )#AUTHOR_TAG , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD )']",['#AUTHOR_TAG'],5
"['fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase#AUTHOR_TAG ,']","['fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase#AUTHOR_TAG ,']","['fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase#AUTHOR_TAG , Human Nomenclature Database']",['#AUTHOR_TAG'],5
"['UMLS -- the Unified Medical Language System ( UMLS ) has been developed and maintained by National Library of Medicine ( NLM )#AUTHOR_TAG .', 'It contains three knowledge sources: the Metathesaurus (META), the SPECIALIST lexicon, and the Semantic Network.', 'The META provides a uniform, integrated platform for over 60 biomedical vocabularies and classifications, and group different names for the same concept.', 'The SPECIALIST lexicon contains syntactic information for many terms,']","['UMLS -- the Unified Medical Language System ( UMLS ) has been developed and maintained by National Library of Medicine ( NLM )#AUTHOR_TAG .', 'It contains three knowledge sources: the Metathesaurus (META), the SPECIALIST lexicon, and the Semantic Network.', 'The META provides a uniform, integrated platform for over 60 biomedical vocabularies and classifications, and group different names for the same concept.', 'The SPECIALIST lexicon contains syntactic information for many terms,']","['UMLS -- the Unified Medical Language System ( UMLS ) has been developed and maintained by National Library of Medicine ( NLM )#AUTHOR_TAG .', 'It contains three knowledge sources: the Metathesaurus (META), the SPECIALIST lexicon, and the Semantic Network.', 'The META provides a uniform, integrated platform for over 60 biomedical vocabularies and classifications, and group different names for the same concept.', 'The SPECIALIST lexicon contains syntactic information for many terms, component words, and English words, including verbs,']","['UMLS -- the Unified Medical Language System ( UMLS ) has been developed and maintained by National Library of Medicine ( NLM )#AUTHOR_TAG .', 'It contains three knowledge sources: the Metathesaurus (META), the SPECIALIST lexicon, and the Semantic Network.', 'The META provides a uniform, integrated platform for over 60 biomedical vocabularies and classifications, and group different names for the same concept.', 'The SPECIALIST lexicon contains syntactic information for many terms, component words, and English words, including verbs, which do not appear in the META.', 'The Semantic Network contains information about the types or categories (e.g., ""Disease or Syndrome"", ""Virus"") to which all META concepts have been assigned']",0
"['the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible#AUTHOR_TAG .', 'One requirement for NLP is the ability to accurately recognize terms that represent biological entities in free text.', 'Another requirement is the ability to associate these terms with corresponding biological entities (']","['the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible#AUTHOR_TAG .', 'One requirement for NLP is the ability to accurately recognize terms that represent biological entities in free text.', 'Another requirement is the ability to associate these terms with corresponding biological entities (i.e., records in biological databases) in order to be']","['the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible#AUTHOR_TAG .', 'One requirement for NLP is the ability to accurately recognize terms that represent biological entities in free text.', 'Another requirement is the ability to associate these terms with corresponding biological entities (']","['the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible#AUTHOR_TAG .', 'One requirement for NLP is the ability to accurately recognize terms that represent biological entities in free text.', 'Another requirement is the ability to associate these terms with corresponding biological entities (i.e., records in biological databases) in order to be used by other automated systems for literature mining.', '']",0
"['23 ] , Online Mendelian Inheritance in Man ( OMIM )#AUTHOR_TAG , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 2']","['23 ] , Online Mendelian Inheritance in Man ( OMIM )#AUTHOR_TAG , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26']","['23 ] , Online Mendelian Inheritance in Man ( OMIM )#AUTHOR_TAG , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 2']",['#AUTHOR_TAG'],5
['clusters based on the CD-HIT algorithm#AUTHOR_TAG'],['clusters based on the CD-HIT algorithm#AUTHOR_TAG'],['clusters based on the CD-HIT algorithm#AUTHOR_TAG'],['#AUTHOR_TAG'],5
"[', fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD )#AUTHOR_TAG , worm -- WormBase [ 22 ] ,']","[', fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD )#AUTHOR_TAG , worm -- WormBase [ 22 ] ,']","[', fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD )#AUTHOR_TAG , worm -- WormBase [ 22 ] , Human Nomenclature Database']",['#AUTHOR_TAG'],5
"['system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq [ 12 ] , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR )#AUTHOR_TAG , and', 'Additionally, several model organism databases or nomenclature databases were used.', 'Correspondences among records from these databases are identified using the']","['system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq [ 12 ] , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR )#AUTHOR_TAG , and', 'Additionally, several model organism databases or nomenclature databases were used.', 'Correspondences among records from these databases are identified using the']","['system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq [ 12 ] , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR )#AUTHOR_TAG , and', 'Additionally, several model organism databases or nomenclature databases were used.', 'Correspondences among records from these databases are identified using']","['system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq [ 12 ] , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR )#AUTHOR_TAG , and', 'Additionally, several model organism databases or nomenclature databases were used.', '']",5
"['molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase#AUTHOR_TAG , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] ,']","['molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase#AUTHOR_TAG , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] ,']","['molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase#AUTHOR_TAG , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database']",['#AUTHOR_TAG'],5
"['Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM )#AUTHOR_TAG']","['Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM )#AUTHOR_TAG']","['Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM )#AUTHOR_TAG']",['#AUTHOR_TAG'],5
"['our experiments, we use naive Bayes as the learning algorithm.', 'The knowledge sources we use include parts-of-speech, local collocations, and surrounding words.', 'These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work( #AUTHOR_TAG )']","['our experiments, we use naive Bayes as the learning algorithm.', 'The knowledge sources we use include parts-of-speech, local collocations, and surrounding words.', 'These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work( #AUTHOR_TAG )']","['our experiments, we use naive Bayes as the learning algorithm.', 'The knowledge sources we use include parts-of-speech, local collocations, and surrounding words.', 'These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work( #AUTHOR_TAG )']","['our experiments, we use naive Bayes as the learning algorithm.', 'The knowledge sources we use include parts-of-speech, local collocations, and surrounding words.', 'These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work( #AUTHOR_TAG )']",2
"['', 'While these approaches have been reasonably successful ( seeMitkov ( 2002 ) ) ,#AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .', '']","['', 'While these approaches have been reasonably successful ( seeMitkov ( 2002 ) ) ,#AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .', '']","['-syntactic cues as knowledge sources in the resolution process (e.g.,Mitkov (1998) ,Tetreault (2001) ).', 'While these approaches have been reasonably successful ( seeMitkov ( 2002 ) ) ,#AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .', '']","['', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g.,Mitkov (1998) ,Tetreault (2001) ).', 'While these approaches have been reasonably successful ( seeMitkov ( 2002 ) ) ,#AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .', '']",0
"['importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g.,#AUTHOR_TAG ) ; and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms']","['importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g.,#AUTHOR_TAG ) ; and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms']","['importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g.,#AUTHOR_TAG ) ; and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms']","['', 'More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g.,#AUTHOR_TAG ) ; and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms']",1
"['the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. ,Mitkov ( 1998 ) ,#AUTHOR_TAG ) .', '']","['the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. ,Mitkov ( 1998 ) ,#AUTHOR_TAG ) .', '']","['the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. ,Mitkov ( 1998 ) ,#AUTHOR_TAG ) .', '']","['the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. ,Mitkov ( 1998 ) ,#AUTHOR_TAG ) .', '']",0
"['our SC classifier; instead, we use the BBN Entity Type Corpus( #AUTHOR_TAG ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.', 'This provides us with a training set that is approximately five times bigger than that of ACE.', '']","['our SC classifier; instead, we use the BBN Entity Type Corpus( #AUTHOR_TAG ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.', 'This provides us with a training set that is approximately five times bigger than that of ACE.', '']","['our SC classifier; instead, we use the BBN Entity Type Corpus( #AUTHOR_TAG ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.', 'This provides us with a training set that is approximately five times bigger than that of ACE.', '']",['( #AUTHOR_TAG )'],5
"['knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g.,Mitkov (1998) ,Tetreault (2001) ).', 'While these approaches have been reasonably successful ( see#AUTHOR_TAG ) ,Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .', '']","['knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g.,Mitkov (1998) ,Tetreault (2001) ).', 'While these approaches have been reasonably successful ( see#AUTHOR_TAG ) ,Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .', '']","['knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g.,Mitkov (1998) ,Tetreault (2001) ).', 'While these approaches have been reasonably successful ( see#AUTHOR_TAG ) ,Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .', '']","['', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g.,Mitkov (1998) ,Tetreault (2001) ).', 'While these approaches have been reasonably successful ( see#AUTHOR_TAG ) ,Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .', '']",0
"['; and (3) train an SVM classifier (using the LIBSVM package( #AUTHOR_TAG ) ) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19']","['instances; and (3) train an SVM classifier (using the LIBSVM package( #AUTHOR_TAG ) ) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8']","['and (3) train an SVM classifier (using the LIBSVM package( #AUTHOR_TAG ) ) on these 20% of the instances, where each instance, i, is represented by']",['( #AUTHOR_TAG )'],5
"[""NE : We use BBN 's IdentiFinder( #AUTHOR_TAG ) , a MUC-style NE recognizer to determine the NE type of NPZ ."", '']","[""NE : We use BBN 's IdentiFinder( #AUTHOR_TAG ) , a MUC-style NE recognizer to determine the NE type of NPZ ."", '']","[""NE : We use BBN 's IdentiFinder( #AUTHOR_TAG ) , a MUC-style NE recognizer to determine the NE type of NPZ ."", '']","[""4 ) NE : We use BBN 's IdentiFinder( #AUTHOR_TAG ) , a MUC-style NE recognizer to determine the NE type of NPZ ."", '']",5
"['learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. ,Soon et al. ( 2001 ) ,#AUTHOR_TAG ) .', 'It is not easy to measure the accuracy of this heuristic,']","['learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. ,Soon et al. ( 2001 ) ,#AUTHOR_TAG ) .', 'It is not easy to measure the accuracy of this heuristic,']","['learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. ,Soon et al. ( 2001 ) ,#AUTHOR_TAG ) .', 'It is not easy to measure the accuracy of this heuristic,']","['', 'However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. ,Soon et al. ( 2001 ) ,#AUTHOR_TAG ) .', '']",0
"['algorithms.', 'We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in#AUTHOR_TAG , motivated by its success in the related tasks of word sense disambiguation( Yarowsky , 1995 ) and NE classification( Collins and Singer , 1999 ) .', '']","['algorithms.', 'We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in#AUTHOR_TAG , motivated by its success in the related tasks of word sense disambiguation( Yarowsky , 1995 ) and NE classification( Collins and Singer , 1999 ) .', '']","['algorithms.', 'We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in#AUTHOR_TAG , motivated by its success in the related tasks of word sense disambiguation( Yarowsky , 1995 ) and NE classification( Collins and Singer , 1999 ) .', '']","['algorithms.', 'We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in#AUTHOR_TAG , motivated by its success in the related tasks of word sense disambiguation( Yarowsky , 1995 ) and NE classification( Collins and Singer , 1999 ) .', '']",5
"['.', 'We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer( #AUTHOR_TAG ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved .', '']","['texts.', 'We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer( #AUTHOR_TAG ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved .', '']","['the 97 test texts.', 'We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer( #AUTHOR_TAG ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved .', '']","['in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.', 'We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer( #AUTHOR_TAG ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved .', '']",5
"['training, the decision tree classifier is used to select an antecedent for each NP in a test text.', 'Following#AUTHOR_TAG , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj.', 'If no such NP exists, no antecedent is selected for NPj']","['training, the decision tree classifier is used to select an antecedent for each NP in a test text.', 'Following#AUTHOR_TAG , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj.', 'If no such NP exists, no antecedent is selected for NPj']","['training, the decision tree classifier is used to select an antecedent for each NP in a test text.', 'Following#AUTHOR_TAG , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj.', 'If no such NP exists, no antecedent is selected for NPj']","['training, the decision tree classifier is used to select an antecedent for each NP in a test text.', 'Following#AUTHOR_TAG , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj.', 'If no such NP exists, no antecedent is selected for NPj']",4
"['ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. ,#AUTHOR_TAG ) .', 'Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location).', '']","['ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. ,#AUTHOR_TAG ) .', 'Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location).', '']","['ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. ,#AUTHOR_TAG ) .', 'Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location).', '']","['ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. ,#AUTHOR_TAG ) .', 'Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location).', '']",1
"['; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\x0e, NPi+2, � � �, NPj positional features that have been employed by high- performing resolvers such asNg and Cardie (2002) and#AUTHOR_TAG , as described below']","['and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\x0e, NPi+2, � � �, NPj positional features that have been employed by high- performing resolvers such asNg and Cardie (2002) and#AUTHOR_TAG , as described below']","['and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\x0e, NPi+2, � � �, NPj positional features that have been employed by high- performing resolvers such asNg and Cardie (2002) and#AUTHOR_TAG , as described below']","['', 'Following previous work (e.g.,Soon et al. (2001) andPonzetto and Strube (2006) ), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\x0e, NPi+2, � � �, NPj positional features that have been employed by high- performing resolvers such asNg and Cardie (2002) and#AUTHOR_TAG , as described below']",1
"['baseline coreference system uses the C4 .5 decision tree learner( #AUTHOR_TAG ) to acquire a classifier on the training texts for determining whether two NPs are coreferent .', '']","['baseline coreference system uses the C4 .5 decision tree learner( #AUTHOR_TAG ) to acquire a classifier on the training texts for determining whether two NPs are coreferent .', '']","['baseline coreference system uses the C4 .5 decision tree learner( #AUTHOR_TAG ) to acquire a classifier on the training texts for determining whether two NPs are coreferent .', '']","['baseline coreference system uses the C4 .5 decision tree learner( #AUTHOR_TAG ) to acquire a classifier on the training texts for determining whether two NPs are coreferent .', '']",5
"['', 'Each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high- performing resolvers such as#AUTHOR_TAG andYang et al. (2003) , as described below']","['', 'Each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high- performing resolvers such as#AUTHOR_TAG andYang et al. (2003) , as described below']","['', 'Each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high- performing resolvers such as#AUTHOR_TAG andYang et al. (2003) , as described below']","['', 'Each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high- performing resolvers such as#AUTHOR_TAG andYang et al. (2003) , as described below']",1
"['', 'Following#AUTHOR_TAG , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition .', '']","['', 'Following#AUTHOR_TAG , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition .', '']","['', 'Following#AUTHOR_TAG , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition .', '']","['', 'Following#AUTHOR_TAG , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition .', '']",5
"['researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. ,Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. ,Poesio et al. ( 2004 ) ) or Wikipedia( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see#AUTHOR_TAG )']","['researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. ,Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. ,Poesio et al. ( 2004 ) ) or Wikipedia( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see#AUTHOR_TAG )']","['researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. ,Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. ,Poesio et al. ( 2004 ) ) or Wikipedia( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see#AUTHOR_TAG )']","['', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. ,Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. ,Poesio et al. ( 2004 ) ) or Wikipedia( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see#AUTHOR_TAG )']",0
"['learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. ,#AUTHOR_TAG ,Markert and Nissim ( 2005 ) ) .', 'It is not easy to measure the']","['learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. ,#AUTHOR_TAG ,Markert and Nissim ( 2005 ) ) .', 'It is not easy to measure the']","['learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. ,#AUTHOR_TAG ,Markert and Nissim ( 2005 ) ) .', 'It is not easy to measure the accuracy']","['', 'However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. ,#AUTHOR_TAG ,Markert and Nissim ( 2005 ) ) .', '']",0
"['baseline coreference system uses the C4.5 decision tree learner(Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Following previous work ( e.g. ,#AUTHOR_TAG andPonzetto and Strube ( 2006 ) ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 .', '., NP j−1 .', '']","['baseline coreference system uses the C4.5 decision tree learner(Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Following previous work ( e.g. ,#AUTHOR_TAG andPonzetto and Strube ( 2006 ) ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 .', '., NP j−1 .', '']","['baseline coreference system uses the C4.5 decision tree learner(Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Following previous work ( e.g. ,#AUTHOR_TAG andPonzetto and Strube ( 2006 ) ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 .', '., NP j−1 .', '']","['baseline coreference system uses the C4.5 decision tree learner(Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Following previous work ( e.g. ,#AUTHOR_TAG andPonzetto and Strube ( 2006 ) ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 .', '., NP j−1 .', '']",5
"[': We use the DL learner as described inCollins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation( #AUTHOR_TAG ) and NE classification( Collins and Singer , 1999 ) .', '']","[': We use the DL learner as described inCollins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation( #AUTHOR_TAG ) and NE classification( Collins and Singer , 1999 ) .', '']","[': We use the DL learner as described inCollins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation( #AUTHOR_TAG ) and NE classification( Collins and Singer , 1999 ) .', '']","['', 'We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described inCollins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation( #AUTHOR_TAG ) and NE classification( Collins and Singer , 1999 ) .', '']",4
"['researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. ,Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. ,Poesio et al. ( 2004 ) ) or Wikipedia( #AUTHOR_TAG ) , and the contextual role played by an NP ( seeBean and Riloff ( 2004 ) )']","['researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. ,Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. ,Poesio et al. ( 2004 ) ) or Wikipedia( #AUTHOR_TAG ) , and the contextual role played by an NP ( seeBean and Riloff ( 2004 ) )']","['researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. ,Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. ,Poesio et al. ( 2004 ) ) or Wikipedia( #AUTHOR_TAG ) , and the contextual role played by an NP ( seeBean and Riloff ( 2004 ) )']","['', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. ,Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. ,Poesio et al. ( 2004 ) ) or Wikipedia( #AUTHOR_TAG ) , and the contextual role played by an NP ( seeBean and Riloff ( 2004 ) )']",0
"['', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. ,#AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. ,Poesio et al. ( 2004 ) ) or Wikipedia( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( seeBean and Riloff ( 2004 ) )']","['', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. ,#AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. ,Poesio et al. ( 2004 ) ) or Wikipedia( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( seeBean and Riloff ( 2004 ) )']","['', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. ,#AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. ,Poesio et al. ( 2004 ) ) or Wikipedia( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( seeBean and Riloff ( 2004 ) )']","['', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. ,#AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. ,Poesio et al. ( 2004 ) ) or Wikipedia( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( seeBean and Riloff ( 2004 ) )']",0
"['in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see#AUTHOR_TAG a ) ) .', 'Motivated by this observation, we']","['in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see#AUTHOR_TAG a ) ) .', 'Motivated by this observation, we']","['in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see#AUTHOR_TAG a ) ) .', 'Motivated by this observation, we']","['', '( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see#AUTHOR_TAG a ) ) .', 'Motivated by this observation, we']",4
"['-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. ,#AUTHOR_TAG ) .', '']","['first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. ,#AUTHOR_TAG ) .', '']","['the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. ,#AUTHOR_TAG ) .', '']",['#AUTHOR_TAG )'],4
"['feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 ) .', 'Our']","['feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 ) .', 'Our']","['feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 ) .', 'Our']","['', 'Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 ) .', '']",0
"['communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme( #AUTHOR_TAG ) .', 'The MU-']","['communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme( #AUTHOR_TAG ) .', 'The MU-MIN']","['communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme( #AUTHOR_TAG ) .', 'The MU-MIN scheme']","['communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme( #AUTHOR_TAG ) .', 'The MU-MIN scheme is a general framework for the study of gestures in interpersonal communication.', '']",5
"['has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. ( 2009 ) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while#AUTHOR_TAG examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .', '']","['has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. ( 2009 ) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while#AUTHOR_TAG examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .', '']","['has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. ( 2009 ) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while#AUTHOR_TAG examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .', '']","['has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. ( 2009 ) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while#AUTHOR_TAG examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .', '']",0
"['feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) .', 'Our']","['feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) .', 'Our']","['feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) .', 'Our']","['', 'Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) .', '']",0
"['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see#AUTHOR_TAG for an overview ) .', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', '']","['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see#AUTHOR_TAG for an overview ) .', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', '']","['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see#AUTHOR_TAG for an overview ) .', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', '']","['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see#AUTHOR_TAG for an overview ) .', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', '']",0
"['', ""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa( Cohen , 1960 ) 1 and corrected kappa( #AUTHOR_TAG ) 2 ."", '']","['', ""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa( Cohen , 1960 ) 1 and corrected kappa( #AUTHOR_TAG ) 2 ."", '']","['', ""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa( Cohen , 1960 ) 1 and corrected kappa( #AUTHOR_TAG ) 2 ."", '']","['', ""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa( Cohen , 1960 ) 1 and corrected kappa( #AUTHOR_TAG ) 2 ."", '']",5
['pauses and dialogue structure in annotated English map-task dialogues( #AUTHOR_TAG ) and find correlations between the various modalities both within and across speakers'],['pauses and dialogue structure in annotated English map-task dialogues( #AUTHOR_TAG ) and find correlations between the various modalities both within and across speakers'],"['pauses and dialogue structure in annotated English map-task dialogues( #AUTHOR_TAG ) and find correlations between the various modalities both within and across speakers .', '']","['', 'Louwerse et al. ( 2006 ) andLouwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues( #AUTHOR_TAG ) and find correlations between the various modalities both within and across speakers .', '']",0
"['as shown in Table 4.', 'These two sets of data were used for automatic dialogue act classification , which was run in the Weka system( #AUTHOR_TAG ) .', 'We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table .', '']","['as shown in Table 4.', 'These two sets of data were used for automatic dialogue act classification , which was run in the Weka system( #AUTHOR_TAG ) .', 'We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table .', '']","['as shown in Table 4.', 'These two sets of data were used for automatic dialogue act classification , which was run in the Weka system( #AUTHOR_TAG ) .', 'We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table .', '']","['', 'These two sets of data were used for automatic dialogue act classification , which was run in the Weka system( #AUTHOR_TAG ) .', 'We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table .', '']",5
"['has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', '#AUTHOR_TAG obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , whileGravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .', '']","['has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', '#AUTHOR_TAG obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , whileGravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .', '']","['has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', '#AUTHOR_TAG obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , whileGravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .', '']","['has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', '#AUTHOR_TAG obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , whileGravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .', '']",0
"['', ""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa( #AUTHOR_TAG ) 1 and corrected kappa( Brennan and Prediger , 1981 ) 2 ."", '']","['', ""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa( #AUTHOR_TAG ) 1 and corrected kappa( Brennan and Prediger , 1981 ) 2 ."", '']","['', ""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa( #AUTHOR_TAG ) 1 and corrected kappa( Brennan and Prediger , 1981 ) 2 ."", '']","['', ""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa( #AUTHOR_TAG ) 1 and corrected kappa( Brennan and Prediger , 1981 ) 2 ."", '']",5
"['feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 ) .', 'Our']","['feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 ) .', 'Our']","['feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 ) .', 'Our']","['', 'Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 ) .', '']",0
"[', thus making the semantic classification very fine-grained.', ""Table 1 Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see#AUTHOR_TAG , it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent(Fleiss, 1971) ."", '']","['time, thus making the semantic classification very fine-grained.', ""Table 1 Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see#AUTHOR_TAG , it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent(Fleiss, 1971) ."", '']","['an Agree and a TurnElicit at the same time, thus making the semantic classification very fine-grained.', ""Table 1 Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see#AUTHOR_TAG , it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent(Fleiss, 1971) ."", '']","['', 'For example, a yes can be an Answer to a question, an Agree and a TurnElicit at the same time, thus making the semantic classification very fine-grained.', ""Table 1 Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see#AUTHOR_TAG , it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent(Fleiss, 1971) ."", '']",0
"['', 'Related are also the studies by Rieks op denAkker and Schulz ( 2008 ) and#AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']","['', 'Related are also the studies by Rieks op denAkker and Schulz ( 2008 ) and#AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']","['', 'Related are also the studies by Rieks op denAkker and Schulz ( 2008 ) and#AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']","['', 'Related are also the studies by Rieks op denAkker and Schulz ( 2008 ) and#AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']",0
"['authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example ,#AUTHOR_TAG andJokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements , whileReidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .', 'Related are also the']","['authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example ,#AUTHOR_TAG andJokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements , whileReidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .', 'Related are also the']","['authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example ,#AUTHOR_TAG andJokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements , whileReidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .', 'Related are also the studies']","['authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example ,#AUTHOR_TAG andJokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements , whileReidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .', '']",0
"['authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example ,Jokinen and Ragni ( 2007 ) and#AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements , whileReidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .', 'Related']","['authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example ,Jokinen and Ragni ( 2007 ) and#AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements , whileReidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .', 'Related']","['authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example ,Jokinen and Ragni ( 2007 ) and#AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements , whileReidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .', '']","['authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example ,Jokinen and Ragni ( 2007 ) and#AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements , whileReidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .', '']",0
"['thus making the semantic classification very fine-grained.', ""Table 1 Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, seeArtstein and Poesio (2008) , it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent( #AUTHOR_TAG ) ."", '']","['thus making the semantic classification very fine-grained.', ""Table 1 Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, seeArtstein and Poesio (2008) , it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent( #AUTHOR_TAG ) ."", '']","['thus making the semantic classification very fine-grained.', ""Table 1 Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, seeArtstein and Poesio (2008) , it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent( #AUTHOR_TAG ) ."", '']","['', ""Table 1 Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, seeArtstein and Poesio (2008) , it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent( #AUTHOR_TAG ) ."", '']",0
"['ures we also measured agreement on gesture segmentation.', 'The figures obtained are given in Table 3.', 'These results are slightly worse than those obtained in previous studies using the same annotation scheme( #AUTHOR_TAG ) , but are still sat -isfactory given the high number of categories provided by the scheme']","['as it was the case for the annotations on feedback expressions.', 'In the case of gestures we also measured agreement on gesture segmentation.', 'The figures obtained are given in Table 3.', 'These results are slightly worse than those obtained in previous studies using the same annotation scheme( #AUTHOR_TAG ) , but are still sat -isfactory given the high number of categories provided by the scheme']","['ures we also measured agreement on gesture segmentation.', 'The figures obtained are given in Table 3.', 'These results are slightly worse than those obtained in previous studies using the same annotation scheme( #AUTHOR_TAG ) , but are still sat -isfactory given the high number of categories provided by the scheme']","['', 'The annotations of this video were then used to measure inter-coder agreement in ANVIL as it was the case for the annotations on feedback expressions.', 'In the case of gestures we also measured agreement on gesture segmentation.', 'The figures obtained are given in Table 3.', 'These results are slightly worse than those obtained in previous studies using the same annotation scheme( #AUTHOR_TAG ) , but are still sat -isfactory given the high number of categories provided by the scheme']",1
"['', 'The results , which partly confirm those obtained on a smaller dataset in#AUTHOR_TAG , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions']","['', 'The results , which partly confirm those obtained on a smaller dataset in#AUTHOR_TAG , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions']","['', 'The results , which partly confirm those obtained on a smaller dataset in#AUTHOR_TAG , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions .', '']","['', 'The results , which partly confirm those obtained on a smaller dataset in#AUTHOR_TAG , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions .', '']",1
"['', 'The best results on most of our data were obtained using Hidden Naive Bayes ( HNB )( #AUTHOR_TAG ) .', 'Therefore, here we show the results of this classifier.', '']","['', 'The best results on most of our data were obtained using Hidden Naive Bayes ( HNB )( #AUTHOR_TAG ) .', 'Therefore, here we show the results of this classifier.', '']","['', 'The best results on most of our data were obtained using Hidden Naive Bayes ( HNB )( #AUTHOR_TAG ) .', 'Therefore, here we show the results of this classifier.', '']","['', 'The best results on most of our data were obtained using Hidden Naive Bayes ( HNB )( #AUTHOR_TAG ) .', 'Therefore, here we show the results of this classifier.', '']",5
"['', 'For this study, we added semantic labels -including dialogue acts -and gesture annotation.', 'Both kinds of annotation were carried out using ANVIL( #AUTHOR_TAG ) .', '']","['', 'For this study, we added semantic labels -including dialogue acts -and gesture annotation.', 'Both kinds of annotation were carried out using ANVIL( #AUTHOR_TAG ) .', '']","['', 'For this study, we added semantic labels -including dialogue acts -and gesture annotation.', 'Both kinds of annotation were carried out using ANVIL( #AUTHOR_TAG ) .', '']","['', 'For this study, we added semantic labels -including dialogue acts -and gesture annotation.', 'Both kinds of annotation were carried out using ANVIL( #AUTHOR_TAG ) .', '']",5
"[', for more information see Gr�nnum (2006).', 'The Praat tool was used( #AUTHOR_TAG )']","['version was produced with the supervision of an expert annotator, for more information see Gr�nnum (2006).', 'The Praat tool was used( #AUTHOR_TAG )']","['version was produced with the supervision of an expert annotator, for more information see Gr�nnum (2006).', 'The Praat tool was used( #AUTHOR_TAG )']","['onetic and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see Gr�nnum (2006).', 'The Praat tool was used( #AUTHOR_TAG )']",5
"['knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""The diagnoser , based on#AUTHOR_TAG b ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer ."", '']","['knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""The diagnoser , based on#AUTHOR_TAG b ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer ."", '']","['knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""The diagnoser , based on#AUTHOR_TAG b ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer ."", '']","['knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""The diagnoser , based on#AUTHOR_TAG b ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer ."", '']",2
"['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', '']",0
"['strategy decision , and a FUF/SURGE( #AUTHOR_TAG ) generation system to produce the appropriate text']","['strategy decision , and a FUF/SURGE( #AUTHOR_TAG ) generation system to produce the appropriate text']","['the strategy decision , and a FUF/SURGE( #AUTHOR_TAG ) generation system to produce the appropriate text .', '']","['', 'The system uses a domain-specific content planner to produce input to the surface realizer based on the strategy decision , and a FUF/SURGE( #AUTHOR_TAG ) generation system to produce the appropriate text .', '']",5
"['', 'Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain( #AUTHOR_TAG ) .', 'Using a deep generator to automatically generate system feedback gives us a level of control over the output and will']","['', 'Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain( #AUTHOR_TAG ) .', 'Using a deep generator to automatically generate system feedback gives us a level of control over the output and will']","['', 'Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain( #AUTHOR_TAG ) .', 'Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow']","['', 'Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain( #AUTHOR_TAG ) .', 'Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail']",3
"['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', '']",0
"['action between components is coordinated by the dialogue manager which uses the informationstate approach( #AUTHOR_TAG ) .', 'The dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not-yet-mentioned parts of the answer.', 'Once the complete answer has been accumulated, the system accepts it and moves on.', 'Tutor hints can contribute']","['action between components is coordinated by the dialogue manager which uses the informationstate approach( #AUTHOR_TAG ) .', 'The dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not-yet-mentioned parts of the answer.', 'Once the complete answer has been accumulated, the system accepts it and moves on.', 'Tutor hints can contribute']","['action between components is coordinated by the dialogue manager which uses the informationstate approach( #AUTHOR_TAG ) .', 'The dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not-yet-mentioned parts of the answer.', 'Once the complete answer has been accumulated, the system accepts it and moves on.', 'Tutor hints can contribute']","['action between components is coordinated by the dialogue manager which uses the informationstate approach( #AUTHOR_TAG ) .', 'The dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not-yet-mentioned parts of the answer.', 'Once the complete answer has been accumulated, the system accepts it and moves on.', 'Tutor hints can contribute parts of the answer to the cumulative state as well, allowing the system to jointly construct the solution with the student']",5
"['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', '']",0
"['key issue we are planning to investigate is that of linguistic alignment.', ""The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed( #AUTHOR_TAG ) ."", 'Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain(Ward and Litman, 2006)']","['key issue we are planning to investigate is that of linguistic alignment.', ""The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed( #AUTHOR_TAG ) ."", 'Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain(Ward and Litman, 2006)']","['the key issue we are planning to investigate is that of linguistic alignment.', ""The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed( #AUTHOR_TAG ) ."", 'Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain(Ward and Litman, 2006) .', '']","['dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.', ""The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed( #AUTHOR_TAG ) ."", 'Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain(Ward and Litman, 2006) .', '']",3
"['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', '']",0
"['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', '']",0
"['1 In addition to a remediation policy , the tutorial planner implements an error recovery policy( #AUTHOR_TAG ) .', '']","['.1 In addition to a remediation policy , the tutorial planner implements an error recovery policy( #AUTHOR_TAG ) .', '']","['1 In addition to a remediation policy , the tutorial planner implements an error recovery policy( #AUTHOR_TAG ) .', '']","['tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy( #AUTHOR_TAG ) .', '']",5
"[""contextual interpreter then uses a reference resolution approach similar to#AUTHOR_TAG , and an ontology mapping mechanism( Dzikovska et al. , 2008 a ) to produce a domain-specific semantic representation of the student 's output""]","[""contextual interpreter then uses a reference resolution approach similar to#AUTHOR_TAG , and an ontology mapping mechanism( Dzikovska et al. , 2008 a ) to produce a domain-specific semantic representation of the student 's output""]","['a domain-independent semantic representation including high-level word senses and semantic role labels.', ""The contextual interpreter then uses a reference resolution approach similar to#AUTHOR_TAG , and an ontology mapping mechanism( Dzikovska et al. , 2008 a ) to produce a domain-specific semantic representation of the student 's output ."", '']","['use the TRIPS dialogue parser(Allen et al., 2007) to parse the utterances.', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', ""The contextual interpreter then uses a reference resolution approach similar to#AUTHOR_TAG , and an ontology mapping mechanism( Dzikovska et al. , 2008 a ) to produce a domain-specific semantic representation of the student 's output ."", '']",1
"['BEETLE II system architecture is designed to overcome these limitations( #AUTHOR_TAG ) .', 'It uses a deep parser and generator, together with a domain reasoner and a diagnoser, to produce detailed analyses of student utterances and generate feedback automatically.', 'This allows the system to consistently apply the same tutorial policy across a range of questions.', '']","['BEETLE II system architecture is designed to overcome these limitations( #AUTHOR_TAG ) .', 'It uses a deep parser and generator, together with a domain reasoner and a diagnoser, to produce detailed analyses of student utterances and generate feedback automatically.', 'This allows the system to consistently apply the same tutorial policy across a range of questions.', '']","['BEETLE II system architecture is designed to overcome these limitations( #AUTHOR_TAG ) .', 'It uses a deep parser and generator, together with a domain reasoner and a diagnoser, to produce detailed analyses of student utterances and generate feedback automatically.', 'This allows the system to consistently apply the same tutorial policy across a range of questions.', '']","['BEETLE II system architecture is designed to overcome these limitations( #AUTHOR_TAG ) .', 'It uses a deep parser and generator, together with a domain reasoner and a diagnoser, to produce detailed analyses of student utterances and generate feedback automatically.', 'This allows the system to consistently apply the same tutorial policy across a range of questions.', 'To some extent, this comes at the expense of being able to address individual student misconceptions.', '']",0
"['', 'At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to#AUTHOR_TAG']","['', 'At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to#AUTHOR_TAG']","['', 'At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to#AUTHOR_TAG']","['', 'At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to#AUTHOR_TAG']",3
"['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', '']",0
"['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', '']",0
"[""and an ontology mapping mechanism( #AUTHOR_TAG a ) to produce a domain-specific semantic representation of the student 's output ."", '']","[""and an ontology mapping mechanism( #AUTHOR_TAG a ) to produce a domain-specific semantic representation of the student 's output ."", '']","[""and an ontology mapping mechanism( #AUTHOR_TAG a ) to produce a domain-specific semantic representation of the student 's output ."", '']","['', ""The contextual interpreter then uses a reference resolution approach similar toByron ( 2002 ) , and an ontology mapping mechanism( #AUTHOR_TAG a ) to produce a domain-specific semantic representation of the student 's output ."", '']",5
['factors such as student confidence could be considered as well( #AUTHOR_TAG )'],['factors such as student confidence could be considered as well( #AUTHOR_TAG )'],['factors such as student confidence could be considered as well( #AUTHOR_TAG )'],['factors such as student confidence could be considered as well( #AUTHOR_TAG )'],3
"['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', '']",0
"['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', '']",0
"['', 'Our recovery policy is modeled on the TargetedHelp( #AUTHOR_TAG ) policy used in task-oriented dialogue .', '']","['', 'Our recovery policy is modeled on the TargetedHelp( #AUTHOR_TAG ) policy used in task-oriented dialogue .', '']","['', 'Our recovery policy is modeled on the TargetedHelp( #AUTHOR_TAG ) policy used in task-oriented dialogue .', '']","['', 'Our recovery policy is modeled on the TargetedHelp( #AUTHOR_TAG ) policy used in task-oriented dialogue .', '']",2
"['use the TRIPS dialogue parser( #AUTHOR_TAG ) to parse the utterances .', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', '']","['use the TRIPS dialogue parser( #AUTHOR_TAG ) to parse the utterances .', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', '']","['use the TRIPS dialogue parser( #AUTHOR_TAG ) to parse the utterances .', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', '']","['use the TRIPS dialogue parser( #AUTHOR_TAG ) to parse the utterances .', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', '']",5
"['system uses a knowledge base implemented in the KM representation language( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .', 'At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits']","['system uses a knowledge base implemented in the KM representation language( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .', 'At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits']","['system uses a knowledge base implemented in the KM representation language( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .', 'At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits']","['system uses a knowledge base implemented in the KM representation language( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .', 'At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits']",5
"['current approaches to monolingual TE , either syntactically oriented( Rus et al. , 2005 ) , or applying logical inference( Tatu and Moldovan , 2005 ) , or adopting transformation-based techniques( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference']","['current approaches to monolingual TE , either syntactically oriented( Rus et al. , 2005 ) , or applying logical inference( Tatu and Moldovan , 2005 ) , or adopting transformation-based techniques( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference']","['current approaches to monolingual TE , either syntactically oriented( Rus et al. , 2005 ) , or applying logical inference( Tatu and Moldovan , 2005 ) , or adopting transformation-based techniques( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .', '']","['current approaches to monolingual TE , either syntactically oriented( Rus et al. , 2005 ) , or applying logical inference( Tatu and Moldovan , 2005 ) , or adopting transformation-based techniques( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .', '']",0
"['rase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language( #AUTHOR_TAG ) .', 'There are several methods to build phrase tables.', '']","['rase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language( #AUTHOR_TAG ) .', 'There are several methods to build phrase tables.', '']","['rase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language( #AUTHOR_TAG ) .', 'There are several methods to build phrase tables.', '']","['rase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language( #AUTHOR_TAG ) .', 'There are several methods to build phrase tables.', '']",0
"[', DIRT( #AUTHOR_TAG ) , VerbOcean( Chklovski and Pantel , 2004 ) , FrameNet( Baker et al. , 1998 ) , and Wikipedia( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs']","[', DIRT( #AUTHOR_TAG ) , VerbOcean( Chklovski and Pantel , 2004 ) , FrameNet( Baker et al. , 1998 ) , and Wikipedia( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs,']","[', DIRT( #AUTHOR_TAG ) , VerbOcean( Chklovski and Pantel , 2004 ) , FrameNet( Baker et al. , 1998 ) , and Wikipedia( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs']","[', DIRT( #AUTHOR_TAG ) , VerbOcean( Chklovski and Pantel , 2004 ) , FrameNet( Baker et al. , 1998 ) , and Wikipedia( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are fre- quently used as a source of precise entailment rules between predicates.', '']",0
"['current approaches to monolingual TE , either syntactically oriented( Rus et al. , 2005 ) , or applying logical inference ( Tatu and#AUTHOR_TAG ) , or adopting transformation-based techniques( Kouleykov and Magnini , 2005 ;  Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .', '']","['current approaches to monolingual TE , either syntactically oriented( Rus et al. , 2005 ) , or applying logical inference ( Tatu and#AUTHOR_TAG ) , or adopting transformation-based techniques( Kouleykov and Magnini , 2005 ;  Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .', '']","['current approaches to monolingual TE , either syntactically oriented( Rus et al. , 2005 ) , or applying logical inference ( Tatu and#AUTHOR_TAG ) , or adopting transformation-based techniques( Kouleykov and Magnini , 2005 ;  Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .', '']","['current approaches to monolingual TE , either syntactically oriented( Rus et al. , 2005 ) , or applying logical inference ( Tatu and#AUTHOR_TAG ) , or adopting transformation-based techniques( Kouleykov and Magnini , 2005 ;  Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .', '']",0
"['', 'Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by( #AUTHOR_TAG ) .', ""The method relies on translation-validation cycles, defined as separate jobs routed to MTurk's workforce."", 'Translation jobs return one Spanish version for each hypothesis.', 'Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference.', '']","['', 'Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by( #AUTHOR_TAG ) .', ""The method relies on translation-validation cycles, defined as separate jobs routed to MTurk's workforce."", 'Translation jobs return one Spanish version for each hypothesis.', 'Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference.', '']","['', 'Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by( #AUTHOR_TAG ) .', ""The method relies on translation-validation cycles, defined as separate jobs routed to MTurk's workforce."", 'Translation jobs return one Spanish version for each hypothesis.', 'Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference.', '']","['', 'Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by( #AUTHOR_TAG ) .', ""The method relies on translation-validation cycles, defined as separate jobs routed to MTurk's workforce."", 'Translation jobs return one Spanish version for each hypothesis.', 'Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference.', 'At each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the CLTE corpus, while wrong translations are sent back to workers in a new translation job.', '']",5
"['-lingual Textual Entailment ( CLTE ) has been proposed by( #AUTHOR_TAG ) as an extension of Textual Entailment( Dagan and Glickman , 2004 ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .', 'The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', '']","['-lingual Textual Entailment ( CLTE ) has been proposed by( #AUTHOR_TAG ) as an extension of Textual Entailment( Dagan and Glickman , 2004 ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .', 'The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', '']","['-lingual Textual Entailment ( CLTE ) has been proposed by( #AUTHOR_TAG ) as an extension of Textual Entailment( Dagan and Glickman , 2004 ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .', 'The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', '']","['-lingual Textual Entailment ( CLTE ) has been proposed by( #AUTHOR_TAG ) as an extension of Textual Entailment( Dagan and Glickman , 2004 ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .', 'The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', '']",0
"['system.', 'Using the basic solution proposed by( #AUTHOR_TAG ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions']","['system.', 'Using the basic solution proposed by( #AUTHOR_TAG ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions']","['of any TE system.', 'Using the basic solution proposed by( #AUTHOR_TAG ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions']","['', 'Using the basic solution proposed by( #AUTHOR_TAG ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions']",1
"['tokenization , and used the Giza + +( #AUTHOR_TAG ) to align the tokenized corpora at the word level .', '']","['tokenization , and used the Giza + +( #AUTHOR_TAG ) to align the tokenized corpora at the word level .', '']","['tokenization , and used the Giza + +( #AUTHOR_TAG ) to align the tokenized corpora at the word level .', '']","['', 'We run TreeTagger( Schmid , 1994 ) for tokenization , and used the Giza + +( #AUTHOR_TAG ) to align the tokenized corpora at the word level .', '']",5
"['aphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation( Iordanskaja et al. , 1991 ) , multidocument summarization( #AUTHOR_TAG ) , automatic evaluation of MT( Denkowski and Lavie , 2010 ) , and TE( Dinu and Wang , 2009 )']","['aphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation( Iordanskaja et al. , 1991 ) , multidocument summarization( #AUTHOR_TAG ) , automatic evaluation of MT( Denkowski and Lavie , 2010 ) , and TE( Dinu and Wang , 2009 )']","['aphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation( Iordanskaja et al. , 1991 ) , multidocument summarization( #AUTHOR_TAG ) , automatic evaluation of MT( Denkowski and Lavie , 2010 ) , and TE( Dinu and Wang , 2009 ) .', '']","['aphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation( Iordanskaja et al. , 1991 ) , multidocument summarization( #AUTHOR_TAG ) , automatic evaluation of MT( Denkowski and Lavie , 2010 ) , and TE( Dinu and Wang , 2009 ) .', '']",4
"['ablation tests reported in(Bentivogli et al., 2010) , even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'Some previous works( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words']","['ablation tests reported in(Bentivogli et al., 2010) , even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'Some previous works( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words']","['the ablation tests reported in(Bentivogli et al., 2010) , even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'Some previous works( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words']","['the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'As emerges from the ablation tests reported in(Bentivogli et al., 2010) , even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'Some previous works( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words']",0
"['combine the phrasal matching scores obtained at each n-gram level , and optimize their relative weights , we trained a Support Vector Machine classifier , SVMlight( #AUTHOR_TAG ) , using each score as a feature']","['combine the phrasal matching scores obtained at each n-gram level , and optimize their relative weights , we trained a Support Vector Machine classifier , SVMlight( #AUTHOR_TAG ) , using each score as a feature']","['combine the phrasal matching scores obtained at each n-gram level , and optimize their relative weights , we trained a Support Vector Machine classifier , SVMlight( #AUTHOR_TAG ) , using each score as a feature']","['combine the phrasal matching scores obtained at each n-gram level , and optimize their relative weights , we trained a Support Vector Machine classifier , SVMlight( #AUTHOR_TAG ) , using each score as a feature']",5
"['aphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation( #AUTHOR_TAG ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT( Denkowski and Lavie , 2010 ) , and TE( Dinu and Wang , 2009 )']","['aphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation( #AUTHOR_TAG ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT( Denkowski and Lavie , 2010 ) , and TE( Dinu and Wang , 2009 )']","['aphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation( #AUTHOR_TAG ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT( Denkowski and Lavie , 2010 ) , and TE( Dinu and Wang , 2009 ) .', '']","['aphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation( #AUTHOR_TAG ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT( Denkowski and Lavie , 2010 ) , and TE( Dinu and Wang , 2009 ) .', '']",4
"['WordNet, the RTE literature documents the use of a variety of lexical information sources(Bentivogli et al., 2010; Dagan et al., 2009) .', 'These include, just to mention the most popular ones , DIRT( Lin and Pantel , 2001 ) , VerbOcean( #AUTHOR_TAG ) , FrameNet( Baker et al. , 1998 ) , and Wikipedia( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical']","['WordNet, the RTE literature documents the use of a variety of lexical information sources(Bentivogli et al., 2010; Dagan et al., 2009) .', 'These include, just to mention the most popular ones , DIRT( Lin and Pantel , 2001 ) , VerbOcean( #AUTHOR_TAG ) , FrameNet( Baker et al. , 1998 ) , and Wikipedia( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical']","['WordNet, the RTE literature documents the use of a variety of lexical information sources(Bentivogli et al., 2010; Dagan et al., 2009) .', 'These include, just to mention the most popular ones , DIRT( Lin and Pantel , 2001 ) , VerbOcean( #AUTHOR_TAG ) , FrameNet( Baker et al. , 1998 ) , and Wikipedia( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lex']","['WordNet, the RTE literature documents the use of a variety of lexical information sources(Bentivogli et al., 2010; Dagan et al., 2009) .', 'These include, just to mention the most popular ones , DIRT( Lin and Pantel , 2001 ) , VerbOcean( #AUTHOR_TAG ) , FrameNet( Baker et al. , 1998 ) , and Wikipedia( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', '']",0
"['', 'Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet( #AUTHOR_TAG ) ) have been created for several languages , with different degrees of coverage .', 'As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet�s synsets, thus making the coverage issue even more problematic than for TE.', '']","['', 'Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet( #AUTHOR_TAG ) ) have been created for several languages , with different degrees of coverage .', 'As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet�s synsets, thus making the coverage issue even more problematic than for TE.', '']","['', 'Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet( #AUTHOR_TAG ) ) have been created for several languages , with different degrees of coverage .', 'As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet�s synsets, thus making the coverage issue even more problematic than for TE.', '']","['', 'Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet( #AUTHOR_TAG ) ) have been created for several languages , with different degrees of coverage .', 'As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet�s synsets, thus making the coverage issue even more problematic than for TE.', '']",0
"['', 'They proved to be useful in a number of NLP applications such as natural language generation( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT( Denkowski and Lavie , 2010 ) , and TE( #AUTHOR_TAG ) .', '']","['the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT( Denkowski and Lavie , 2010 ) , and TE( #AUTHOR_TAG ) .', '']","['', 'They proved to be useful in a number of NLP applications such as natural language generation( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT( Denkowski and Lavie , 2010 ) , and TE( #AUTHOR_TAG ) .', '']","['aphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT( Denkowski and Lavie , 2010 ) , and TE( #AUTHOR_TAG ) .', '']",4
"['', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus( #AUTHOR_TAG ) .', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', '']","['', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus( #AUTHOR_TAG ) .', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', '']","['', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus( #AUTHOR_TAG ) .', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', '']","['', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus( #AUTHOR_TAG ) .', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', '']",0
"['WordNet, the RTE literature documents the use of a variety of lexical information sources(Bentivogli et al., 2010;Dagan et al., 2009) .', 'These include, just to mention the most popular ones , DIRT( Lin and Pantel , 2001 ) , VerbOcean( Chklovski and Pantel , 2004 ) , FrameNet( Baker et al. , 1998 ) , and Wikipedia( #AUTHOR_TAG ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet']","['WordNet, the RTE literature documents the use of a variety of lexical information sources(Bentivogli et al., 2010;Dagan et al., 2009) .', 'These include, just to mention the most popular ones , DIRT( Lin and Pantel , 2001 ) , VerbOcean( Chklovski and Pantel , 2004 ) , FrameNet( Baker et al. , 1998 ) , and Wikipedia( #AUTHOR_TAG ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet']","['WordNet, the RTE literature documents the use of a variety of lexical information sources(Bentivogli et al., 2010;Dagan et al., 2009) .', 'These include, just to mention the most popular ones , DIRT( Lin and Pantel , 2001 ) , VerbOcean( Chklovski and Pantel , 2004 ) , FrameNet( Baker et al. , 1998 ) , and Wikipedia( #AUTHOR_TAG ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet']","['WordNet, the RTE literature documents the use of a variety of lexical information sources(Bentivogli et al., 2010;Dagan et al., 2009) .', 'These include, just to mention the most popular ones , DIRT( Lin and Pantel , 2001 ) , VerbOcean( Chklovski and Pantel , 2004 ) , FrameNet( Baker et al. , 1998 ) , and Wikipedia( #AUTHOR_TAG ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', '']",0
"['-lingual Textual Entailment ( CLTE ) has been proposed by( Mehdad et al. , 2010 ) as an extension of Textual Entailment( #AUTHOR_TAG ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .', 'The task is inherently difficult']","['-lingual Textual Entailment ( CLTE ) has been proposed by( Mehdad et al. , 2010 ) as an extension of Textual Entailment( #AUTHOR_TAG ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .', 'The task is inherently difficult,']","['-lingual Textual Entailment ( CLTE ) has been proposed by( Mehdad et al. , 2010 ) as an extension of Textual Entailment( #AUTHOR_TAG ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .', 'The task is inherently difficult']","['-lingual Textual Entailment ( CLTE ) has been proposed by( Mehdad et al. , 2010 ) as an extension of Textual Entailment( #AUTHOR_TAG ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .', 'The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', '']",0
"['WordNet, the RTE literature documents the use of a variety of lexical information sources(Bentivogli et al., 2010; Dagan et al., 2009) .', 'These include, just to mention the most popular ones , DIRT( Lin and Pantel , 2001 ) , VerbOcean( Chklovski and Pantel , 2004 ) , FrameNet( #AUTHOR_TAG ) , and Wikipedia( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is']","['WordNet, the RTE literature documents the use of a variety of lexical information sources(Bentivogli et al., 2010; Dagan et al., 2009) .', 'These include, just to mention the most popular ones , DIRT( Lin and Pantel , 2001 ) , VerbOcean( Chklovski and Pantel , 2004 ) , FrameNet( #AUTHOR_TAG ) , and Wikipedia( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is']","['WordNet, the RTE literature documents the use of a variety of lexical information sources(Bentivogli et al., 2010; Dagan et al., 2009) .', 'These include, just to mention the most popular ones , DIRT( Lin and Pantel , 2001 ) , VerbOcean( Chklovski and Pantel , 2004 ) , FrameNet( #AUTHOR_TAG ) , and Wikipedia( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is often']","['WordNet, the RTE literature documents the use of a variety of lexical information sources(Bentivogli et al., 2010; Dagan et al., 2009) .', 'These include, just to mention the most popular ones , DIRT( Lin and Pantel , 2001 ) , VerbOcean( Chklovski and Pantel , 2004 ) , FrameNet( #AUTHOR_TAG ) , and Wikipedia( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', '']",0
"['', 'After the extraction , pruning techniques( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases']","['all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction , pruning techniques( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases']","['', 'After the extraction , pruning techniques( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases']","['', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction , pruning techniques( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases']",0
"['aphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT( #AUTHOR_TAG ) , and TE( Dinu and Wang , 2009 )']","['aphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT( #AUTHOR_TAG ) , and TE( Dinu and Wang , 2009 )']","['aphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT( #AUTHOR_TAG ) , and TE( Dinu and Wang , 2009 ) .', '']","['aphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT( #AUTHOR_TAG ) , and TE( Dinu and Wang , 2009 ) .', '']",4
"['ablation tests reported in(Bentivogli et al., 2010) , even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'Some previous works( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words']","['ablation tests reported in(Bentivogli et al., 2010) , even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'Some previous works( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words']","['the ablation tests reported in(Bentivogli et al., 2010) , even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'Some previous works( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words']","['the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'As emerges from the ablation tests reported in(Bentivogli et al., 2010) , even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'Some previous works( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words']",0
"[""the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by( #AUTHOR_TAG ) ."", 'Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of']","[""the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by( #AUTHOR_TAG ) ."", 'Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of']","[""the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by( #AUTHOR_TAG ) ."", 'Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of']","[""the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by( #AUTHOR_TAG ) ."", '']",1
"['its use for TE and CLTE.', 'On one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by( #AUTHOR_TAG ) .', '']","['its use for TE and CLTE.', 'On one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by( #AUTHOR_TAG ) .', '']","['its use for TE and CLTE.', 'On one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by( #AUTHOR_TAG ) .', 'Another interesting direction is to investigate the potential of paraphrase patterns (i.e.', 'patterns including']","['future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE.', 'On one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by( #AUTHOR_TAG ) .', 'Another interesting direction is to investigate the potential of paraphrase patterns (i.e.', '']",3
"['(WIKI).', 'We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .', '']","['(WIKI).', 'We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .', '']","['(WIKI).', 'We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .', '']","['(WIKI).', 'We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .', '']",5
"['.', 'Second , in line with the findings of( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .', '63.50%).', 'Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%).', '']","['table.', 'Second , in line with the findings of( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .', '63.50%).', 'Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%).', '']","['', 'Second , in line with the findings of( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .', '63.50%).', 'Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%).', '']","['', 'This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table.', 'Second , in line with the findings of( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .', '63.50%).', 'Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%).', '']",1
"['sequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit( #AUTHOR_TAG ) .', '']","['', 'Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit( #AUTHOR_TAG ) .', '']","['sequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit( #AUTHOR_TAG ) .', '']","['', 'Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit( #AUTHOR_TAG ) .', '']",5
"['WordNet , the RTE literature documents the use of a variety of lexical information sources( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .', 'These include, just to mention the most popular ones, DIRT(Lin and Pantel, 2001) , VerbOcean(Chklovski and Pantel, 2004) , FrameNet(Baker et al., 1998) , andWikipedia Kouylekov et al., 2009) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a']","['WordNet , the RTE literature documents the use of a variety of lexical information sources( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .', 'These include, just to mention the most popular ones, DIRT(Lin and Pantel, 2001) , VerbOcean(Chklovski and Pantel, 2004) , FrameNet(Baker et al., 1998) , andWikipedia Kouylekov et al., 2009) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a']","['WordNet , the RTE literature documents the use of a variety of lexical information sources( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .', 'These include, just to mention the most popular ones, DIRT(Lin and Pantel, 2001) , VerbOcean(Chklovski and Pantel, 2004) , FrameNet(Baker et al., 1998) , andWikipedia Kouylekov et al., 2009) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is']","['WordNet , the RTE literature documents the use of a variety of lexical information sources( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .', 'These include, just to mention the most popular ones, DIRT(Lin and Pantel, 2001) , VerbOcean(Chklovski and Pantel, 2004) , FrameNet(Baker et al., 1998) , andWikipedia Kouylekov et al., 2009) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', '']",0
"['questions are typically answered by designing appropriate priming experiments( #AUTHOR_TAG ) or other lexical decision tasks .', 'The reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain.', '(See Sec. 2 for models of morphological organization and access and related experiments']","['questions are typically answered by designing appropriate priming experiments( #AUTHOR_TAG ) or other lexical decision tasks .', 'The reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain.', '(See Sec. 2 for models of morphological organization and access and related experiments']","['questions are typically answered by designing appropriate priming experiments( #AUTHOR_TAG ) or other lexical decision tasks .', 'The reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain.', '(See Sec. 2 for models of morphological organization and access and related experiments']","['questions are typically answered by designing appropriate priming experiments( #AUTHOR_TAG ) or other lexical decision tasks .', 'The reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain.', '(See Sec. 2 for models of morphological organization and access and related experiments']",0
"['', 'We measure the inter annotator agreement using the Fleiss Kappa( #AUTHOR_TAG ) measure ( x ) where the agreement lies around 0.79 .', 'Next']","['', 'We measure the inter annotator agreement using the Fleiss Kappa( #AUTHOR_TAG ) measure ( x ) where the agreement lies around 0.79 .', 'Next,']","['', 'We measure the inter annotator agreement using the Fleiss Kappa( #AUTHOR_TAG ) measure ( x ) where the agreement lies around 0.79 .', 'Next']","['', 'We measure the inter annotator agreement using the Fleiss Kappa( #AUTHOR_TAG ) measure ( x ) where the agreement lies around 0.79 .', 'Next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 V1+V2 pairs and presented them to 36 native Bangla speakers.', 'We ask each subjects to give a']",5
"['to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by#AUTHOR_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints .', '']","['to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by#AUTHOR_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints .', '']","['to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by#AUTHOR_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints .', '']","['', 'Bashir (1993) tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by#AUTHOR_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints .', '']",0
"['order to validate such claim we perform a lexical', 'decision experiment using native Bangla speakers with 92 different verb sequences. We followed the same experimental procedure as discussed in( #AUTHOR_TAG ) for English polymorphemic words .']","['order to validate such claim we perform a lexical', 'decision experiment using native Bangla speakers with 92 different verb sequences. We followed the same experimental procedure as discussed in( #AUTHOR_TAG ) for English polymorphemic words .']","['order to validate such claim we perform a lexical', 'decision experiment using native Bangla speakers with 92 different verb sequences. We followed the same experimental procedure as discussed in( #AUTHOR_TAG ) for English polymorphemic words .']","['', 'to a single expression of meaning. In order to validate such claim we perform a lexical', 'decision experiment using native Bangla speakers with 92 different verb sequences. We followed the same experimental procedure as discussed in( #AUTHOR_TAG ) for English polymorphemic words . However, rather than derived', '']",5
"['', '#AUTHOR_TAG tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', '']","['', '#AUTHOR_TAG tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', '']","['', '#AUTHOR_TAG tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', '']","['', '#AUTHOR_TAG tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', '']",0
"['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  Taft 1975 ;#AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose .', '']","['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  Taft 1975 ;#AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose .', '']","['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  Taft 1975 ;#AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose .', '']","['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  Taft 1975 ;#AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose .', '']",0
"['the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.', 'Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model.', 'The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon( #AUTHOR_TAG ; Butterworth , 1983 ) .', '']","['the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.', 'Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model.', 'The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon( #AUTHOR_TAG ; Butterworth , 1983 ) .', '']","['the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.', 'Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model.', 'The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon( #AUTHOR_TAG ; Butterworth , 1983 ) .', '']","['the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.', 'Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model.', 'The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon( #AUTHOR_TAG ; Butterworth , 1983 ) .', 'On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.', '']",0
"['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', '']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', '']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', '']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', '']",0
"['.', '#AUTHOR_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure .', '']","['', '#AUTHOR_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure .', '']","['', '#AUTHOR_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure .', '']","['', '#AUTHOR_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure .', '']",0
"['respect to this , we apply the different priming and other lexical decision experiments , described in literature( #AUTHOR_TAG ; Bentin , S. andFeldman , 1990 ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla .', '']","['respect to this , we apply the different priming and other lexical decision experiments , described in literature( #AUTHOR_TAG ; Bentin , S. andFeldman , 1990 ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla .', '']","['respect to this , we apply the different priming and other lexical decision experiments , described in literature( #AUTHOR_TAG ; Bentin , S. andFeldman , 1990 ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla .', '']","['respect to this , we apply the different priming and other lexical decision experiments , described in literature( #AUTHOR_TAG ; Bentin , S. andFeldman , 1990 ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla .', '']",5
"['apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .', '']","['apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .', 'Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming).', 'The']","['apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .', '']","['apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .', 'Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming).', '']",5
"['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; Grainger , et al. , 1991 ;Drews and Zwitserlood , 1995 ) .', '']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; Grainger , et al. , 1991 ;Drews and Zwitserlood , 1995 ) .', '']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; Grainger , et al. , 1991 ;Drews and Zwitserlood , 1995 ) .', '']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; Grainger , et al. , 1991 ;Drews and Zwitserlood , 1995 ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', '']",0
"['apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .', '']","['apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .', 'Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming).', 'The']","['apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .', '']","['apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .', 'Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming).', '']",5
"['', 'Their computational significance arises from the issue of their storage in lexical resources like WordNet( #AUTHOR_TAG ) and raises the questions like , how to store morphologically complex words , in a lexical resource like WordNet keeping in mind the storage and access efficiency']","['', 'Their computational significance arises from the issue of their storage in lexical resources like WordNet( #AUTHOR_TAG ) and raises the questions like , how to store morphologically complex words , in a lexical resource like WordNet keeping in mind the storage and access efficiency']","['', 'Their computational significance arises from the issue of their storage in lexical resources like WordNet( #AUTHOR_TAG ) and raises the questions like , how to store morphologically complex words , in a lexical resource like WordNet keeping in mind the storage and access efficiency']","['', 'Their computational significance arises from the issue of their storage in lexical resources like WordNet( #AUTHOR_TAG ) and raises the questions like , how to store morphologically complex words , in a lexical resource like WordNet keeping in mind the storage and access efficiency']",0
"['', 'On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent( #AUTHOR_TAG ) .', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages']","['', 'On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent( #AUTHOR_TAG ) .', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages']","['', 'On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent( #AUTHOR_TAG ) .', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages']","['', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent( #AUTHOR_TAG ) .', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages']",0
"['', 'Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.', 'For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model( #AUTHOR_TAG )']","['', 'Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.', 'For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model( #AUTHOR_TAG )']","['', 'Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.', 'For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model( #AUTHOR_TAG )']","['', 'Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.', 'For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model( #AUTHOR_TAG )']",0
"['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', '#AUTHOR_TAG considers the second verb V2 as an aspectual complex comparable to the auxiliaries .', '']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', '#AUTHOR_TAG considers the second verb V2 as an aspectual complex comparable to the auxiliaries .', '']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', '#AUTHOR_TAG considers the second verb V2 as an aspectual complex comparable to the auxiliaries .', '']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', '#AUTHOR_TAG considers the second verb V2 as an aspectual complex comparable to the auxiliaries .', '']",0
"['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  Taft 1975 ;Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .', '']","['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  Taft 1975 ;Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .', '']","['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  Taft 1975 ;Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .', '']","['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  Taft 1975 ;Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .', '']",0
"['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ;  Grainger , et al. , 1991 ;#AUTHOR_TAG ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ;  Grainger , et al. , 1991 ;#AUTHOR_TAG ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', '']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ;  Grainger , et al. , 1991 ;#AUTHOR_TAG ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ;  Grainger , et al. , 1991 ;#AUTHOR_TAG ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', '']",0
"['', 'Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based( #AUTHOR_TAG c ) .', 'The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in']","['', 'Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based( #AUTHOR_TAG c ) .', 'The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in']","['', 'Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based( #AUTHOR_TAG c ) .', 'The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in']","['', 'Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based( #AUTHOR_TAG c ) .', '']",0
"['the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation( Brown et al. , 1988 ; #AUTHOR_TAG ; Brown et al. , 1993 a ) .', '']","['the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation( Brown et al. , 1988 ; #AUTHOR_TAG ; Brown et al. , 1993 a ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', '']","['the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation( Brown et al. , 1988 ; #AUTHOR_TAG ; Brown et al. , 1993 a ) .', '']","['the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation( Brown et al. , 1988 ; #AUTHOR_TAG ; Brown et al. , 1993 a ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', '']",0
"['.', ""Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates ''( #AUTHOR_TAG ) ."", 'Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based(Melamed, 1996']","['uk.', ""Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates ''( #AUTHOR_TAG ) ."", 'Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based(Melamed, 1996']","['.', ""Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates ''( #AUTHOR_TAG ) ."", 'Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based']","['', ""Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates ''( #AUTHOR_TAG ) ."", 'Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based(Melamed, 1996 c).', '']",0
"[', certain machine-assisted translation tools (e.g.( #AUTHOR_TAG ; Melamed , 1996 b ) ), concordancing for bilingual lexicography(Catizone et al., 1993; Gale & Church, 1991) , computer- assisted language learning, corpus linguistics (Melby.']","[', certain machine-assisted translation tools (e.g.( #AUTHOR_TAG ; Melamed , 1996 b ) ), concordancing for bilingual lexicography(Catizone et al., 1993; Gale & Church, 1991) , computer- assisted language learning, corpus linguistics (Melby. 1981), and']","[', certain machine-assisted translation tools (e.g.( #AUTHOR_TAG ; Melamed , 1996 b ) ), concordancing for bilingual lexicography(Catizone et al., 1993; Gale & Church, 1991) , computer- assisted language learning, corpus linguistics (Melby.']","['the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation(Brown et al., 1988; Brown et al., 1990; Brown et al., 1993 a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993) , certain machine-assisted translation tools (e.g.( #AUTHOR_TAG ; Melamed , 1996 b ) ), concordancing for bilingual lexicography(Catizone et al., 1993; Gale & Church, 1991) , computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996)']",0
"['from other published translation models.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .', 'The likelihoods in the word-to-word model remain un']","['from other published translation models.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete']","['from other published translation models.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .', 'The likelihoods in the word-to-word model remain un']","['some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete']",0
"[', certain machine-assisted translation tools (e.g.(Macklovitch, 1994; Melamed, 1996 b)), concordancing for bilingual lexicography(Catizone et al., 1993; #AUTHOR_TAG ) , computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996)']","[', certain machine-assisted translation tools (e.g.(Macklovitch, 1994; Melamed, 1996 b)), concordancing for bilingual lexicography(Catizone et al., 1993; #AUTHOR_TAG ) , computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996)']","[', certain machine-assisted translation tools (e.g.(Macklovitch, 1994; Melamed, 1996 b)), concordancing for bilingual lexicography(Catizone et al., 1993; #AUTHOR_TAG ) , computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996)']","['the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation(Brown et al., 1988; Brown et al., 1990; Brown et al., 1993 a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993) , certain machine-assisted translation tools (e.g.(Macklovitch, 1994; Melamed, 1996 b)), concordancing for bilingual lexicography(Catizone et al., 1993; #AUTHOR_TAG ) , computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996)']",0
"[', certain machine-assisted translation tools (e.g.(Macklovitch, 1994; #AUTHOR_TAG b )), concordancing for bilingual lexicography(Catizone et al., 1993; Gale & Church , 1991 ) , computer- assisted']","[', certain machine-assisted translation tools (e.g.(Macklovitch, 1994; #AUTHOR_TAG b )), concordancing for bilingual lexicography(Catizone et al., 1993; Gale & Church , 1991 ) , computer- assisted']","[', certain machine-assisted translation tools (e.g.(Macklovitch, 1994; #AUTHOR_TAG b )), concordancing for bilingual lexicography(Catizone et al., 1993; Gale & Church , 1991 ) , computer']","['(Macklovitch, 1994; #AUTHOR_TAG']",0
"['2We could just as easily use other symmetric ""association"" measures, such as 02( Gale & Church , 1991 ) or the Dice coefficient( #AUTHOR_TAG ) .', '']","['2We could just as easily use other symmetric ""association"" measures, such as 02( Gale & Church , 1991 ) or the Dice coefficient( #AUTHOR_TAG ) .', '']","['2We could just as easily use other symmetric ""association"" measures, such as 02( Gale & Church , 1991 ) or the Dice coefficient( #AUTHOR_TAG ) .', '']","['2We could just as easily use other symmetric ""association"" measures, such as 02( Gale & Church , 1991 ) or the Dice coefficient( #AUTHOR_TAG ) .', '']",1
"['the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993 a ) .', '']","['the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993 a ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', '']","['the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993 a ) .', '']","['the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993 a ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', '']",0
"['', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other( Gale & Church , 1991 ; #AUTHOR_TAG a ) .', '']","['', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other( Gale & Church , 1991 ; #AUTHOR_TAG a ) .', '']","['', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other( Gale & Church , 1991 ; #AUTHOR_TAG a ) .', '']","['', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other( Gale & Church , 1991 ; #AUTHOR_TAG a ) .', '']",0
"['induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in( #AUTHOR_TAG )']","['induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in( #AUTHOR_TAG )']","['induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in( #AUTHOR_TAG )']","['induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in( #AUTHOR_TAG )']",5
"[') < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ) .', '']","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ) .', '']","[') < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ) .', '']","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ) .', '']",0
"['model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .', 'Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', '']","['model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .', 'Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', '']","['model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .', 'Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', '']","['model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .', 'Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', '']",0
"['-occurrence With the exception of(Fung, 1998 b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998 a;Melamed, 1995) .', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways']","['-occurrence With the exception of(Fung, 1998 b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998 a;Melamed, 1995) .', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', '']","['-occurrence With the exception of(Fung, 1998 b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998 a;Melamed, 1995) .', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', '']","['-occurrence With the exception of(Fung, 1998 b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998 a;Melamed, 1995) .', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each otherMelamed, 1996 a).', '']",0
"['including ""crummy"" MT on the World Wide Web( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g.( Macklovitch , 1994 ; Melamed , 1996 b ) ) , concordancing for bilingual lexicography( #AUTHOR_TAG ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996)']","['including ""crummy"" MT on the World Wide Web( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g.( Macklovitch , 1994 ; Melamed , 1996 b ) ) , concordancing for bilingual lexicography( #AUTHOR_TAG ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996)']","['including ""crummy"" MT on the World Wide Web( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g.( Macklovitch , 1994 ; Melamed , 1996 b ) ) , concordancing for bilingual lexicography( #AUTHOR_TAG ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996)']","['the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation(Brown et al., 1988; Brown et al., 1990; Brown et al., 1993 a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including ""crummy"" MT on the World Wide Web( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g.( Macklovitch , 1994 ; Melamed , 1996 b ) ) , concordancing for bilingual lexicography( #AUTHOR_TAG ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996)']",0
"['better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy .', ""Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units( #AUTHOR_TAG ) ."", 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.', 'In this manner, the model can account for a wider range of translation phenomena']","['better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy .', ""Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units( #AUTHOR_TAG ) ."", 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.', 'In this manner, the model can account for a wider range of translation phenomena']","['better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy .', ""Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units( #AUTHOR_TAG ) ."", 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.', 'In this manner, the model can account for a wider range of translation phenomena']","['better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy .', ""Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units( #AUTHOR_TAG ) ."", 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.', 'In this manner, the model can account for a wider range of translation phenomena']",3
"['', 'For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( â\x80\x9e , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following( #AUTHOR_TAG ) 2 .', ""When the L(u, v) are re-estimated, the model's hidden parameters come into play""]","['', 'For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( â\x80\x9e , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following( #AUTHOR_TAG ) 2 .', ""When the L(u, v) are re-estimated, the model's hidden parameters come into play""]","['', 'For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( â\x80\x9e , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following( #AUTHOR_TAG ) 2 .', ""When the L(u, v) are re-estimated, the model's hidden parameters come into play""]","['', 'For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( â\x80\x9e , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following( #AUTHOR_TAG ) 2 .', ""When the L(u, v) are re-estimated, the model's hidden parameters come into play""]",5
"['the exception of( Fung , 1995 b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995 a ;#AUTHOR_TAG ) .', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of']","['the exception of( Fung , 1995 b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995 a ;#AUTHOR_TAG ) .', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of']","['the exception of( Fung , 1995 b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995 a ;#AUTHOR_TAG ) .', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of']","['the exception of( Fung , 1995 b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995 a ;#AUTHOR_TAG ) .', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other(Gale & Church, 1991; Melamed, 1996 a).', '']",0
"[') < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ) .', '']","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ) .', '']","[') < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ) .', '']","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ) .', '']",1
"[""advantage that Brown et al.'s Model i has over our word-to-word model is that their objective function has no local maxima."", 'By using the EM algorithm( #AUTHOR_TAG ) , they can guarantee convergence towards the globally optimum parameter set .', '']","[""advantage that Brown et al.'s Model i has over our word-to-word model is that their objective function has no local maxima."", 'By using the EM algorithm( #AUTHOR_TAG ) , they can guarantee convergence towards the globally optimum parameter set .', '']","[""advantage that Brown et al.'s Model i has over our word-to-word model is that their objective function has no local maxima."", 'By using the EM algorithm( #AUTHOR_TAG ) , they can guarantee convergence towards the globally optimum parameter set .', '']","[""advantage that Brown et al.'s Model i has over our word-to-word model is that their objective function has no local maxima."", 'By using the EM algorithm( #AUTHOR_TAG ) , they can guarantee convergence towards the globally optimum parameter set .', 'In contrast, the dynamic nature of the competitive linking algorithm changes the Pr(datalmodel ) in a non-monotonic fashion.', '']",0
"['better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy( #AUTHOR_TAG ) .', '']","['better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy( #AUTHOR_TAG ) .', '']","['better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy( #AUTHOR_TAG ) .', 'Another interesting extension is to broaden the definition']","['better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy( #AUTHOR_TAG ) .', '']",3
"['the basic word-to-word model, the hidden parameters A + and A-depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'More accurate models can be induced by taking into account various features of the linked tokens.', 'For example , frequent words are translated less consistently than rare words( #AUTHOR_TAG )']","['the basic word-to-word model, the hidden parameters A + and A-depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'More accurate models can be induced by taking into account various features of the linked tokens.', 'For example , frequent words are translated less consistently than rare words( #AUTHOR_TAG )']","['the basic word-to-word model, the hidden parameters A + and A-depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'More accurate models can be induced by taking into account various features of the linked tokens.', 'For example , frequent words are translated less consistently than rare words( #AUTHOR_TAG )']","['the basic word-to-word model, the hidden parameters A + and A-depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'More accurate models can be induced by taking into account various features of the linked tokens.', 'For example , frequent words are translated less consistently than rare words( #AUTHOR_TAG )']",0
"[') < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ) .', '']","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ) .', '']","[') < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ) .', '']","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ) .', '']",1
"[""2We could just as easily use other symmetric `` association '' measures , such as 02( #AUTHOR_TAG ) or the Dice coefficient( Smadja , 1992 ) ."", '']","[""2We could just as easily use other symmetric `` association '' measures , such as 02( #AUTHOR_TAG ) or the Dice coefficient( Smadja , 1992 ) ."", '']","[""2We could just as easily use other symmetric `` association '' measures , such as 02( #AUTHOR_TAG ) or the Dice coefficient( Smadja , 1992 ) ."", '']","[""2We could just as easily use other symmetric `` association '' measures , such as 02( #AUTHOR_TAG ) or the Dice coefficient( Smadja , 1992 ) ."", '']",1
"['the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAG a ) .', '']","['the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAG a ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', '']","['the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAG a ) .', '']","['the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAG a ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', '']",0
"['from other published translation models.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .', 'The likelihoods in the word-to-word model remain un']","['from other published translation models.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete']","['from other published translation models.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .', 'The likelihoods in the word-to-word model remain un']","['some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete']",0
"['', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models( #AUTHOR_TAG b ) .', 'for their models(Brown et al., 1993 b']","['', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models( #AUTHOR_TAG b ) .', 'for their models(Brown et al., 1993 b).', 'When the hidden parameters are conditioned on different link classes, the estimation method does not change; it is just repeated for each link class']","['', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models( #AUTHOR_TAG b ) .', 'for their models(Brown et al., 1993 b']","['', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models( #AUTHOR_TAG b ) .', 'for their models(Brown et al., 1993 b).', 'When the hidden parameters are conditioned on different link classes, the estimation method does not change; it is just repeated for each link class']",1
"['', ""The most detailed evaluation of link tokens to date was performed by( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards ."", 'These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.', '']","['', ""The most detailed evaluation of link tokens to date was performed by( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards ."", 'These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.', '']","['', ""The most detailed evaluation of link tokens to date was performed by( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards ."", 'These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.', '']","['', ""The most detailed evaluation of link tokens to date was performed by( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards ."", 'These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.', '']",1
"['11', '11 From( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .', 'Thus, to be convenient, we only conduct experiments with the SAMT system']","['11', '11 From( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .', 'Thus, to be convenient, we only conduct experiments with the SAMT system']","['11', '11 From( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .', 'Thus, to be convenient, we only conduct experiments with the SAMT system']","['', '11 From( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .', 'Thus, to be convenient, we only conduct experiments with the SAMT system']",4
"['', 'Differently ,#AUTHOR_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .', 'We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'This subject will']","['', 'Differently ,#AUTHOR_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .', 'We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'This subject will']","['', 'Differently ,#AUTHOR_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .', 'We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'This subject will be']","['', 'Differently ,#AUTHOR_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .', 'We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'This subject will be one of our future work topics']",4
['obtained SCFG is further used in a phrase-based and hierarchical phrase-based system( #AUTHOR_TAG )'],['obtained SCFG is further used in a phrase-based and hierarchical phrase-based system( #AUTHOR_TAG )'],"['obtained SCFG is further used in a phrase-based and hierarchical phrase-based system( #AUTHOR_TAG ) .', '']","['', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system( #AUTHOR_TAG ) .', '']",1
"['', '#AUTHOR_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .', '']","['', '#AUTHOR_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .', '']","['', '#AUTHOR_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .', '']","['', '#AUTHOR_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system(Chiang, 2007) .', '']",1
"['translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on( Galley et al. , 2006 ) and( #AUTHOR_TAG ) .', '']","['translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on( Galley et al. , 2006 ) and( #AUTHOR_TAG ) .', '']","['translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on( Galley et al. , 2006 ) and( #AUTHOR_TAG ) .', '']","['translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on( Galley et al. , 2006 ) and( #AUTHOR_TAG ) .', '']",5
"['', 'Our previous work( #AUTHOR_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models .', 'This work differs from the above']","['', 'Our previous work( #AUTHOR_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models .', 'This work differs from the above']","['', 'Our previous work( #AUTHOR_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models .', 'This work differs from the above work']","['', 'Our previous work( #AUTHOR_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models .', '']",1
"['2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser(Petrov et al., 2006) .', 'Then , we binarize the English parse trees using the head binarization approach( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system']","['s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser(Petrov et al., 2006) .', 'Then , we binarize the English parse trees using the head binarization approach( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system']","['build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser(Petrov et al., 2006) .', 'Then , we binarize the English parse trees using the head binarization approach( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system']","['build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser(Petrov et al., 2006) .', 'Then , we binarize the English parse trees using the head binarization approach( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system']",5
"['recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality( Liu et al. , 2006 ,  2009 ;Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ;#AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']","['recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality( Liu et al. , 2006 ,  2009 ;Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ;#AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']","['recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality( Liu et al. , 2006 ,  2009 ;Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ;#AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']","['recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality( Liu et al. , 2006 ,  2009 ;Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ;#AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']",0
"['recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality( Liu et al. , 2006 ,  2009 ;Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ;Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011 b )']","['recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality( Liu et al. , 2006 ,  2009 ;Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ;Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011 b )']","['recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality( Liu et al. , 2006 ,  2009 ;Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ;Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011 b )']","['recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality( Liu et al. , 2006 ,  2009 ;Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ;Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011 b )']",0
"['', '#AUTHOR_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees']","['', '#AUTHOR_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees']","['', '#AUTHOR_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .', '']","['', '#AUTHOR_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .', '']",1
['we follow#AUTHOR_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows'],['we follow#AUTHOR_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows'],['we follow#AUTHOR_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows'],"['', 'Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow#AUTHOR_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows']",5
"['', '#AUTHOR_TAG further labeled the SCFG rules with POS tags and unsupervised word classes .', '']","['', '#AUTHOR_TAG further labeled the SCFG rules with POS tags and unsupervised word classes .', '']","['', '#AUTHOR_TAG further labeled the SCFG rules with POS tags and unsupervised word classes .', '']","['', '#AUTHOR_TAG further labeled the SCFG rules with POS tags and unsupervised word classes .', '']",1
"['the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes( #AUTHOR_TAG ) .', 'Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'For example, the bold italic nodes with shadows in Figure 2 are frontier nodes']","['the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes( #AUTHOR_TAG ) .', 'Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'For example, the bold italic nodes with shadows in Figure 2 are frontier nodes.', '']","['the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes( #AUTHOR_TAG ) .', 'Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'For example, the bold italic nodes with shadows in Figure 2 are frontier nodes.', '']","['the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes( #AUTHOR_TAG ) .', 'Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'For example, the bold italic nodes with shadows in Figure 2 are frontier nodes.', '']",5
"['', '#AUTHOR_TAG re-trained the linguistic parsers bilingually based on word alignment']","['', '#AUTHOR_TAG re-trained the linguistic parsers bilingually based on word alignment']","['', '#AUTHOR_TAG re-trained the linguistic parsers bilingually based on word alignment .', '']","['', '#AUTHOR_TAG re-trained the linguistic parsers bilingually based on word alignment .', '']",1
"['', 'The statistical significance test is performed by the re-sampling approach( #AUTHOR_TAG )']","['', 'The statistical significance test is performed by the re-sampling approach( #AUTHOR_TAG )']","['', 'The statistical significance test is performed by the re-sampling approach( #AUTHOR_TAG )']","['', 'The statistical significance test is performed by the re-sampling approach( #AUTHOR_TAG )']",5
"['translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on( #AUTHOR_TAG ) and ( Marcu et al. 2006 ) .', '']","['translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on( #AUTHOR_TAG ) and ( Marcu et al. 2006 ) .', '']","['translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on( #AUTHOR_TAG ) and ( Marcu et al. 2006 ) .', '']","['translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on( #AUTHOR_TAG ) and ( Marcu et al. 2006 ) .', '']",5
"['', '#AUTHOR_TAG employed a Bayesian method to learn discontinuous SCFG rules']","['', '#AUTHOR_TAG employed a Bayesian method to learn discontinuous SCFG rules']","['', '#AUTHOR_TAG employed a Bayesian method to learn discontinuous SCFG rules .', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', '']","['', '#AUTHOR_TAG employed a Bayesian method to learn discontinuous SCFG rules .', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', '']",1
"['tree-based translation models than SCFG.', '#AUTHOR_TAG andBurkett et al. ( 2010 ) focused on joint parsing and alignment']","['tree-based translation models than SCFG.', '#AUTHOR_TAG andBurkett et al. ( 2010 ) focused on joint parsing and alignment']","['tree-based translation models than SCFG.', '#AUTHOR_TAG andBurkett et al. ( 2010 ) focused on joint parsing and alignment .', '']","['', '#AUTHOR_TAG andBurkett et al. ( 2010 ) focused on joint parsing and alignment .', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', '']",1
['9 We only use the minimal GHKM rules( #AUTHOR_TAG ) here to reduce the complexity of the sampler'],"['', '9 We only use the minimal GHKM rules( #AUTHOR_TAG ) here to reduce the complexity of the sampler']",['9 We only use the minimal GHKM rules( #AUTHOR_TAG ) here to reduce the complexity of the sampler'],"['', '9 We only use the minimal GHKM rules( #AUTHOR_TAG ) here to reduce the complexity of the sampler']",5
"['', 'In the system , we extract both the minimal GHKM rules( #AUTHOR_TAG ) , and the rules of SPMT Model 1( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side']","['', 'In the system , we extract both the minimal GHKM rules( #AUTHOR_TAG ) , and the rules of SPMT Model 1( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side']","['', 'In the system , we extract both the minimal GHKM rules( #AUTHOR_TAG ) , and the rules of SPMT Model 1( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side']","['', 'In the system , we extract both the minimal GHKM rules( #AUTHOR_TAG ) , and the rules of SPMT Model 1( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side']",5
"['-trees.', 'Using the GHKM algorithm( #AUTHOR_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .', 'Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own.', ""In the two derivations, the STSG rules defined by the two states include the one rooted at the s-node's lowest ancestor frontier node, and the one rooted at the s-node if it is a frontier node."", '']","['U-trees.', 'Using the GHKM algorithm( #AUTHOR_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .', 'Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own.', ""In the two derivations, the STSG rules defined by the two states include the one rooted at the s-node's lowest ancestor frontier node, and the one rooted at the s-node if it is a frontier node."", '']","['two different U-trees.', 'Using the GHKM algorithm( #AUTHOR_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .', 'Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own.', ""In the two derivations, the STSG rules defined by the two states include the one rooted at the s-node's lowest ancestor frontier node, and the one rooted at the s-node if it is a frontier node."", '']","['', 'Using the GHKM algorithm( #AUTHOR_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .', 'Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own.', ""In the two derivations, the STSG rules defined by the two states include the one rooted at the s-node's lowest ancestor frontier node, and the one rooted at the s-node if it is a frontier node."", '']",5
"['recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality( Liu et al. , 2006 ,  2009 ;Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ;Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']","['recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality( Liu et al. , 2006 ,  2009 ;Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ;Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']","['recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality( Liu et al. , 2006 ,  2009 ;Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ;Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']","['recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality( Liu et al. , 2006 ,  2009 ;Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ;Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']",0
"['recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality( Liu et al. , 2006 ,  2009 ;#AUTHOR_TAG ; Galley et al. , 2004 ,  2006 ;Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']","['recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality( Liu et al. , 2006 ,  2009 ;#AUTHOR_TAG ; Galley et al. , 2004 ,  2006 ;Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']","['recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality( Liu et al. , 2006 ,  2009 ;#AUTHOR_TAG ; Galley et al. , 2004 ,  2006 ;Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']","['recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality( Liu et al. , 2006 ,  2009 ;#AUTHOR_TAG ; Galley et al. , 2004 ,  2006 ;Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']",0
"['create the baseline system , we use the opensource Joshua 4.0 system( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system( #AUTHOR_TAG ) respectively']","['create the baseline system , we use the opensource Joshua 4.0 system( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system( #AUTHOR_TAG ) respectively']","['create the baseline system , we use the opensource Joshua 4.0 system( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system( #AUTHOR_TAG ) respectively']","['create the baseline system , we use the opensource Joshua 4.0 system( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system( #AUTHOR_TAG ) respectively']",5
"['build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser( #AUTHOR_TAG ) .', 'Then, we binarize the English parse trees']","['build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser( #AUTHOR_TAG ) .', 'Then, we binarize the English parse trees']","['build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser( #AUTHOR_TAG ) .', 'Then, we binarize the English parse trees using']","['build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser( #AUTHOR_TAG ) .', 'Then, we binarize the English parse trees using the head binarization approach(Wang et al., 2007) and use the resulting binary parse trees to build another s2t system']",5
"['the probability of producing the target tree fragment frag.', 'To generate frag, used a geometric prior to decide how many child nodes to assign each node.', 'Differently, we require that each multi-word non-terminal node must have two child nodes.', 'This is because the binary structure has been verified to be very effective for tree-based translation( #AUTHOR_TAG ; Zhang et al. , 2011 a )']","['the probability of producing the target tree fragment frag.', 'To generate frag, used a geometric prior to decide how many child nodes to assign each node.', 'Differently, we require that each multi-word non-terminal node must have two child nodes.', 'This is because the binary structure has been verified to be very effective for tree-based translation( #AUTHOR_TAG ; Zhang et al. , 2011 a )']","['the probability of producing the target tree fragment frag.', 'To generate frag, used a geometric prior to decide how many child nodes to assign each node.', 'Differently, we require that each multi-word non-terminal node must have two child nodes.', 'This is because the binary structure has been verified to be very effective for tree-based translation( #AUTHOR_TAG ; Zhang et al. , 2011 a )']","['the probability of producing the target tree fragment frag.', 'To generate frag, used a geometric prior to decide how many child nodes to assign each node.', 'Differently, we require that each multi-word non-terminal node must have two child nodes.', 'This is because the binary structure has been verified to be very effective for tree-based translation( #AUTHOR_TAG ; Zhang et al. , 2011 a )']",4
"['recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality( #AUTHOR_TAG , 2009 ;Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ;Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']","['recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality( #AUTHOR_TAG , 2009 ;Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ;Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']","['recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality( #AUTHOR_TAG , 2009 ;Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ;Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']","['recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality( #AUTHOR_TAG , 2009 ;Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ;Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']",0
"['-bank resources for training.', '2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'This indicates that parse trees are usually not the optimal choice for training tree-based translation models( #AUTHOR_TAG )']","['Tree-bank resources for training.', '2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'This indicates that parse trees are usually not the optimal choice for training tree-based translation models( #AUTHOR_TAG )']","['-bank resources for training.', '2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'This indicates that parse trees are usually not the optimal choice for training tree-based translation models( #AUTHOR_TAG )']","[', for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training.', '2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'This indicates that parse trees are usually not the optimal choice for training tree-based translation models( #AUTHOR_TAG )']",0
['#AUTHOR_TAG focused on joint parsing and alignment'],['#AUTHOR_TAG focused on joint parsing and alignment'],"['#AUTHOR_TAG focused on joint parsing and alignment .', '']","['', 'Burkett and Klein ( 2008 ) and#AUTHOR_TAG focused on joint parsing and alignment .', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', '']",1
"['', '#AUTHOR_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation']","['', '#AUTHOR_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation']","['', '#AUTHOR_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .', '']","['', '#AUTHOR_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .', '']",1
"['(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by( Blunsom et al. , 2009 ) and( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string']","['(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by( Blunsom et al. , 2009 ) and( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string']","['(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by( Blunsom et al. , 2009 ) and( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string']","['(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by( Blunsom et al. , 2009 ) and( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string']",4
"['(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by( #AUTHOR_TAG ) and( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string']","['(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by( #AUTHOR_TAG ) and( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string']","['(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by( #AUTHOR_TAG ) and( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string']","['(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by( #AUTHOR_TAG ) and( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string']",4
"['addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar( #AUTHOR_TAG ) 5 .', 'This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.', '']","['addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar( #AUTHOR_TAG ) 5 .', 'This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.', '']","['addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar( #AUTHOR_TAG ) 5 .', 'This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.', '']","['addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar( #AUTHOR_TAG ) 5 .', 'This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.', '']",1
"['', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', '']","['we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', '']","['we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', '']","['', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', '']",1
"['', 'This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search( #AUTHOR_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses']","['', 'This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search( #AUTHOR_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses']","['or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan(Kobayashi et al., 1992) .', 'This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search( #AUTHOR_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses .', '']","['hypotheses.', 'As the recogn/fion engine, either VoiceRex, developed by NTI""(Noda et al., 1998) , or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan(Kobayashi et al., 1992) .', 'This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search( #AUTHOR_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses .', '']",0
"['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain']",0
"['have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests( #AUTHOR_TAG ; Chu-Carroll , 1999 ) .', '']","['have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests( #AUTHOR_TAG ; Chu-Carroll , 1999 ) .', '']","['have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests( #AUTHOR_TAG ; Chu-Carroll , 1999 ) .', '']","['have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests( #AUTHOR_TAG ; Chu-Carroll , 1999 ) .', 'Incorporating such techniques would deo crease the system developer workload.', '']",3
"['ctions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of#AUTHOR_TAG']","['ctions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of#AUTHOR_TAG']","['ctions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of#AUTHOR_TAG']","['ctions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of#AUTHOR_TAG']",1
"['hypotheses.', 'As the recogn/fion engine, either VoiceRex, developed by NTI""(Noda et al., 1998) , or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan( #AUTHOR_TAG ) .', '']","['hypotheses.', 'As the recogn/fion engine, either VoiceRex, developed by NTI""(Noda et al., 1998) , or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan( #AUTHOR_TAG ) .', '']","['hypotheses.', 'As the recogn/fion engine, either VoiceRex, developed by NTI""(Noda et al., 1998) , or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan( #AUTHOR_TAG ) .', '']","['hypotheses.', 'As the recogn/fion engine, either VoiceRex, developed by NTI""(Noda et al., 1998) , or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan( #AUTHOR_TAG ) .', '']",5
"['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain']",0
"['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain']",0
"['has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system( #AUTHOR_TAG b ) , a video-recording programming system , a schedule management system( Nakano et al. , 1999 a ) , and a weather infomiation system( Dohsaka et al. , 2000 ) .', '']","['has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system( #AUTHOR_TAG b ) , a video-recording programming system , a schedule management system( Nakano et al. , 1999 a ) , and a weather infomiation system( Dohsaka et al. , 2000 ) .', '']","['has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system( #AUTHOR_TAG b ) , a video-recording programming system , a schedule management system( Nakano et al. , 1999 a ) , and a weather infomiation system( Dohsaka et al. , 2000 ) .', '']","['has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system( #AUTHOR_TAG b ) , a video-recording programming system , a schedule management system( Nakano et al. , 1999 a ) , and a weather infomiation system( Dohsaka et al. , 2000 ) .', '']",2
"['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain']",0
"['', 'Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered.', 'For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ) .', 'The language generation module features Common Lisp functions, so there is']","['', 'Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered.', 'For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ) .', 'The language generation module features Common Lisp functions, so there is']","['', 'Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered.', 'For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ) .', 'The language generation module features Common Lisp functions, so there is']","['', 'Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered.', 'For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ) .', 'The language generation module features Common Lisp functions, so there is no limitation on the description.', '']",3
"['the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking( #AUTHOR_TAG a ) .', 'Thus the system can respond immediately after user pauses when the user has the initiative.', '']","['the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking( #AUTHOR_TAG a ) .', 'Thus the system can respond immediately after user pauses when the user has the initiative.', '']","['the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking( #AUTHOR_TAG a ) .', 'Thus the system can respond immediately after user pauses when the user has the initiative.', '']","['the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking( #AUTHOR_TAG a ) .', 'Thus the system can respond immediately after user pauses when the user has the initiative.', '']",0
"['has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system( Nakano et al. , 1999 b ) , a video-recording programming system , a schedule management system( Nakano et al. , 1999 a ) , and a weather infomiation system( #AUTHOR_TAG ) .', '']","['has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system( Nakano et al. , 1999 b ) , a video-recording programming system , a schedule management system( Nakano et al. , 1999 a ) , and a weather infomiation system( #AUTHOR_TAG ) .', '']","['has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system( Nakano et al. , 1999 b ) , a video-recording programming system , a schedule management system( Nakano et al. , 1999 a ) , and a weather infomiation system( #AUTHOR_TAG ) .', '']","['has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system( Nakano et al. , 1999 b ) , a video-recording programming system , a schedule management system( Nakano et al. , 1999 a ) , and a weather infomiation system( #AUTHOR_TAG ) .', '']",2
"['', 'WIT features an incremental understanding method( #AUTHOR_TAG b ) that makes it possible to build a robust and real-time system .', '']","['', 'WIT features an incremental understanding method( #AUTHOR_TAG b ) that makes it possible to build a robust and real-time system .', '']","['', 'WIT features an incremental understanding method( #AUTHOR_TAG b ) that makes it possible to build a robust and real-time system .', '']","['', 'WIT features an incremental understanding method( #AUTHOR_TAG b ) that makes it possible to build a robust and real-time system .', 'In addition, WIT compiles domain-dependent system specifications into internal knowledge sources so that building systems is easier.', '']",5
"['domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.', 'Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions( #AUTHOR_TAG ) .', '']","['domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.', 'Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions( #AUTHOR_TAG ) .', '']","['domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.', 'Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions( #AUTHOR_TAG ) .', '']","['domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.', 'Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions( #AUTHOR_TAG ) .', 'When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase.', '']",5
"['199 I), but we do not explain these constraints in detail in this paper.', 'The priorities are used for disambiguating interpretation in the incremental understanding method( #AUTHOR_TAG b ) .', '']","['is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper.', 'The priorities are used for disambiguating interpretation in the incremental understanding method( #AUTHOR_TAG b ) .', '']","['199 I), but we do not explain these constraints in detail in this paper.', 'The priorities are used for disambiguating interpretation in the incremental understanding method( #AUTHOR_TAG b ) .', '']","['', 'It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper.', 'The priorities are used for disambiguating interpretation in the incremental understanding method( #AUTHOR_TAG b ) .', '']",5
"['this end , several toolkits for building spoken dialogue systems have been developed( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit']","['this end , several toolkits for building spoken dialogue systems have been developed( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit']","['this end , several toolkits for building spoken dialogue systems have been developed( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit']","['this end , several toolkits for building spoken dialogue systems have been developed( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit(Sutton et al., 1998) , which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model.', '']",0
"['', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search )( #AUTHOR_TAG b ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most pl']","['', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search )( #AUTHOR_TAG b ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most']","['', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search )( #AUTHOR_TAG b ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most pl']","['', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search )( #AUTHOR_TAG b ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence.', '']",5
"['common approach is term translation, e.g., via a bilingual lexicon.(Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996) .', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', '']","['common approach is term translation, e.g., via a bilingual lexicon.(Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996) .', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', '']","['common approach is term translation, e.g., via a bilingual lexicon.(Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996) .', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', '']","['common approach is term translation, e.g., via a bilingual lexicon.(Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996) .', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', '']",1
"['second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ) .', 'One such method is to treat different translations of the same term as synonyms.', 'Ball']","['second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ) .', 'One such method is to treat different translations of the same term as synonyms.', '']","['second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ) .', 'One such method is to treat different translations of the same term as synonyms.', 'Ball']","['second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ) .', 'One such method is to treat different translations of the same term as synonyms.', 'Ballesteros, for example, used the INQUERY(Callan et al, 1995) synonym operator to group translations of different query terms.', '']",1
"['results in Table 4 show that manual disambiguation improves performance by 17% on Trec5C, 4% on Trec4S, but not at all on Trec6C.', 'Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries.', 'The one-sided t-test( #AUTHOR_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant']","['results in Table 4 show that manual disambiguation improves performance by 17% on Trec5C, 4% on Trec4S, but not at all on Trec6C.', 'Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries.', 'The one-sided t-test( #AUTHOR_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant']","['results in Table 4 show that manual disambiguation improves performance by 17% on Trec5C, 4% on Trec4S, but not at all on Trec6C.', 'Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries.', 'The one-sided t-test( #AUTHOR_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant']","['results in Table 4 show that manual disambiguation improves performance by 17% on Trec5C, 4% on Trec4S, but not at all on Trec6C.', 'Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries.', 'The one-sided t-test( #AUTHOR_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant']",5
"['third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI )( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) ,( #AUTHOR_TAG ) .', 'We believe our approach is computationally less costly than (LSI and GVSM) and assumes less resources (WordNet in']","['third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI )( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) ,( #AUTHOR_TAG ) .', 'We believe our approach is computationally less costly than (LSI and GVSM) and assumes less resources (WordNet in']","['third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI )( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) ,( #AUTHOR_TAG ) .', 'We believe our approach is computationally less costly than (LSI and GVSM) and assumes less resources (WordNet in']","['third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI )( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) ,( #AUTHOR_TAG ) .', 'We believe our approach is computationally less costly than (LSI and GVSM) and assumes less resources (WordNet inDiekema et al., 1999)']",1
['\x80¢ The transition probability a is 0.7 using the EM algorithm( #AUTHOR_TAG ) on the TREC4 ad-hoc query set'],['\x80¢ The transition probability a is 0.7 using the EM algorithm( #AUTHOR_TAG ) on the TREC4 ad-hoc query set'],['\x80¢ The transition probability a is 0.7 using the EM algorithm( #AUTHOR_TAG ) on the TREC4 ad-hoc query set'],['\x80¢ The transition probability a is 0.7 using the EM algorithm( #AUTHOR_TAG ) on the TREC4 ad-hoc query set'],5
"['', '#AUTHOR_TAG studied the issue of disambiguation for mono-lingual M']","['', '#AUTHOR_TAG studied the issue of disambiguation for mono-lingual M']","['', '#AUTHOR_TAG studied the issue of disambiguation for mono-lingual M']","['', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and deJong, 1999; Hull, 1997 .', '#AUTHOR_TAG studied the issue of disambiguation for mono-lingual M']",0
"['.', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and deJong , 1999 ; #AUTHOR_TAG .', 'Sanderson, 1994  studied the issue of disarnbiguation for mono-lingual IR']","['.', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and deJong , 1999 ; #AUTHOR_TAG .', 'Sanderson, 1994  studied the issue of disarnbiguation for mono-lingual IR']","['', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996) .', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and deJong , 1999 ; #AUTHOR_TAG .', 'Sanderson, 1994  studied the issue of disarnbiguation for mono-lingual IR']","['', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996) .', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and deJong , 1999 ; #AUTHOR_TAG .', 'Sanderson, 1994  studied the issue of disarnbiguation for mono-lingual IR']",0
"['approaches to cross-lingual IR have been published.', 'One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries( Gey et al , 1999 ; #AUTHOR_TAG ) .', 'For most languages, there are no MT systems at all.', 'Our']","['approaches to cross-lingual IR have been published.', 'One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries( Gey et al , 1999 ; #AUTHOR_TAG ) .', 'For most languages, there are no MT systems at all.', 'Our']","['approaches to cross-lingual IR have been published.', 'One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries( Gey et al , 1999 ; #AUTHOR_TAG ) .', 'For most languages, there are no MT systems at all.', 'Our']","['approaches to cross-lingual IR have been published.', 'One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries( Gey et al , 1999 ; #AUTHOR_TAG ) .', 'For most languages, there are no MT systems at all.', '']",1
"['second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .', 'One such method is to treat different translations of the same term as synonyms.', 'Ball']","['second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .', 'One such method is to treat different translations of the same term as synonyms.', '']","['second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .', 'One such method is to treat different translations of the same term as synonyms.', 'Ball']","['second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .', 'One such method is to treat different translations of the same term as synonyms.', '']",1
['the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur( #AUTHOR_TAG )'],['the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur( #AUTHOR_TAG )'],['the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur( #AUTHOR_TAG )'],"['', 'However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur( #AUTHOR_TAG )']",0
"['', 'A cooccurrence based stemmer( #AUTHOR_TAG ) was used to stem Spanish words .', '']","['', 'A cooccurrence based stemmer( #AUTHOR_TAG ) was used to stem Spanish words .', '']","['', 'A cooccurrence based stemmer( #AUTHOR_TAG ) was used to stem Spanish words .', '']","['', 'A cooccurrence based stemmer( #AUTHOR_TAG ) was used to stem Spanish words .', '']",5
"['studies which view lR as a query generation process includeMaron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999  .', 'Our work has focused on cross-lingual retrieval']","['studies which view lR as a query generation process includeMaron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999  .', 'Our work has focused on cross-lingual retrieval']","['studies which view lR as a query generation process includeMaron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999  .', 'Our work has focused on cross-lingual retrieval']","['studies which view lR as a query generation process includeMaron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999  .', 'Our work has focused on cross-lingual retrieval']",1
"['studies which view lR as a query generation process include#AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999  .', 'Our work has focused on cross-lingual retrieval']","['studies which view lR as a query generation process include#AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999  .', 'Our work has focused on cross-lingual retrieval']","['studies which view lR as a query generation process include#AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999  .', 'Our work has focused on cross-lingual retrieval']","['studies which view lR as a query generation process include#AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999  .', 'Our work has focused on cross-lingual retrieval']",1
"['technique is automatic discovery of translations from parallel or non-parallel corpora( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies']","['technique is automatic discovery of translations from parallel or non-parallel corpora( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies']","['technique is automatic discovery of translations from parallel or non-parallel corpora( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies']","['technique is automatic discovery of translations from parallel or non-parallel corpora( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies']",0
"['#AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', '']","['#AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', '']","['#AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', '']","['#AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', '']",5
"['examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers( #AUTHOR_TAG ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers( #AUTHOR_TAG ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers( #AUTHOR_TAG ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['', 'would be chunked as follows(Tjong Kim Sang and Buchholz, 2000) : While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers( #AUTHOR_TAG ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim#AUTHOR_TAG ) .', 'In this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'The goal in this case is therefore to accurately predict a collection of ¢ £ ¢ different types of phrases.', 'The chunk types are based on the syntactic category part of the bracket label in the Treebank']","['first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim#AUTHOR_TAG ) .', 'In this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'The goal in this case is therefore to accurately predict a collection of ¢ £ ¢ different types of phrases.', 'The chunk types are based on the syntactic category part of the bracket label in the Treebank.', '']","['first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim#AUTHOR_TAG ) .', 'In this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'The goal in this case is therefore to accurately predict a collection of ¢ £ ¢ different types of phrases.', 'The chunk types are based on the syntactic category part of the bracket label in the Treebank.', '']","['first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim#AUTHOR_TAG ) .', 'In this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'The goal in this case is therefore to accurately predict a collection of ¢ £ ¢ different types of phrases.', 'The chunk types are based on the syntactic category part of the bracket label in the Treebank.', '']",5
"['allow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .', '']","['allow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .', '']","['allow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .', '']","['allow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .', '']",0
"['examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; #AUTHOR_TAG a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; #AUTHOR_TAG a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; #AUTHOR_TAG a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['', 'would be chunked as follows(Tjong Kim Sang and Buchholz, 2000) : While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; #AUTHOR_TAG a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time( Charniak , 1997 b ;Charniak , 1997 a ;#AUTHOR_TAG ; Ratnaparkhi , 1997 )']","[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time( Charniak , 1997 b ;Charniak , 1997 a ;#AUTHOR_TAG ; Ratnaparkhi , 1997 )']","[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time( Charniak , 1997 b ;Charniak , 1997 a ;#AUTHOR_TAG ; Ratnaparkhi , 1997 )']","[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time( Charniak , 1997 b ;Charniak , 1997 a ;#AUTHOR_TAG ; Ratnaparkhi , 1997 )']",0
"['perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins( Collins , 1996 ; #AUTHOR_TAG ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'After']","['perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins( Collins , 1996 ; #AUTHOR_TAG ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'After']","['perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins( Collins , 1996 ; #AUTHOR_TAG ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'After']","['perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins( Collins , 1996 ; #AUTHOR_TAG ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'After that, it will choose the candidate parse tree with the highest probability as output.', '']",5
"[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time( #AUTHOR_TAG b ;Charniak , 1997 a ;Collins , 1997 ; Ratnaparkhi , 1997 )']","[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time( #AUTHOR_TAG b ;Charniak , 1997 a ;Collins , 1997 ; Ratnaparkhi , 1997 )']","[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time( #AUTHOR_TAG b ;Charniak , 1997 a ;Collins , 1997 ; Ratnaparkhi , 1997 )']","[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time( #AUTHOR_TAG b ;Charniak , 1997 a ;Collins , 1997 ; Ratnaparkhi , 1997 )']",0
"['', 'SNoW( #AUTHOR_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', '']","['', 'SNoW( #AUTHOR_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', '']","['shallow parser used is the SNoW-based CSCL parser(Punyakanok and Roth, 2001;Munoz et al., 1999) .', 'SNoW( #AUTHOR_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', '']","['shallow parser used is the SNoW-based CSCL parser(Punyakanok and Roth, 2001;Munoz et al., 1999) .', 'SNoW( #AUTHOR_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', '']",5
"['years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","[', along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","[', along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['allow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ) .', '']","['allow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ) .', '']","['allow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ) .', '']","['allow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ) .', '']",0
"['allow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ) .', '']","['allow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ) .', '']","['allow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ) .', '']","['allow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ) .', '']",0
"['noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization( Grishman , 1995 ; #AUTHOR_TAG ) .', '']","['noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization( Grishman , 1995 ; #AUTHOR_TAG ) .', '']","['that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization( Grishman , 1995 ; #AUTHOR_TAG ) .', '']","[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization( Grishman , 1995 ; #AUTHOR_TAG ) .', '']",0
"['we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim#AUTHOR_TAG ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1']","['we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim#AUTHOR_TAG ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1']","['we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim#AUTHOR_TAG ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1']","['earlier versions of the SNoW based CSCL were used only to identify single phrases( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim#AUTHOR_TAG ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1']",5
"['years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","[', along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time( Charniak , 1997 b ;Charniak , 1997 a ;Collins , 1997 ; #AUTHOR_TAG )']","[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time( Charniak , 1997 b ;Charniak , 1997 a ;Collins , 1997 ; #AUTHOR_TAG )']","[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time( Charniak , 1997 b ;Charniak , 1997 a ;Collins , 1997 ; #AUTHOR_TAG )']","[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time( Charniak , 1997 b ;Charniak , 1997 a ;Collins , 1997 ; #AUTHOR_TAG )']",0
"['shallow parser used is the SNoW-based CSCL parser( #AUTHOR_TAG ; Munoz et al. , 1999 ) .', 'SNoW(Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example']","['shallow parser used is the SNoW-based CSCL parser( #AUTHOR_TAG ; Munoz et al. , 1999 ) .', 'SNoW(Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.', '']","['shallow parser used is the SNoW-based CSCL parser( #AUTHOR_TAG ; Munoz et al. , 1999 ) .', 'SNoW(Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', '']","['shallow parser used is the SNoW-based CSCL parser( #AUTHOR_TAG ; Munoz et al. , 1999 ) .', 'SNoW(Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', '']",5
"['start by reporting the results in which we compare the full parser and the shallow parser on the ""clean"" WSJ data.', 'Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim#AUTHOR_TAG ) terminology .', 'The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.', '']","['start by reporting the results in which we compare the full parser and the shallow parser on the ""clean"" WSJ data.', 'Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim#AUTHOR_TAG ) terminology .', 'The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.', '']","['start by reporting the results in which we compare the full parser and the shallow parser on the ""clean"" WSJ data.', 'Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim#AUTHOR_TAG ) terminology .', 'The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.', '']","['start by reporting the results in which we compare the full parser and the shallow parser on the ""clean"" WSJ data.', 'Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim#AUTHOR_TAG ) terminology .', 'The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.', 'Next, we compared the performance of the parsers on the task of identifying atomic phrases 2 .', '']",5
"['years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","[', along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['earlier versions of the SNoW based CSCL were used only to identify single phrases( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers']","['earlier versions of the SNoW based CSCL were used only to identify single phrases( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers']","['earlier versions of the SNoW based CSCL were used only to identify single phrases( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows']","['earlier versions of the SNoW based CSCL were used only to identify single phrases( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', '']",2
"['on shallow parsing was inspired by psycholinguistics arguments( #AUTHOR_TAG ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint']","['on shallow parsing was inspired by psycholinguistics arguments( #AUTHOR_TAG ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint']","['on shallow parsing was inspired by psycholinguistics arguments( #AUTHOR_TAG ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint']","['on shallow parsing was inspired by psycholinguistics arguments( #AUTHOR_TAG ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint']",0
"['perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'After']","['perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'After']","['perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'After']","['perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'After that, it will choose the candidate parse tree with the highest probability as output.', '']",5
"['years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","[', along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['shallow parser used is the SNoW-based CSCL parser( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) .', 'SNoW(Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example']","['shallow parser used is the SNoW-based CSCL parser( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) .', 'SNoW(Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.', '']","['shallow parser used is the SNoW-based CSCL parser( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) .', 'SNoW(Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', '']","['shallow parser used is the SNoW-based CSCL parser( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) .', 'SNoW(Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', '']",5
"['', 'SNoW( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', '']","['', 'SNoW( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', '']","['shallow parser used is the SNoW-based CSCL parser(Punyakanok and Roth, 2001;Munoz et al., 1999) .', 'SNoW( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', '']","['shallow parser used is the SNoW-based CSCL parser(Punyakanok and Roth, 2001;Munoz et al., 1999) .', 'SNoW( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', '']",5
"['the sentence �He reckons the current account deficit will narrow to only $ 1.8 billion in September .�', 'would be chunked as follows ( Tjong Kim#AUTHOR_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September]']","['the sentence �He reckons the current account deficit will narrow to only $ 1.8 billion in September .�', 'would be chunked as follows ( Tjong Kim#AUTHOR_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September]']","[', the sentence �He reckons the current account deficit will narrow to only $ 1.8 billion in September .�', 'would be chunked as follows ( Tjong Kim#AUTHOR_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September]']","['', 'A lot of recent work on shallow parsing has been influenced by Abney�s work(Abney, 1991) , who has suggested to �chunk� sentences to base level phrases.', 'For example, the sentence �He reckons the current account deficit will narrow to only $ 1.8 billion in September .�', 'would be chunked as follows ( Tjong Kim#AUTHOR_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September]']",0
"['noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization( #AUTHOR_TAG ; Appelt et al. , 1993 ) .', '']","['noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization( #AUTHOR_TAG ; Appelt et al. , 1993 ) .', '']","['that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization( #AUTHOR_TAG ; Appelt et al. , 1993 ) .', '']","[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization( #AUTHOR_TAG ; Appelt et al. , 1993 ) .', '']",0
"['years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","[', along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 )']","['years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 )']","[', along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 )']","['', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;#AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;#AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;#AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers( Collins , 1997 ; Charniak , 1997 a ;Charniak , 1997 b ;#AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['earlier versions of the SNoW based CSCL were used only to identify single phrases( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers']","['earlier versions of the SNoW based CSCL were used only to identify single phrases( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers']","['earlier versions of the SNoW based CSCL were used only to identify single phrases( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows']","['earlier versions of the SNoW based CSCL were used only to identify single phrases( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', '']",2
"['was done on the Penn Treebank( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .', '']","['was done on the Penn Treebank( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .', '']","['was done on the Penn Treebank( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .', '']","['was done on the Penn Treebank( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .', '']",5
"['ateisi et al. also translated LTAG into HPSG( #AUTHOR_TAG ) .', 'However, their method depended on translator�s intuitive analy- sis of the original grammar.', 'Thus the transla- tion was manual and grammar dependent.', 'The manual translation demanded considerable efforts from the translator, and obscures the equiva- lence between the original and obtained gram- mars.', 'Other works(Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars.', '']","['ateisi et al. also translated LTAG into HPSG( #AUTHOR_TAG ) .', 'However, their method depended on translator�s intuitive analy- sis of the original grammar.', 'Thus the transla- tion was manual and grammar dependent.', 'The manual translation demanded considerable efforts from the translator, and obscures the equiva- lence between the original and obtained gram- mars.', 'Other works(Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars.', '']","['ateisi et al. also translated LTAG into HPSG( #AUTHOR_TAG ) .', 'However, their method depended on translator�s intuitive analy- sis of the original grammar.', 'Thus the transla- tion was manual and grammar dependent.', 'The manual translation demanded considerable efforts from the translator, and obscures the equiva- lence between the original and obtained gram- mars.', 'Other works(Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars.', '']","['ateisi et al. also translated LTAG into HPSG( #AUTHOR_TAG ) .', 'However, their method depended on translator�s intuitive analy- sis of the original grammar.', 'Thus the transla- tion was manual and grammar dependent.', 'The manual translation demanded considerable efforts from the translator, and obscures the equiva- lence between the original and obtained gram- mars.', 'Other works(Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars.', '']",1
"['have been many studies on parsing techniques( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models( Chiang , 2000 ; #AUTHOR_TAG ) , and ones on programming/grammar-development environ -(Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998) .', 'These works are restricted to each closed community, and the relation between them is not well discussed.', 'Invest']","['have been many studies on parsing techniques( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models( Chiang , 2000 ; #AUTHOR_TAG ) , and ones on programming/grammar-development environ -(Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998) .', 'These works are restricted to each closed community, and the relation between them is not well discussed.', '']","['have been many studies on parsing techniques( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models( Chiang , 2000 ; #AUTHOR_TAG ) , and ones on programming/grammar-development environ -(Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998) .', 'These works are restricted to each closed community, and the relation between them is not well discussed.', 'Invest']","['have been many studies on parsing techniques( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models( Chiang , 2000 ; #AUTHOR_TAG ) , and ones on programming/grammar-development environ -(Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998) .', 'These works are restricted to each closed community, and the relation between them is not well discussed.', '']",0
"['which is a large-scale FB-LTAG grammar.', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser( #AUTHOR_TAG ) .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing']","['which is a large-scale FB-LTAG grammar.', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser( #AUTHOR_TAG ) .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing']","['which is a large-scale FB-LTAG grammar.', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser( #AUTHOR_TAG ) .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing']","['', 'We apply our system to the latest version of the XTAG English grammar (The XTAGResearch Group, 2001) , which is a large-scale FB-LTAG grammar.', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser( #AUTHOR_TAG ) .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing']",1
"['2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (vanNoord, 1994) without features (phase 1), and then executes feature unification (phase 2).', 'TNT refers to the HPSG parser( #AUTHOR_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .', 'Table']","['2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (vanNoord, 1994) without features (phase 1), and then executes feature unification (phase 2).', 'TNT refers to the HPSG parser( #AUTHOR_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .', 'Table']","['2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (vanNoord, 1994) without features (phase 1), and then executes feature unification (phase 2).', 'TNT refers to the HPSG parser( #AUTHOR_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .', 'Table']","['2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (vanNoord, 1994) without features (phase 1), and then executes feature unification (phase 2).', 'TNT refers to the HPSG parser( #AUTHOR_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .', 'Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.', '']",1
"['paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ;  Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG )( #AUTHOR_TAG ) by a method of grammar conversion .', 'The']","['paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ;  Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG )( #AUTHOR_TAG ) by a method of grammar conversion .', 'The']","['paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ;  Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG )( #AUTHOR_TAG ) by a method of grammar conversion .', '']","['paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ;  Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG )( #AUTHOR_TAG ) by a method of grammar conversion .', '']",0
"['node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).', 'FBLTAG( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the LTAG formalism .', '']","['node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).', 'FBLTAG( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the LTAG formalism .', '']","['foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).', 'FBLTAG( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the LTAG formalism .', '']","['', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).', 'FBLTAG( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the LTAG formalism .', '']",0
"['( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) .', 'These works are re- stricted to each closed community, and the rela- tion between them is not well discussed.', 'Investi- gating the relation will be apparently valuable for both communities']","['( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) .', 'These works are re- stricted to each closed community, and the rela- tion between them is not well discussed.', 'Investi- gating the relation will be apparently valuable for both communities']","['( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) .', 'These works are re- stricted to each closed community, and the rela- tion between them is not well discussed.', 'Investi- gating the relation will be apparently valuable for both communities']","['( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) .', 'These works are re- stricted to each closed community, and the rela- tion between them is not well discussed.', 'Investi- gating the relation will be apparently valuable for both communities']",0
"['paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 )( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and Head-Driven Phrase Structure Grammar ( HPSG )( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent']","['paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 )( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and Head-Driven Phrase Structure Grammar ( HPSG )( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent']","['paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 )( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and Head-Driven Phrase Structure Grammar ( HPSG )( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into']","['paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 )( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and Head-Driven Phrase Structure Grammar ( HPSG )( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', '']",0
['TAG( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG gram'],['TAG( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG gram'],['TAG( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG gram'],['TAG( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG gram'],0
"['', 'In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project( #AUTHOR_TAG ) .', '']","['', 'In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project( #AUTHOR_TAG ) .', '']","['', 'In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project( #AUTHOR_TAG ) .', '']","['', 'In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project( #AUTHOR_TAG ) .', '']",0
"['', 'Our group has developed a wide-coverage HPSG grammar for Japanese( #AUTHOR_TAG ) , which is used in a high-accuracy Japanese dependency analyzer( Kanayama et al. , 2000 )']","['', 'Our group has developed a wide-coverage HPSG grammar for Japanese( #AUTHOR_TAG ) , which is used in a high-accuracy Japanese dependency analyzer( Kanayama et al. , 2000 )']","['', 'Our group has developed a wide-coverage HPSG grammar for Japanese( #AUTHOR_TAG ) , which is used in a high-accuracy Japanese dependency analyzer( Kanayama et al. , 2000 )']","['', 'Our group has developed a wide-coverage HPSG grammar for Japanese( #AUTHOR_TAG ) , which is used in a high-accuracy Japanese dependency analyzer( Kanayama et al. , 2000 )']",0
"['', 'Other works( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert HPSG grammars into LTAG grammars .', '']","['', 'Other works( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert HPSG grammars into LTAG grammars .', '']","['', 'Other works( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert HPSG grammars into LTAG grammars .', '']","['', 'Other works( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert HPSG grammars into LTAG grammars .', 'However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar.', '']",1
"['', 'Our group has developed a wide-coverage HPSG grammar for Japanese( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer( #AUTHOR_TAG )']","['', 'Our group has developed a wide-coverage HPSG grammar for Japanese( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer( #AUTHOR_TAG )']","['', 'Our group has developed a wide-coverage HPSG grammar for Japanese( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer( #AUTHOR_TAG )']","['', 'Our group has developed a wide-coverage HPSG grammar for Japanese( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer( #AUTHOR_TAG )']",0
"['HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures( #AUTHOR_TAG ) .', 'A lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category.', 'An ID grammar']","['HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures( #AUTHOR_TAG ) .', 'A lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category.', 'An ID grammar']","['HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures( #AUTHOR_TAG ) .', 'A lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category.', 'An ID grammar rule']","['HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures( #AUTHOR_TAG ) .', 'A lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category.', '']",0
"['', 'We apply our system to the latest version of the XTAG English grammar ( The XTAG Research#AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing L']","['', 'We apply our system to the latest version of the XTAG English grammar ( The XTAG Research#AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG']","['', 'We apply our system to the latest version of the XTAG English grammar ( The XTAG Research#AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', '']","['', 'We apply our system to the latest version of the XTAG English grammar ( The XTAG Research#AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', '']",5
"['', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAGResearch Group, 2001) .', 'The XTAG group( #AUTHOR_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .', 'Development of a large-scale French grammar (Abeillé andCandito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7']","['', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAGResearch Group, 2001) .', 'The XTAG group( #AUTHOR_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .', 'Development of a large-scale French grammar (Abeillé andCandito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7']","['', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAGResearch Group, 2001) .', 'The XTAG group( #AUTHOR_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .', 'Development of a large-scale French grammar (Abeillé andCandito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7']","['', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAGResearch Group, 2001) .', 'The XTAG group( #AUTHOR_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .', 'Development of a large-scale French grammar (Abeillé andCandito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7']",0
"['', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar( #AUTHOR_TAG ) .', '']","['', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar( #AUTHOR_TAG ) .', '']","['', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar( #AUTHOR_TAG ) .', '']","['paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoin-ing Grammar (FB-LTAG 1 ) (Vijay-Shanker, 1987; Vijay-Shanker and Joshi, 1988) and Head-Driven Phrase Structure Grammar (HPSG)(Pollard and Sag, 1994) by a method of grammar conversion.', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar( #AUTHOR_TAG ) .', '']",0
"['2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2 , lem refers to the LTAG parser( #AUTHOR_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( vanNoord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .', 'TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CF']","['2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2 , lem refers to the LTAG parser( #AUTHOR_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( vanNoord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .', 'TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG']","['2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2 , lem refers to the LTAG parser( #AUTHOR_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( vanNoord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .', 'TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG']","['2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2 , lem refers to the LTAG parser( #AUTHOR_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( vanNoord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .', 'TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2).', '']",1
"['node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).', 'FBLTAG( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .', '']","['node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).', 'FBLTAG( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .', '']","['foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).', 'FBLTAG( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .', '']","['', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).', 'FBLTAG( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .', '']",0
"['are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project( #AUTHOR_TAG ) .', '']","['are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project( #AUTHOR_TAG ) .', '']","['are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project( #AUTHOR_TAG ) .', '']","['are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project( #AUTHOR_TAG ) .', '']",0
"['', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques(Poller and Becker, 1998; Flickinger et al., 2000) , ones on disambiguation models(Chiang, 2000; Kanayama et al., 2000) , and ones on programming/grammar-development environment( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities']","['', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques(Poller and Becker, 1998; Flickinger et al., 2000) , ones on disambiguation models(Chiang, 2000; Kanayama et al., 2000) , and ones on programming/grammar-development environment( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities']","['', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques(Poller and Becker, 1998; Flickinger et al., 2000) , ones on disambiguation models(Chiang, 2000; Kanayama et al., 2000) , and ones on programming/grammar-development environment( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities']","['', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques(Poller and Becker, 1998; Flickinger et al., 2000) , ones on disambiguation models(Chiang, 2000; Kanayama et al., 2000) , and ones on programming/grammar-development environment( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities']",0
"['RenTAL system is implemented in LiL-FeS(Makino et al., 1998) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system( Nishida et al. , 1999 ; #AUTHOR_TAG ) .', 'We applied our system to the XTAG English grammar (']","['RenTAL system is implemented in LiL-FeS(Makino et al., 1998) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system( Nishida et al. , 1999 ; #AUTHOR_TAG ) .', 'We applied our system to the XTAG English grammar (The']","['RenTAL system is implemented in LiL-FeS(Makino et al., 1998) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system( Nishida et al. , 1999 ; #AUTHOR_TAG ) .', 'We applied our system to the XTAG English grammar (']","['RenTAL system is implemented in LiL-FeS(Makino et al., 1998) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system( Nishida et al. , 1999 ; #AUTHOR_TAG ) .', '']",0
"['', 'Another paper( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance']","['', 'Another paper( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance']","['', 'Another paper( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance']","['', 'Another paper( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance']",0
"['', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus( #AUTHOR_TAG ) 6 ( the average length is 6.32 words ) .', 'This result empirically attested the strong equivalence of our algorithm']","['a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus( #AUTHOR_TAG ) 6 ( the average length is 6.32 words ) .', 'This result empirically attested the strong equivalence of our algorithm']","['', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus( #AUTHOR_TAG ) 6 ( the average length is 6.32 words ) .', 'This result empirically attested the strong equivalence of our algorithm']","['', 'We applied our system to the XTAG English grammar (The XTAGResearch Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus( #AUTHOR_TAG ) 6 ( the average length is 6.32 words ) .', 'This result empirically attested the strong equivalence of our algorithm']",5
"['RenTAL system is implemented in LiLFeS( #AUTHOR_TAG ) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system(Nishida et al., 1999; .', 'We applied our system to the XTAG English grammar']","['RenTAL system is implemented in LiLFeS( #AUTHOR_TAG ) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system(Nishida et al., 1999; .', 'We applied our system to the XTAG English grammar']","['RenTAL system is implemented in LiLFeS( #AUTHOR_TAG ) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system(Nishida et al., 1999; .', 'We applied our system to the XTAG English grammar']","['RenTAL system is implemented in LiLFeS( #AUTHOR_TAG ) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system(Nishida et al., 1999; .', '']",5
"['paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 )( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG )( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent']","['paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 )( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG )( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent']","['paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 )( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG )( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into']","['paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 )( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG )( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', '']",0
"['', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques(Poller and Becker, 1998; Flickinger et al., 2000) , ones on disambiguation models(Chiang, 2000; Kanayama et al., 2000) , and ones on programming/grammar-development( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities']","['grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques(Poller and Becker, 1998; Flickinger et al., 2000) , ones on disambiguation models(Chiang, 2000; Kanayama et al., 2000) , and ones on programming/grammar-development( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities']","['', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques(Poller and Becker, 1998; Flickinger et al., 2000) , ones on disambiguation models(Chiang, 2000; Kanayama et al., 2000) , and ones on programming/grammar-development( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities']","['', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques(Poller and Becker, 1998; Flickinger et al., 2000) , ones on disambiguation models(Chiang, 2000; Kanayama et al., 2000) , and ones on programming/grammar-development( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities']",0
"['', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment(Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998) .', 'These works are restricted to each closed community, and the relation']","['grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment(Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998) .', 'These works are restricted to each closed community, and the relation']","['', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment(Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998) .', 'These works are restricted to each closed community, and the relation']","['', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment(Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998) .', 'These works are restricted to each closed community, and the relation between them is not well discussed.', '']",0
['grammar conversion from LTAG to HPSG( #AUTHOR_TAG ) is the core portion of the RenTAL system'],['grammar conversion from LTAG to HPSG( #AUTHOR_TAG ) is the core portion of the RenTAL system'],['grammar conversion from LTAG to HPSG( #AUTHOR_TAG ) is the core portion of the RenTAL system'],['grammar conversion from LTAG to HPSG( #AUTHOR_TAG ) is the core portion of the RenTAL system'],0
"['feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research#AUTHOR_TAG ) .', 'The XTAG group(Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeillé andCandito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7']","['feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research#AUTHOR_TAG ) .', 'The XTAG group(Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeillé andCandito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7']","['a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research#AUTHOR_TAG ) .', 'The XTAG group(Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeillé andCandito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7']","['', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research#AUTHOR_TAG ) .', 'The XTAG group(Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeillé andCandito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7']",0
"['', 'We applied our system to the XTAG English grammar ( The XTAG Research#AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser']","['', 'We applied our system to the XTAG English grammar ( The XTAG Research#AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser']","['', 'We applied our system to the XTAG English grammar ( The XTAG Research#AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', '']","['', 'We applied our system to the XTAG English grammar ( The XTAG Research#AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', '']",5
"['has been some controversy , at least for simple stemmers( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Emp']","['has been some controversy , at least for simple stemmers( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical']","['has been some controversy , at least for simple stemmers( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Emp']","['has been some controversy , at least for simple stemmers( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', '']",0
"['an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG )']","['an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG )']","['an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG )']","['', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG )']",0
"['efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance .', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992; Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001) .', '']","['efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance .', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992; Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001) .', '']","['efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance .', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992; Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001) .', '']","['efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance .', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992; Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001) .', '']",0
"['has been some controversy , at least for simple stemmers( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Emp']","['has been some controversy , at least for simple stemmers( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical']","['has been some controversy , at least for simple stemmers( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Emp']","['has been some controversy , at least for simple stemmers( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', '']",0
"['efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance .', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992; Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001) .', '']","['efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance .', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992; Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001) .', '']","['efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance .', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992; Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001) .', '']","['efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance .', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992; Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001) .', '']",0
"['phological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system( Choueka , 1990 ;  J Â¨ appinen and Niemist Â¨ o , 1988 ;#AUTHOR_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', '']","['phological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system( Choueka , 1990 ;  J Â¨ appinen and Niemist Â¨ o , 1988 ;#AUTHOR_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', '']","['phological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system( Choueka , 1990 ;  J Â¨ appinen and Niemist Â¨ o , 1988 ;#AUTHOR_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', '']","['phological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system( Choueka , 1990 ;  J Â¨ appinen and Niemist Â¨ o , 1988 ;#AUTHOR_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', '']",0
"['an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']",0
"['has been some controversy , at least for simple stemmers( Lovins , 1968 ; #AUTHOR_TAG ) , about the effectiveness of morphological analysis for document retrieval( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems']","['has been some controversy , at least for simple stemmers( Lovins , 1968 ; #AUTHOR_TAG ) , about the effectiveness of morphological analysis for document retrieval( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems']","['has been some controversy , at least for simple stemmers( Lovins , 1968 ; #AUTHOR_TAG ) , about the effectiveness of morphological analysis for document retrieval( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems']","['has been some controversy , at least for simple stemmers( Lovins , 1968 ; #AUTHOR_TAG ) , about the effectiveness of morphological analysis for document retrieval( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', '']",0
"['but at high ones its precision decreases almost dramatically.', 'Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in( #AUTHOR_TAG ) ) one can not really recommend this method']","['but at high ones its precision decreases almost dramatically.', 'Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in( #AUTHOR_TAG ) ) one can not really recommend this method']","['but at high ones its precision decreases almost dramatically.', 'Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in( #AUTHOR_TAG ) ) one can not really recommend this method']","['but at high ones its precision decreases almost dramatically.', 'Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in( #AUTHOR_TAG ) ) one can not really recommend this method']",1
"['an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 )']",0
"['.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 )']","['dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 )']","['.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 )']","['', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 )']",0
"['', 'The retrieval process relies on the vector space model( #AUTHOR_TAG ) , with the cosine measure expressing the similarity between a query and a document .', '']","['', 'The retrieval process relies on the vector space model( #AUTHOR_TAG ) , with the cosine measure expressing the similarity between a query and a document .', '']","['', 'The retrieval process relies on the vector space model( #AUTHOR_TAG ) , with the cosine measure expressing the similarity between a query and a document .', '']","['', 'The retrieval process relies on the vector space model( #AUTHOR_TAG ) , with the cosine measure expressing the similarity between a query and a document .', '']",5
"['has been some controversy , at least for simple stemmers( #AUTHOR_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems']","['has been some controversy , at least for simple stemmers( #AUTHOR_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems']","['has been some controversy , at least for simple stemmers( #AUTHOR_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems']","['has been some controversy , at least for simple stemmers( #AUTHOR_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', '']",0
"['', 'While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting( #AUTHOR_TAG )']","['', 'While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting( #AUTHOR_TAG )']","[', medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language (e.g., German), often referred to as neo-classical compounding (Mc-Cray et al., 1988) .', 'While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting( #AUTHOR_TAG )']","[', medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language (e.g., German), often referred to as neo-classical compounding (Mc-Cray et al., 1988) .', 'While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting( #AUTHOR_TAG )']",0
"['to a complete mismatch between query and documents.', 'This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH ,( #AUTHOR_TAG ) ) are incorporated into our system .', '']","['to a complete mismatch between query and documents.', 'This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH ,( #AUTHOR_TAG ) ) are incorporated into our system .', '']","['to a complete mismatch between query and documents.', 'This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH ,( #AUTHOR_TAG ) ) are incorporated into our system .', '']","['', 'This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH ,( #AUTHOR_TAG ) ) are incorporated into our system .', '']",3
"['has been some controversy , at least for simple stemmers( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Emp']","['has been some controversy , at least for simple stemmers( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical']","['has been some controversy , at least for simple stemmers( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Emp']","['has been some controversy , at least for simple stemmers( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', '']",0
"['efforts required for performing morphologi- cal analysis vary from language to language.', 'For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers(Lovins, 1968; Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ;Choueka , 1990 ; #AUTHOR_TAG ; Ekmekc Â¸ ioglu et al. , 1995 ;Hedlund et al. , 2001 ; Pirkola , 2001 ) .', '']","['efforts required for performing morphologi- cal analysis vary from language to language.', 'For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers(Lovins, 1968; Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ;Choueka , 1990 ; #AUTHOR_TAG ; Ekmekc Â¸ ioglu et al. , 1995 ;Hedlund et al. , 2001 ; Pirkola , 2001 ) .', '']","['efforts required for performing morphologi- cal analysis vary from language to language.', 'For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers(Lovins, 1968; Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ;Choueka , 1990 ; #AUTHOR_TAG ; Ekmekc Â¸ ioglu et al. , 1995 ;Hedlund et al. , 2001 ; Pirkola , 2001 ) .', '']","['efforts required for performing morphologi- cal analysis vary from language to language.', 'For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers(Lovins, 1968; Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ;Choueka , 1990 ; #AUTHOR_TAG ; Ekmekc Â¸ ioglu et al. , 1995 ;Hedlund et al. , 2001 ; Pirkola , 2001 ) .', '']",0
"['an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']",0
"['an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']",0
"[')) are incorporated into our system.', 'Alternatively , we may think of user-centered comparative studies( #AUTHOR_TAG )']","['are incorporated into our system.', 'Alternatively , we may think of user-centered comparative studies( #AUTHOR_TAG )']","[')) are incorporated into our system.', 'Alternatively , we may think of user-centered comparative studies( #AUTHOR_TAG )']","['', 'Alternatively , we may think of user-centered comparative studies( #AUTHOR_TAG )']",3
"['.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG )']","['dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG )']","['.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG )']","['', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG )']",0
"['documents.', 'Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French( #AUTHOR_TAG ) , turns out to be infeasible , at least for German and related languages']","['documents.', 'Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French( #AUTHOR_TAG ) , turns out to be infeasible , at least for German and related languages']","['relevant documents.', 'Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French( #AUTHOR_TAG ) , turns out to be infeasible , at least for German and related languages']","['', 'Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French( #AUTHOR_TAG ) , turns out to be infeasible , at least for German and related languages']",0
"['approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ;Choueka , 1990 ; Popovic and Willett , 1992 ;  Ekmekc Â¸ ioglu et al. , 1995 ;Hedlund et al. , 2001 ; #AUTHOR_TAG ) .', '']","['approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ;Choueka , 1990 ; Popovic and Willett , 1992 ;  Ekmekc Â¸ ioglu et al. , 1995 ;Hedlund et al. , 2001 ; #AUTHOR_TAG ) .', '']","['( J Â¨ appinen and Niemist Â¨ o , 1988 ;Choueka , 1990 ; Popovic and Willett , 1992 ;  Ekmekc Â¸ ioglu et al. , 1995 ;Hedlund et al. , 2001 ; #AUTHOR_TAG ) .', '']","['', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ;Choueka , 1990 ; Popovic and Willett , 1992 ;  Ekmekc Â¸ ioglu et al. , 1995 ;Hedlund et al. , 2001 ; #AUTHOR_TAG ) .', '']",0
"['phological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system( #AUTHOR_TAG ; J Â¨ appinen and Niemist Â¨ o , 1988 ;Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved']","['phological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system( #AUTHOR_TAG ; J Â¨ appinen and Niemist Â¨ o , 1988 ;Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved']","['phological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system( #AUTHOR_TAG ; J Â¨ appinen and Niemist Â¨ o , 1988 ;Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', '']","['phological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system( #AUTHOR_TAG ; J Â¨ appinen and Niemist Â¨ o , 1988 ;Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', '']",0
"[', medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding( #AUTHOR_TAG ) .', 'While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting(Wolff, 1984)']","[', medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding( #AUTHOR_TAG ) .', 'While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting(Wolff, 1984)']","[', medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding( #AUTHOR_TAG ) .', 'While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting(Wolff, 1984)']","[', medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding( #AUTHOR_TAG ) .', 'While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting(Wolff, 1984)']",0
"['', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( #AUTHOR_TAG ) ,( Meyers et al. , 2000 ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']","['', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( #AUTHOR_TAG ) ,( Meyers et al. , 2000 ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']","['', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( #AUTHOR_TAG ) ,( Meyers et al. , 2000 ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']","['', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( #AUTHOR_TAG ) ,( Meyers et al. , 2000 ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']",0
"['correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( Meyers et al. , 2000 ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) ,( #AUTHOR_TAG )']","['correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( Meyers et al. , 2000 ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) ,( #AUTHOR_TAG )']","['structural correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( Meyers et al. , 2000 ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) ,( #AUTHOR_TAG )']","['', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( Meyers et al. , 2000 ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) ,( #AUTHOR_TAG )']",0
"['this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC )( #AUTHOR_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .', 'The correspondence between the string and its associated representation tree structure is defined in terms of the sub-correspondence between parts of the string (substrings) and parts of the tree structure (subtrees), which can']","['this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC )( #AUTHOR_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .', 'The correspondence between the string and its associated representation tree structure is defined in terms of the sub-correspondence between parts of the string (substrings) and parts of the tree structure (subtrees), which can']","['this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC )( #AUTHOR_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .', 'The correspondence between the string and its associated representation tree structure is defined in terms of the sub-correspondence between parts of the string (substrings) and parts of the tree structure (subtrees), which']","['this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC )( #AUTHOR_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .', 'The correspondence between the string and its associated representation tree structure is defined in terms of the sub-correspondence between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation.', '']",0
"['', 'These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies( #AUTHOR_TAG ) .', 'crossed dependencies(Tang & Zaharin, 1995)']","['', 'These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies( #AUTHOR_TAG ) .', 'crossed dependencies(Tang & Zaharin, 1995)']","['the facility to specify the correspondence between the string and the associated tree which can be nonprojective(Boitet & Zaharin, 1988) .', 'These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies( #AUTHOR_TAG ) .', 'crossed dependencies(Tang & Zaharin, 1995)']","['SSTC is a general structure that can associate an arbitrary tree structure to string in a language as desired by the annotator to be the interpretation structure of the string, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be nonprojective(Boitet & Zaharin, 1988) .', 'These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies( #AUTHOR_TAG ) .', 'crossed dependencies(Tang & Zaharin, 1995)']",0
"['', 'Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in#AUTHOR_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC']","['', 'Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in#AUTHOR_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC']","['', 'Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in#AUTHOR_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC']","['', 'Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in#AUTHOR_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC']",0
"['', '#AUTHOR_TAG presented an approach for constructing a BKB based on the S-SSTC']","['', '#AUTHOR_TAG presented an approach for constructing a BKB based on the S-SSTC']","['', '#AUTHOR_TAG presented an approach for constructing a BKB based on the S-SSTC']","['', '#AUTHOR_TAG presented an approach for constructing a BKB based on the S-SSTC']",0
"['', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( #AUTHOR_TAG ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']","['', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( #AUTHOR_TAG ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']","['', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( #AUTHOR_TAG ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']","['', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( #AUTHOR_TAG ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']",0
"['', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by(Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL( Shieber , 1994 ) ,( #AUTHOR_TAG ) .', '']","['', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by(Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL( Shieber , 1994 ) ,( #AUTHOR_TAG ) .', '']","['', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by(Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL( Shieber , 1994 ) ,( #AUTHOR_TAG ) .', '']","['', 'For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeillé et al.,1990),(Harbusch & Poller,1996) , or for relating a syntactic TAG and semantic one for the same language(Shieber & Schabes,1990) .', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by(Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL( Shieber , 1994 ) ,( #AUTHOR_TAG ) .', 'As a result,Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.', '']",0
"['the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts( #AUTHOR_TAG ) .', 'The MTT point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community']","['the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts( #AUTHOR_TAG ) .', 'The MTT point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community']","['the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts( #AUTHOR_TAG ) .', 'The MTT point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community']","['the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts( #AUTHOR_TAG ) .', 'The MTT point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community']",0
"['or find structural correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( Meyers et al. , 2000 ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( #AUTHOR_TAG ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']","['or find structural correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( Meyers et al. , 2000 ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( #AUTHOR_TAG ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']","['or find structural correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( Meyers et al. , 2000 ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( #AUTHOR_TAG ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']","['', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( Meyers et al. , 2000 ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( #AUTHOR_TAG ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']",0
"['mentioned earlier, there are some non-standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g.#AUTHOR_TAG cases ) .', '']","['mentioned earlier, there are some non-standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g.#AUTHOR_TAG cases ) .', '']","['mentioned earlier, there are some non-standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g.#AUTHOR_TAG cases ) .', '']","['mentioned earlier, there are some non-standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g.#AUTHOR_TAG cases ) .', '']",0
"['such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( Meyers et al. , 2000 ) ,( #AUTHOR_TAG ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']","['such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( Meyers et al. , 2000 ) ,( #AUTHOR_TAG ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']","['such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( Meyers et al. , 2000 ) ,( #AUTHOR_TAG ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']","['', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( Meyers et al. , 2000 ) ,( #AUTHOR_TAG ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']",0
"['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures( #AUTHOR_TAG ) , such as the relation between syntax and semantic']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures( #AUTHOR_TAG ) , such as the relation between syntax and semantic']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures( #AUTHOR_TAG ) , such as the relation between syntax and semantic']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures( #AUTHOR_TAG ) , such as the relation between syntax and semantic']",0
"['', ""For instance, when building translation units in EBMT approaches( Richardson et al. , 2001 ) ,( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) ,( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed cor""]","['', ""For instance, when building translation units in EBMT approaches( Richardson et al. , 2001 ) ,( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) ,( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed cor""]","['', ""For instance, when building translation units in EBMT approaches( Richardson et al. , 2001 ) ,( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) ,( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed cor""]","['', ""For instance, when building translation units in EBMT approaches( Richardson et al. , 2001 ) ,( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) ,( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed cor""]",0
"['', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning( #AUTHOR_TAG ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( Meyers et al. , 2000 ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']","['', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning( #AUTHOR_TAG ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( Meyers et al. , 2000 ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']","['to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning( #AUTHOR_TAG ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( Meyers et al. , 2000 ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']","['', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning( #AUTHOR_TAG ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( Meyers et al. , 2000 ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']",0
"['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures( #AUTHOR_TAG )']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures( #AUTHOR_TAG )']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures( #AUTHOR_TAG )']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures( #AUTHOR_TAG )']",0
"['box up"".', 'For more details on the proprieties of SSTC , see#AUTHOR_TAG']","['box up"".', 'For more details on the proprieties of SSTC , see#AUTHOR_TAG']","['the box up"".', 'For more details on the proprieties of SSTC , see#AUTHOR_TAG']","['', 'For more details on the proprieties of SSTC , see#AUTHOR_TAG']",0
"['', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in( #AUTHOR_TAG ) andBoitet & Zaharin ( 1988 ) .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree']","['', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in( #AUTHOR_TAG ) andBoitet & Zaharin ( 1988 ) .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree']","['', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in( #AUTHOR_TAG ) andBoitet & Zaharin ( 1988 ) .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree']","['', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in( #AUTHOR_TAG ) andBoitet & Zaharin ( 1988 ) .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree']",5
"[', 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( #AUTHOR_TAG ) , ( Al-Adhaileh & Tang , 1999 )']","[', 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( #AUTHOR_TAG ) , ( Al-Adhaileh & Tang , 1999 )']","['( Watanabe et al. , 2000 ) ,( Meyers et al. , 2000 ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( #AUTHOR_TAG ) , ( Al-Adhaileh & Tang , 1999 )']","['', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( Meyers et al. , 2000 ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( Sato , 1991 ) ,( #AUTHOR_TAG ) , ( Al-Adhaileh & Tang , 1999 )']",0
"['', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by(Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL( #AUTHOR_TAG ) ,( Harbusch & Poller , 2000 )']","['', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by(Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL( #AUTHOR_TAG ) ,( Harbusch & Poller , 2000 )']","['', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by(Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL( #AUTHOR_TAG ) ,( Harbusch & Poller , 2000 ) .', '']","['', 'For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeillé et al.,1990),(Harbusch & Poller,1996) , or for relating a syntactic TAG and semantic one for the same language(Shieber & Schabes,1990) .', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by(Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL( #AUTHOR_TAG ) ,( Harbusch & Poller , 2000 ) .', '']",0
"['structural correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( Meyers et al. , 2000 ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( #AUTHOR_TAG ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']","['structural correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( Meyers et al. , 2000 ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( #AUTHOR_TAG ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']","['structural correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( Meyers et al. , 2000 ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( #AUTHOR_TAG ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']","['', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning( Menezes & Richardson , 2001 ) ,( Aramaki et al. , 2001 ) ,( Watanabe et al. , 2000 ) ,( Meyers et al. , 2000 ) ,( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3( Sato & Nagao , 1990 ) ,( #AUTHOR_TAG ) ,( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 )']",0
"['and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in( Tang , 1994 ) and#AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree']","['and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in( Tang , 1994 ) and#AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree']","['and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in( Tang , 1994 ) and#AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree']","['2 illustrates the sentence ""John picks the box up"" with its corresponding SSTC.', 'It contains a nonprojective correspondence.', 'An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in( Tang , 1994 ) and#AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree']",5
"[', the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data#AUTHOR_TAG ) .', 'Recent work(Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data.', 'Also, many potential applications of NLP will involve processing very large text databases.', '']","[', the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data#AUTHOR_TAG ) .', 'Recent work(Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data.', 'Also, many potential applications of NLP will involve processing very large text databases.', '']","[', the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data#AUTHOR_TAG ) .', 'Recent work(Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data.', 'Also, many potential applications of NLP will involve processing very large text databases.', '']","[', the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data#AUTHOR_TAG ) .', 'Recent work(Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data.', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.', '']",0
"['of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems( #AUTHOR_TAG ) and speech recognition( Hacioglu and Pellom , 2003 ) .', '']","['of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems( #AUTHOR_TAG ) and speech recognition( Hacioglu and Pellom , 2003 ) .', '']","['of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems( #AUTHOR_TAG ) and speech recognition( Hacioglu and Pellom , 2003 ) .', '']","['', 'This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems( #AUTHOR_TAG ) and speech recognition( Hacioglu and Pellom , 2003 ) .', '']",0
"['discussed earlier, there are two main requirements of the system that are covered by ""high performance"": speed and state of the art accuracy.', 'Efficiency is required both in training and processing.', 'Efficient training is required because the amount of data available for training will increase significantly.', 'Also , advanced methods often require many training iterations , for example active learning( Dagan and Engelson ,1995 ) and co-training( #AUTHOR_TAG ) .', 'Processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many']","['discussed earlier, there are two main requirements of the system that are covered by ""high performance"": speed and state of the art accuracy.', 'Efficiency is required both in training and processing.', 'Efficient training is required because the amount of data available for training will increase significantly.', 'Also , advanced methods often require many training iterations , for example active learning( Dagan and Engelson ,1995 ) and co-training( #AUTHOR_TAG ) .', 'Processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many']","['discussed earlier, there are two main requirements of the system that are covered by ""high performance"": speed and state of the art accuracy.', 'Efficiency is required both in training and processing.', 'Efficient training is required because the amount of data available for training will increase significantly.', 'Also , advanced methods often require many training iterations , for example active learning( Dagan and Engelson ,1995 ) and co-training( #AUTHOR_TAG ) .', 'Processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or']","['discussed earlier, there are two main requirements of the system that are covered by ""high performance"": speed and state of the art accuracy.', 'Efficiency is required both in training and processing.', 'Efficient training is required because the amount of data available for training will increase significantly.', 'Also , advanced methods often require many training iterations , for example active learning( Dagan and Engelson ,1995 ) and co-training( #AUTHOR_TAG ) .', 'Processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly']",0
"['are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE )( #AUTHOR_TAG ) and the Alembic Workbench( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors(Cunningham et al., 2002) .', 'GATE goes beyond earlier systems by using a component-based infrastructure(Cunningham, 2000) which the GUI is built on top of.', '']","['are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE )( #AUTHOR_TAG ) and the Alembic Workbench( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors(Cunningham et al., 2002) .', 'GATE goes beyond earlier systems by using a component-based infrastructure(Cunningham, 2000) which the GUI is built on top of.', '']","['are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE )( #AUTHOR_TAG ) and the Alembic Workbench( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors(Cunningham et al., 2002) .', 'GATE goes beyond earlier systems by using a component-based infrastructure(Cunningham, 2000) which the GUI is built on top of.', '']","['are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE )( #AUTHOR_TAG ) and the Alembic Workbench( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors(Cunningham et al., 2002) .', 'GATE goes beyond earlier systems by using a component-based infrastructure(Cunningham, 2000) which the GUI is built on top of.', '']",0
"['', 'This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems( Bayer et al. , 2001 ) and speech recognition( #AUTHOR_TAG ) .', 'Web services will']","['', 'This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems( Bayer et al. , 2001 ) and speech recognition( #AUTHOR_TAG ) .', 'Web services will']","['', 'This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems( Bayer et al. , 2001 ) and speech recognition( #AUTHOR_TAG ) .', 'Web services will allow']","['', 'This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems( Bayer et al. , 2001 ) and speech recognition( #AUTHOR_TAG ) .', '']",0
"['elemental components.', 'For instance , implementing an efficient version of the MXPOST POS tagger( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component']","['elemental components.', 'For instance , implementing an efficient version of the MXPOST POS tagger( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component']","['many elemental components.', 'For instance , implementing an efficient version of the MXPOST POS tagger( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component']","['Generative Programming approach to NLP infrastructure development will allow tools such as sentence boundary detectors, POS taggers, chunkers and named entity recognisers to be rapidly composed from many elemental components.', 'For instance , implementing an efficient version of the MXPOST POS tagger( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component']",3
"['implementation has been inspired by experience in extracting information from very large corpora( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'We have already implemented a POS tagger']","['implementation has been inspired by experience in extracting information from very large corpora( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'We have already implemented a POS tagger,']","['implementation has been inspired by experience in extracting information from very large corpora( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', '']","['implementation has been inspired by experience in extracting information from very large corpora( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', '']",4
"['implementation has been inspired by experience in extracting information from very large corpora( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging( Curran and Clark , 2003 ; Clark et al. , 2003 ) .', '']","['implementation has been inspired by experience in extracting information from very large corpora( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging( Curran and Clark , 2003 ; Clark et al. , 2003 ) .', '']","['implementation has been inspired by experience in extracting information from very large corpora( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging( Curran and Clark , 2003 ; Clark et al. , 2003 ) .', '']","['implementation has been inspired by experience in extracting information from very large corpora( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging( Curran and Clark , 2003 ; Clark et al. , 2003 ) .', 'We have already implemented a P O S tagger, chunker, C C G supertagger and named entity recogniser using the infrastructure.', '']",4
"['iciency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by#AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly( Malouf , 2002 ) .', '']","['iciency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by#AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly( Malouf , 2002 ) .', '']","['iciency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by#AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly( Malouf , 2002 ) .', '']","['iciency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by#AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly( Malouf , 2002 ) .', '']",0
"['', 'Other tools have been designed around particular techniques , such as finite state machines( Karttunen et al. , 1997 ; #AUTHOR_TAG ) .', '']","['', 'Other tools have been designed around particular techniques , such as finite state machines( Karttunen et al. , 1997 ; #AUTHOR_TAG ) .', 'However, the source code for these tools is not freely available, so they cannot be extended']","['', 'Other tools have been designed around particular techniques , such as finite state machines( Karttunen et al. , 1997 ; #AUTHOR_TAG ) .', '']","['', 'Other tools have been designed around particular techniques , such as finite state machines( Karttunen et al. , 1997 ; #AUTHOR_TAG ) .', 'However, the source code for these tools is not freely available, so they cannot be extended']",0
"['', 'Recent work( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web']","['', 'Recent work( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.', '']","[', the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus(Linguistic Data Consortium, 2003) .', 'Recent work( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.', '']","[', the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus(Linguistic Data Consortium, 2003) .', 'Recent work( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.', '']",0
"['', 'For example , 10 million words of the American National Corpus( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank( #AUTHOR_TAG ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations']","['', 'Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank( #AUTHOR_TAG ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations']","['', 'For example , 10 million words of the American National Corpus( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank( #AUTHOR_TAG ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations']","['LP is experiencing an explosion in the quantity of electronic text available.', 'Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank( #AUTHOR_TAG ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations']",0
['already been used to implement a framework for teaching NLP( #AUTHOR_TAG )'],['already been used to implement a framework for teaching NLP( #AUTHOR_TAG )'],['been used to implement a framework for teaching NLP( #AUTHOR_TAG )'],"['', 'It has already been used to implement a framework for teaching NLP( #AUTHOR_TAG )']",2
"['are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE )( Cunningham et al. , 1997 ) and the Alembic Workbench( #AUTHOR_TAG ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors(Cunningham et al., 2002) .', 'GATE goes beyond earlier systems by using a component-based infrastructure(Cunningham, 2000) which the GUI is built on top']","['are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE )( Cunningham et al. , 1997 ) and the Alembic Workbench( #AUTHOR_TAG ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors(Cunningham et al., 2002) .', 'GATE goes beyond earlier systems by using a component-based infrastructure(Cunningham, 2000) which the GUI is built on top']","['are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE )( Cunningham et al. , 1997 ) and the Alembic Workbench( #AUTHOR_TAG ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors(Cunningham et al., 2002) .', 'GATE goes beyond earlier systems by using a component-based infrastructure(Cunningham, 2000) which the GUI is built on']","['are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE )( Cunningham et al. , 1997 ) and the Alembic Workbench( #AUTHOR_TAG ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors(Cunningham et al., 2002) .', 'GATE goes beyond earlier systems by using a component-based infrastructure(Cunningham, 2000) which the GUI is built on top of.', '']",0
"['.', 'For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors( #AUTHOR_TAG ) .', 'GATE goes beyond earlier systems by using a component-based infrastructure(Cunningham, 2000) which the GUI is built on top of.', '']","['', 'For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors( #AUTHOR_TAG ) .', 'GATE goes beyond earlier systems by using a component-based infrastructure(Cunningham, 2000) which the GUI is built on top of.', '']","['', 'For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors( #AUTHOR_TAG ) .', 'GATE goes beyond earlier systems by using a component-based infrastructure(Cunningham, 2000) which the GUI is built on top of.', '']","['', 'For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors( #AUTHOR_TAG ) .', 'GATE goes beyond earlier systems by using a component-based infrastructure(Cunningham, 2000) which the GUI is built on top of.', '']",0
"['engineering research on Generative Programming( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .', 'Our infrastructure']","['engineering research on Generative Programming( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .', 'Our infrastructure']","['engineering research on Generative Programming( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .', 'Our infrastructure']","['engineering research on Generative Programming( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .', '']",0
"['LP is experiencing an explosion in the quantity of electronic text available.', 'Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus( #AUTHOR_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank( Marcus et al. , 1993 ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations']","['LP is experiencing an explosion in the quantity of electronic text available.', 'Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus( #AUTHOR_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank( Marcus et al. , 1993 ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations']","['LP is experiencing an explosion in the quantity of electronic text available.', 'Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus( #AUTHOR_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank( Marcus et al. , 1993 ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations']","['LP is experiencing an explosion in the quantity of electronic text available.', 'Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus( #AUTHOR_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank( Marcus et al. , 1993 ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations']",0
"['number of stand-alone tools have also been developed.', 'For example , the suite of LT tools( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files']","['number of stand-alone tools have also been developed.', 'For example , the suite of LT tools( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', '']","['number of stand-alone tools have also been developed.', 'For example , the suite of LT tools( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', '']","['number of stand-alone tools have also been developed.', 'For example , the suite of LT tools( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', '']",0
"['', 'To provide the required configurability in the static version of the code we will use policy templates( #AUTHOR_TAG ) , and for the dynamic version we will use configuration classes']","['', 'To provide the required configurability in the static version of the code we will use policy templates( #AUTHOR_TAG ) , and for the dynamic version we will use configuration classes']","['', 'To provide the required configurability in the static version of the code we will use policy templates( #AUTHOR_TAG ) , and for the dynamic version we will use configuration classes']","['', 'To provide the required configurability in the static version of the code we will use policy templates( #AUTHOR_TAG ) , and for the dynamic version we will use configuration classes']",5
"['', 'Recent work( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web']","['', 'Recent work( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.', '']","[', the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus(Linguistic Data Consortium, 2003) .', 'Recent work( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.', '']","[', the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus(Linguistic Data Consortium, 2003) .', 'Recent work( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.', '']",0
"['number of stand-alone tools have also been developed.', 'For example , the suite of LT tools( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files']","['number of stand-alone tools have also been developed.', 'For example , the suite of LT tools( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', '']","['number of stand-alone tools have also been developed.', 'For example , the suite of LT tools( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', '']","['number of stand-alone tools have also been developed.', 'For example , the suite of LT tools( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', '']",0
"['learning methods should be interchangeable : Transformation-based learning ( TBL )( #AUTHOR_TAG ) and Memory-based learning ( MBL )( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .', 'We will base these components on the design of Weka(Witten and Frank, 1999)']","['learning methods should be interchangeable : Transformation-based learning ( TBL )( #AUTHOR_TAG ) and Memory-based learning ( MBL )( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .', 'We will base these components on the design of Weka(Witten and Frank, 1999)']","['learning methods should be interchangeable : Transformation-based learning ( TBL )( #AUTHOR_TAG ) and Memory-based learning ( MBL )( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .', 'We will base these components on the design of Weka(Witten and Frank, 1999)']","['learning methods should be interchangeable : Transformation-based learning ( TBL )( #AUTHOR_TAG ) and Memory-based learning ( MBL )( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .', 'We will base these components on the design of Weka(Witten and Frank, 1999)']",4
"['', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit( #AUTHOR_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging( Roche and Schabes , 1997 ) .', 'The T']","['', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit( #AUTHOR_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging( Roche and Schabes , 1997 ) .', 'The TNT']","['', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit( #AUTHOR_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging( Roche and Schabes , 1997 ) .', '']","['iciency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used byRatnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly(Malouf, 2002) .', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit( #AUTHOR_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging( Roche and Schabes , 1997 ) .', '']",0
"['basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'An example']","['basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'An example']","['basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'An example']","['basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'An example of using the Python tagger interface is shown in Figure 1']",0
"['.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing( #AUTHOR_TAG ) .', '']","['', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing( #AUTHOR_TAG ) .', '']","['', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing( #AUTHOR_TAG ) .', '']","['', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing( #AUTHOR_TAG ) .', '']",5
"['', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging( #AUTHOR_TAG ) .', '']","['', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging( #AUTHOR_TAG ) .', '']","['', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging( #AUTHOR_TAG ) .', '']","['', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging( #AUTHOR_TAG ) .', '']",0
"[', the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python( #AUTHOR_TAG ) .', 'Python scripting is extremely simple to learn, read and write']","[', the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python( #AUTHOR_TAG ) .', 'Python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is']","[', the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python( #AUTHOR_TAG ) .', 'Python scripting is extremely simple to learn, read and write']","[', the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python( #AUTHOR_TAG ) .', 'Python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple']",0
"['implementation has been inspired by experience in extracting information from very large corpora( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'We have already implemented a POS tagger']","['implementation has been inspired by experience in extracting information from very large corpora( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'We have already implemented a POS tagger,']","['implementation has been inspired by experience in extracting information from very large corpora( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', '']","['implementation has been inspired by experience in extracting information from very large corpora( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', '']",4
"['iciency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used byRatnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly( #AUTHOR_TAG ) .', '']","['iciency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used byRatnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly( #AUTHOR_TAG ) .', '']","['iciency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used byRatnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly( #AUTHOR_TAG ) .', '']","['iciency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used byRatnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly( #AUTHOR_TAG ) .', '']",0
"['basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'An example']","['basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'An example']","['basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'An example']","['basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'An example of using the Python tagger interface is shown in Figure 1']",0
"['', 'The TNT POS tagger( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second']","['', 'The TNT POS tagger( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second']","['', 'The TNT POS tagger( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second']","['', 'The TNT POS tagger( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second']",0
"['', 'GATE goes beyond earlier systems by using a component-based infrastructure( #AUTHOR_TAG ) which the GUI is built on top of .', 'This allows components to be highly configurable and simplifies the addition of new components to the system']","['', 'GATE goes beyond earlier systems by using a component-based infrastructure( #AUTHOR_TAG ) which the GUI is built on top of .', 'This allows components to be highly configurable and simplifies the addition of new components to the system']","['', 'GATE goes beyond earlier systems by using a component-based infrastructure( #AUTHOR_TAG ) which the GUI is built on top of .', 'This allows components to be highly configurable and simplifies the addition of new components to the system']","['', 'GATE goes beyond earlier systems by using a component-based infrastructure( #AUTHOR_TAG ) which the GUI is built on top of .', 'This allows components to be highly configurable and simplifies the addition of new components to the system']",0
"['for instance ,( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics']","['for instance ,( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics']","['for instance ,( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics']","['Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance ,( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics']",0
"['ases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available( #AUTHOR_TAG ) .', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', '']","['ases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available( #AUTHOR_TAG ) .', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', '']","['ases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available( #AUTHOR_TAG ) .', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', '']","['ases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available( #AUTHOR_TAG ) .', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', '']",0
"['( Barzilay and Lee , 2003 ) and( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', '']","['( Barzilay and Lee , 2003 ) and( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', '']","['( Barzilay and Lee , 2003 ) and( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', '']","['', 'Similarly ,( Barzilay and Lee , 2003 ) and( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', '']",0
['( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts'],['( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts'],['( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts'],"['', 'And( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']",0
"['.', 'To address this problem , we are currently working on developing a metagrammar in the sense of( #AUTHOR_TAG ) .', 'This metagrammar allows us to factorise both syntactic and semantic information.', 'Syntactic information is factorised in the usual way.', 'For instance, there will be a class NOVN1 which groups together all the initial trees representing the possible syntactic configurations in which a transitive verb with two nominal arguments can occur.', 'But additionnally there will be semantic classes such as, ""binary predicate of semantic type X"" which will be associated with the relevant syntactic classes for instance, NOVN1 (the class of transitive verbs with nominal arguments),']","['', 'To address this problem , we are currently working on developing a metagrammar in the sense of( #AUTHOR_TAG ) .', 'This metagrammar allows us to factorise both syntactic and semantic information.', 'Syntactic information is factorised in the usual way.', 'For instance, there will be a class NOVN1 which groups together all the initial trees representing the possible syntactic configurations in which a transitive verb with two nominal arguments can occur.', 'But additionnally there will be semantic classes such as, ""binary predicate of semantic type X"" which will be associated with the relevant syntactic classes for instance, NOVN1 (the class of transitive verbs with nominal arguments),']","['.', 'To address this problem , we are currently working on developing a metagrammar in the sense of( #AUTHOR_TAG ) .', 'This metagrammar allows us to factorise both syntactic and semantic information.', 'Syntactic information is factorised in the usual way.', 'For instance, there will be a class NOVN1 which groups together all the initial trees representing the possible syntactic configurations in which a transitive verb with two nominal arguments can occur.', 'But additionnally there will be semantic classes such as, ""binary predicate of semantic type X"" which will be associated with the relevant syntactic classes for instance, NOVN1 (the class of transitive verbs with nominal arguments), BINARY NPRED (the class of binary predicative nouns), NOVSUPNN1']","['', 'To address this problem , we are currently working on developing a metagrammar in the sense of( #AUTHOR_TAG ) .', 'This metagrammar allows us to factorise both syntactic and semantic information.', 'Syntactic information is factorised in the usual way.', 'For instance, there will be a class NOVN1 which groups together all the initial trees representing the possible syntactic configurations in which a transitive verb with two nominal arguments can occur.', 'But additionnally there will be semantic classes such as, ""binary predicate of semantic type X"" which will be associated with the relevant syntactic classes for instance, NOVN1 (the class of transitive verbs with nominal arguments), BINARY NPRED (the class of binary predicative nouns), NOVSUPNN1 , the class of support verb constructions taking two nominal arguments.', '']",3
"['', 'Similarly ,( #AUTHOR_TAG ) and( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', 'And(Glickman and Dagan, 2003)']","['', 'Similarly ,( #AUTHOR_TAG ) and( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', 'And(Glickman and Dagan, 2003)']","['', 'Similarly ,( #AUTHOR_TAG ) and( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', 'And(Glickman and Dagan, 2003)']","['', 'Similarly ,( #AUTHOR_TAG ) and( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', '']",0
"['represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG ) .', 'Johnson et al., 2002) .', 'FrameNet is an online lexical resource for English based on the principles of Frame Semantics.', 'In this approach, a word evokes a frame i.e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a']","['represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG ) .', 'Johnson et al., 2002) .', 'FrameNet is an online lexical resource for English based on the principles of Frame Semantics.', 'In this approach, a word evokes a frame i.e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a']","['represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG ) .', 'Johnson et al., 2002) .', 'FrameNet is an online lexical resource for English based on the principles of Frame Semantics.', 'In this approach, a word evokes a frame i.e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a']","['represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG ) .', 'Johnson et al., 2002) .', 'FrameNet is an online lexical resource for English based on the principles of Frame Semantics.', 'In this approach, a word evokes a frame i.e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a given role in the frame.', 'Finally each frame is associated with a set of target words, the words that evoke that frame']",5
"['', 'For instance ,( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .', '']","['', 'For instance ,( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .', '']","['', 'For instance ,( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .', '']","['', 'For instance ,( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .', '']",0
"['ases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available(Ploux, 1997) .', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity( Pereira et al. , 1993 ; #AUTHOR_TAG ) .', '']","['ases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available(Ploux, 1997) .', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity( Pereira et al. , 1993 ; #AUTHOR_TAG ) .', 'techniques']","['ases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available(Ploux, 1997) .', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity( Pereira et al. , 1993 ; #AUTHOR_TAG ) .', '']","['ases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available(Ploux, 1997) .', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity( Pereira et al. , 1993 ; #AUTHOR_TAG ) .', 'techniques']",3
"['', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena( #AUTHOR_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information( Oepen and Flickinger , 1998 )']","['', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena( #AUTHOR_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information( Oepen and Flickinger , 1998 )']","['', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena( #AUTHOR_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information( Oepen and Flickinger , 1998 ) .', '']","['', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena( #AUTHOR_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information( Oepen and Flickinger , 1998 ) .', '']",1
"['language chosen for semantic representation is a flat semantics along the line of( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ) .', '']","['language chosen for semantic representation is a flat semantics along the line of( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ) .', '']","['language chosen for semantic representation is a flat semantics along the line of( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ) .', '']","['language chosen for semantic representation is a flat semantics along the line of( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ) .', '']",1
"['', 'While corpus driven efforts along the PARSEVAL lines( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .', '']","['', 'While corpus driven efforts along the PARSEVAL lines( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .', '']","['', 'While corpus driven efforts along the PARSEVAL lines( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .', '']","['', 'While corpus driven efforts along the PARSEVAL lines( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .', '']",0
"['language chosen for semantic representation is a flat semantics along the line of( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .', '']","['language chosen for semantic representation is a flat semantics along the line of( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .', '']","['language chosen for semantic representation is a flat semantics along the line of( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .', '']","['language chosen for semantic representation is a flat semantics along the line of( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .', '']",1
"['', 'In particular ,( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns']","['', 'In particular ,( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns']","['', 'In particular ,( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns']","['', 'In particular ,( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns']",3
"['we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar( #AUTHOR_TAG ) thus ensuring an additional level of abstraction .', 'The metagrammar is an abstract spec']","['we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar( #AUTHOR_TAG ) thus ensuring an additional level of abstraction .', 'The metagrammar is an abstract specification']","['we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar( #AUTHOR_TAG ) thus ensuring an additional level of abstraction .', 'The metagrammar is an abstract specification']","['we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar( #AUTHOR_TAG ) thus ensuring an additional level of abstraction .', 'The metagrammar is an abstract specification of the linguistic properties (phrase structure, valency, realisation of grammatical functions etc.) encoded in the grammar basic units.', '']",5
"['shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.', 'For complementing this database and for converse constructions , the LADL tables( #AUTHOR_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .', 'In particular,(Gross, 1989) lists the converses of some 3 ']","['shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.', 'For complementing this database and for converse constructions , the LADL tables( #AUTHOR_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .', 'In particular,(Gross, 1989) lists the converses of some 3 500 predicative nouns']","['shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.', 'For complementing this database and for converse constructions , the LADL tables( #AUTHOR_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .', 'In particular,(Gross, 1989) lists the converses of some 3 500 predicative nouns']","['shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.', 'For complementing this database and for converse constructions , the LADL tables( #AUTHOR_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .', 'In particular,(Gross, 1989) lists the converses of some 3 500 predicative nouns']",3
"['and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information( #AUTHOR_TAG ) .', 'Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', '']","['and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information( #AUTHOR_TAG ) .', 'Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', '']","['and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information( #AUTHOR_TAG ) .', 'Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', '']",['( #AUTHOR_TAG )'],0
"['ases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available(Ploux, 1997) .', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity( #AUTHOR_TAG ; Lin , 1998 ) .', '']","['ases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available(Ploux, 1997) .', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity( #AUTHOR_TAG ; Lin , 1998 ) .', 'techniques']","['ases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available(Ploux, 1997) .', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity( #AUTHOR_TAG ; Lin , 1998 ) .', '']","['ases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available(Ploux, 1997) .', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity( #AUTHOR_TAG ; Lin , 1998 ) .', 'techniques']",3
"['for instance ,( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and( #AUTHOR_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics']","['for instance ,( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and( #AUTHOR_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics']","['for instance ,( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and( #AUTHOR_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics']","['', 'Thus for instance ,( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and( #AUTHOR_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics']",0
['antic construction proceeds from the derived tree( #AUTHOR_TAG ) rather than -- as is more common in TAG -- from the derivation tree'],['antic construction proceeds from the derived tree( #AUTHOR_TAG ) rather than -- as is more common in TAG -- from the derivation tree'],"['antic construction proceeds from the derived tree( #AUTHOR_TAG ) rather than -- as is more common in TAG -- from the derivation tree .', '']","['antic construction proceeds from the derived tree( #AUTHOR_TAG ) rather than -- as is more common in TAG -- from the derivation tree .', '']",0
"['for instance ,( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics']","['for instance ,( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics']","['for instance ,( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics']","['Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance ,( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics']",0
"['language chosen for semantic representation is a flat semantics along the line of( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', '']","['language chosen for semantic representation is a flat semantics along the line of( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', '']","['language chosen for semantic representation is a flat semantics along the line of( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', '']","['language chosen for semantic representation is a flat semantics along the line of( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', '']",1
"['shuffling paraphrases , french alternations are partially described in( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', '']","['shuffling paraphrases , french alternations are partially described in( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', '']","['shuffling paraphrases , french alternations are partially described in( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', '']","['shuffling paraphrases , french alternations are partially described in( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', '']",0
"['', 'In this work, we use the Arabic root extraction technique in(El Kourdi, 2004) .', 'It compares favorably to other stemming or root extraction algorithms( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', '']","['', 'In this work, we use the Arabic root extraction technique in(El Kourdi, 2004) .', 'It compares favorably to other stemming or root extraction algorithms( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', '']","['', 'In this work, we use the Arabic root extraction technique in(El Kourdi, 2004) .', 'It compares favorably to other stemming or root extraction algorithms( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', '']","['', 'In this work, we use the Arabic root extraction technique in(El Kourdi, 2004) .', 'It compares favorably to other stemming or root extraction algorithms( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', '']",4
"['', 'In this work, we use the Arabic root extraction technique in(El Kourdi, 2004) .', 'It compares favorably to other stemming or root extraction algorithms( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', '']","['', 'In this work, we use the Arabic root extraction technique in(El Kourdi, 2004) .', 'It compares favorably to other stemming or root extraction algorithms( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', '']","['', 'In this work, we use the Arabic root extraction technique in(El Kourdi, 2004) .', 'It compares favorably to other stemming or root extraction algorithms( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', '']","['', 'In this work, we use the Arabic root extraction technique in(El Kourdi, 2004) .', 'It compares favorably to other stemming or root extraction algorithms( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', '']",4
"['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG )( #AUTHOR_TAG ) , minimum description length principal( Lang , 1995 ) , and the X2 statistic .', '']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG )( #AUTHOR_TAG ) , minimum description length principal( Lang , 1995 ) , and the X2 statistic .', '']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG )( #AUTHOR_TAG ) , minimum description length principal( Lang , 1995 ) , and the X2 statistic .', '']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG )( #AUTHOR_TAG ) , minimum description length principal( Lang , 1995 ) , and the X2 statistic .', '']",0
"['', 'This work is a continuation of that initiated in( #AUTHOR_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .', 'A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories']","['', 'This work is a continuation of that initiated in( #AUTHOR_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .', 'A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories).', '']","['', 'This work is a continuation of that initiated in( #AUTHOR_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .', 'A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories).', '']","['', 'This work is a continuation of that initiated in( #AUTHOR_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .', 'A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories).', '']",2
"['sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in( #AUTHOR_TAG ) .', '']","['sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in( #AUTHOR_TAG ) .', '']","['sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in( #AUTHOR_TAG ) .', '']","['sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in( #AUTHOR_TAG ) .', '']",1
"['good study comparing document categorization algorithms can be found in( #AUTHOR_TAG ) .', 'More recently,(Sebastiani, 2002) has performed a good survey of document categorization; recent works can also be found in(Joachims, 2002) ,(Crammer and Singer, 2003) , and(Lewis et al., 2004)']","['good study comparing document categorization algorithms can be found in( #AUTHOR_TAG ) .', 'More recently,(Sebastiani, 2002) has performed a good survey of document categorization; recent works can also be found in(Joachims, 2002) ,(Crammer and Singer, 2003) , and(Lewis et al., 2004)']","['good study comparing document categorization algorithms can be found in( #AUTHOR_TAG ) .', 'More recently,(Sebastiani, 2002) has performed a good survey of document categorization; recent works can also be found in(Joachims, 2002) ,(Crammer and Singer, 2003) , and(Lewis et al., 2004)']","['good study comparing document categorization algorithms can be found in( #AUTHOR_TAG ) .', 'More recently,(Sebastiani, 2002) has performed a good survey of document categorization; recent works can also be found in(Joachims, 2002) ,(Crammer and Singer, 2003) , and(Lewis et al., 2004)']",0
"['Arabic, however, the use of stems will not yield satisfactory categorization.', 'This is mainly due to the fact that Arabic is a non-concatenative language( #AUTHOR_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the']","['Arabic, however, the use of stems will not yield satisfactory categorization.', 'This is mainly due to the fact that Arabic is a non-concatenative language( #AUTHOR_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the']","['Arabic, however, the use of stems will not yield satisfactory categorization.', 'This is mainly due to the fact that Arabic is a non-concatenative language( #AUTHOR_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called']","['Arabic, however, the use of stems will not yield satisfactory categorization.', 'This is mainly due to the fact that Arabic is a non-concatenative language( #AUTHOR_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root .', '']",0
"['ious feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG)(Tzeras and Hartman, 1993) , minimum description length principal(Lang, 1995) , and the χ 2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term .', '']","['', 'Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG)(Tzeras and Hartman, 1993) , minimum description length principal(Lang, 1995) , and the χ 2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term .', '']","['ious feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG)(Tzeras and Hartman, 1993) , minimum description length principal(Lang, 1995) , and the χ 2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term .', '']","['', 'Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG)(Tzeras and Hartman, 1993) , minimum description length principal(Lang, 1995) , and the χ 2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term .', '']",0
"['good study comparing document categorization algorithms can be found in(Yang and Liu, 1999) .', 'More recently ,( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in( Joachims , 2002 ) ,( Crammer and Singer , 2003 ) , and( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in(Yang and Liu, 1999) .', 'More recently ,( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in( Joachims , 2002 ) ,( Crammer and Singer , 2003 ) , and( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in(Yang and Liu, 1999) .', 'More recently ,( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in( Joachims , 2002 ) ,( Crammer and Singer , 2003 ) , and( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in(Yang and Liu, 1999) .', 'More recently ,( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in( Joachims , 2002 ) ,( Crammer and Singer , 2003 ) , and( Lewis et al. , 2004 )']",0
"['retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre-specified subject themes.', 'Automatic text categorization has been used in search engines , digital library systems , and document management systems( #AUTHOR_TAG ) .', '']","['retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre-specified subject themes.', 'Automatic text categorization has been used in search engines , digital library systems , and document management systems( #AUTHOR_TAG ) .', '']","['information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre-specified subject themes.', 'Automatic text categorization has been used in search engines , digital library systems , and document management systems( #AUTHOR_TAG ) .', '']","['', 'As such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre-specified subject themes.', 'Automatic text categorization has been used in search engines , digital library systems , and document management systems( #AUTHOR_TAG ) .', '']",0
"['', 'In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in( #AUTHOR_TAG ) .', 'TF-ID']","['', 'In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in( #AUTHOR_TAG ) .', 'TF-IDF (term frequency-inverse document frequency)']","['', 'In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in( #AUTHOR_TAG ) .', 'TF-ID']","['', 'In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in( #AUTHOR_TAG ) .', '']",1
"['( #AUTHOR_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .', '']","['( #AUTHOR_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .', 'Thus, the weight of each term/root in a document is given by w D,t = TF D,t * IDF']","['( #AUTHOR_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .', '']",['( #AUTHOR_TAG )'],4
"['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG )( Tzeras and Hartman , 1993 ) , minimum description length principal( #AUTHOR_TAG ) , and the X2 statistic .', '']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG )( Tzeras and Hartman , 1993 ) , minimum description length principal( #AUTHOR_TAG ) , and the X2 statistic .', '']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG )( Tzeras and Hartman , 1993 ) , minimum description length principal( #AUTHOR_TAG ) , and the X2 statistic .', '']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG )( Tzeras and Hartman , 1993 ) , minimum description length principal( #AUTHOR_TAG ) , and the X2 statistic .', '']",0
"['machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in( #AUTHOR_TAG ) ,( Creecy and Masand , 1992 ) and( Wiene and Pedersen , 1995 ) , respectively']","['machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in( #AUTHOR_TAG ) ,( Creecy and Masand , 1992 ) and( Wiene and Pedersen , 1995 ) , respectively']","['machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in( #AUTHOR_TAG ) ,( Creecy and Masand , 1992 ) and( Wiene and Pedersen , 1995 ) , respectively']","['machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in( #AUTHOR_TAG ) ,( Creecy and Masand , 1992 ) and( Wiene and Pedersen , 1995 ) , respectively']",0
"['', 'TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval( #AUTHOR_TAG ) .', '']","['', 'TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval( #AUTHOR_TAG ) .', '']","['', 'TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval( #AUTHOR_TAG ) .', '']","['', 'TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval( #AUTHOR_TAG ) .', '']",0
"['good study comparing document categorization algorithms can be found in(Yang and Liu, 1999) .', 'More recently ,( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in( #AUTHOR_TAG ) ,( Crammer and Singer , 2003 ) , and( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in(Yang and Liu, 1999) .', 'More recently ,( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in( #AUTHOR_TAG ) ,( Crammer and Singer , 2003 ) , and( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in(Yang and Liu, 1999) .', 'More recently ,( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in( #AUTHOR_TAG ) ,( Crammer and Singer , 2003 ) , and( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in(Yang and Liu, 1999) .', 'More recently ,( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in( #AUTHOR_TAG ) ,( Crammer and Singer , 2003 ) , and( Lewis et al. , 2004 )']",0
"[',( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces']","[',( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces']","[',( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces']","['bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.', 'For example ,( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces']",0
"['the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques(Church and Hanks, 1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist )( Oueslati , 1999 ) or , more frequently , on a combination of the two( Smadja , 1993 ; #AUTHOR_TAG , for example ) .', 'It']","['the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques(Church and Hanks, 1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist )( Oueslati , 1999 ) or , more frequently , on a combination of the two( Smadja , 1993 ; #AUTHOR_TAG , for example ) .', 'It']","['the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques(Church and Hanks, 1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist )( Oueslati , 1999 ) or , more frequently , on a combination of the two( Smadja , 1993 ; #AUTHOR_TAG , for example ) .', 'It is']","['the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques(Church and Hanks, 1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist )( Oueslati , 1999 ) or , more frequently , on a combination of the two( Smadja , 1993 ; #AUTHOR_TAG , for example ) .', '']",1
"['', 'The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles( #AUTHOR_TAG ) .', '']","['and called TermoStat.', 'The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles( #AUTHOR_TAG ) .', '']","['', 'The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles( #AUTHOR_TAG ) .', '']","['', 'The terms have been identified as the most specific to our corpus by a program developed byDrouin (2003) and called TermoStat.', 'The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles( #AUTHOR_TAG ) .', '']",5
"['number of applications have relied on distributional analysis(Harris, 1971) in order to build classes of semantically related terms.', 'This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness( #AUTHOR_TAG , for example ) , does not specify the relationship itself .', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not di\x1berentiated']","['number of applications have relied on distributional analysis(Harris, 1971) in order to build classes of semantically related terms.', 'This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness( #AUTHOR_TAG , for example ) , does not specify the relationship itself .', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not di\x1berentiated']","['number of applications have relied on distributional analysis(Harris, 1971) in order to build classes of semantically related terms.', 'This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness( #AUTHOR_TAG , for example ) , does not specify the relationship itself .', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not di\x1berentiated']","['number of applications have relied on distributional analysis(Harris, 1971) in order to build classes of semantically related terms.', 'This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness( #AUTHOR_TAG , for example ) , does not specify the relationship itself .', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not di\x1berentiated']",0
"['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms :#AUTHOR_TAG uses derivational morphology ;Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms :#AUTHOR_TAG uses derivational morphology ;Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms :#AUTHOR_TAG uses derivational morphology ;Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms :#AUTHOR_TAG uses derivational morphology ;Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters']",0
"['(system), utilisateur (user ).', 'The terms have been identified as the most specific to our corpus by a program developed by#AUTHOR_TAG and called TER1vloSTAT .', 'The ten most speci c nouns have been produced by comparing our corpus of computing to the French corpus Le']","['(system), utilisateur (user ).', 'The terms have been identified as the most specific to our corpus by a program developed by#AUTHOR_TAG and called TER1vloSTAT .', 'The ten most speci c nouns have been produced by comparing our corpus of computing to the French corpus Le']","['(system), utilisateur (user ).', 'The terms have been identified as the most specific to our corpus by a program developed by#AUTHOR_TAG and called TER1vloSTAT .', 'The ten most speci c nouns have been produced by comparing our corpus of computing to the French corpus Le Monde,']","['', 'The terms have been identified as the most specific to our corpus by a program developed by#AUTHOR_TAG and called TER1vloSTAT .', 'The ten most speci c nouns have been produced by comparing our corpus of computing to the French corpus Le Monde, composed of newspaper articles(Lemay et al., 2004) .', '']",5
"['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms :Daille ( 2003 ) uses derivational morphology ;#AUTHOR_TAG use , as a starting point , a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms :Daille ( 2003 ) uses derivational morphology ;#AUTHOR_TAG use , as a starting point , a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms :Daille ( 2003 ) uses derivational morphology ;#AUTHOR_TAG use , as a starting point , a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms :Daille ( 2003 ) uses derivational morphology ;#AUTHOR_TAG use , as a starting point , a number of identical characters']",0
"['the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques(Church and Hanks, 1990) , on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist)( #AUTHOR_TAG ) or, more frequently, on a combination of the two(Smadja, 1993;  Kilgarri\x1b andTugwell, 2001,  for example).', 'It']","['the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques(Church and Hanks, 1990) , on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist)( #AUTHOR_TAG ) or, more frequently, on a combination of the two(Smadja, 1993;  Kilgarri\x1b andTugwell, 2001,  for example).', 'It']","['the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques(Church and Hanks, 1990) , on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist)( #AUTHOR_TAG ) or, more frequently, on a combination of the two(Smadja, 1993;  Kilgarri\x1b andTugwell, 2001,  for example).', 'It is']","['the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques(Church and Hanks, 1990) , on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist)( #AUTHOR_TAG ) or, more frequently, on a combination of the two(Smadja, 1993;  Kilgarri\x1b andTugwell, 2001,  for example).', '']",1
"['SARES is based on a Machine Learning technique , Inductive Logic Programming ( ILP )( #AUTHOR_TAG ) , which infers general morpho-syntactic patterns from a set of examples ( this set is noted E + hereafter ) and counter-examples ( E â\x88\x92 ) of the elements one wants to acquire and their context.', 'The contextual patterns produced can then be applied to the corpus in order to retrieve new elements.', 'The acquisition process can be summarized in 3 steps']","['SARES is based on a Machine Learning technique , Inductive Logic Programming ( ILP )( #AUTHOR_TAG ) , which infers general morpho-syntactic patterns from a set of examples ( this set is noted E + hereafter ) and counter-examples ( E â\x88\x92 ) of the elements one wants to acquire and their context.', 'The contextual patterns produced can then be applied to the corpus in order to retrieve new elements.', 'The acquisition process can be summarized in 3 steps']","['SARES is based on a Machine Learning technique , Inductive Logic Programming ( ILP )( #AUTHOR_TAG ) , which infers general morpho-syntactic patterns from a set of examples ( this set is noted E + hereafter ) and counter-examples ( E â\x88\x92 ) of the elements one wants to acquire and their context.', 'The contextual patterns produced can then be applied to the corpus in order to retrieve new elements.', 'The acquisition process can be summarized in 3 steps']","['SARES is based on a Machine Learning technique , Inductive Logic Programming ( ILP )( #AUTHOR_TAG ) , which infers general morpho-syntactic patterns from a set of examples ( this set is noted E + hereafter ) and counter-examples ( E â\x88\x92 ) of the elements one wants to acquire and their context.', 'The contextual patterns produced can then be applied to the corpus in order to retrieve new elements.', 'The acquisition process can be summarized in 3 steps']",0
"[', most strategies are based on `` internal or `` external methods( #AUTHOR_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .', '(In some cases, an additional resource, such as a dictionary or a thesaurus, is used during the']","[', most strategies are based on `` internal or `` external methods( #AUTHOR_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .', '(In some cases, an additional resource, such as a dictionary or a thesaurus, is used during the']","[', most strategies are based on `` internal or `` external methods( #AUTHOR_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .', '(In some cases, an additional resource, such as a dictionary or a thesaurus, is used during']","[', most strategies are based on `` internal or `` external methods( #AUTHOR_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .', '(In some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identification process.)', '']",1
"['this paper, the method is applied to a French corpus on computing to and noun-verb combinations in which verbs convey a meaning of realization.', 'The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information( #AUTHOR_TAG )']","['this paper, the method is applied to a French corpus on computing to and noun-verb combinations in which verbs convey a meaning of realization.', 'The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information( #AUTHOR_TAG )']","['this paper, the method is applied to a French corpus on computing to and noun-verb combinations in which verbs convey a meaning of realization.', 'The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information( #AUTHOR_TAG )']","['this paper, the method is applied to a French corpus on computing to and noun-verb combinations in which verbs convey a meaning of realization.', 'The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information( #AUTHOR_TAG )']",4
"['addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques( #AUTHOR_TAG )']","['addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques( #AUTHOR_TAG )']","['addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques( #AUTHOR_TAG )']","['addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques( #AUTHOR_TAG )']",4
"['SARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework( #AUTHOR_TAG ) and called qualia relations( Bouillon et al. , 2001 ) .', '']","['SARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework( #AUTHOR_TAG ) and called qualia relations( Bouillon et al. , 2001 ) .', '']","['SARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework( #AUTHOR_TAG ) and called qualia relations( Bouillon et al. , 2001 ) .', '']","['SARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework( #AUTHOR_TAG ) and called qualia relations( Bouillon et al. , 2001 ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.', '']",0
['such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by#AUTHOR_TAG with WoRDNET relations )'],['such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by#AUTHOR_TAG with WoRDNET relations )'],['such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by#AUTHOR_TAG with WoRDNET relations )'],"['', 'Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by#AUTHOR_TAG with WoRDNET relations )']",1
"['SARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework( Pustejovsky , 1995 ) and called qualia relations( #AUTHOR_TAG ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.', '']","['SARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework( Pustejovsky , 1995 ) and called qualia relations( #AUTHOR_TAG ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.', '']","['SARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework( Pustejovsky , 1995 ) and called qualia relations( #AUTHOR_TAG ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.', '']","['SARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework( Pustejovsky , 1995 ) and called qualia relations( #AUTHOR_TAG ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.', '']",0
"['main benefits of this acquisition technique lie in the inferred patterns.', 'Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see( #AUTHOR_TAG ) for a review ) , these patterns allow']","['main benefits of this acquisition technique lie in the inferred patterns.', 'Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see( #AUTHOR_TAG ) for a review ) , these patterns allow']","['main benefits of this acquisition technique lie in the inferred patterns.', 'Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see( #AUTHOR_TAG ) for a review ) , these patterns allow']","['main benefits of this acquisition technique lie in the inferred patterns.', 'Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see( #AUTHOR_TAG ) for a review ) , these patterns allow']",0
"['method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool.', 'ASARES is presented in detail in( #AUTHOR_TAG ) .', 'We simply give a short account of its basic principles herein']","['method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool.', 'ASARES is presented in detail in( #AUTHOR_TAG ) .', 'We simply give a short account of its basic principles herein']","['method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool.', 'ASARES is presented in detail in( #AUTHOR_TAG ) .', 'We simply give a short account of its basic principles herein']","['method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool.', 'ASARES is presented in detail in( #AUTHOR_TAG ) .', 'We simply give a short account of its basic principles herein']",5
"['number of applications have relied on distributional analysis( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness(Habert et al., 1996,  for example), does not specify the']","['number of applications have relied on distributional analysis( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness(Habert et al., 1996,  for example), does not specify the']","['number of applications have relied on distributional analysis( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness(Habert et al., 1996,  for example), does not specify the relationship itself.', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not differentiated']","['number of applications have relied on distributional analysis( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness(Habert et al., 1996,  for example), does not specify the relationship itself.', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not differentiated']",0
"['different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging( #AUTHOR_TAG ; Xu et al. , 2002 )']","['different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging( #AUTHOR_TAG ; Xu et al. , 2002 )']","['the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging( #AUTHOR_TAG ; Xu et al. , 2002 )']","['', 'In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging( #AUTHOR_TAG ; Xu et al. , 2002 )']",0
"['', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']","['', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']","['', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']","['this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']",0
"['we will adopt the nomenclature of the Automatic Content Extraction program( #AUTHOR_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which']","['we will adopt the nomenclature of the Automatic Content Extraction program( #AUTHOR_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which']","['we will adopt the nomenclature of the Automatic Content Extraction program( #AUTHOR_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which']","['', 'Instead , we will adopt the nomenclature of the Automatic Content Extraction program( #AUTHOR_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) .', '']",5
"['addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation( #AUTHOR_TAG ) .', '']","['addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation( #AUTHOR_TAG ) .', '']","['addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation( #AUTHOR_TAG ) .', '']","['addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation( #AUTHOR_TAG ) .', '']",1
"['types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the#AUTHOR_TAG Arabic data .', 'The experiments are performed on a clearly specified partition of the data']","['types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the#AUTHOR_TAG Arabic data .', 'The experiments are performed on a clearly specified partition of the data,']","['types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the#AUTHOR_TAG Arabic data .', 'The experiments are performed on a clearly specified partition of the data,']","['types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the#AUTHOR_TAG Arabic data .', '']",5
"['', 'Each punctuation symbol is considered a separate token.', 'Character classes , such as punctuation , are defined according to the Unicode Standard( #AUTHOR_TAG )']","['', 'Each punctuation symbol is considered a separate token.', 'Character classes , such as punctuation , are defined according to the Unicode Standard( #AUTHOR_TAG )']","['', 'Each punctuation symbol is considered a separate token.', 'Character classes , such as punctuation , are defined according to the Unicode Standard( #AUTHOR_TAG )']","['', 'Each punctuation symbol is considered a separate token.', 'Character classes , such as punctuation , are defined according to the Unicode Standard( #AUTHOR_TAG )']",5
"['different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging( Xu et al. , 2001 ; #AUTHOR_TAG )']","['different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging( Xu et al. , 2001 ; #AUTHOR_TAG )']","['the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging( Xu et al. , 2001 ; #AUTHOR_TAG )']","['', 'In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging( Xu et al. , 2001 ; #AUTHOR_TAG )']",0
['coreference system system is similar to the Bell tree algorithm as described by( #AUTHOR_TAG )'],['coreference system system is similar to the Bell tree algorithm as described by( #AUTHOR_TAG )'],['coreference system system is similar to the Bell tree algorithm as described by( #AUTHOR_TAG )'],['coreference system system is similar to the Bell tree algorithm as described by( #AUTHOR_TAG )'],1
"['ic has two kinds of plurals : broken plurals and sound plurals( #AUTHOR_TAG ; Chen and Gey , 2002 ) .', 'The formation of broken plurals is common, more complex and often ir']","['ic has two kinds of plurals : broken plurals and sound plurals( #AUTHOR_TAG ; Chen and Gey , 2002 ) .', 'The formation of broken plurals is common, more complex and often']","['ic has two kinds of plurals : broken plurals and sound plurals( #AUTHOR_TAG ; Chen and Gey , 2002 ) .', 'The formation of broken plurals is common, more complex and often ir']","['ic has two kinds of plurals : broken plurals and sound plurals( #AUTHOR_TAG ; Chen and Gey , 2002 ) .', 'The formation of broken plurals is common, more complex and often irregular.', '']",0
"['', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']","['', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']","['', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']","['this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']",0
"['', 'We use a maximum entropy Markov model (MEMM) classifier.', 'The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model )( #AUTHOR_TAG ) .', '']","['', 'We use a maximum entropy Markov model (MEMM) classifier.', 'The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model )( #AUTHOR_TAG ) .', '']","['', 'We use a maximum entropy Markov model (MEMM) classifier.', 'The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model )( #AUTHOR_TAG ) .', '']","['', 'We use a maximum entropy Markov model (MEMM) classifier.', 'The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model )( #AUTHOR_TAG ) .', '']",0
"['mention sub-type , which is a sub-category of the mention type( #AUTHOR_TAG ) ( e.g. OrgGovernmental , FacilityPath , etc. )']","['mention sub-type , which is a sub-category of the mention type( #AUTHOR_TAG ) ( e.g. OrgGovernmental , FacilityPath , etc. )']","['mention sub-type , which is a sub-category of the mention type( #AUTHOR_TAG ) ( e.g. OrgGovernmental , FacilityPath , etc. )']","['mention sub-type , which is a sub-category of the mention type( #AUTHOR_TAG ) ( e.g. OrgGovernmental , FacilityPath , etc. )']",5
"['', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']","['', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']","['', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']","['this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']",0
"['demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .', 'A trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules']","['demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .', 'A trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules']","['demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .', 'A trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules']","['demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .', 'A trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules']",5
"['tasks are performed with a statistical framework: the mention detection system is similar to the one presented in(Florian et al., 2004) and the coreference resolution system is similar to the one described in .', 'Both systems are built around from the maximum-entropy technique( #AUTHOR_TAG ) .', '']","['tasks are performed with a statistical framework: the mention detection system is similar to the one presented in(Florian et al., 2004) and the coreference resolution system is similar to the one described in .', 'Both systems are built around from the maximum-entropy technique( #AUTHOR_TAG ) .', '']","['tasks are performed with a statistical framework: the mention detection system is similar to the one presented in(Florian et al., 2004) and the coreference resolution system is similar to the one described in .', 'Both systems are built around from the maximum-entropy technique( #AUTHOR_TAG ) .', '']","['tasks are performed with a statistical framework: the mention detection system is similar to the one presented in(Florian et al., 2004) and the coreference resolution system is similar to the one described in .', 'Both systems are built around from the maximum-entropy technique( #AUTHOR_TAG ) .', '']",5
"['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in( Luo et al. , 2004 ) .', '']","['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in( Luo et al. , 2004 ) .', '']","['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in( Luo et al. , 2004 ) .', '']","['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in( Luo et al. , 2004 ) .', '']",1
"['context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not( #AUTHOR_TAG ) .', 'We denote these features as backward token tri-grams and forward token tri-grams for the previous and next context of t i respectively.', 'For a token t i , the backward token n-gram feature will contains the previous n − 1 tokens in the history (t i−n+']","['context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not( #AUTHOR_TAG ) .', 'We denote these features as backward token tri-grams and forward token tri-grams for the previous and next context of t i respectively.', 'For a token t i , the backward token n-gram feature will contains the previous n − 1 tokens in the history (t i−n+1']","['context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not( #AUTHOR_TAG ) .', 'We denote these features as backward token tri-grams and forward token tri-grams for the previous and next context of t i respectively.', 'For a token t i , the backward token n-gram feature will contains the previous n − 1 tokens in the history (t i−n+1']","['context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not( #AUTHOR_TAG ) .', 'We denote these features as backward token tri-grams and forward token tri-grams for the previous and next context of t i respectively.', 'For a token t i , the backward token n-gram feature will contains the previous n − 1 tokens in the history (t i−n+1 , . . .', 't i−1 ) and the forward token n-gram feature will contains the next n − 1 tokens (t i+1 , . . .', '']",0
"['using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution( #AUTHOR_TAG ) .', 'For Arabic, since words are morphologically derived from a list of roots (']","['using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution( #AUTHOR_TAG ) .', 'For Arabic, since words are morphologically derived from a list of roots']","['using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution( #AUTHOR_TAG ) .', 'For Arabic, since words are morphologically derived from a list of roots (']","['using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution( #AUTHOR_TAG ) .', '']",5
"['', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']","['', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']","['', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']","['this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']",0
"['', 'We seeked to exploit this ability to generalize to improve the dictionary based model.', 'As in( #AUTHOR_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .', '']","['', 'We seeked to exploit this ability to generalize to improve the dictionary based model.', 'As in( #AUTHOR_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .', '']","['', 'We seeked to exploit this ability to generalize to improve the dictionary based model.', 'As in( #AUTHOR_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .', '']","['', 'We seeked to exploit this ability to generalize to improve the dictionary based model.', 'As in( #AUTHOR_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .', '']",5
"['', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']","['', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']","['', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']","['this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']",0
"['', 'We introduce here a clearly defined and replicable split of the#AUTHOR_TAG data , so that future investigations can accurately and correctly compare against the results presented here']","['', 'We introduce here a clearly defined and replicable split of the#AUTHOR_TAG data , so that future investigations can accurately and correctly compare against the results presented here']","['', 'We introduce here a clearly defined and replicable split of the#AUTHOR_TAG data , so that future investigations can accurately and correctly compare against the results presented here']","['', 'We introduce here a clearly defined and replicable split of the#AUTHOR_TAG data , so that future investigations can accurately and correctly compare against the results presented here']",5
"['', 'The final machine is a trigram language model , specifically a Kneser-Ney( #AUTHOR_TAG ) based backoff language model .', '']","['', 'The final machine is a trigram language model , specifically a Kneser-Ney( #AUTHOR_TAG ) based backoff language model .', '']","['', 'The final machine is a trigram language model , specifically a Kneser-Ney( #AUTHOR_TAG ) based backoff language model .', '']","['', 'The final machine is a trigram language model , specifically a Kneser-Ney( #AUTHOR_TAG ) based backoff language model .', '']",5
"['', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']","['', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']","['', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']","['this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']",0
"['', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']","['', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']","['', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']","['this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", '']",0
"['', 'We report results with two metrics: ECM-F and ACE- Value.', 'ECM-F is an entity-constrained mention Fmeasure ( cfXXX( #AUTHOR_TAG ) for how ECM-F is computed ) , and A']","['', 'We report results with two metrics: ECM-F and ACE- Value.', 'ECM-F is an entity-constrained mention Fmeasure ( cfXXX( #AUTHOR_TAG ) for how ECM-F is computed ) , and']","['', 'We report results with two metrics: ECM-F and ACE- Value.', 'ECM-F is an entity-constrained mention Fmeasure ( cfXXX( #AUTHOR_TAG ) for how ECM-F is computed ) , and A']","['', 'We report results with two metrics: ECM-F and ACE- Value.', 'ECM-F is an entity-constrained mention Fmeasure ( cfXXX( #AUTHOR_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .', '']",5
"['ic has two kinds of plurals : broken plurals and sound plurals( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .', 'The formation of broken plurals is common, more complex and often ir']","['ic has two kinds of plurals : broken plurals and sound plurals( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .', 'The formation of broken plurals is common, more complex and often']","['ic has two kinds of plurals : broken plurals and sound plurals( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .', 'The formation of broken plurals is common, more complex and often ir']","['ic has two kinds of plurals : broken plurals and sound plurals( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .', 'The formation of broken plurals is common, more complex and often irregular.', '']",0
"['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in( #AUTHOR_TAG ) .', 'Both systems are built around from the maximum-entropy technique(Berger et al., 1996)']","['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in( #AUTHOR_TAG ) .', 'Both systems are built around from the maximum-entropy technique(Berger et al., 1996)']","['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in( #AUTHOR_TAG ) .', 'Both systems are built around from the maximum-entropy technique(Berger et al., 1996) .', '']","['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in( #AUTHOR_TAG ) .', 'Both systems are built around from the maximum-entropy technique(Berger et al., 1996) .', '']",1
"['want to investigate the usefulness of stem n- gram features in the mention detection system.', ""As stated before , the experiments are run in the ACE '04 framework( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."", 'Detecting the mention']","['want to investigate the usefulness of stem n- gram features in the mention detection system.', ""As stated before , the experiments are run in the ACE '04 framework( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."", 'Detecting the mention']","['want to investigate the usefulness of stem n- gram features in the mention detection system.', ""As stated before , the experiments are run in the ACE '04 framework( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."", 'Detecting the mention']","['want to investigate the usefulness of stem n- gram features in the mention detection system.', ""As stated before , the experiments are run in the ACE '04 framework( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."", '']",5
"['mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model( #AUTHOR_TAG )']","['mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model( #AUTHOR_TAG )']","['mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model( #AUTHOR_TAG )']","['mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model( #AUTHOR_TAG )']",5
"['experiment.', '#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .', 'Their results differed particularly in cases of antonymy or distributionally related pairs.', 'We']","['experiment.', '#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .', 'Their results differed particularly in cases of antonymy or distributionally related pairs.', 'We']","['the experiment.', '#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .', 'Their results differed particularly in cases of antonymy or distributionally related pairs.', 'We created']","['', '#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .', 'Their results differed particularly in cases of antonymy or distributionally related pairs.', 'We created a manual with a detailed introduction to SR stressing the crucial points.', '']",4
"['spelling correction.', 'It is defined on different kinds of textual units, e.g.', 'documents, parts of a document (e.g.', 'words and their surrounding context), words or concepts( #AUTHOR_TAG ) .']","['spelling correction.', 'It is defined on different kinds of textual units, e.g.', 'documents, parts of a document (e.g.', 'words and their surrounding context), words or concepts( #AUTHOR_TAG ) .']","['spelling correction.', 'It is defined on different kinds of textual units, e.g.', 'documents, parts of a document (e.g.', 'words and their surrounding context), words or concepts( #AUTHOR_TAG ) .']",['( #AUTHOR_TAG )'],0
"['', 'In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im#AUTHOR_TAG ) . Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from']","['', 'In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im#AUTHOR_TAG ) . Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from']","['', 'In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im#AUTHOR_TAG ) . Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from']","['', 'In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im#AUTHOR_TAG ) . Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', '']",0
"['to#AUTHOR_TAG , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments']","['to#AUTHOR_TAG , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments']","['to#AUTHOR_TAG , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments']","['to#AUTHOR_TAG , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments']",0
"['even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .', 'This empty band is not observed here']","['even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .', 'This empty band is not observed here.', 'However, Figure 4 shows the distribution of averaged judgments with the highest agreement between annotators (standard deviation < 0.8).', 'The plot clearly shows an empty horizontal band with']","['even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .', 'This empty band is not observed here']","['', 'However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .', 'This empty band is not observed here.', 'However, Figure 4 shows the distribution of averaged judgments with the highest agreement between annotators (standard deviation < 0.8).', 'The plot clearly shows an empty horizontal band with no judgments.', 'The connection between averaged judgments and standard deviation is plotted in Figure 5']",1
"['', 'Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German.', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in#AUTHOR_TAG .', '']","['', 'Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German.', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in#AUTHOR_TAG .', '']","['', 'Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German.', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in#AUTHOR_TAG .', '']","['', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', 'Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German.', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in#AUTHOR_TAG .', '']",0
"['', 'This experiment was again replicated by#AUTHOR_TAG with 10 subjects .', 'Table ']","['', 'This experiment was again replicated by#AUTHOR_TAG with 10 subjects .', 'Table ']","['', 'This experiment was again replicated by#AUTHOR_TAG with 10 subjects .', 'Table ']","['', 'This experiment was again replicated by#AUTHOR_TAG with 10 subjects .', 'Table ']",0
"['extracted word pairs from three different domain-specific corpora (see Table 2).', 'This is motivated by the aim to enable research in information retrieval incorporating SR measures.', ""In particular , the `` Semantic Information Retrieval '' project ( SIR#AUTHOR_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems""]","['extracted word pairs from three different domain-specific corpora (see Table 2).', 'This is motivated by the aim to enable research in information retrieval incorporating SR measures.', ""In particular , the `` Semantic Information Retrieval '' project ( SIR#AUTHOR_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems""]","['extracted word pairs from three different domain-specific corpora (see Table 2).', 'This is motivated by the aim to enable research in information retrieval incorporating SR measures.', ""In particular , the `` Semantic Information Retrieval '' project ( SIR#AUTHOR_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems""]","['extracted word pairs from three different domain-specific corpora (see Table 2).', 'This is motivated by the aim to enable research in information retrieval incorporating SR measures.', ""In particular , the `` Semantic Information Retrieval '' project ( SIR#AUTHOR_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems""]",4
"['ious approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based( Lesk , 1986 ) , ontology-based( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional( #AUTHOR_TAG ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['ious approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based( Lesk , 1986 ) , ontology-based( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional( #AUTHOR_TAG ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['ious approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based( Lesk , 1986 ) , ontology-based( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional( #AUTHOR_TAG ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['ious approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based( Lesk , 1986 ) , ontology-based( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional( #AUTHOR_TAG ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['were noun-noun pairs, 15% noun-verb pairs and so on.', 'Word pairs containing polysemous words are expanded to concept pairs using GermaNet( #AUTHOR_TAG ) , the German equivalent to WordNet , as a sense inventory for each word .', 'It is the most complete resource of this type for German']","['were noun-noun pairs, 15% noun-verb pairs and so on.', 'Word pairs containing polysemous words are expanded to concept pairs using GermaNet( #AUTHOR_TAG ) , the German equivalent to WordNet , as a sense inventory for each word .', 'It is the most complete resource of this type for German']","['were noun-noun pairs, 15% noun-verb pairs and so on.', 'Word pairs containing polysemous words are expanded to concept pairs using GermaNet( #AUTHOR_TAG ) , the German equivalent to WordNet , as a sense inventory for each word .', 'It is the most complete resource of this type for German']","['', 'We used the following parameters: were noun-noun pairs, 15% noun-verb pairs and so on.', 'Word pairs containing polysemous words are expanded to concept pairs using GermaNet( #AUTHOR_TAG ) , the German equivalent to WordNet , as a sense inventory for each word .', 'It is the most complete resource of this type for German']",5
"['ical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric( #AUTHOR_TAG ) .4', 'However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application']","['ical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric( #AUTHOR_TAG ) .4', 'However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application']","['ical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric( #AUTHOR_TAG ) .4', 'However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application']","['ical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric( #AUTHOR_TAG ) .4', 'However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application']",0
"['', '#AUTHOR_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .', 'The values may again not be compared directly.', 'Furthermore, we cannot generalize from these results']","['', '#AUTHOR_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .', 'The values may again not be compared directly.', 'Furthermore, we cannot generalize from these results,']","['', '#AUTHOR_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .', 'The values may again not be compared directly.', 'Furthermore, we cannot generalize from these results,']","['', '#AUTHOR_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .', 'The values may again not be compared directly.', 'Furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low']",1
"['', 'We used the revised experimental setup( #AUTHOR_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .', '']","['', 'We used the revised experimental setup( #AUTHOR_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .', '']","['', 'We used the revised experimental setup( #AUTHOR_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .', '']","['', 'We used the revised experimental setup( #AUTHOR_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .', '']",5
"['our experiment , we annotated a high number of pairs similar in size to the test sets byFinkelstein ( 2002 ) and#AUTHOR_TAG .', 'We used the rev']","['our experiment , we annotated a high number of pairs similar in size to the test sets byFinkelstein ( 2002 ) and#AUTHOR_TAG .', 'We used the revised']","['our experiment , we annotated a high number of pairs similar in size to the test sets byFinkelstein ( 2002 ) and#AUTHOR_TAG .', 'We used']","['our experiment , we annotated a high number of pairs similar in size to the test sets byFinkelstein ( 2002 ) and#AUTHOR_TAG .', '']",1
"['ious approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based( Lesk , 1986 ) , ontology-based( Wu and Palmer , 1994 ; #AUTHOR_TAG ) , information-based( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['ious approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based( Lesk , 1986 ) , ontology-based( Wu and Palmer , 1994 ; #AUTHOR_TAG ) , information-based( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['ious approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based( Lesk , 1986 ) , ontology-based( Wu and Palmer , 1994 ; #AUTHOR_TAG ) , information-based( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['ious approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based( Lesk , 1986 ) , ontology-based( Wu and Palmer , 1994 ; #AUTHOR_TAG ) , information-based( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['3', 'Dissimilar words can be semantically related, e.g.', 'via functional relationships (night -dark) or when they are antonyms (high -low).', 'Many NLP applications require knowledge about semantic relatedness rather than just similarity( #AUTHOR_TAG )']","['3', 'Dissimilar words can be semantically related, e.g.', 'via functional relationships (night -dark) or when they are antonyms (high -low).', 'Many NLP applications require knowledge about semantic relatedness rather than just similarity( #AUTHOR_TAG )']","['3', 'Dissimilar words can be semantically related, e.g.', 'via functional relationships (night -dark) or when they are antonyms (high -low).', 'Many NLP applications require knowledge about semantic relatedness rather than just similarity( #AUTHOR_TAG )']","['antic similarity is typically defined via the lexical relations of synonymy (automobile -car) and hypernymy (vehicle -car), while semantic relatedness (SR) is defined to cover any kind of lexical or functional association that may exist be-tween two words(Gurevych, 2005) . 3', 'Dissimilar words can be semantically related, e.g.', 'via functional relationships (night -dark) or when they are antonyms (high -low).', 'Many NLP applications require knowledge about semantic relatedness rather than just similarity( #AUTHOR_TAG )']",0
"['the seminal work by#AUTHOR_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .', 'Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.', '']","['the seminal work by#AUTHOR_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .', 'Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.', '']","['the seminal work by#AUTHOR_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .', 'Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.', '']","['the seminal work by#AUTHOR_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .', 'Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.', '']",0
"['latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation( #AUTHOR_TAG ) or malapropism detection( Budanitsky and Hirst , 2006 ) .']","['latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation( #AUTHOR_TAG ) or malapropism detection( Budanitsky and Hirst , 2006 ) .']","['latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation( #AUTHOR_TAG ) or malapropism detection( Budanitsky and Hirst , 2006 ) .']","['latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation( #AUTHOR_TAG ) or malapropism detection( Budanitsky and Hirst , 2006 ) .Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task.', '']",0
"['', '#AUTHOR_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task .', '']","['', '#AUTHOR_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task .', '']","['apropism detection(Budanitsky and Hirst, 2006) .', '#AUTHOR_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task .', '']","['latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g.', 'word sense disambiguation(Patwardhan et al., 2003) or malapropism detection(Budanitsky and Hirst, 2006) .', '#AUTHOR_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task .', 'But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', '']",0
"['three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger(Schmid, 1995) .', ""The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme( #AUTHOR_TAG )""]","['three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger(Schmid, 1995) .', ""The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme( #AUTHOR_TAG )""]","['three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger(Schmid, 1995) .', ""The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme( #AUTHOR_TAG )""]","['three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger(Schmid, 1995) .', ""The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme( #AUTHOR_TAG )""]",5
"['for their larger dataset.', '#AUTHOR_TAG reported a correlation of r = .69 .', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically']","['for their larger dataset.', '#AUTHOR_TAG reported a correlation of r = .69 .', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically']","['for their larger dataset.', '#AUTHOR_TAG reported a correlation of r = .69 .', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically']","['', '#AUTHOR_TAG reported a correlation of r = .69 .', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically']",1
"['maNet contains only a few conceptual glosses.', 'As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling vs. `` brother , comrade , friend( #AUTHOR_TAG ) .', 'We']","['maNet contains only a few conceptual glosses.', 'As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling vs. `` brother , comrade , friend( #AUTHOR_TAG ) .', 'We']","['maNet contains only a few conceptual glosses.', 'As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling vs. `` brother , comrade , friend( #AUTHOR_TAG ) .', 'We removed']","['maNet contains only a few conceptual glosses.', 'As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling vs. `` brother , comrade , friend( #AUTHOR_TAG ) .', 'We removed words which had more than three senses']",5
"['', '#AUTHOR_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity']","['lexical-semantic relations.', '#AUTHOR_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity']","['-semantic relations.', '#AUTHOR_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity']","['', '#AUTHOR_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity']",0
"['', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ) , too .', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', '']","['', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ) , too .', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', '']","['', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ) , too .', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', '']","['', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ) , too .', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', '']",0
"['three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger( #AUTHOR_TAG ) .', ""The resulting list of POS-tagged lemmas is weighted using the SMART 'ltc' ""]","['three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger( #AUTHOR_TAG ) .', ""The resulting list of POS-tagged lemmas is weighted using the SMART 'ltc' 8""]","['three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger( #AUTHOR_TAG ) .', ""The resulting list of POS-tagged lemmas is weighted using the SMART 'ltc' ""]","['three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger( #AUTHOR_TAG ) .', ""The resulting list of POS-tagged lemmas is weighted using the SMART 'ltc' 8 tf.idf-weighting scheme(Salton, 1989)""]",5
"['ious approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based( Lesk , 1986 ) , ontology-based( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['ious approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based( Lesk , 1986 ) , ontology-based( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['ious approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based( Lesk , 1986 ) , ontology-based( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['ious approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based( Lesk , 1986 ) , ontology-based( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['ious approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based( Lesk , 1986 ) , ontology-based( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['ious approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based( Lesk , 1986 ) , ontology-based( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['ious approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based( Lesk , 1986 ) , ontology-based( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['ious approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based( Lesk , 1986 ) , ontology-based( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['ious approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based( #AUTHOR_TAG ) , ontology-based( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['ious approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based( #AUTHOR_TAG ) , ontology-based( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['ious approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based( #AUTHOR_TAG ) , ontology-based( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['ious approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based( #AUTHOR_TAG ) , ontology-based( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans( #AUTHOR_TAG ) . 6', 'Pairs containing such words are not suitable for evaluation.', 'To limit']","['differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans( #AUTHOR_TAG ) . 6', 'Pairs containing such words are not suitable for evaluation.', 'To limit']","['differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans( #AUTHOR_TAG ) . 6', 'Pairs containing such words are not suitable for evaluation.', 'To limit']","['differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans( #AUTHOR_TAG ) . 6', 'Pairs containing such words are not suitable for evaluation.', '']",0
"['', 'Furthermore , manually selected word pairs are often biased towards highly related pairs( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .', 'Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations']","['', 'Furthermore , manually selected word pairs are often biased towards highly related pairs( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .', 'Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', '']","['', 'Furthermore , manually selected word pairs are often biased towards highly related pairs( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .', 'Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', '']","['', 'Furthermore , manually selected word pairs are often biased towards highly related pairs( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .', 'Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', '']",0
"['.', '#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .', '']","['of semantic relatedness.', '#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .', '']","['of semantic relatedness.', '#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .', '']","['', '#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .', '']",1
"['10', '#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness']","['summarized inter-subject correlation between 21 subjects was r=.478 (cf. is statistically significant at p < .05.', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', '#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness']","['summarized inter-subject correlation between 21 subjects was r=.478 (cf. is statistically significant at p < .05.', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', '#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .', '']","['summarized inter-subject correlation between 21 subjects was r=.478 (cf. is statistically significant at p < .05.', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', '#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .', '']",1
"['', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup']","['semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup']","['semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup']","['', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown inGurevych (2006) .', '']",0
"['to the corpusbased approach, many domain-specific concept pairs are introduced into the test set.', 'Therefore , inter-subject correlation is lower than the results obtained by#AUTHOR_TAG']","['to the corpusbased approach, many domain-specific concept pairs are introduced into the test set.', 'Therefore , inter-subject correlation is lower than the results obtained by#AUTHOR_TAG']","['to the corpusbased approach, many domain-specific concept pairs are introduced into the test set.', 'Therefore , inter-subject correlation is lower than the results obtained by#AUTHOR_TAG']","['', 'Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set.', 'Therefore , inter-subject correlation is lower than the results obtained by#AUTHOR_TAG']",1
"['ious approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based( Lesk , 1986 ) , ontology-based( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based( Resnik , 1995 ; #AUTHOR_TAG ) or distributional( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['ious approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based( Lesk , 1986 ) , ontology-based( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based( Resnik , 1995 ; #AUTHOR_TAG ) or distributional( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['ious approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based( Lesk , 1986 ) , ontology-based( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based( Resnik , 1995 ; #AUTHOR_TAG ) or distributional( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['ious approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based( Lesk , 1986 ) , ontology-based( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based( Resnik , 1995 ; #AUTHOR_TAG ) or distributional( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['ly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by#AUTHOR_TAG b ) and shingling techniques described byChakrabarti ( 2002 )']","['', 'Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by#AUTHOR_TAG b ) and shingling techniques described byChakrabarti ( 2002 )']","['ly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by#AUTHOR_TAG b ) and shingling techniques described byChakrabarti ( 2002 )']","['', 'Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by#AUTHOR_TAG b ) and shingling techniques described byChakrabarti ( 2002 )']",3
"['', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp( Kehoe and Renouf , 2002 ) , KWiCFinder( Fletcher , 2004 a ) and the Linguist 's Search Engine( #AUTHOR_TAG ; Resnik and Elkiss , 2003 )""]","['', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp( Kehoe and Renouf , 2002 ) , KWiCFinder( Fletcher , 2004 a ) and the Linguist 's Search Engine( #AUTHOR_TAG ; Resnik and Elkiss , 2003 )""]","['', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp( Kehoe and Renouf , 2002 ) , KWiCFinder( Fletcher , 2004 a ) and the Linguist 's Search Engine( #AUTHOR_TAG ; Resnik and Elkiss , 2003 )""]","['', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp( Kehoe and Renouf , 2002 ) , KWiCFinder( Fletcher , 2004 a ) and the Linguist 's Search Engine( #AUTHOR_TAG ; Resnik and Elkiss , 2003 )""]",0
"['corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible( Kennedy , 1998  : 56 ) unless the web is used as a corpus( #AUTHOR_TAG ) .', '']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible( Kennedy , 1998  : 56 ) unless the web is used as a corpus( #AUTHOR_TAG ) .', '']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible( Kennedy , 1998  : 56 ) unless the web is used as a corpus( #AUTHOR_TAG ) .', '']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible( Kennedy , 1998  : 56 ) unless the web is used as a corpus( #AUTHOR_TAG ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem(Keller et al., 2002) .', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', '']",0
"['the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience( #AUTHOR_TAG ; Hughes et al , 2004 ) .', 'While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to lingu']","['the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience( #AUTHOR_TAG ; Hughes et al , 2004 ) .', 'While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists']","['the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience( #AUTHOR_TAG ; Hughes et al , 2004 ) .', 'While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to lingu']","['the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience( #AUTHOR_TAG ; Hughes et al , 2004 ) .', 'While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach.', '']",0
"['', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp( Kehoe and Renouf , 2002 ) , KWiCFinder( Fletcher , 2004 a ) and the Linguist 's Search Engine( Kilgarriff , 2003 ; #AUTHOR_TAG )""]","['', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp( Kehoe and Renouf , 2002 ) , KWiCFinder( Fletcher , 2004 a ) and the Linguist 's Search Engine( Kilgarriff , 2003 ; #AUTHOR_TAG )""]","['', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp( Kehoe and Renouf , 2002 ) , KWiCFinder( Fletcher , 2004 a ) and the Linguist 's Search Engine( Kilgarriff , 2003 ; #AUTHOR_TAG )""]","['', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp( Kehoe and Renouf , 2002 ) , KWiCFinder( Fletcher , 2004 a ) and the Linguist 's Search Engine( Kilgarriff , 2003 ; #AUTHOR_TAG )""]",0
"['', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp( Kehoe and Renouf , 2002 ) , KWiCFinder( #AUTHOR_TAG a ) and the Linguist 's Search Engine( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 )""]","['', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp( Kehoe and Renouf , 2002 ) , KWiCFinder( #AUTHOR_TAG a ) and the Linguist 's Search Engine( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 )""]","['', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp( Kehoe and Renouf , 2002 ) , KWiCFinder( #AUTHOR_TAG a ) and the Linguist 's Search Engine( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 )""]","['', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp( Kehoe and Renouf , 2002 ) , KWiCFinder( #AUTHOR_TAG a ) and the Linguist 's Search Engine( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 )""]",0
"['', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG , 2004b ) and received a special issue of the journal Computational Linguistics( Kilgarriff and Grefenstette , 2003 ) .', '']","['', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG , 2004b ) and received a special issue of the journal Computational Linguistics( Kilgarriff and Grefenstette , 2003 ) .', '']","['', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG , 2004b ) and received a special issue of the journal Computational Linguistics( Kilgarriff and Grefenstette , 2003 ) .', '']","['', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG , 2004b ) and received a special issue of the journal Computational Linguistics( Kilgarriff and Grefenstette , 2003 ) .', '']",0
"['key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies.', 'Word frequency counts in internet search engines are inconsistent and unreliable( #AUTHOR_TAG ) .', 'Tools based on static corpora do not suffer from']","['key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies.', 'Word frequency counts in internet search engines are inconsistent and unreliable( #AUTHOR_TAG ) .', 'Tools based on static corpora do not suffer from']","['key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies.', 'Word frequency counts in internet search engines are inconsistent and unreliable( #AUTHOR_TAG ) .', 'Tools based on static corpora do not suffer from']","['key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies.', 'Word frequency counts in internet search engines are inconsistent and unreliable( #AUTHOR_TAG ) .', 'Tools based on static corpora do not suffer from this problem, e.g.', 'BNCweb 7 , developed at the University of Zurich, and View 8 (Variation in English Words and Phrases, developed at Brigham Young University) are both based on the British National Corpus.', '']",0
"['', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal Computational Linguistics( Kilgarriff and Grefenstette , 2003 ) .', '']","['', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal Computational Linguistics( Kilgarriff and Grefenstette , 2003 ) .', '']","['', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal Computational Linguistics( Kilgarriff and Grefenstette , 2003 ) .', '']","['', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal Computational Linguistics( Kilgarriff and Grefenstette , 2003 ) .', '']",0
"['', 'In addition , the advantages of using linguistically annotated data over raw data are well documented( Mair , 2005 ; #AUTHOR_TAG ) .', '']","['at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented( Mair , 2005 ; #AUTHOR_TAG ) .', '']","['at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented( Mair , 2005 ; #AUTHOR_TAG ) .', '']","['', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented( Mair , 2005 ; #AUTHOR_TAG ) .', '']",0
"['corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible( #AUTHOR_TAG : 56 ) unless the web is used as a corpus( Kilgarriff and Grefenstette , 2003 ) .', '']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible( #AUTHOR_TAG : 56 ) unless the web is used as a corpus( Kilgarriff and Grefenstette , 2003 ) .', '']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible( #AUTHOR_TAG : 56 ) unless the web is used as a corpus( Kilgarriff and Grefenstette , 2003 ) .', '']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible( #AUTHOR_TAG : 56 ) unless the web is used as a corpus( Kilgarriff and Grefenstette , 2003 ) .', '']",0
"['', 'We have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding( #AUTHOR_TAG ) , distributed virtual worlds( Hughes et al. , 2005 ) and digital library management( Walkerdine and Rayson , 2004 ) .', 'It']","['', 'We have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding( #AUTHOR_TAG ) , distributed virtual worlds( Hughes et al. , 2005 ) and digital library management( Walkerdine and Rayson , 2004 ) .', 'It']","['', 'We have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding( #AUTHOR_TAG ) , distributed virtual worlds( Hughes et al. , 2005 ) and digital library management( Walkerdine and Rayson , 2004 ) .', 'It is']","['', 'We have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding( #AUTHOR_TAG ) , distributed virtual worlds( Hughes et al. , 2005 ) and digital library management( Walkerdine and Rayson , 2004 ) .', '']",0
"['to linguists via a P2P approach.', 'In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet( #AUTHOR_TAG ) .', '']","['to linguists via a P2P approach.', 'In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet( #AUTHOR_TAG ) .', '']","['to linguists via a P2P approach.', 'In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet( #AUTHOR_TAG ) .', '']","['', 'In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet( #AUTHOR_TAG ) .', '']",0
"['', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding( Hughes and Walkerdine , 2005 ) , distributed virtual worlds( Hughes et al. , 2005 ) and digital library management( #AUTHOR_TAG ) .', '']","['', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding( Hughes and Walkerdine , 2005 ) , distributed virtual worlds( Hughes et al. , 2005 ) and digital library management( #AUTHOR_TAG ) .', '']","['', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding( Hughes and Walkerdine , 2005 ) , distributed virtual worlds( Hughes et al. , 2005 ) and digital library management( #AUTHOR_TAG ) .', '']","['', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding( Hughes and Walkerdine , 2005 ) , distributed virtual worlds( Hughes et al. , 2005 ) and digital library management( #AUTHOR_TAG ) .', '']",0
"['', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal Computational Linguistics( Kilgarriff and Grefenstette , 2003 ) .', '']","['', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal Computational Linguistics( Kilgarriff and Grefenstette , 2003 ) .', '']","['', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal Computational Linguistics( Kilgarriff and Grefenstette , 2003 ) .', '']","['', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal Computational Linguistics( Kilgarriff and Grefenstette , 2003 ) .', '']",0
"['', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal Computational Linguistics( #AUTHOR_TAG ) .', '']","['', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal Computational Linguistics( #AUTHOR_TAG ) .', '']","['', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal Computational Linguistics( #AUTHOR_TAG ) .', '']","['', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal Computational Linguistics( #AUTHOR_TAG ) .', 'Studies have used several different methods to mine web data.', '']",0
"['', '#AUTHOR_TAG built a corpus by iteratively searching Google for a small set of seed terms .', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp(Kehoe and Renouf, 2002) , KWiCFinder(Fletcher, 2004 a) and the Linguist's Search Engine(Kilgarriff, 2003;Resnik and Elkiss, 2003)""]","['web crawler.', '#AUTHOR_TAG built a corpus by iteratively searching Google for a small set of seed terms .', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp(Kehoe and Renouf, 2002) , KWiCFinder(Fletcher, 2004 a) and the Linguist's Search Engine(Kilgarriff, 2003;Resnik and Elkiss, 2003)""]","['a web crawler.', '#AUTHOR_TAG built a corpus by iteratively searching Google for a small set of seed terms .', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp(Kehoe and Renouf, 2002) , KWiCFinder(Fletcher, 2004 a) and the Linguist's Search Engine(Kilgarriff, 2003;Resnik and Elkiss, 2003)""]","['', '#AUTHOR_TAG built a corpus by iteratively searching Google for a small set of seed terms .', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp(Kehoe and Renouf, 2002) , KWiCFinder(Fletcher, 2004 a) and the Linguist's Search Engine(Kilgarriff, 2003;Resnik and Elkiss, 2003)""]",0
"['key aspect of our case study research will be to investigate extending corpus collection to new document types.', 'Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3( #AUTHOR_TAG )']","['key aspect of our case study research will be to investigate extending corpus collection to new document types.', 'Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3( #AUTHOR_TAG )']","['key aspect of our case study research will be to investigate extending corpus collection to new document types.', 'Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3( #AUTHOR_TAG )']","['key aspect of our case study research will be to investigate extending corpus collection to new document types.', 'Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3( #AUTHOR_TAG )']",0
"['', 'Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem( #AUTHOR_TAG ) .', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', '']","['', 'Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem( #AUTHOR_TAG ) .', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', '']","['', 'Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem( #AUTHOR_TAG ) .', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', '']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible(Kennedy, 1998 : 56) unless the web is used as a corpus(Kilgarriff and Grefenstette, 2003) .', 'Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem( #AUTHOR_TAG ) .', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', '']",0
"['', 'The Gsearch system( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .', 'Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up']","['', 'The Gsearch system( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .', 'Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up.', '']","['', 'The Gsearch system( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .', 'Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up.', '']","['', 'The Gsearch system( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .', 'Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up.', '']",0
"['inguistic annotation of corpora contributes crucially to the study of language at several levels: morphology, syntax, semantics, and discourse.', 'Its significance is reflected both in the growing interest in annotation software for word sense tagging( #AUTHOR_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages']","['inguistic annotation of corpora contributes crucially to the study of language at several levels: morphology, syntax, semantics, and discourse.', 'Its significance is reflected both in the growing interest in annotation software for word sense tagging( #AUTHOR_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages']","['inguistic annotation of corpora contributes crucially to the study of language at several levels: morphology, syntax, semantics, and discourse.', 'Its significance is reflected both in the growing interest in annotation software for word sense tagging( #AUTHOR_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages']","['inguistic annotation of corpora contributes crucially to the study of language at several levels: morphology, syntax, semantics, and discourse.', 'Its significance is reflected both in the growing interest in annotation software for word sense tagging( #AUTHOR_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages']",0
"['', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal Computational Linguistics( Kilgarriff and Grefenstette , 2003 ) .', '']","['', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal Computational Linguistics( Kilgarriff and Grefenstette , 2003 ) .', '']","['', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal Computational Linguistics( Kilgarriff and Grefenstette , 2003 ) .', '']","['', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal Computational Linguistics( Kilgarriff and Grefenstette , 2003 ) .', '']",0
"['', '#AUTHOR_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .', 'Baroni and Bernardini (2004)']","['several different methods to mine web data.', '#AUTHOR_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .', 'Baroni and Bernardini (2004)']","['several different methods to mine web data.', '#AUTHOR_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .', 'Baroni and Bernardini (2004)']","['', 'Studies have used several different methods to mine web data.', '#AUTHOR_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .', '']",0
"['given the dynamic nature of the web, this is not sufficiently robust.', 'Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by#AUTHOR_TAG a ) .', '']","['given the dynamic nature of the web, this is not sufficiently robust.', 'Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by#AUTHOR_TAG a ) .', '']","['given the dynamic nature of the web, this is not sufficiently robust.', 'Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by#AUTHOR_TAG a ) .', '']","['', 'Current practise elsewhere includes the distribution of URL lists, but given the dynamic nature of the web, this is not sufficiently robust.', 'Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by#AUTHOR_TAG a ) .', '']",0
"['', 'In addition , the advantages of using linguistically annotated data over raw data are well documented( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .', '']","['at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .', '']","['at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .', '']","['', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .', '']",0
[''],[''],[''],[''],0
"['.', 'We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve( #AUTHOR_TAG ) .', 'Narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture(Stallman, 2001)']","['work.', 'We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve( #AUTHOR_TAG ) .', 'Narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture(Stallman, 2001)']","['.', 'We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve( #AUTHOR_TAG ) .', 'Narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture(Stallman, 2001)']","['believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work.', 'We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve( #AUTHOR_TAG ) .', 'Narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture(Stallman, 2001)']",5
"['from wikis, blogs, and cognitive mapping, we were also inspired by the experiences of early hypertext writing tools, in particular Intermedia and Storyspace.', 'In fact, they were used expecially in academic writing with some success.', 'Intermedia is no more developed and nobody of us had the opportunity to try it( #AUTHOR_TAG ) .', 'Story']","['from wikis, blogs, and cognitive mapping, we were also inspired by the experiences of early hypertext writing tools, in particular Intermedia and Storyspace.', 'In fact, they were used expecially in academic writing with some success.', 'Intermedia is no more developed and nobody of us had the opportunity to try it( #AUTHOR_TAG ) .', 'Storyspace']","['from wikis, blogs, and cognitive mapping, we were also inspired by the experiences of early hypertext writing tools, in particular Intermedia and Storyspace.', 'In fact, they were used expecially in academic writing with some success.', 'Intermedia is no more developed and nobody of us had the opportunity to try it( #AUTHOR_TAG ) .', 'Story']","['from wikis, blogs, and cognitive mapping, we were also inspired by the experiences of early hypertext writing tools, in particular Intermedia and Storyspace.', 'In fact, they were used expecially in academic writing with some success.', 'Intermedia is no more developed and nobody of us had the opportunity to try it( #AUTHOR_TAG ) .', '']",0
"[""the example of#AUTHOR_TAG , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined byRoland Barthes ( 1970 ) ."", '']","[""the example of#AUTHOR_TAG , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined byRoland Barthes ( 1970 ) ."", '']","[""the example of#AUTHOR_TAG , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined byRoland Barthes ( 1970 ) ."", '']","[""the example of#AUTHOR_TAG , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined byRoland Barthes ( 1970 ) ."", '']",5
"['', ""For example , a ` web page ' is more similar to an infinite canvas than a written page( #AUTHOR_TAG ) ."", '']","['', ""For example , a ` web page ' is more similar to an infinite canvas than a written page( #AUTHOR_TAG ) ."", '']","['', ""For example , a ` web page ' is more similar to an infinite canvas than a written page( #AUTHOR_TAG ) ."", '']","['', 'Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999) .', ""For example , a ` web page ' is more similar to an infinite canvas than a written page( #AUTHOR_TAG ) ."", '']",0
"['', 'Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture( #AUTHOR_TAG )']","['', 'Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture( #AUTHOR_TAG )']","['', 'Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture( #AUTHOR_TAG )']","['', 'Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture( #AUTHOR_TAG )']",0
"['the other side , wikis started as collective works where each entry is not owned by a single author e.g.#AUTHOR_TAG .', 'Wikipedia (2005)']","['the other side , wikis started as collective works where each entry is not owned by a single author e.g.#AUTHOR_TAG .', 'Wikipedia (2005)']","['the other side , wikis started as collective works where each entry is not owned by a single author e.g.#AUTHOR_TAG .', 'Wikipedia (2005) .', '']","['', 'On the other side , wikis started as collective works where each entry is not owned by a single author e.g.#AUTHOR_TAG .', 'Wikipedia (2005) .', '']",0
"['(see Figure 2).', 'Authors may choose this right with the No-Deriv option of the Creative Commons licences( #AUTHOR_TAG ) .', 'If nobody claims the document for himself, it will fall in the public domain']","['(see Figure 2).', 'Authors may choose this right with the No-Deriv option of the Creative Commons licences( #AUTHOR_TAG ) .', 'If nobody claims the document for himself, it will fall in the public domain.', '']","['(see Figure 2).', 'Authors may choose this right with the No-Deriv option of the Creative Commons licences( #AUTHOR_TAG ) .', 'If nobody claims the document for himself, it will fall in the public domain.', '']","['', 'Authors may choose this right with the No-Deriv option of the Creative Commons licences( #AUTHOR_TAG ) .', 'If nobody claims the document for himself, it will fall in the public domain.', '']",0
"['main source of Novelle are wikis and blogs.', 'While wikis have spread from a detailed design( #AUTHOR_TAG ) , unfortunately blogs have not been designed under a model .', 'So we have tested and compared the most used tools available for blogging']","['main source of Novelle are wikis and blogs.', 'While wikis have spread from a detailed design( #AUTHOR_TAG ) , unfortunately blogs have not been designed under a model .', 'So we have tested and compared the most used tools available for blogging:']","['main source of Novelle are wikis and blogs.', 'While wikis have spread from a detailed design( #AUTHOR_TAG ) , unfortunately blogs have not been designed under a model .', 'So we have tested and compared the most used tools available for blogging']","['main source of Novelle are wikis and blogs.', 'While wikis have spread from a detailed design( #AUTHOR_TAG ) , unfortunately blogs have not been designed under a model .', 'So we have tested and compared the most used tools available for blogging: Bloggers, WordPress, MovableType and LiveJournal']",0
"['', 'We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences( #AUTHOR_TAG )']","['', 'We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences( #AUTHOR_TAG )']","['', 'We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences( #AUTHOR_TAG )']","['', 'We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences( #AUTHOR_TAG )']",5
"['ally speaking , we find that the personal public diary metaphor behind blogs( #AUTHOR_TAG ) may bring to an unsatisfactory representation of the context .', '']","['ally speaking , we find that the personal public diary metaphor behind blogs( #AUTHOR_TAG ) may bring to an unsatisfactory representation of the context .', '']","['ally speaking , we find that the personal public diary metaphor behind blogs( #AUTHOR_TAG ) may bring to an unsatisfactory representation of the context .', '']","['ally speaking , we find that the personal public diary metaphor behind blogs( #AUTHOR_TAG ) may bring to an unsatisfactory representation of the context .', ""The only way to retrieve information is through a search engine or a calendar, i.e. the date of the 'post' -a lexia in the jargon of bloggers""]",0
"['emphasis on narrativity takes into account the use of blogs as public diaries on the web , that is still the main current interpretation of this literary genre , or metagenre( #AUTHOR_TAG ) .', 'Furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', '']","['emphasis on narrativity takes into account the use of blogs as public diaries on the web , that is still the main current interpretation of this literary genre , or metagenre( #AUTHOR_TAG ) .', 'Furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', '']","['emphasis on narrativity takes into account the use of blogs as public diaries on the web , that is still the main current interpretation of this literary genre , or metagenre( #AUTHOR_TAG ) .', 'Furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', '']","['emphasis on narrativity takes into account the use of blogs as public diaries on the web , that is still the main current interpretation of this literary genre , or metagenre( #AUTHOR_TAG ) .', 'Furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', '']",0
"['criticism -- unlike in the previous times( #AUTHOR_TAG ) .', '']","['criticism -- unlike in the previous times( #AUTHOR_TAG ) .', '']","['-- unlike in the previous times( #AUTHOR_TAG ) .', '']",['( #AUTHOR_TAG )'],0
"['1 Hypertext as a New Writing Space#AUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""Terms as 'chapter', 'page' or 'footnote' simply become meaningless in the new texts, or they highly change their meaning""]","['Hypertext as a New Writing Space#AUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""Terms as 'chapter', 'page' or 'footnote' simply become meaningless in the new texts, or they highly change their meaning."", '']","['1 Hypertext as a New Writing Space#AUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""Terms as 'chapter', 'page' or 'footnote' simply become meaningless in the new texts, or they highly change their meaning""]","['1.1 Hypertext as a New Writing Space#AUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""Terms as 'chapter', 'page' or 'footnote' simply become meaningless in the new texts, or they highly change their meaning."", '']",0
"['DOM ) , the XMLHTTPRequest object( #AUTHOR_TAG )']","['DOM ) , the XMLHTTPRequest object( #AUTHOR_TAG )']","['DOM ) , the XMLHTTPRequest object( #AUTHOR_TAG )']",['( #AUTHOR_TAG )'],0
"[""cept mapping has been used at least in education for over thirty years, in particular at the Cornell University, where Piaget's ideas gave the roots to the assimilation theory by David Ausubel."", 'Very briefly, concept maps show the relationships between concepts labelling both nodes and arcs.', 'Every arc always has a definite direction , i.e. arcs are arrows( #AUTHOR_TAG )']","[""cept mapping has been used at least in education for over thirty years, in particular at the Cornell University, where Piaget's ideas gave the roots to the assimilation theory by David Ausubel."", 'Very briefly, concept maps show the relationships between concepts labelling both nodes and arcs.', 'Every arc always has a definite direction , i.e. arcs are arrows( #AUTHOR_TAG )']","[""cept mapping has been used at least in education for over thirty years, in particular at the Cornell University, where Piaget's ideas gave the roots to the assimilation theory by David Ausubel."", 'Very briefly, concept maps show the relationships between concepts labelling both nodes and arcs.', 'Every arc always has a definite direction , i.e. arcs are arrows( #AUTHOR_TAG )']","[""cept mapping has been used at least in education for over thirty years, in particular at the Cornell University, where Piaget's ideas gave the roots to the assimilation theory by David Ausubel."", 'Very briefly, concept maps show the relationships between concepts labelling both nodes and arcs.', 'Every arc always has a definite direction , i.e. arcs are arrows( #AUTHOR_TAG )']",0
['Ruby on#AUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes'],['Ruby on#AUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes'],['Ruby on#AUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes'],['Ruby on#AUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes'],5
"[', a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history( #AUTHOR_TAG ) .', 'Figure 1 shows the model.', 'History snapshots']","[', a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history( #AUTHOR_TAG ) .', 'Figure 1 shows the model.', 'History snapshots']","[', a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history( #AUTHOR_TAG ) .', 'Figure 1 shows the model.', 'History snapshots']","['', 'Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history( #AUTHOR_TAG ) .', 'Figure 1 shows the model.', '']",0
"['a more pessimistic one, an author may feel to have lost power in this openness.', 'Henceforth the collaborative traits of blogs and wikis( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .', 'They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.', 'This situation could']","['a more pessimistic one, an author may feel to have lost power in this openness.', 'Henceforth the collaborative traits of blogs and wikis( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .', 'They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.', 'This situation could']","['a more pessimistic one, an author may feel to have lost power in this openness.', 'Henceforth the collaborative traits of blogs and wikis( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .', 'They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.', 'This situation could make']","['', 'From a more pessimistic one, an author may feel to have lost power in this openness.', 'Henceforth the collaborative traits of blogs and wikis( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .', 'They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.', '']",0
"['', 'The paradigm is ""write many , read many""( #AUTHOR_TAG )']","['', 'The paradigm is ""write many , read many""( #AUTHOR_TAG )']","['', 'The paradigm is ""write many , read many""( #AUTHOR_TAG )']","['', 'The paradigm is ""write many , read many""( #AUTHOR_TAG )']",0
"['AJAX) paradigm to create the graphical user interface.', 'AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML( #AUTHOR_TAG )']","['AJAX) paradigm to create the graphical user interface.', 'AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML( #AUTHOR_TAG )']","['or AJAX) paradigm to create the graphical user interface.', 'AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML( #AUTHOR_TAG )']","['used the Asyncronous Javascript And XML (or AJAX) paradigm to create the graphical user interface.', 'AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML( #AUTHOR_TAG )']",0
"['is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'This is noticeable for German( Brants et al. , 2002 ) and Portuguese( #AUTHOR_TAG ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van derBeek et al. , 2002 ) and Slovene ( DË\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively']","['is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'This is noticeable for German( Brants et al. , 2002 ) and Portuguese( #AUTHOR_TAG ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van derBeek et al. , 2002 ) and Slovene ( DË\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively']","['is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'This is noticeable for German( Brants et al. , 2002 ) and Portuguese( #AUTHOR_TAG ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van derBeek et al. , 2002 ) and Slovene ( DË\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .', '']","['second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'This is noticeable for German( Brants et al. , 2002 ) and Portuguese( #AUTHOR_TAG ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van derBeek et al. , 2002 ) and Slovene ( DË\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .', '']",1
['Graph transformations for recovering nonprojective structures( #AUTHOR_TAG )'],['Graph transformations for recovering nonprojective structures( #AUTHOR_TAG )'],['¢ Graph transformations for recovering nonprojective structures( #AUTHOR_TAG )'],"['A deterministic algorithm for building labeled projective dependency graphs(Nivre, 2006) .', '• History-based feature models for predicting the next parser action(Black et al., 1992) .', '• Support vector machines for mapping histories to parser actions(Kudo and Matsumoto, 2002) .', 'â\x80¢ Graph transformations for recovering nonprojective structures( #AUTHOR_TAG )']",5
"['second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'This is noticeable for German( #AUTHOR_TAG ) and Portuguese( Afonso et al. , 2002 ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van derBeek et al. , 2002 ) and Slovene ( DË\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively']","['second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'This is noticeable for German( #AUTHOR_TAG ) and Portuguese( Afonso et al. , 2002 ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van derBeek et al. , 2002 ) and Slovene ( DË\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively']","['second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'This is noticeable for German( #AUTHOR_TAG ) and Portuguese( Afonso et al. , 2002 ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van derBeek et al. , 2002 ) and Slovene ( DË\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .', '']","['second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'This is noticeable for German( #AUTHOR_TAG ) and Portuguese( Afonso et al. , 2002 ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van derBeek et al. , 2002 ) and Slovene ( DË\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .', '']",1
['parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing byNivre ( 2003 ) and extended to labeled dependency parsing by#AUTHOR_TAG'],['parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing byNivre ( 2003 ) and extended to labeled dependency parsing by#AUTHOR_TAG'],['parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing byNivre ( 2003 ) and extended to labeled dependency parsing by#AUTHOR_TAG'],['parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing byNivre ( 2003 ) and extended to labeled dependency parsing by#AUTHOR_TAG'],5
"['', 'Typical examples are Bulgarian( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese( Chen et al. , 2003 ) , Danish( Kromann , 2003 ) , and Swedish( #AUTHOR_TAG ) .', 'Japanese(Kawata and Bartels, 2000)']","['', 'Typical examples are Bulgarian( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese( Chen et al. , 2003 ) , Danish( Kromann , 2003 ) , and Swedish( #AUTHOR_TAG ) .', 'Japanese(Kawata and Bartels, 2000)']","['', 'Typical examples are Bulgarian( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese( Chen et al. , 2003 ) , Danish( Kromann , 2003 ) , and Swedish( #AUTHOR_TAG ) .', 'Japanese(Kawata and Bartels, 2000)']","['', 'Typical examples are Bulgarian( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese( Chen et al. , 2003 ) , Danish( Kromann , 2003 ) , and Swedish( #AUTHOR_TAG ) .', '']",0
"['.', 'Typical examples are Bulgarian( #AUTHOR_TAG ; Simov and Osenova , 2003 ) , Chinese( Chen et al. , 2003 ) , Danish( Kromann , 2003 ) , and Swedish( Nilsson et al. , 2005 ) .', '']","['', 'Typical examples are Bulgarian( #AUTHOR_TAG ; Simov and Osenova , 2003 ) , Chinese( Chen et al. , 2003 ) , Danish( Kromann , 2003 ) , and Swedish( Nilsson et al. , 2005 ) .', '']","['.', 'Typical examples are Bulgarian( #AUTHOR_TAG ; Simov and Osenova , 2003 ) , Chinese( Chen et al. , 2003 ) , Danish( Kromann , 2003 ) , and Swedish( Nilsson et al. , 2005 ) .', '']","['', 'Typical examples are Bulgarian( #AUTHOR_TAG ; Simov and Osenova , 2003 ) , Chinese( Chen et al. , 2003 ) , Danish( Kromann , 2003 ) , and Swedish( Nilsson et al. , 2005 ) .', '']",0
"['', 'Typical examples are Bulgarian( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese( #AUTHOR_TAG ) , Danish( Kromann , 2003 ) , and Swedish( Nilsson et al. , 2005 ) .', '']","['', 'Typical examples are Bulgarian( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese( #AUTHOR_TAG ) , Danish( Kromann , 2003 ) , and Swedish( Nilsson et al. , 2005 ) .', '']","['', 'Typical examples are Bulgarian( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese( #AUTHOR_TAG ) , Danish( Kromann , 2003 ) , and Swedish( Nilsson et al. , 2005 ) .', '']","['', 'Typical examples are Bulgarian( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese( #AUTHOR_TAG ) , Danish( Kromann , 2003 ) , and Swedish( Nilsson et al. , 2005 ) .', '']",0
"['A deterministic algorithm for building labeled projective dependency graphs(Nivre, 2006) .', 'â\x80¢ History-based feature models for predicting the next parser action( #AUTHOR_TAG ) .', '• Support vector machines for mapping histories to parser actions(Kudo and Matsumoto, 2002) .', '• Graph transform']","['A deterministic algorithm for building labeled projective dependency graphs(Nivre, 2006) .', 'â\x80¢ History-based feature models for predicting the next parser action( #AUTHOR_TAG ) .', '• Support vector machines for mapping histories to parser actions(Kudo and Matsumoto, 2002) .', '• Graph transformations']","['A deterministic algorithm for building labeled projective dependency graphs(Nivre, 2006) .', 'â\x80¢ History-based feature models for predicting the next parser action( #AUTHOR_TAG ) .', '• Support vector machines for mapping histories to parser actions(Kudo and Matsumoto, 2002) .', '• Graph transformations']","['A deterministic algorithm for building labeled projective dependency graphs(Nivre, 2006) .', 'â\x80¢ History-based feature models for predicting the next parser action( #AUTHOR_TAG ) .', '• Support vector machines for mapping histories to parser actions(Kudo and Matsumoto, 2002) .', '• Graph transformations for recovering nonprojective structures']",5
"['experiments have been performed using MaltParser( #AUTHOR_TAG ) , version 0.4 , which is made available together with the suite of programs used for preand post-processing .']","['experiments have been performed using MaltParser( #AUTHOR_TAG ) , version 0.4 , which is made available together with the suite of programs used for preand post-processing .']","['experiments have been performed using MaltParser( #AUTHOR_TAG ) , version 0.4 , which is made available together with the suite of programs used for preand post-processing .']","['experiments have been performed using MaltParser( #AUTHOR_TAG ) , version 0.4 , which is made available together with the suite of programs used for preand post-processing .']",5
"['use support vector machines to predict the next parser action from a feature vector representing the history.', 'More specifically , we use LIBSVM( #AUTHOR_TAG ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification .', '']","['use support vector machines to predict the next parser action from a feature vector representing the history.', 'More specifically , we use LIBSVM( #AUTHOR_TAG ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification .', '']","['use support vector machines to predict the next parser action from a feature vector representing the history.', 'More specifically , we use LIBSVM( #AUTHOR_TAG ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification .', '']","['use support vector machines to predict the next parser action from a feature vector representing the history.', 'More specifically , we use LIBSVM( #AUTHOR_TAG ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification .', '']",5
['6The analysis is reminiscent of the treatment of coordination in the Collins parser( #AUTHOR_TAG )'],['6The analysis is reminiscent of the treatment of coordination in the Collins parser( #AUTHOR_TAG )'],['6The analysis is reminiscent of the treatment of coordination in the Collins parser( #AUTHOR_TAG )'],['6The analysis is reminiscent of the treatment of coordination in the Collins parser( #AUTHOR_TAG )'],1
"['the parser only derives projective graphs , the fact that graphs are labeled allows non-projective dependencies to be captured using the pseudoprojective approach of#AUTHOR_TAG']","['the parser only derives projective graphs , the fact that graphs are labeled allows non-projective dependencies to be captured using the pseudoprojective approach of#AUTHOR_TAG']","['the parser only derives projective graphs , the fact that graphs are labeled allows non-projective dependencies to be captured using the pseudoprojective approach of#AUTHOR_TAG']","['the parser only derives projective graphs , the fact that graphs are labeled allows non-projective dependencies to be captured using the pseudoprojective approach of#AUTHOR_TAG']",0
"['some languages , we divide the training data into smaller sets , based on some feature s ( normally the CPOS or POS of the next input token ) , which may reduce training times without a significant loss in accuracy( #AUTHOR_TAG ) .', '']","['some languages , we divide the training data into smaller sets , based on some feature s ( normally the CPOS or POS of the next input token ) , which may reduce training times without a significant loss in accuracy( #AUTHOR_TAG ) .', '']","['some languages , we divide the training data into smaller sets , based on some feature s ( normally the CPOS or POS of the next input token ) , which may reduce training times without a significant loss in accuracy( #AUTHOR_TAG ) .', '']","['some languages , we divide the training data into smaller sets , based on some feature s ( normally the CPOS or POS of the next input token ) , which may reduce training times without a significant loss in accuracy( #AUTHOR_TAG ) .', '']",0
"['.', 'Typical examples are Bulgarian( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese( Chen et al. , 2003 ) , Danish( Kromann , 2003 ) , and Swedish( Nilsson et al. , 2005 ) .', '']","['', 'Typical examples are Bulgarian( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese( Chen et al. , 2003 ) , Danish( Kromann , 2003 ) , and Swedish( Nilsson et al. , 2005 ) .', '']","['.', 'Typical examples are Bulgarian( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese( Chen et al. , 2003 ) , Danish( Kromann , 2003 ) , and Swedish( Nilsson et al. , 2005 ) .', '']","['', 'Typical examples are Bulgarian( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese( Chen et al. , 2003 ) , Danish( Kromann , 2003 ) , and Swedish( Nilsson et al. , 2005 ) .', '']",0
"['', 'Japanese( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances']","['', 'Japanese( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances']","['', 'Japanese( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances']","['', 'Japanese( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances']",1
"['A deterministic algorithm for building labeled projective dependency graphs(Nivre, 2006) .', '• History-based feature models for predicting the next parser action(Black et al., 1992) .', 'Support vector machines for mapping histories to parser actions( #AUTHOR_TAG ) .', '• Graph transformations for recovering nonprojective structures']","['A deterministic algorithm for building labeled projective dependency graphs(Nivre, 2006) .', '• History-based feature models for predicting the next parser action(Black et al., 1992) .', 'Support vector machines for mapping histories to parser actions( #AUTHOR_TAG ) .', '• Graph transformations for recovering nonprojective structures']","['A deterministic algorithm for building labeled projective dependency graphs(Nivre, 2006) .', '• History-based feature models for predicting the next parser action(Black et al., 1992) .', 'Support vector machines for mapping histories to parser actions( #AUTHOR_TAG ) .', '• Graph transformations for recovering nonprojective structures']","['A deterministic algorithm for building labeled projective dependency graphs(Nivre, 2006) .', '• History-based feature models for predicting the next parser action(Black et al., 1992) .', 'Support vector machines for mapping histories to parser actions( #AUTHOR_TAG ) .', '• Graph transformations for recovering nonprojective structures']",5
"['', 'By contrast , Turkish( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .', 'It']","['', 'By contrast , Turkish( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .', 'It']","['', 'By contrast , Turkish( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .', 'It is']","['', 'By contrast , Turkish( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .', '']",1
"['', '#AUTHOR_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques .', '']","['studying the role of semantics in various tasks.', '#AUTHOR_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques .', '']","['studying the role of semantics in various tasks.', '#AUTHOR_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques .', '']","['', '#AUTHOR_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques .', '']",0
"['#AUTHOR_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score .', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', '']","['#AUTHOR_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score .', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', '']","['#AUTHOR_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score .', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', '']","['', 'For example ,#AUTHOR_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score .', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', '']",0
"['to further boost performance, we employed Linear Discriminant Analysis (LDA) to find a linear projection of the four-dimensional vec- tors that maximizes the separation of the Gaussians (corresponding to the HMM states).', '#AUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the LDA transform matrix , which entails computing the withinand between-covariance matrices of the classes , and using Singular Value Decomposition ( SVD ) to compute the eigenvectors of the new space .', '']","['to further boost performance, we employed Linear Discriminant Analysis (LDA) to find a linear projection of the four-dimensional vec- tors that maximizes the separation of the Gaussians (corresponding to the HMM states).', '#AUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the LDA transform matrix , which entails computing the withinand between-covariance matrices of the classes , and using Singular Value Decomposition ( SVD ) to compute the eigenvectors of the new space .', '']","['to further boost performance, we employed Linear Discriminant Analysis (LDA) to find a linear projection of the four-dimensional vec- tors that maximizes the separation of the Gaussians (corresponding to the HMM states).', '#AUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the LDA transform matrix , which entails computing the withinand between-covariance matrices of the classes , and using Singular Value Decomposition ( SVD ) to compute the eigenvectors of the new space .', '']","['an attempt to further boost performance, we employed Linear Discriminant Analysis (LDA) to find a linear projection of the four-dimensional vec- tors that maximizes the separation of the Gaussians (corresponding to the HMM states).', '#AUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the LDA transform matrix , which entails computing the withinand between-covariance matrices of the classes , and using Singular Value Decomposition ( SVD ) to compute the eigenvectors of the new space .', '']",5
"['', 'Such a component would serve as the first stage of a clinical question answering system( #AUTHOR_TAG ) or summarization system ( McKeown et al. , 2003 )']","['', 'Such a component would serve as the first stage of a clinical question answering system( #AUTHOR_TAG ) or summarization system ( McKeown et al. , 2003 )']","['', 'Such a component would serve as the first stage of a clinical question answering system( #AUTHOR_TAG ) or summarization system ( McKeown et al. , 2003 ) .', 'We chose to']","['', 'Such a component would serve as the first stage of a clinical question answering system( #AUTHOR_TAG ) or summarization system ( McKeown et al. , 2003 ) .', '']",3
"['criminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example ,( Joachims , 1998 ; #AUTHOR_TAG ) .', 'However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing.', '']","['criminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example ,( Joachims , 1998 ; #AUTHOR_TAG ) .', 'However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing.', '']","['criminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example ,( Joachims , 1998 ; #AUTHOR_TAG ) .', 'However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing.', '']","['criminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example ,( Joachims , 1998 ; #AUTHOR_TAG ) .', 'However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing.', '']",0
"['and#AUTHOR_TAG , we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts .', 'The four states in our H']","['and#AUTHOR_TAG , we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts .', 'The four states in our']","['and#AUTHOR_TAG , we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts .', 'The four states in our H']","['and#AUTHOR_TAG , we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts .', 'The four states in our HMMs correspond to the information that characterizes each section (""introduction"", ""methods"", ""results"", and ""conclusions"") and state transitions capture the discourse flow from section to section']",5
"['scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990; Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization( #AUTHOR_TAG ) , information retrieval( Tbahriti et al. , 2005 ) , information extraction( Mizuta et al. , 2005 ) , and question answering .', '']","['scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990; Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization( #AUTHOR_TAG ) , information retrieval( Tbahriti et al. , 2005 ) , information extraction( Mizuta et al. , 2005 ) , and question answering .', '']","['scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990; Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization( #AUTHOR_TAG ) , information retrieval( Tbahriti et al. , 2005 ) , information extraction( Mizuta et al. , 2005 ) , and question answering .', '']","['', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990; Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization( #AUTHOR_TAG ) , information retrieval( Tbahriti et al. , 2005 ) , information extraction( Mizuta et al. , 2005 ) , and question answering .', '']",0
"['', 'Using the section labels , the HMM was trained using the HTK toolkit( #AUTHOR_TAG ) , which efficiently performs the forward-backward algorithm and BaumWelch estimation .', '']","['', 'Using the section labels , the HMM was trained using the HTK toolkit( #AUTHOR_TAG ) , which efficiently performs the forward-backward algorithm and BaumWelch estimation .', '']","['', 'Using the section labels , the HMM was trained using the HTK toolkit( #AUTHOR_TAG ) , which efficiently performs the forward-backward algorithm and BaumWelch estimation .', '']","['', 'Using the section labels , the HMM was trained using the HTK toolkit( #AUTHOR_TAG ) , which efficiently performs the forward-backward algorithm and BaumWelch estimation .', '']",5
"['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"" , ""methods"" , ""results"" , and ""conclusions""( SalangerMeyer , 1990 ; #AUTHOR_TAG ; OrË\x98asan , 2001 ) .', 'The ability to explicitly identify these sections in un-structured text could play an important role in applications such as document summarization']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"" , ""methods"" , ""results"" , and ""conclusions""( SalangerMeyer , 1990 ; #AUTHOR_TAG ; OrË\x98asan , 2001 ) .', 'The ability to explicitly identify these sections in un-structured text could play an important role in applications such as document summarization']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"" , ""methods"" , ""results"" , and ""conclusions""( SalangerMeyer , 1990 ; #AUTHOR_TAG ; OrË\x98asan , 2001 ) .', 'The ability to explicitly identify these sections in un-structured text could play an important role in applications such as document summarization']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"" , ""methods"" , ""results"" , and ""conclusions""( SalangerMeyer , 1990 ; #AUTHOR_TAG ; OrË\x98asan , 2001 ) .', 'The ability to explicitly identify these sections in un-structured text could play an important role in applications such as document summarization(Teufel and Moens, 2000) , information retrieval(Tbahriti et al., 2005) , information extraction(Mizuta et al., 2005) , and question answering.', '']",0
"['interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in( #AUTHOR_TAG ) .', 'This technique provides two important advantages.', '']","['interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in( #AUTHOR_TAG ) .', 'This technique provides two important advantages.', '']","['interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in( #AUTHOR_TAG ) .', 'This technique provides two important advantages.', '']","['interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in( #AUTHOR_TAG ) .', 'This technique provides two important advantages.', '']",1
"['criminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example ,( #AUTHOR_TAG ; Ng and Jordan , 2001 ) .', 'However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing.', '']","['criminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example ,( #AUTHOR_TAG ; Ng and Jordan , 2001 ) .', 'However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing.', '']","['criminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example ,( #AUTHOR_TAG ; Ng and Jordan , 2001 ) .', 'However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing.', '']","['criminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example ,( #AUTHOR_TAG ; Ng and Jordan , 2001 ) .', 'However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing.', '']",0
"['the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS )( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap( #AUTHOR_TAG ) for concept identification and SemRep( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks']","['the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS )( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap( #AUTHOR_TAG ) for concept identification and SemRep( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks']","['the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS )( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap( #AUTHOR_TAG ) for concept identification and SemRep( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .', '']","['', 'Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS )( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap( #AUTHOR_TAG ) for concept identification and SemRep( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .', '']",0
"['this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements( McKeown , 1985 ; #AUTHOR_TAG ) .', 'Our task']","['this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements( McKeown , 1985 ; #AUTHOR_TAG ) .', 'Our task']","['this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements( McKeown , 1985 ; #AUTHOR_TAG ) .', 'Our task']","['this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements( McKeown , 1985 ; #AUTHOR_TAG ) .', '']",1
"['', 'The need for information systems to support physicians at the point of care has been well studied( Covell et al. , 1985 ; Gorman et al. , 1994 ; #AUTHOR_TAG ) .', 'Retriev']","['variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied( Covell et al. , 1985 ; Gorman et al. , 1994 ; #AUTHOR_TAG ) .', 'Retrieval']","['', 'The need for information systems to support physicians at the point of care has been well studied( Covell et al. , 1985 ; Gorman et al. , 1994 ; #AUTHOR_TAG ) .', 'Retriev']","['', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied( Covell et al. , 1985 ; Gorman et al. , 1994 ; #AUTHOR_TAG ) .', '']",0
"['', 'Our task is closer to the work of#AUTHOR_TAG , who looked at the problem of intellectual attribution in scientific texts']","['', 'Our task is closer to the work of#AUTHOR_TAG , who looked at the problem of intellectual attribution in scientific texts']","['', 'Our task is closer to the work of#AUTHOR_TAG , who looked at the problem of intellectual attribution in scientific texts']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements (McKeown, 1985;Marcu and Echihabi, 2002) .', 'Our task is closer to the work of#AUTHOR_TAG , who looked at the problem of intellectual attribution in scientific texts']",1
"['', 'Such a component would serve as the first stage of a clinical question answering system ( Demner-Fushman and Lin , 2005 ) or summarization system( #AUTHOR_TAG ) .', 'We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured']","['', 'Such a component would serve as the first stage of a clinical question answering system ( Demner-Fushman and Lin , 2005 ) or summarization system( #AUTHOR_TAG ) .', 'We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured']","['', 'Such a component would serve as the first stage of a clinical question answering system ( Demner-Fushman and Lin , 2005 ) or summarization system( #AUTHOR_TAG ) .', 'We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured']","['', 'Such a component would serve as the first stage of a clinical question answering system ( Demner-Fushman and Lin , 2005 ) or summarization system( #AUTHOR_TAG ) .', 'We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured']",3
['(b) again reproduces the results from#AUTHOR_TAG (2003) for a comparable task on a different subset of 206 unstructured abstracts'],['(b) again reproduces the results from#AUTHOR_TAG (2003) for a comparable task on a different subset of 206 unstructured abstracts'],['(b) again reproduces the results from#AUTHOR_TAG (2003) for a comparable task on a different subset of 206 unstructured abstracts'],['(b) again reproduces the results from#AUTHOR_TAG (2003) for a comparable task on a different subset of 206 unstructured abstracts'],1
"['scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990; Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization( Teufel and Moens , 2000 ) , information retrieval( Tbahriti et al. , 2005 ) , information extraction( #AUTHOR_TAG ) , and question answering .', '']","['scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990; Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization( Teufel and Moens , 2000 ) , information retrieval( Tbahriti et al. , 2005 ) , information extraction( #AUTHOR_TAG ) , and question answering .', '']","['scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990; Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization( Teufel and Moens , 2000 ) , information retrieval( Tbahriti et al. , 2005 ) , information extraction( #AUTHOR_TAG ) , and question answering .', '']","['', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990; Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization( Teufel and Moens , 2000 ) , information retrieval( Tbahriti et al. , 2005 ) , information extraction( #AUTHOR_TAG ) , and question answering .', '']",0
"['', 'The need for information systems to support physicians at the point of care has been well studied( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 ) .', 'Retriev']","['variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 ) .', 'Retrieval']","['', 'The need for information systems to support physicians at the point of care has been well studied( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 ) .', 'Retriev']","['', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 ) .', '']",0
"[', scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990; Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization( Teufel and Moens , 2000 ) , information retrieval( #AUTHOR_TAG ) , information extraction( Mizuta et al. , 2005 ) , and question answering .', '']","['example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990; Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization( Teufel and Moens , 2000 ) , information retrieval( #AUTHOR_TAG ) , information extraction( Mizuta et al. , 2005 ) , and question answering .', '']","['scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990; Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization( Teufel and Moens , 2000 ) , information retrieval( #AUTHOR_TAG ) , information extraction( Mizuta et al. , 2005 ) , and question answering .', '']","['', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990; Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization( Teufel and Moens , 2000 ) , information retrieval( #AUTHOR_TAG ) , information extraction( Mizuta et al. , 2005 ) , and question answering .', '']",0
"['reduces error rate.', 'Table 2(b) reports accuracy, precision, recall, and F- measure for four separate binary classifiers (HMM with LDA) specifically trained for each of the sections (one per row in the table).', 'The table also presents the closest comparable experimental results reported by#AUTHOR_TAG .1 McK']","['reduces error rate.', 'Table 2(b) reports accuracy, precision, recall, and F- measure for four separate binary classifiers (HMM with LDA) specifically trained for each of the sections (one per row in the table).', 'The table also presents the closest comparable experimental results reported by#AUTHOR_TAG .1 McKnight']","['further reduces error rate.', 'Table 2(b) reports accuracy, precision, recall, and F- measure for four separate binary classifiers (HMM with LDA) specifically trained for each of the sections (one per row in the table).', 'The table also presents the closest comparable experimental results reported by#AUTHOR_TAG .1 McKnight']",['#AUTHOR_TAG'],1
"['this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements( #AUTHOR_TAG ; Marcu and Echihabi , 2002 ) .', 'Our task']","['this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements( #AUTHOR_TAG ; Marcu and Echihabi , 2002 ) .', 'Our task']","['this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements( #AUTHOR_TAG ; Marcu and Echihabi , 2002 ) .', 'Our task']","['this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements( #AUTHOR_TAG ; Marcu and Echihabi , 2002 ) .', '']",1
"['in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX( #AUTHOR_TAG ) .', '']","['in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX( #AUTHOR_TAG ) .', '']","['in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX( #AUTHOR_TAG ) .', '']","['', 'Building on the work ofRuch et al. ( 2003 ) in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX( #AUTHOR_TAG ) .', '']",0
"['presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above( #AUTHOR_TAG ) .', '']","['presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above( #AUTHOR_TAG ) .', '']","['presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above( #AUTHOR_TAG ) .', '']","['', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above( #AUTHOR_TAG ) .', '']",0
"['', 'The need for information systems to support physicians at the point of care has been well studied( Covell et al. , 1985 ; #AUTHOR_TAG ; Ely et al. , 2005 ) .', 'Retriev']","['variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied( Covell et al. , 1985 ; #AUTHOR_TAG ; Ely et al. , 2005 ) .', 'Retrieval']","['', 'The need for information systems to support physicians at the point of care has been well studied( Covell et al. , 1985 ; #AUTHOR_TAG ; Ely et al. , 2005 ) .', 'Retriev']","['', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied( Covell et al. , 1985 ; #AUTHOR_TAG ; Ely et al. , 2005 ) .', '']",0
"['the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS )( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap( Aronson , 2001 ) for concept identification and SemRep( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks']","['the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS )( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap( Aronson , 2001 ) for concept identification and SemRep( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks']","['the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS )( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap( Aronson , 2001 ) for concept identification and SemRep( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .', '']","['', 'Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS )( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap( Aronson , 2001 ) for concept identification and SemRep( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .', '']",0
"['not the first to employ a generative approach to directly model content , the seminal work of#AUTHOR_TAG is a noteworthy point of reference and comparison .', '']","['not the first to employ a generative approach to directly model content , the seminal work of#AUTHOR_TAG is a noteworthy point of reference and comparison .', 'However, our study differs in several important respects.', '']","['not the first to employ a generative approach to directly model content , the seminal work of#AUTHOR_TAG is a noteworthy point of reference and comparison .', '']","['not the first to employ a generative approach to directly model content , the seminal work of#AUTHOR_TAG is a noteworthy point of reference and comparison .', 'However, our study differs in several important respects.', '']",1
"['using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition( #AUTHOR_TAG )']","['using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition( #AUTHOR_TAG )']","['using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition( #AUTHOR_TAG )']","['', 'Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition( #AUTHOR_TAG )']",0
"['work with a semi-technical text on meteorological phenomena( #AUTHOR_TAG ) , meant for primary school students .', 'The text gradually introduces concepts related to precipitation, and explains them.', 'Its nature makes it appropriate for the semantic analysis task in an incremental approach.', 'The system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text']","['work with a semi-technical text on meteorological phenomena( #AUTHOR_TAG ) , meant for primary school students .', 'The text gradually introduces concepts related to precipitation, and explains them.', 'Its nature makes it appropriate for the semantic analysis task in an incremental approach.', 'The system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text']","['work with a semi-technical text on meteorological phenomena( #AUTHOR_TAG ) , meant for primary school students .', 'The text gradually introduces concepts related to precipitation, and explains them.', 'Its nature makes it appropriate for the semantic analysis task in an incremental approach.', 'The system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text']","['work with a semi-technical text on meteorological phenomena( #AUTHOR_TAG ) , meant for primary school students .', 'The text gradually introduces concepts related to precipitation, and explains them.', 'Its nature makes it appropriate for the semantic analysis task in an incremental approach.', 'The system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text']",5
"['', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain(Rosario and Hearst, 2001) or the system(Gomez, 1998) .', 'Such systems extract information from some types of syntactic units ( clauses in( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information']","['', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain(Rosario and Hearst, 2001) or the system(Gomez, 1998) .', 'Such systems extract information from some types of syntactic units ( clauses in( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts(Baker et al., 1998) .', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain(Rosario and Hearst, 2001) or the system(Gomez, 1998) .', 'Such systems extract information from some types of syntactic units ( clauses in( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts(Baker et al., 1998) .', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain(Rosario and Hearst, 2001) or the system(Gomez, 1998) .', 'Such systems extract information from some types of syntactic units ( clauses in( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information']",0
"['process a pair P not encountered previously, the system builds a graph centered on the main element (often the head) of P .', ""This idea was inspired by#AUTHOR_TAG , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) ."", '']","['process a pair P not encountered previously, the system builds a graph centered on the main element (often the head) of P .', ""This idea was inspired by#AUTHOR_TAG , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) ."", '']","['process a pair P not encountered previously, the system builds a graph centered on the main element (often the head) of P .', ""This idea was inspired by#AUTHOR_TAG , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) ."", '']","['process a pair P not encountered previously, the system builds a graph centered on the main element (often the head) of P .', ""This idea was inspired by#AUTHOR_TAG , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) ."", '']",4
"['current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet( #AUTHOR_TAG ) and FrameNet( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 )']","['current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet( #AUTHOR_TAG ) and FrameNet( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 )']","['current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet( #AUTHOR_TAG ) and FrameNet( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 )']","['current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet( #AUTHOR_TAG ) and FrameNet( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 )']",0
"['', 'The list , a synthesis of a number of relation lists cited in the literature , has been designed to be general , domainindependent( #AUTHOR_TAG a )']","['', 'The list , a synthesis of a number of relation lists cited in the literature , has been designed to be general , domainindependent( #AUTHOR_TAG a )']","['', 'The list , a synthesis of a number of relation lists cited in the literature , has been designed to be general , domainindependent( #AUTHOR_TAG a )']","['', 'The list , a synthesis of a number of relation lists cited in the literature , has been designed to be general , domainindependent( #AUTHOR_TAG a )']",4
"['', 'In other methods , lexical resources are specifically tailored to meet the requirements of the domain( #AUTHOR_TAG ) or the system( Gomez , 1998 ) .', '']","['', 'In other methods , lexical resources are specifically tailored to meet the requirements of the domain( #AUTHOR_TAG ) or the system( Gomez , 1998 ) .', '']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts(Baker et al., 1998) .', 'In other methods , lexical resources are specifically tailored to meet the requirements of the domain( #AUTHOR_TAG ) or the system( Gomez , 1998 ) .', '']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts(Baker et al., 1998) .', 'In other methods , lexical resources are specifically tailored to meet the requirements of the domain( #AUTHOR_TAG ) or the system( Gomez , 1998 ) .', '']",0
"['', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain(Rosario and Hearst, 2001) or the system(Gomez, 1998) .', 'Such systems extract information from some types of syntactic units ( clauses in( Fillmore and Atkins , 1998 ; #AUTHOR_TAG ; Hull and Gomez , 1996 ) ; noun phrases in( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information']","['', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain(Rosario and Hearst, 2001) or the system(Gomez, 1998) .', 'Such systems extract information from some types of syntactic units ( clauses in( Fillmore and Atkins , 1998 ; #AUTHOR_TAG ; Hull and Gomez , 1996 ) ; noun phrases in( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts(Baker et al., 1998) .', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain(Rosario and Hearst, 2001) or the system(Gomez, 1998) .', 'Such systems extract information from some types of syntactic units ( clauses in( Fillmore and Atkins , 1998 ; #AUTHOR_TAG ; Hull and Gomez , 1996 ) ; noun phrases in( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts(Baker et al., 1998) .', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain(Rosario and Hearst, 2001) or the system(Gomez, 1998) .', 'Such systems extract information from some types of syntactic units ( clauses in( Fillmore and Atkins , 1998 ; #AUTHOR_TAG ; Hull and Gomez , 1996 ) ; noun phrases in( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information']",0
"['the Rapid Knowledge Formation Project (RKF) a support system was developed for domain experts.', 'It helps them build complex knowledge bases by combining components : events , entities and modifiers( #AUTHOR_TAG ) .', ""The system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary""]","['the Rapid Knowledge Formation Project (RKF) a support system was developed for domain experts.', 'It helps them build complex knowledge bases by combining components : events , entities and modifiers( #AUTHOR_TAG ) .', ""The system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary""]","['the Rapid Knowledge Formation Project (RKF) a support system was developed for domain experts.', 'It helps them build complex knowledge bases by combining components : events , entities and modifiers( #AUTHOR_TAG ) .', ""The system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary""]","['the Rapid Knowledge Formation Project (RKF) a support system was developed for domain experts.', 'It helps them build complex knowledge bases by combining components : events , entities and modifiers( #AUTHOR_TAG ) .', ""The system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary""]",0
"['methods of semantic relation analysis rely on predefined templates filled with information from processed texts(Baker et al., 1998) .', 'In other methods , lexical resources are specifically tailored to meet the requirements of the domain( Rosario and Hearst , 2001 ) or the system( #AUTHOR_TAG ) .', '']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts(Baker et al., 1998) .', 'In other methods , lexical resources are specifically tailored to meet the requirements of the domain( Rosario and Hearst , 2001 ) or the system( #AUTHOR_TAG ) .', '']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts(Baker et al., 1998) .', 'In other methods , lexical resources are specifically tailored to meet the requirements of the domain( Rosario and Hearst , 2001 ) or the system( #AUTHOR_TAG ) .', '']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts(Baker et al., 1998) .', 'In other methods , lexical resources are specifically tailored to meet the requirements of the domain( Rosario and Hearst , 2001 ) or the system( #AUTHOR_TAG ) .', '']",0
"['', 'He was a grammarian who analysed Sanskrit( #AUTHOR_TAG ) .', '']","['', 'He was a grammarian who analysed Sanskrit( #AUTHOR_TAG ) .', '']","['', 'He was a grammarian who analysed Sanskrit( #AUTHOR_TAG ) .', '']","['', 'This is an old idea.', 'The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century B.C. and the work of Panini1 .', 'He was a grammarian who analysed Sanskrit( #AUTHOR_TAG ) .', '']",0
"['current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet( Kipper et al. , 2000 ) and FrameNet( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 )']","['current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet( Kipper et al. , 2000 ) and FrameNet( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 )']","['current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet( Kipper et al. , 2000 ) and FrameNet( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 )']","['current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet( Kipper et al. , 2000 ) and FrameNet( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 )']",0
"['nière (1959), who proposes a grouping of verb arguments into actants and circumstances, gives a set of rules to connect specific types of actants -for example, agent or instrument -to such grammatical elements as subject, direct object, indirect object.', 'This idea was expanded to include nouns and their modifiers through verb nominalizations( #AUTHOR_TAG ; Quirk et al. , 1985 )']","['', 'Tesnière (1959), who proposes a grouping of verb arguments into actants and circumstances, gives a set of rules to connect specific types of actants -for example, agent or instrument -to such grammatical elements as subject, direct object, indirect object.', 'This idea was expanded to include nouns and their modifiers through verb nominalizations( #AUTHOR_TAG ; Quirk et al. , 1985 )']","['nière (1959), who proposes a grouping of verb arguments into actants and circumstances, gives a set of rules to connect specific types of actants -for example, agent or instrument -to such grammatical elements as subject, direct object, indirect object.', 'This idea was expanded to include nouns and their modifiers through verb nominalizations( #AUTHOR_TAG ; Quirk et al. , 1985 )']","['', 'Tesnière (1959), who proposes a grouping of verb arguments into actants and circumstances, gives a set of rules to connect specific types of actants -for example, agent or instrument -to such grammatical elements as subject, direct object, indirect object.', 'This idea was expanded to include nouns and their modifiers through verb nominalizations( #AUTHOR_TAG ; Quirk et al. , 1985 )']",0
"['current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet( Kipper et al. , 2000 ) and FrameNet( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; #AUTHOR_TAG )']","['current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet( Kipper et al. , 2000 ) and FrameNet( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; #AUTHOR_TAG )']","['current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet( Kipper et al. , 2000 ) and FrameNet( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; #AUTHOR_TAG )']","['current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet( Kipper et al. , 2000 ) and FrameNet( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; #AUTHOR_TAG )']",0
"['syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT( #AUTHOR_TAG ) .', 'The parser']","['syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT( #AUTHOR_TAG ) .', 'The parser,']","['syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT( #AUTHOR_TAG ) .', 'The parser,']","['syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT( #AUTHOR_TAG ) .', '']",5
"['', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain(Rosario and Hearst, 2001) or the system(Gomez, 1998) .', 'Such systems extract information from some types of syntactic units ( clauses in( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in( Hull and Gomez , 1996 ; #AUTHOR_TAG ) ) .', 'Lists of semantic relations are designed to capture salient domain information']","['', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain(Rosario and Hearst, 2001) or the system(Gomez, 1998) .', 'Such systems extract information from some types of syntactic units ( clauses in( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in( Hull and Gomez , 1996 ; #AUTHOR_TAG ) ) .', 'Lists of semantic relations are designed to capture salient domain information']","['', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain(Rosario and Hearst, 2001) or the system(Gomez, 1998) .', 'Such systems extract information from some types of syntactic units ( clauses in( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in( Hull and Gomez , 1996 ; #AUTHOR_TAG ) ) .', 'Lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts(Baker et al., 1998) .', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain(Rosario and Hearst, 2001) or the system(Gomez, 1998) .', 'Such systems extract information from some types of syntactic units ( clauses in( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in( Hull and Gomez , 1996 ; #AUTHOR_TAG ) ) .', 'Lists of semantic relations are designed to capture salient domain information']",0
"['', 'The idea resurfaced forcefully at several points in the more recent history of linguistic research( Tesni`ere , 1959 ; #AUTHOR_TAG ; Fillmore , 1968 ) .', '']","['', 'The idea resurfaced forcefully at several points in the more recent history of linguistic research( Tesni`ere , 1959 ; #AUTHOR_TAG ; Fillmore , 1968 ) .', '']","['anskrit(Misra, 1966) .', 'The idea resurfaced forcefully at several points in the more recent history of linguistic research( Tesni`ere , 1959 ; #AUTHOR_TAG ; Fillmore , 1968 ) .', '']","['', 'He was a grammarian who analysed Sanskrit(Misra, 1966) .', 'The idea resurfaced forcefully at several points in the more recent history of linguistic research( Tesni`ere , 1959 ; #AUTHOR_TAG ; Fillmore , 1968 ) .', 'Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling(Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005)']",0
"['', 'The idea resurfaced forcefully at several points in the more recent history of linguistic research( Tesni`ere , 1959 ; Gruber , 1965 ; #AUTHOR_TAG ) .', '']","['', 'The idea resurfaced forcefully at several points in the more recent history of linguistic research( Tesni`ere , 1959 ; Gruber , 1965 ; #AUTHOR_TAG ) .', '']","['anskrit(Misra, 1966) .', 'The idea resurfaced forcefully at several points in the more recent history of linguistic research( Tesni`ere , 1959 ; Gruber , 1965 ; #AUTHOR_TAG ) .', '']","['', 'He was a grammarian who analysed Sanskrit(Misra, 1966) .', 'The idea resurfaced forcefully at several points in the more recent history of linguistic research( Tesni`ere , 1959 ; Gruber , 1965 ; #AUTHOR_TAG ) .', 'Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling(Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005)']",0
"['system begins operation with a minimum of manually encoded knowledge, and accumulates information as it processes the text.', 'This design idea was adopted from TANKA( #AUTHOR_TAG b ) .', '']","['system begins operation with a minimum of manually encoded knowledge, and accumulates information as it processes the text.', 'This design idea was adopted from TANKA( #AUTHOR_TAG b ) .', '']","['system begins operation with a minimum of manually encoded knowledge, and accumulates information as it processes the text.', 'This design idea was adopted from TANKA( #AUTHOR_TAG b ) .', '']","['system begins operation with a minimum of manually encoded knowledge, and accumulates information as it processes the text.', 'This design idea was adopted from TANKA( #AUTHOR_TAG b ) .', '']",5
"['', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain(Rosario and Hearst, 2001) or the system(Gomez, 1998) .', 'Such systems extract information from some types of syntactic units ( clauses in( #AUTHOR_TAG ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information']","['', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain(Rosario and Hearst, 2001) or the system(Gomez, 1998) .', 'Such systems extract information from some types of syntactic units ( clauses in( #AUTHOR_TAG ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts(Baker et al., 1998) .', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain(Rosario and Hearst, 2001) or the system(Gomez, 1998) .', 'Such systems extract information from some types of syntactic units ( clauses in( #AUTHOR_TAG ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts(Baker et al., 1998) .', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain(Rosario and Hearst, 2001) or the system(Gomez, 1998) .', 'Such systems extract information from some types of syntactic units ( clauses in( #AUTHOR_TAG ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information']",0
"['methods of semantic relation analysis rely on predefined templates filled with information from processed texts( #AUTHOR_TAG ) .', '']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts( #AUTHOR_TAG ) .', '']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts( #AUTHOR_TAG ) .', '']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts( #AUTHOR_TAG ) .', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain(Rosario and Hearst, 2001) or the system(Gomez, 1998) .', '']",0
"['list of semantic relations with which we work is based on extensive literature study( #AUTHOR_TAG a ) .', 'Three lists of relations for three syntactic levels -inter-clause, intra-clause (case) and nounmodifier relations -were next combined based on syntactic and semantic phenomena.', 'The resulting list is the one used in the experiments']","['list of semantic relations with which we work is based on extensive literature study( #AUTHOR_TAG a ) .', 'Three lists of relations for three syntactic levels -inter-clause, intra-clause (case) and nounmodifier relations -were next combined based on syntactic and semantic phenomena.', 'The resulting list is the one used in the experiments']","['list of semantic relations with which we work is based on extensive literature study( #AUTHOR_TAG a ) .', 'Three lists of relations for three syntactic levels -inter-clause, intra-clause (case) and nounmodifier relations -were next combined based on syntactic and semantic phenomena.', 'The resulting list is the one used in the experiments']","['list of semantic relations with which we work is based on extensive literature study( #AUTHOR_TAG a ) .', 'Three lists of relations for three syntactic levels -inter-clause, intra-clause (case) and nounmodifier relations -were next combined based on syntactic and semantic phenomena.', 'The resulting list is the one used in the experiments we present in this paper.', '']",5
"['reordering between German and English is a complex problem.', 'Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish( #AUTHOR_TAG ) , we tried to adapt the same approach to the German-English language pair .', 'It turned out that there is a larger variety of long reordering patterns in this case.', '']","['reordering between German and English is a complex problem.', 'Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish( #AUTHOR_TAG ) , we tried to adapt the same approach to the German-English language pair .', 'It turned out that there is a larger variety of long reordering patterns in this case.', '']","['reordering between German and English is a complex problem.', 'Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish( #AUTHOR_TAG ) , we tried to adapt the same approach to the German-English language pair .', 'It turned out that there is a larger variety of long reordering patterns in this case.', '']","['reordering between German and English is a complex problem.', 'Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish( #AUTHOR_TAG ) , we tried to adapt the same approach to the German-English language pair .', 'It turned out that there is a larger variety of long reordering patterns in this case.', '']",4
"['', 'Future research should apply the work of#AUTHOR_TAG andBlunsom and Osborne ( 2008 ) , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars']","['', 'Future research should apply the work of#AUTHOR_TAG andBlunsom and Osborne ( 2008 ) , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars']","['', 'Future research should apply the work of#AUTHOR_TAG andBlunsom and Osborne ( 2008 ) , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars']","['', 'Future research should apply the work of#AUTHOR_TAG andBlunsom and Osborne ( 2008 ) , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars']",3
"['', 'Future research should apply the work ofBlunsom et al. ( 2008 ) and#AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars']","['', 'Future research should apply the work ofBlunsom et al. ( 2008 ) and#AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars']","['', 'Future research should apply the work ofBlunsom et al. ( 2008 ) and#AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars']","['', 'Future research should apply the work ofBlunsom et al. ( 2008 ) and#AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars']",3
"['our prior work( #AUTHOR_TAG ) , we examined whether techniques used for predicting the helpfulness of product reviews( Kim et al. , 2006 ) could be tailored to our peer-review domain , where the definition of helpfulness is largely influenced by the educational context of peer review .', 'While previously we used the average of two expert-provided ratings as our gold standard of peer-review helpfulness 1 , there are other types of helpfulness rating']","['our prior work( #AUTHOR_TAG ) , we examined whether techniques used for predicting the helpfulness of product reviews( Kim et al. , 2006 ) could be tailored to our peer-review domain , where the definition of helpfulness is largely influenced by the educational context of peer review .', 'While previously we used the average of two expert-provided ratings as our gold standard of peer-review helpfulness 1 , there are other types of helpfulness rating']","['our prior work( #AUTHOR_TAG ) , we examined whether techniques used for predicting the helpfulness of product reviews( Kim et al. , 2006 ) could be tailored to our peer-review domain , where the definition of helpfulness is largely influenced by the educational context of peer review .', 'While previously we used the average of two expert-provided ratings as our gold standard of peer-review helpfulness 1 , there are other types of helpfulness rating']","['our prior work( #AUTHOR_TAG ) , we examined whether techniques used for predicting the helpfulness of product reviews( Kim et al. , 2006 ) could be tailored to our peer-review domain , where the definition of helpfulness is largely influenced by the educational context of peer review .', 'While previously we used the average of two expert-provided ratings as our gold standard of peer-review helpfulness 1 , there are other types of helpfulness rating (e.g.', '']",2
"['', '( Details of how the average-expert model performs can be found in our prior work( #AUTHOR_TAG ) .']","['', '( Details of how the average-expert model performs can be found in our prior work( #AUTHOR_TAG ) .']","['', '( Details of how the average-expert model performs can be found in our prior work( #AUTHOR_TAG ) .']","['', '( Details of how the average-expert model performs can be found in our prior work( #AUTHOR_TAG ) .']",2
"['selecting features for Korean, we have to ac- count for relatively free word order(Chung et al., 2010) .', 'We follow our previous work( #AUTHOR_TAG ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see alsoTetreault and Chodorow , 2008 ) .', '']","['selecting features for Korean, we have to ac- count for relatively free word order(Chung et al., 2010) .', 'We follow our previous work( #AUTHOR_TAG ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see alsoTetreault and Chodorow , 2008 ) .', '']","['selecting features for Korean, we have to ac- count for relatively free word order(Chung et al., 2010) .', 'We follow our previous work( #AUTHOR_TAG ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see alsoTetreault and Chodorow , 2008 ) .', '']","['selecting features for Korean, we have to ac- count for relatively free word order(Chung et al., 2010) .', 'We follow our previous work( #AUTHOR_TAG ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see alsoTetreault and Chodorow , 2008 ) .', '']",2
"['same annotation scheme as in our previous work on anger detection has been applied , see e.g.( #AUTHOR_TAG ) .', '(Schmitt et al., 2009) .', '']","['same annotation scheme as in our previous work on anger detection has been applied , see e.g.( #AUTHOR_TAG ) .', '(Schmitt et al., 2009) .', '']","['same annotation scheme as in our previous work on anger detection has been applied , see e.g.( #AUTHOR_TAG ) .', '(Schmitt et al., 2009) .', '']","['same annotation scheme as in our previous work on anger detection has been applied , see e.g.( #AUTHOR_TAG ) .', '(Schmitt et al., 2009) .', '']",2
"['', 'This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications( #AUTHOR_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 ) .', '']","['doing so until all classes have been merged.', 'This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications( #AUTHOR_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 ) .', '']","['', 'This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications( #AUTHOR_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 ) .', '']","['', 'This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications( #AUTHOR_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 ) .', '']",4
"['#AUTHOR_TAG , we also compare the performance of our system with a system using features based on the Brown clusters of the word types in a document .', '']","['#AUTHOR_TAG , we also compare the performance of our system with a system using features based on the Brown clusters of the word types in a document .', '']","['#AUTHOR_TAG , we also compare the performance of our system with a system using features based on the Brown clusters of the word types in a document .', '']","['#AUTHOR_TAG , we also compare the performance of our system with a system using features based on the Brown clusters of the word types in a document .', '']",5
"['this section we describe in detail the baseline NER system we use.', 'It is inspired by the system described in#AUTHOR_TAG .', 'Because NER annotations are commonly not nested (for example,']","['this section we describe in detail the baseline NER system we use.', 'It is inspired by the system described in#AUTHOR_TAG .', 'Because NER annotations are commonly not nested (for example,']","['this section we describe in detail the baseline NER system we use.', 'It is inspired by the system described in#AUTHOR_TAG .', 'Because NER annotations are commonly not nested (for example,']","['this section we describe in detail the baseline NER system we use.', 'It is inspired by the system described in#AUTHOR_TAG .', '']",4
"['implementing our scheme.', 'This choice is inspired by recent work on learning syntactic categories( #AUTHOR_TAG ) , which successfully utilized such language models to represent word window contexts of target words .', '']","['implementing our scheme.', 'This choice is inspired by recent work on learning syntactic categories( #AUTHOR_TAG ) , which successfully utilized such language models to represent word window contexts of target words .', '']","['our scheme.', 'This choice is inspired by recent work on learning syntactic categories( #AUTHOR_TAG ) , which successfully utilized such language models to represent word window contexts of target words .', '']","['', 'This choice is inspired by recent work on learning syntactic categories( #AUTHOR_TAG ) , which successfully utilized such language models to represent word window contexts of target words .', '']",4
"['', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry( Greene et al. , 2010 )( Colton et al. , 2012 )( Jiang and Zhou , 2008 ) or song lyrics( #AUTHOR_TAG ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .', '']","['', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry( Greene et al. , 2010 )( Colton et al. , 2012 )( Jiang and Zhou , 2008 ) or song lyrics( #AUTHOR_TAG ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .', '']","['', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry( Greene et al. , 2010 )( Colton et al. , 2012 )( Jiang and Zhou , 2008 ) or song lyrics( #AUTHOR_TAG ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .', '']","['', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry( Greene et al. , 2010 )( Colton et al. , 2012 )( Jiang and Zhou , 2008 ) or song lyrics( #AUTHOR_TAG ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .', 'In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']",1
"['motivation for generation of material for language education exists in work such asSumita et al. ( 2005 ) and#AUTHOR_TAG , which deal with automatic generation of classic fill in the blank questions .', 'Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences']","['motivation for generation of material for language education exists in work such asSumita et al. ( 2005 ) and#AUTHOR_TAG , which deal with automatic generation of classic fill in the blank questions .', 'Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences']","['motivation for generation of material for language education exists in work such asSumita et al. ( 2005 ) and#AUTHOR_TAG , which deal with automatic generation of classic fill in the blank questions .', 'Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences']","['motivation for generation of material for language education exists in work such asSumita et al. ( 2005 ) and#AUTHOR_TAG , which deal with automatic generation of classic fill in the blank questions .', 'Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences']",4
"['', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry( #AUTHOR_TAG )( Colton et al. , 2012 )( Jiang and Zhou , 2008 ) or song lyrics( Wu et al. , 2013 ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .', '']","['', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry( #AUTHOR_TAG )( Colton et al. , 2012 )( Jiang and Zhou , 2008 ) or song lyrics( Wu et al. , 2013 ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .', '']","['', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry( #AUTHOR_TAG )( Colton et al. , 2012 )( Jiang and Zhou , 2008 ) or song lyrics( Wu et al. , 2013 ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .', '']","['', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry( #AUTHOR_TAG )( Colton et al. , 2012 )( Jiang and Zhou , 2008 ) or song lyrics( Wu et al. , 2013 ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .', 'In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']",1
"['', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry( Greene et al. , 2010 )( Colton et al. , 2012 )( Jiang and Zhou , 2008 ) or song lyrics( Wu et al. , 2013 ) ( Ramakrishnan#AUTHOR_TAG ) , where specified meter or rhyme schemes are enforced .', '']","['', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry( Greene et al. , 2010 )( Colton et al. , 2012 )( Jiang and Zhou , 2008 ) or song lyrics( Wu et al. , 2013 ) ( Ramakrishnan#AUTHOR_TAG ) , where specified meter or rhyme schemes are enforced .', '']","['', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry( Greene et al. , 2010 )( Colton et al. , 2012 )( Jiang and Zhou , 2008 ) or song lyrics( Wu et al. , 2013 ) ( Ramakrishnan#AUTHOR_TAG ) , where specified meter or rhyme schemes are enforced .', '']","['', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry( Greene et al. , 2010 )( Colton et al. , 2012 )( Jiang and Zhou , 2008 ) or song lyrics( Wu et al. , 2013 ) ( Ramakrishnan#AUTHOR_TAG ) , where specified meter or rhyme schemes are enforced .', 'In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']",1
"['of words (L): Instead of using the raw length value as a feature , we follow our previous work( #AUTHOR_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) .', 'We use length as the only feature to train a decision tree for each fold and use the nodes']","['of words (L): Instead of using the raw length value as a feature , we follow our previous work( #AUTHOR_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) .', 'We use length as the only feature to train a decision tree for each fold and use the nodes']","['of words (L): Instead of using the raw length value as a feature , we follow our previous work( #AUTHOR_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) .', 'We use length as the only feature to train a decision tree for each fold and use the nodes']","['', '3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work( #AUTHOR_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) .', 'We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.', '']",2
"['ence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments.', 'raw length value as a feature , we follow our previous work( Rubino et al. , 2013 ; #AUTHOR_TAG ) and create multiple features for length using a decision tree ( J48 ) .', '']","['Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments.', 'raw length value as a feature , we follow our previous work( Rubino et al. , 2013 ; #AUTHOR_TAG ) and create multiple features for length using a decision tree ( J48 ) .', '']","['in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments.', 'raw length value as a feature , we follow our previous work( Rubino et al. , 2013 ; #AUTHOR_TAG ) and create multiple features for length using a decision tree ( J48 ) .', '']","['', '2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments.', 'raw length value as a feature , we follow our previous work( Rubino et al. , 2013 ; #AUTHOR_TAG ) and create multiple features for length using a decision tree ( J48 ) .', '']",2
"['shape-based metric.', ""The only disambiguation metric that we used in our previous work( #AUTHOR_TAG b ) was the shape-based metric , according to which the `` best '' trees are those that are skewed to the right ."", 'The explanation for this metric is that text processing is, essentially, a left-to-right process.', 'In many genres, people write texts so that the most important ideas go first, both at the paragraph and at the text']","['shape-based metric.', ""The only disambiguation metric that we used in our previous work( #AUTHOR_TAG b ) was the shape-based metric , according to which the `` best '' trees are those that are skewed to the right ."", 'The explanation for this metric is that text processing is, essentially, a left-to-right process.', 'In many genres, people write texts so that the most important ideas go first, both at the paragraph and at the text']","['shape-based metric.', ""The only disambiguation metric that we used in our previous work( #AUTHOR_TAG b ) was the shape-based metric , according to which the `` best '' trees are those that are skewed to the right ."", 'The explanation for this metric is that text processing is, essentially, a left-to-right process.', 'In many genres, people write texts so that the most important ideas go first, both at the paragraph and at the text levels.', '']","['shape-based metric.', ""The only disambiguation metric that we used in our previous work( #AUTHOR_TAG b ) was the shape-based metric , according to which the `` best '' trees are those that are skewed to the right ."", 'The explanation for this metric is that text processing is, essentially, a left-to-right process.', 'In many genres, people write texts so that the most important ideas go first, both at the paragraph and at the text levels.', '']",2
"['', 'A number of alignment techniques have been proposed , varying from statistical methods( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods( Kay and Roscheisen , 1993 ; Chen , 1993 ) .', '']","['', 'A number of alignment techniques have been proposed , varying from statistical methods( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods( Kay and Roscheisen , 1993 ; Chen , 1993 ) .', '']","['', 'A number of alignment techniques have been proposed , varying from statistical methods( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods( Kay and Roscheisen , 1993 ; Chen , 1993 ) .', '']","['', 'A number of alignment techniques have been proposed , varying from statistical methods( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods( Kay and Roscheisen , 1993 ; Chen , 1993 ) .', '']",0
"[', a major obstacle to this approach is the lack of parallel corpora for model training.', 'Only a few such corpora exist , including the Hansard English-French corpus and the HKUST EnglishChinese corpus( #AUTHOR_TAG ) .', 'In this paper, we will describe a method which automatically searches for parallel texts on the Web.', '']","[', a major obstacle to this approach is the lack of parallel corpora for model training.', 'Only a few such corpora exist , including the Hansard English-French corpus and the HKUST EnglishChinese corpus( #AUTHOR_TAG ) .', 'In this paper, we will describe a method which automatically searches for parallel texts on the Web.', '']","[', a major obstacle to this approach is the lack of parallel corpora for model training.', 'Only a few such corpora exist , including the Hansard English-French corpus and the HKUST EnglishChinese corpus( #AUTHOR_TAG ) .', 'In this paper, we will describe a method which automatically searches for parallel texts on the Web.', '']","[', a major obstacle to this approach is the lack of parallel corpora for model training.', 'Only a few such corpora exist , including the Hansard English-French corpus and the HKUST EnglishChinese corpus( #AUTHOR_TAG ) .', 'In this paper, we will describe a method which automatically searches for parallel texts on the Web.', '']",0
"['', 'A number of alignment techniques have been proposed , varying from statistical methods( Brown et al. , 1991 ; Gale and Church , 1991 ) to lexical methods( Kay and Roscheisen , 1993 ; #AUTHOR_TAG ) .', 'The method we adopted']","['', 'A number of alignment techniques have been proposed , varying from statistical methods( Brown et al. , 1991 ; Gale and Church , 1991 ) to lexical methods( Kay and Roscheisen , 1993 ; #AUTHOR_TAG ) .', 'The method we adopted']","['', 'A number of alignment techniques have been proposed , varying from statistical methods( Brown et al. , 1991 ; Gale and Church , 1991 ) to lexical methods( Kay and Roscheisen , 1993 ; #AUTHOR_TAG ) .', 'The method we adopted is']","['', 'A number of alignment techniques have been proposed , varying from statistical methods( Brown et al. , 1991 ; Gale and Church , 1991 ) to lexical methods( Kay and Roscheisen , 1993 ; #AUTHOR_TAG ) .', '']",0
"['ide HTML markups, other criteria may also be incorporated.', 'For example , it would be helpful to consider strong correspondence between certain English and Chinese words , as in( #AUTHOR_TAG ) .', 'We hope to implement such correspondences in our future research']","['ide HTML markups, other criteria may also be incorporated.', 'For example , it would be helpful to consider strong correspondence between certain English and Chinese words , as in( #AUTHOR_TAG ) .', 'We hope to implement such correspondences in our future research']","['ide HTML markups, other criteria may also be incorporated.', 'For example , it would be helpful to consider strong correspondence between certain English and Chinese words , as in( #AUTHOR_TAG ) .', 'We hope to implement such correspondences in our future research']","['ide HTML markups, other criteria may also be incorporated.', 'For example , it would be helpful to consider strong correspondence between certain English and Chinese words , as in( #AUTHOR_TAG ) .', 'We hope to implement such correspondences in our future research']",3
"['', 'A number of alignment techniques have been proposed , varying from statistical methods( #AUTHOR_TAG ; Gale and Church , 1991 ) to lexical methods( Kay and Roscheisen , 1993 ; Chen , 1993 ) .', '']","['', 'A number of alignment techniques have been proposed , varying from statistical methods( #AUTHOR_TAG ; Gale and Church , 1991 ) to lexical methods( Kay and Roscheisen , 1993 ; Chen , 1993 ) .', '']","['', 'A number of alignment techniques have been proposed , varying from statistical methods( #AUTHOR_TAG ; Gale and Church , 1991 ) to lexical methods( Kay and Roscheisen , 1993 ; Chen , 1993 ) .', '']","['', 'A number of alignment techniques have been proposed , varying from statistical methods( #AUTHOR_TAG ; Gale and Church , 1991 ) to lexical methods( Kay and Roscheisen , 1993 ; Chen , 1993 ) .', '']",0
"['', 'Figure 4 shows the architecture of Atlas-Andes; any other system built with APE would look similar.', ""Robust natural language understanding in Atlas-Andes is provided by RosÃ© 's CARMEL system ( RosÃ© 2000 ) ; it uses the spelling correction algorithm devised by#AUTHOR_TAG""]","['', 'Figure 4 shows the architecture of Atlas-Andes; any other system built with APE would look similar.', ""Robust natural language understanding in Atlas-Andes is provided by RosÃ© 's CARMEL system ( RosÃ© 2000 ) ; it uses the spelling correction algorithm devised by#AUTHOR_TAG""]","['', 'Figure 4 shows the architecture of Atlas-Andes; any other system built with APE would look similar.', ""Robust natural language understanding in Atlas-Andes is provided by RosÃ© 's CARMEL system ( RosÃ© 2000 ) ; it uses the spelling correction algorithm devised by#AUTHOR_TAG""]","['', 'Figure 4 shows the architecture of Atlas-Andes; any other system built with APE would look similar.', ""Robust natural language understanding in Atlas-Andes is provided by RosÃ© 's CARMEL system ( RosÃ© 2000 ) ; it uses the spelling correction algorithm devised by#AUTHOR_TAG""]",5
"[""2 See( #AUTHOR_TAG ) for how MIMIC 's dialoguelevel knowledge is used to override default prosodic assignments for concept-to-speech generation""]","[""2 See( #AUTHOR_TAG ) for how MIMIC 's dialoguelevel knowledge is used to override default prosodic assignments for concept-to-speech generation""]","[""2 See( #AUTHOR_TAG ) for how MIMIC 's dialoguelevel knowledge is used to override default prosodic assignments for concept-to-speech generation""]","[""2 See( #AUTHOR_TAG ) for how MIMIC 's dialoguelevel knowledge is used to override default prosodic assignments for concept-to-speech generation""]",0
"['strategies employed when MIMIC has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems ( e.g. , ( Bennacef et at. , 1996 ;#AUTHOR_TAG ) ) .', '']","['strategies employed when MIMIC has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems ( e.g. , ( Bennacef et at. , 1996 ;#AUTHOR_TAG ) ) .', '']","['strategies employed when MIMIC has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems ( e.g. , ( Bennacef et at. , 1996 ;#AUTHOR_TAG ) ) .', '']","['strategies employed when MIMIC has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems ( e.g. , ( Bennacef et at. , 1996 ;#AUTHOR_TAG ) ) .', '']",1
"['vious work has argued that initiative affects the degree of control an agent has in the dialogue interaction( Whittaker and Stenton , 1988 ; #AUTHOR_TAG ; Chu-Carroll and Brown , 1998 ) .', 'Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', '']","['vious work has argued that initiative affects the degree of control an agent has in the dialogue interaction( Whittaker and Stenton , 1988 ; #AUTHOR_TAG ; Chu-Carroll and Brown , 1998 ) .', 'Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', '']","['vious work has argued that initiative affects the degree of control an agent has in the dialogue interaction( Whittaker and Stenton , 1988 ; #AUTHOR_TAG ; Chu-Carroll and Brown , 1998 ) .', 'Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', '']","['vious work has argued that initiative affects the degree of control an agent has in the dialogue interaction( Whittaker and Stenton , 1988 ; #AUTHOR_TAG ; Chu-Carroll and Brown , 1998 ) .', 'Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', '']",0
"['vious work has argued that initiative affects the degree of control an agent has in the dialogue interaction( #AUTHOR_TAG ; Walker and Whittaker , 1990 ; Chu-Carroll and Brown , 1998 ) .', 'Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', '']","['vious work has argued that initiative affects the degree of control an agent has in the dialogue interaction( #AUTHOR_TAG ; Walker and Whittaker , 1990 ; Chu-Carroll and Brown , 1998 ) .', 'Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', '']","['vious work has argued that initiative affects the degree of control an agent has in the dialogue interaction( #AUTHOR_TAG ; Walker and Whittaker , 1990 ; Chu-Carroll and Brown , 1998 ) .', 'Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', '']","['vious work has argued that initiative affects the degree of control an agent has in the dialogue interaction( #AUTHOR_TAG ; Walker and Whittaker , 1990 ; Chu-Carroll and Brown , 1998 ) .', 'Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', '']",0
"['a nonadaptive mixed-initiative version of MIMIC that resembles the behavior of many existing dialogue systems.', 'In this section we summarize these experiments and their results.', 'A companion paper describes the evaluation process and results in further detail( #AUTHOR_TAG ) .', 'Each experiment involved eight users interacting with MIMIC and MIMIC-SI or MIMIC-MI to perform a set of tasks, each requiring the user to obtain specific movie information.', '']","['MIMIC-MI, a nonadaptive mixed-initiative version of MIMIC that resembles the behavior of many existing dialogue systems.', 'In this section we summarize these experiments and their results.', 'A companion paper describes the evaluation process and results in further detail( #AUTHOR_TAG ) .', 'Each experiment involved eight users interacting with MIMIC and MIMIC-SI or MIMIC-MI to perform a set of tasks, each requiring the user to obtain specific movie information.', '']","['a nonadaptive mixed-initiative version of MIMIC that resembles the behavior of many existing dialogue systems.', 'In this section we summarize these experiments and their results.', 'A companion paper describes the evaluation process and results in further detail( #AUTHOR_TAG ) .', 'Each experiment involved eight users interacting with MIMIC and MIMIC-SI or MIMIC-MI to perform a set of tasks, each requiring the user to obtain specific movie information.', '']","[""conducted two experiments to evaluate MIMIC's automatic adaptation capabilities."", 'We compared MIMIC with two control systems: MIMIC-SI, a system-initiative version of MIMIC in which the system retains both initiatives throughout the dialogue, and MIMIC-MI, a nonadaptive mixed-initiative version of MIMIC that resembles the behavior of many existing dialogue systems.', 'In this section we summarize these experiments and their results.', 'A companion paper describes the evaluation process and results in further detail( #AUTHOR_TAG ) .', 'Each experiment involved eight users interacting with MIMIC and MIMIC-SI or MIMIC-MI to perform a set of tasks, each requiring the user to obtain specific movie information.', '']",2
"['5An alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results( #AUTHOR_TAG ) , which we leave for future work']","['5An alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results( #AUTHOR_TAG ) , which we leave for future work']","['5An alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results( #AUTHOR_TAG ) , which we leave for future work']","['5An alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results( #AUTHOR_TAG ) , which we leave for future work']",3
"[', a number of performance features , largely based on the PARADISE dialogue evaluation scheme( #AUTHOR_TAG ) , were automatically logged , derived , or manually annotated .', '']","[', a number of performance features , largely based on the PARADISE dialogue evaluation scheme( #AUTHOR_TAG ) , were automatically logged , derived , or manually annotated .', '']","[', a number of performance features , largely based on the PARADISE dialogue evaluation scheme( #AUTHOR_TAG ) , were automatically logged , derived , or manually annotated .', '']","['', 'Furthermore , a number of performance features , largely based on the PARADISE dialogue evaluation scheme( #AUTHOR_TAG ) , were automatically logged , derived , or manually annotated .', '']",5
"['', 'The goal of the JAVOX toolkit is to speech-enable traditional desktop applications -- this is similar to the goals of the MELISSA project( #AUTHOR_TAG ) .', 'It is intended to both speed the development of SLSs and to localize the speech-specific code within the application.', 'JAVOX allows developers to add speech']","['', 'The goal of the JAVOX toolkit is to speech-enable traditional desktop applications -- this is similar to the goals of the MELISSA project( #AUTHOR_TAG ) .', 'It is intended to both speed the development of SLSs and to localize the speech-specific code within the application.', 'JAVOX allows developers to add speech']","['', 'The goal of the JAVOX toolkit is to speech-enable traditional desktop applications -- this is similar to the goals of the MELISSA project( #AUTHOR_TAG ) .', 'It is intended to both speed the development of SLSs and to localize the speech-specific code within the application.', 'JAVOX allows developers to add']","['', 'The goal of the JAVOX toolkit is to speech-enable traditional desktop applications -- this is similar to the goals of the MELISSA project( #AUTHOR_TAG ) .', 'It is intended to both speed the development of SLSs and to localize the speech-specific code within the application.', '']",1
"['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints(Ganssier, 1995;Kupiec, 1993; hua Chen and Chen, 94;Fung, 1995;Evans and Zhai, 1996) .', 'It is also possible to focus on non-compositional compounds , a key point in bilingual applications( #AUTHOR_TAG ; Melamed , 1997 ;  Lin , 99 ) .', '']","['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints(Ganssier, 1995;Kupiec, 1993; hua Chen and Chen, 94;Fung, 1995;Evans and Zhai, 1996) .', 'It is also possible to focus on non-compositional compounds , a key point in bilingual applications( #AUTHOR_TAG ; Melamed , 1997 ;  Lin , 99 ) .', '']","['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints(Ganssier, 1995;Kupiec, 1993; hua Chen and Chen, 94;Fung, 1995;Evans and Zhai, 1996) .', 'It is also possible to focus on non-compositional compounds , a key point in bilingual applications( #AUTHOR_TAG ; Melamed , 1997 ;  Lin , 99 ) .', '']","['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints(Ganssier, 1995;Kupiec, 1993; hua Chen and Chen, 94;Fung, 1995;Evans and Zhai, 1996) .', 'It is also possible to focus on non-compositional compounds , a key point in bilingual applications( #AUTHOR_TAG ; Melamed , 1997 ;  Lin , 99 ) .', '']",0
"['', 'This method allows the efficient retrieval of arbitrary length n-grams ( Nagao and Mori , 94 ; Haruno et al. , 96 ; Ikehaxa et al. , 96 ;#AUTHOR_TAG ; Russell , 1998 )']","['', 'This method allows the efficient retrieval of arbitrary length n-grams ( Nagao and Mori , 94 ; Haruno et al. , 96 ; Ikehaxa et al. , 96 ;#AUTHOR_TAG ; Russell , 1998 )']","['', 'This method allows the efficient retrieval of arbitrary length n-grams ( Nagao and Mori , 94 ; Haruno et al. , 96 ; Ikehaxa et al. , 96 ;#AUTHOR_TAG ; Russell , 1998 )']","['', 'This method allows the efficient retrieval of arbitrary length n-grams ( Nagao and Mori , 94 ; Haruno et al. , 96 ; Ikehaxa et al. , 96 ;#AUTHOR_TAG ; Russell , 1998 )']",0
"['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints( Gaussier , 1995 ; #AUTHOR_TAG ; hua Chen and Chen , 94 ;Fung , 1995 ; Evans and Zhai , 1996 ) .', 'It is also possible to focus on non-compositional compounds, a key point in bilingual applications(Su et al., 1994;Melamed, 1997; Lin, 99).', '']","['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints( Gaussier , 1995 ; #AUTHOR_TAG ; hua Chen and Chen , 94 ;Fung , 1995 ; Evans and Zhai , 1996 ) .', 'It is also possible to focus on non-compositional compounds, a key point in bilingual applications(Su et al., 1994;Melamed, 1997; Lin, 99).', '']","['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints( Gaussier , 1995 ; #AUTHOR_TAG ; hua Chen and Chen , 94 ;Fung , 1995 ; Evans and Zhai , 1996 ) .', 'It is also possible to focus on non-compositional compounds, a key point in bilingual applications(Su et al., 1994;Melamed, 1997; Lin, 99).', '']","['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints( Gaussier , 1995 ; #AUTHOR_TAG ; hua Chen and Chen , 94 ;Fung , 1995 ; Evans and Zhai , 1996 ) .', 'It is also possible to focus on non-compositional compounds, a key point in bilingual applications(Su et al., 1994;Melamed, 1997; Lin, 99).', '']",5
"['speech and language processing architecture is based on that of the SRI CommandTalk system( #AUTHOR_TAG ; Stent et a. , 1999 ) .', 'The system']","['speech and language processing architecture is based on that of the SRI CommandTalk system( #AUTHOR_TAG ; Stent et a. , 1999 ) .', 'The system']","['speech and language processing architecture is based on that of the SRI CommandTalk system( #AUTHOR_TAG ; Stent et a. , 1999 ) .', 'The system']","['speech and language processing architecture is based on that of the SRI CommandTalk system( #AUTHOR_TAG ; Stent et a. , 1999 ) .', '']",5
"['.', 'CornmandTalk( #AUTHOR_TAG ) , Circuit Fix-It Shop( Smith , 1997 ) and TRAINS-96( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents .', '']","['task.', 'CornmandTalk( #AUTHOR_TAG ) , Circuit Fix-It Shop( Smith , 1997 ) and TRAINS-96( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents .', '']","['', 'CornmandTalk( #AUTHOR_TAG ) , Circuit Fix-It Shop( Smith , 1997 ) and TRAINS-96( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents .', '']","['', 'CornmandTalk( #AUTHOR_TAG ) , Circuit Fix-It Shop( Smith , 1997 ) and TRAINS-96( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents .', '']",0
"['', 'We do this with a first-order HMM part-ofspeech tagger ( Merialdo#AUTHOR_TAG ) .', '']","['', 'We do this with a first-order HMM part-ofspeech tagger ( Merialdo#AUTHOR_TAG ) .', '']","['', 'We do this with a first-order HMM part-ofspeech tagger ( Merialdo#AUTHOR_TAG ) .', '']","['', 'We do this with a first-order HMM part-ofspeech tagger ( Merialdo#AUTHOR_TAG ) .', '']",5
"['', 'This alignment is done on the basis of both length ( Gale and Church#AUTHOR_TAG ) and a notion of cognateness ( Simard [ 16 ] )']","['', 'This alignment is done on the basis of both length ( Gale and Church#AUTHOR_TAG ) and a notion of cognateness ( Simard [ 16 ] )']","['', 'This alignment is done on the basis of both length ( Gale and Church#AUTHOR_TAG ) and a notion of cognateness ( Simard [ 16 ] )']","['', 'After identification of word and sentence boundaries the text is processed into a bi-text by an alignment program.', 'This alignment is done on the basis of both length ( Gale and Church#AUTHOR_TAG ) and a notion of cognateness ( Simard [ 16 ] ) .', '']",5
"['\x80¢ Before indexing the text , we process it with Textract( Byrd and Ravin , 1998 ; #AUTHOR_TAG ) , which performs lemmatization , and discovers proper names and technical terms .', '']","['\x80¢ Before indexing the text , we process it with Textract( Byrd and Ravin , 1998 ; #AUTHOR_TAG ) , which performs lemmatization , and discovers proper names and technical terms .', '']","['\x80¢ Before indexing the text , we process it with Textract( Byrd and Ravin , 1998 ; #AUTHOR_TAG ) , which performs lemmatization , and discovers proper names and technical terms .', '']","['\x80¢ Before indexing the text , we process it with Textract( Byrd and Ravin , 1998 ; #AUTHOR_TAG ) , which performs lemmatization , and discovers proper names and technical terms .', '']",5
"['phAna: Morphological Analysis provided by sines yields the word stems of nouns, verbs and adjectives, as well as the full forms of unknown words.', 'We are using a lexicon of approx.', '100000 word stems of German( #AUTHOR_TAG )']","['phAna: Morphological Analysis provided by sines yields the word stems of nouns, verbs and adjectives, as well as the full forms of unknown words.', 'We are using a lexicon of approx.', '100000 word stems of German( #AUTHOR_TAG )']","['phAna: Morphological Analysis provided by sines yields the word stems of nouns, verbs and adjectives, as well as the full forms of unknown words.', 'We are using a lexicon of approx.', '100000 word stems of German( #AUTHOR_TAG )']","['phAna: Morphological Analysis provided by sines yields the word stems of nouns, verbs and adjectives, as well as the full forms of unknown words.', 'We are using a lexicon of approx.', '100000 word stems of German( #AUTHOR_TAG )']",5
"['inguistic preprocessing of text documents is carried out by re-using smes , an information extraction core system for real-world German text processing( #AUTHOR_TAG ) .', 'The fundamental design criterion of sines is to provide a set of basic']","['inguistic preprocessing of text documents is carried out by re-using smes , an information extraction core system for real-world German text processing( #AUTHOR_TAG ) .', 'The fundamental design criterion of sines is to provide a set of basic,']","['inguistic preprocessing of text documents is carried out by re-using smes , an information extraction core system for real-world German text processing( #AUTHOR_TAG ) .', 'The fundamental design criterion of sines is to provide a set of basic']","['inguistic preprocessing of text documents is carried out by re-using smes , an information extraction core system for real-world German text processing( #AUTHOR_TAG ) .', '']",5
"['NP-based QA System.', 'Our implementation of the NP-based QA system uses the Empire noun phrase finder , which is described in detail in#AUTHOR_TAG .', 'Empire identifies base NPs --non-recursive noun phrases --using a very simple algorithm that matches part-of-speech tag sequences based on a learned noun phrase grammar.', 'The approach is able to achieve 94% precision and recall for base NPs derived from the Penn Treebank Wall Street Journal(Marcus et al., 1993) .', '']","['NP-based QA System.', 'Our implementation of the NP-based QA system uses the Empire noun phrase finder , which is described in detail in#AUTHOR_TAG .', 'Empire identifies base NPs --non-recursive noun phrases --using a very simple algorithm that matches part-of-speech tag sequences based on a learned noun phrase grammar.', 'The approach is able to achieve 94% precision and recall for base NPs derived from the Penn Treebank Wall Street Journal(Marcus et al., 1993) .', '']","['NP-based QA System.', 'Our implementation of the NP-based QA system uses the Empire noun phrase finder , which is described in detail in#AUTHOR_TAG .', 'Empire identifies base NPs --non-recursive noun phrases --using a very simple algorithm that matches part-of-speech tag sequences based on a learned noun phrase grammar.', 'The approach is able to achieve 94% precision and recall for base NPs derived from the Penn Treebank Wall Street Journal(Marcus et al., 1993) .', '']","['NP-based QA System.', 'Our implementation of the NP-based QA system uses the Empire noun phrase finder , which is described in detail in#AUTHOR_TAG .', 'Empire identifies base NPs --non-recursive noun phrases --using a very simple algorithm that matches part-of-speech tag sequences based on a learned noun phrase grammar.', 'The approach is able to achieve 94% precision and recall for base NPs derived from the Penn Treebank Wall Street Journal(Marcus et al., 1993) .', '']",5
"['', 'Although a number of methods for query-dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings( #AUTHOR_TAG ) , we again propose the use of vector space methods from IR , which can be easily extended to the summarization task( Salton et al. , 1994 )']","['', 'Although a number of methods for query-dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings( #AUTHOR_TAG ) , we again propose the use of vector space methods from IR , which can be easily extended to the summarization task( Salton et al. , 1994 )']","['', 'Although a number of methods for query-dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings( #AUTHOR_TAG ) , we again propose the use of vector space methods from IR , which can be easily extended to the summarization task( Salton et al. , 1994 )']","['', 'Although a number of methods for query-dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings( #AUTHOR_TAG ) , we again propose the use of vector space methods from IR , which can be easily extended to the summarization task( Salton et al. , 1994 )']",1
"['', 'The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in( #AUTHOR_TAG ) .', '']","['', 'The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in( #AUTHOR_TAG ) .', '']","['', 'The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in( #AUTHOR_TAG ) .', '']","['aim of this paper is to give a detailed account of the techniques used in TnT.', 'Additionally, we present results of the tagger on the NEGRA corpus(Brants et al., 1999) and the Penn Treebank(Marcus et al., 1993) .', 'The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in( #AUTHOR_TAG ) .', '']",1
"['', 'According to current tagger comparisons ( vanHalteren et al. , 1998 ; Zavrel and Daelemans , 1999 ) , and according to a comparsion of the results presented here with those in( #AUTHOR_TAG ) , the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here .', 'It is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both']","['', 'According to current tagger comparisons ( vanHalteren et al. , 1998 ; Zavrel and Daelemans , 1999 ) , and according to a comparsion of the results presented here with those in( #AUTHOR_TAG ) , the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here .', 'It is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both']","['', 'According to current tagger comparisons ( vanHalteren et al. , 1998 ; Zavrel and Daelemans , 1999 ) , and according to a comparsion of the results presented here with those in( #AUTHOR_TAG ) , the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here .', 'It is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both']","['', 'According to current tagger comparisons ( vanHalteren et al. , 1998 ; Zavrel and Daelemans , 1999 ) , and according to a comparsion of the results presented here with those in( #AUTHOR_TAG ) , the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here .', 'It is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both']",1
"['', ""This confirms that although Kozima 's approach( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation""]","['', ""This confirms that although Kozima 's approach( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation""]","['', ""This confirms that although Kozima 's approach( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation""]","['', ""This confirms that although Kozima 's approach( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation""]",1
"[""98 ( , , , , â\x80\x9e ) uses a variant of Kozima 's semantic similarity measure( #AUTHOR_TAG ) to compute block similarity ."", 'Word similarity']","[""98 ( , , , , â\x80\x9e ) uses a variant of Kozima 's semantic similarity measure( #AUTHOR_TAG ) to compute block similarity ."", 'Word similarity']","[""98 ( , , , , â\x80\x9e ) uses a variant of Kozima 's semantic similarity measure( #AUTHOR_TAG ) to compute block similarity ."", 'Word similarity']","[""98 ( , , , , â\x80\x9e ) uses a variant of Kozima 's semantic similarity measure( #AUTHOR_TAG ) to compute block similarity ."", 'Word similarity is a function of word co- occurrence statistics in the given document.', '']",2
"['related work,Miyao (1999) describes an ap- proach to packing in which alternative feature structures are represented as packed, distributed disjunctions of feature structure fragments.', 'Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of#AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .', '']","['related work,Miyao (1999) describes an ap- proach to packing in which alternative feature structures are represented as packed, distributed disjunctions of feature structure fragments.', 'Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of#AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .', '']","['related work,Miyao (1999) describes an ap- proach to packing in which alternative feature structures are represented as packed, distributed disjunctions of feature structure fragments.', 'Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of#AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .', '']","['related work,Miyao (1999) describes an ap- proach to packing in which alternative feature structures are represented as packed, distributed disjunctions of feature structure fragments.', 'Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of#AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .', '']",1
"['', 'We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in( #AUTHOR_TAG ) and( Chelba and Jelinek , 1998 ) .', 'We leave this for future work']","['', 'We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in( #AUTHOR_TAG ) and( Chelba and Jelinek , 1998 ) .', 'We leave this for future work']","['see Section 1).', 'We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in( #AUTHOR_TAG ) and( Chelba and Jelinek , 1998 ) .', 'We leave this for future work']","['', 'We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in( #AUTHOR_TAG ) and( Chelba and Jelinek , 1998 ) .', 'We leave this for future work']",3
"['', 'We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in( Ratnaparkhi , 1997 ) and( #AUTHOR_TAG ) .', 'We leave this for future work']","['', 'We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in( Ratnaparkhi , 1997 ) and( #AUTHOR_TAG ) .', 'We leave this for future work']","['see Section 1).', 'We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in( Ratnaparkhi , 1997 ) and( #AUTHOR_TAG ) .', 'We leave this for future work']","['', 'We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in( Ratnaparkhi , 1997 ) and( #AUTHOR_TAG ) .', 'We leave this for future work']",3
"['uracy is not the best measure to assess segmentation quality , therefore we also conducted experiments using the WindowDiff measure as proposed by#AUTHOR_TAG .', 'WindowDiff returns 0 in case of a perfect segmentation; 1 is the worst possible score.', 'However, it only takes into account segment boundaries and disregards segment types.', '']","['uracy is not the best measure to assess segmentation quality , therefore we also conducted experiments using the WindowDiff measure as proposed by#AUTHOR_TAG .', 'WindowDiff returns 0 in case of a perfect segmentation; 1 is the worst possible score.', 'However, it only takes into account segment boundaries and disregards segment types.', '']","['uracy is not the best measure to assess segmentation quality , therefore we also conducted experiments using the WindowDiff measure as proposed by#AUTHOR_TAG .', 'WindowDiff returns 0 in case of a perfect segmentation; 1 is the worst possible score.', 'However, it only takes into account segment boundaries and disregards segment types.', '']","['uracy is not the best measure to assess segmentation quality , therefore we also conducted experiments using the WindowDiff measure as proposed by#AUTHOR_TAG .', 'WindowDiff returns 0 in case of a perfect segmentation; 1 is the worst possible score.', 'However, it only takes into account segment boundaries and disregards segment types.', '']",5
"['some relations, there is no contradiction when y 1 and y 2 share a meronym, i.e. ""part of"" relation.', 'For example, in the set born in(Mozart,•) there is no contradiction between the y values ""Salzburg"" and ""Austria"", but ""Salzburg"" conflicts with ""Vienna"".', 'Although this is only true in cases where y occurs in an upward monotone context( #AUTHOR_TAG ) , in practice genuine contradictions between y-values sharing a meronym relationship are extremely rare']","['some relations, there is no contradiction when y 1 and y 2 share a meronym, i.e. ""part of"" relation.', 'For example, in the set born in(Mozart,•) there is no contradiction between the y values ""Salzburg"" and ""Austria"", but ""Salzburg"" conflicts with ""Vienna"".', 'Although this is only true in cases where y occurs in an upward monotone context( #AUTHOR_TAG ) , in practice genuine contradictions between y-values sharing a meronym relationship are extremely rare']","['some relations, there is no contradiction when y 1 and y 2 share a meronym, i.e. ""part of"" relation.', 'For example, in the set born in(Mozart,•) there is no contradiction between the y values ""Salzburg"" and ""Austria"", but ""Salzburg"" conflicts with ""Vienna"".', 'Although this is only true in cases where y occurs in an upward monotone context( #AUTHOR_TAG ) , in practice genuine contradictions between y-values sharing a meronym relationship are extremely rare .', '']","['onyms: For some relations, there is no contradiction when y 1 and y 2 share a meronym, i.e. ""part of"" relation.', 'For example, in the set born in(Mozart,•) there is no contradiction between the y values ""Salzburg"" and ""Austria"", but ""Salzburg"" conflicts with ""Vienna"".', 'Although this is only true in cases where y occurs in an upward monotone context( #AUTHOR_TAG ) , in practice genuine contradictions between y-values sharing a meronym relationship are extremely rare .', 'We therefore simply assigned contradictions between meronyms a probability close to zero.', '']",4
"['4', 'We parse each sentence with the Collins parser( #AUTHOR_TAG ) .', 'Then the document has one big parse tree,']","['', 'We parse each sentence with the Collins parser( #AUTHOR_TAG ) .', 'Then the document has one big parse tree,']","['', 'We parse each sentence with the Collins parser( #AUTHOR_TAG ) .', 'Then the document has one big parse tree,']","['', 'We parse each sentence with the Collins parser( #AUTHOR_TAG ) .', '']",5
"['', 'It is based on the dataset of#AUTHOR_TAG ,9 which consists of 1000 positive and 1000 negative movie reviews , tokenized and divided into 10 folds ( F0 -- F9 ) .', '']","[', we introduced the �Movie Review Polarity Dataset Enriched with Annotator Rationales.�8', 'It is based on the dataset of#AUTHOR_TAG ,9 which consists of 1000 positive and 1000 negative movie reviews , tokenized and divided into 10 folds ( F0 -- F9 ) .', '']","[', we introduced the �Movie Review Polarity Dataset Enriched with Annotator Rationales.�8', 'It is based on the dataset of#AUTHOR_TAG ,9 which consists of 1000 positive and 1000 negative movie reviews , tokenized and divided into 10 folds ( F0 -- F9 ) .', '']","[', we introduced the �Movie Review Polarity Dataset Enriched with Annotator Rationales.�8', 'It is based on the dataset of#AUTHOR_TAG ,9 which consists of 1000 positive and 1000 negative movie reviews , tokenized and divided into 10 folds ( F0 -- F9 ) .', 'All our experiments use F9 as their final blind test set']",2
"['', 'We collect substring rationales for a sentiment classification task( #AUTHOR_TAG ) and use them to obtain significant accuracy improvements for each annotator']","['', 'We collect substring rationales for a sentiment classification task( #AUTHOR_TAG ) and use them to obtain significant accuracy improvements for each annotator']","['', 'We collect substring rationales for a sentiment classification task( #AUTHOR_TAG ) and use them to obtain significant accuracy improvements for each annotator .', '']","['', 'We collect substring rationales for a sentiment classification task( #AUTHOR_TAG ) and use them to obtain significant accuracy improvements for each annotator .', '']",5
"['', 'We use the same set of binary features as in previous work on this dataset( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .', '']","['is a normalizer.', 'We use the same set of binary features as in previous work on this dataset( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .', '']","['is a normalizer.', 'We use the same set of binary features as in previous work on this dataset( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .', '']","['f (•) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and Z θ (x) def = y u(x, y) is a normalizer.', 'We use the same set of binary features as in previous work on this dataset( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .', 'Specifically, let V = {v 1 , ..., v 17744 } be the set of word types with count ≥ 4 in the full 2000-document corpus.', '']",5
"['', 'We gather similar words using#AUTHOR_TAG a ) , mining similar verbs from a comparable-sized parsed corpus , and collecting similar nouns from a broader 10 GB corpus of English text']","['', 'We gather similar words using#AUTHOR_TAG a ) , mining similar verbs from a comparable-sized parsed corpus , and collecting similar nouns from a broader 10 GB corpus of English text']","['', 'We gather similar words using#AUTHOR_TAG a ) , mining similar verbs from a comparable-sized parsed corpus , and collecting similar nouns from a broader 10 GB corpus of English text .4']","['', ""We gather similar words using#AUTHOR_TAG a ) , mining similar verbs from a comparable-sized parsed corpus , and collecting similar nouns from a broader 10 GB corpus of English text .4 We also useKeller and Lapata ( 2003 ) 's approach to obtaining web-counts""]",5
"['also made use of the person-name/instance pairs automatically extracted by#AUTHOR_TAG .2This data provides counts for pairs such as ""Edwin Moses , hurdler"" and ""William Farley , industrialist.', 'We have features for all concepts and therefore learn their association with each verb']","['also made use of the person-name/instance pairs automatically extracted by#AUTHOR_TAG .2This data provides counts for pairs such as ""Edwin Moses , hurdler"" and ""William Farley , industrialist.', 'We have features for all concepts and therefore learn their association with each verb']","['also made use of the person-name/instance pairs automatically extracted by#AUTHOR_TAG .2This data provides counts for pairs such as ""Edwin Moses , hurdler"" and ""William Farley , industrialist.', 'We have features for all concepts and therefore learn their association with each verb']","['also made use of the person-name/instance pairs automatically extracted by#AUTHOR_TAG .2This data provides counts for pairs such as ""Edwin Moses , hurdler"" and ""William Farley , industrialist.', 'We have features for all concepts and therefore learn their association with each verb']",5
"['', ""We study the cases where a 9Recall that even the#AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) ."", '']","['', ""We study the cases where a 9Recall that even the#AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) ."", '']","['', ""We study the cases where a 9Recall that even the#AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) ."", '']","[', we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text(Dagan and Itai, 1990;Kehler et al., 2004) .', ""We study the cases where a 9Recall that even the#AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) ."", '']",1
"['', 'The DSP parameters for eat, for example, place high weight on features like Pr(n|braise), Pr(n|ration), and Pr(n|garnish).', ""#AUTHOR_TAG a ) 's similar word list for eat misses these but includes sleep ( ranked 6 ) and sit ( ranked 14 ) , because these have similar subjects to eat ."", '']","['', 'The DSP parameters for eat, for example, place high weight on features like Pr(n|braise), Pr(n|ration), and Pr(n|garnish).', ""#AUTHOR_TAG a ) 's similar word list for eat misses these but includes sleep ( ranked 6 ) and sit ( ranked 14 ) , because these have similar subjects to eat ."", '']","['', 'The DSP parameters for eat, for example, place high weight on features like Pr(n|braise), Pr(n|ration), and Pr(n|garnish).', ""#AUTHOR_TAG a ) 's similar word list for eat misses these but includes sleep ( ranked 6 ) and sit ( ranked 14 ) , because these have similar subjects to eat ."", '']","['', 'The DSP parameters for eat, for example, place high weight on features like Pr(n|braise), Pr(n|ration), and Pr(n|garnish).', ""#AUTHOR_TAG a ) 's similar word list for eat misses these but includes sleep ( ranked 6 ) and sit ( ranked 14 ) , because these have similar subjects to eat ."", '']",0
"['each example.', 'The similarity-smoothed examples will be undefined if SIMS(w) is empty.', 'Also , the#AUTHOR_TAG approach will be undefined if the pair is unobserved on the web .', 'As a reasonable default for these cases, we assign them a negative decision']","['each example.', 'The similarity-smoothed examples will be undefined if SIMS(w) is empty.', 'Also , the#AUTHOR_TAG approach will be undefined if the pair is unobserved on the web .', 'As a reasonable default for these cases, we assign them a negative decision']","['each example.', 'The similarity-smoothed examples will be undefined if SIMS(w) is empty.', 'Also , the#AUTHOR_TAG approach will be undefined if the pair is unobserved on the web .', 'As a reasonable default for these cases, we assign them a negative decision']","['', 'not be able to provide a score for each example.', 'The similarity-smoothed examples will be undefined if SIMS(w) is empty.', 'Also , the#AUTHOR_TAG approach will be undefined if the pair is unobserved on the web .', 'As a reasonable default for these cases, we assign them a negative decision']",5
"['training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) .', 'This data consists of triples (v, n, n ′ ) where v, n is a predicateargument pair observed in the corpus and v, n ′ has not been observed.', 'The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones.', '']","['training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) .', 'This data consists of triples (v, n, n ′ ) where v, n is a predicateargument pair observed in the corpus and v, n ′ has not been observed.', 'The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones.', '']","['training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) .', 'This data consists of triples (v, n, n ′ ) where v, n is a predicateargument pair observed in the corpus and v, n ′ has not been observed.', 'The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones.', 'We refer to this as Pair']","['training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) .', 'This data consists of triples (v, n, n ′ ) where v, n is a predicateargument pair observed in the corpus and v, n ′ has not been observed.', 'The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones.', 'We refer to this as Pairwise Disambiguation.', '']",1
"['advantage of tuning similarity to the application of interest has been shown previously by#AUTHOR_TAG .', 'They optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation.', '']","['advantage of tuning similarity to the application of interest has been shown previously by#AUTHOR_TAG .', 'They optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation.', '']","['advantage of tuning similarity to the application of interest has been shown previously by#AUTHOR_TAG .', 'They optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation.', '']","['advantage of tuning similarity to the application of interest has been shown previously by#AUTHOR_TAG .', 'They optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation.', '']",1
"[""MI is proportional toKeller and Lapata (2003) 's conditional probability scores for pseudodisambiguation of (v, n, n ′ ) triples: Pr(v|n) = Pr(v, n)/Pr(n), which was shown to be a better measure of association than co-occurrence frequency f (v, n)."", 'Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs.', 'MI was also recently used for inference-rule SPs by#AUTHOR_TAG']","[""MI is proportional toKeller and Lapata (2003) 's conditional probability scores for pseudodisambiguation of (v, n, n ′ ) triples: Pr(v|n) = Pr(v, n)/Pr(n), which was shown to be a better measure of association than co-occurrence frequency f (v, n)."", 'Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs.', 'MI was also recently used for inference-rule SPs by#AUTHOR_TAG']","[""is proportional toKeller and Lapata (2003) 's conditional probability scores for pseudodisambiguation of (v, n, n ′ ) triples: Pr(v|n) = Pr(v, n)/Pr(n), which was shown to be a better measure of association than co-occurrence frequency f (v, n)."", 'Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs.', 'MI was also recently used for inference-rule SPs by#AUTHOR_TAG']","['', ""Thus rather than a single training procedure, we can actually partition the examples by predicate, and train a For a fixed verb, MI is proportional toKeller and Lapata (2003) 's conditional probability scores for pseudodisambiguation of (v, n, n ′ ) triples: Pr(v|n) = Pr(v, n)/Pr(n), which was shown to be a better measure of association than co-occurrence frequency f (v, n)."", 'Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs.', 'MI was also recently used for inference-rule SPs by#AUTHOR_TAG']",0
"['training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) .', 'This data consists of triples (v, n, n ′ ) where v, n is a predicateargument pair observed in the corpus and v, n ′ has not been observed.', 'The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones.', '']","['training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) .', 'This data consists of triples (v, n, n ′ ) where v, n is a predicateargument pair observed in the corpus and v, n ′ has not been observed.', 'The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones.', '']","['training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) .', 'This data consists of triples (v, n, n ′ ) where v, n is a predicateargument pair observed in the corpus and v, n ′ has not been observed.', 'The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones.', 'We refer to this as Pair']","['training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) .', 'This data consists of triples (v, n, n ′ ) where v, n is a predicateargument pair observed in the corpus and v, n ′ has not been observed.', 'The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones.', 'We refer to this as Pairwise Disambiguation.', '']",1
"['that the entire class is suitable for eating.', 'Usually , the classes are from WordNet( Miller et al. , 1990 ) , although they can also be inferred from clustering( #AUTHOR_TAG ) .', '']","['that the entire class is suitable for eating.', 'Usually , the classes are from WordNet( Miller et al. , 1990 ) , although they can also be inferred from clustering( #AUTHOR_TAG ) .', '']","['that the entire class is suitable for eating.', 'Usually , the classes are from WordNet( Miller et al. , 1990 ) , although they can also be inferred from clustering( #AUTHOR_TAG ) .', '']","['approaches to SPs generalize from observed predicate-argument pairs to semantically similar ones by modeling the semantic class of the argument, followingResnik (1996) .', 'For example, we might have a class Mexican Food and learn that the entire class is suitable for eating.', 'Usually , the classes are from WordNet( Miller et al. , 1990 ) , although they can also be inferred from clustering( #AUTHOR_TAG ) .', '']",0
"['umerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) .', 'Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', '']","['umerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) .', 'Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', '']","['umerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) .', 'Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', '']","['umerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) .', 'Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', '']",1
"['al Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules( #AUTHOR_TAG ; Roberto et al. , 2007 ) .', 'Inferences such as ""[X wins Y] ⇒ [X plays Y]"" are only valid for certain argu-ments X and Y.', '']","['al Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules( #AUTHOR_TAG ; Roberto et al. , 2007 ) .', 'Inferences such as ""[X wins Y] ⇒ [X plays Y]"" are only valid for certain argu-ments X and Y.', '']","['al Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules( #AUTHOR_TAG ; Roberto et al. , 2007 ) .', 'Inferences such as ""[X wins Y] ⇒ [X plays Y]"" are only valid for certain argu-ments X and Y.', '']","['al Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules( #AUTHOR_TAG ; Roberto et al. , 2007 ) .', 'Inferences such as ""[X wins Y] ⇒ [X plays Y]"" are only valid for certain argu-ments X and Y.', '']",0
"['', ""Erk ( 2007 ) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and#AUTHOR_TAG a ) 's information-theoretic metric work best ."", '']","['', ""Erk ( 2007 ) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and#AUTHOR_TAG a ) 's information-theoretic metric work best ."", '']","['', ""Erk ( 2007 ) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and#AUTHOR_TAG a ) 's information-theoretic metric work best ."", '']","['', ""Erk ( 2007 ) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and#AUTHOR_TAG a ) 's information-theoretic metric work best ."", '']",0
"['parsed the 3 GB AQUAINT corpus( Voorhees , 2002 ) using Minipar( #AUTHOR_TAG b ) , and collected verb-object and verb-subject frequencies , building an empirical MI model from this data']","['parsed the 3 GB AQUAINT corpus( Voorhees , 2002 ) using Minipar( #AUTHOR_TAG b ) , and collected verb-object and verb-subject frequencies , building an empirical MI model from this data']","['parsed the 3 GB AQUAINT corpus( Voorhees , 2002 ) using Minipar( #AUTHOR_TAG b ) , and collected verb-object and verb-subject frequencies , building an empirical MI model from this data .', '']","['parsed the 3 GB AQUAINT corpus( Voorhees , 2002 ) using Minipar( #AUTHOR_TAG b ) , and collected verb-object and verb-subject frequencies , building an empirical MI model from this data .', '']",5
"['umerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) .', 'Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', '']","['umerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) .', 'Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', '']","['umerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) .', 'Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', '']","['umerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) .', 'Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', '']",1
"['then extract the predicate-argument pairs that have a statistical association in this data.', 'We measure this association using pointwise Mutual Information ( MI )( #AUTHOR_TAG ) .', 'The MI between a verb predicate, v, and its object argument, n, is']","['then extract the predicate-argument pairs that have a statistical association in this data.', 'We measure this association using pointwise Mutual Information ( MI )( #AUTHOR_TAG ) .', 'The MI between a verb predicate, v, and its object argument, n, is']","['then extract the predicate-argument pairs that have a statistical association in this data.', 'We measure this association using pointwise Mutual Information ( MI )( #AUTHOR_TAG ) .', 'The MI between a verb predicate, v, and its object argument, n, is']","['', 'To create the positives, we automatically parse a large corpus, and then extract the predicate-argument pairs that have a statistical association in this data.', 'We measure this association using pointwise Mutual Information ( MI )( #AUTHOR_TAG ) .', 'The MI between a verb predicate, v, and its object argument, n, is']",5
"[""For a fixed verb , MI is proportional to#AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n â\x80² ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) ."", 'Normalizing by Pr(v)']","[""a fixed verb , MI is proportional to#AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n â\x80² ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) ."", 'Normalizing by Pr(v) (yielding MI)']","[""a 1For a fixed verb , MI is proportional to#AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n â\x80² ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) ."", 'Normalizing by Pr(v)']","['', ""Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to#AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n â\x80² ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) ."", 'Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs.', '']",4
"['OLMES is given the following set of six domainindependent rules , which are similar to the upward monotone rules introduced by( #AUTHOR_TAG )']","['OLMES is given the following set of six domainindependent rules , which are similar to the upward monotone rules introduced by( #AUTHOR_TAG )']","['OLMES is given the following set of six domainindependent rules , which are similar to the upward monotone rules introduced by( #AUTHOR_TAG )']","['OLMES is given the following set of six domainindependent rules , which are similar to the upward monotone rules introduced by( #AUTHOR_TAG )']",1
"['', ""While many approaches have addressed this problem , our work is most closely related to that of( Raina et al. , 2005 ; #AUTHOR_TAG ; Tatu and Moldovan , 2006 ; Braz et al. , 2005 ) , which convert the inputs into logical forms and then attempt to ` prove ' H from T plus a set of axioms ."", '']","['', ""While many approaches have addressed this problem , our work is most closely related to that of( Raina et al. , 2005 ; #AUTHOR_TAG ; Tatu and Moldovan , 2006 ; Braz et al. , 2005 ) , which convert the inputs into logical forms and then attempt to ` prove ' H from T plus a set of axioms ."", '']","['', ""While many approaches have addressed this problem , our work is most closely related to that of( Raina et al. , 2005 ; #AUTHOR_TAG ; Tatu and Moldovan , 2006 ; Braz et al. , 2005 ) , which convert the inputs into logical forms and then attempt to ` prove ' H from T plus a set of axioms ."", '']","['ual Entailment systems are given two textual fragments, text T and hypothesis H, and attempt to decide if the meaning of H can be inferred from the meaning of T(Dagan et al., 2005) .', ""While many approaches have addressed this problem , our work is most closely related to that of( Raina et al. , 2005 ; #AUTHOR_TAG ; Tatu and Moldovan , 2006 ; Braz et al. , 2005 ) , which convert the inputs into logical forms and then attempt to ` prove ' H from T plus a set of axioms ."", '']",1
"['studies presented by#AUTHOR_TAG andJohnson ( 2007 ) differed in the number of states that they used .', 'evaluated against the reduced tag set of 17 tags developed bySmith and Eisner (2005) , whileJohnson (2007) evaluated against the full Penn Treebank tag set.', '']","['studies presented by#AUTHOR_TAG andJohnson ( 2007 ) differed in the number of states that they used .', 'evaluated against the reduced tag set of 17 tags developed bySmith and Eisner (2005) , whileJohnson (2007) evaluated against the full Penn Treebank tag set.', '']","['studies presented by#AUTHOR_TAG andJohnson ( 2007 ) differed in the number of states that they used .', 'evaluated against the reduced tag set of 17 tags developed bySmith and Eisner (2005) , whileJohnson (2007) evaluated against the full Penn Treebank tag set.', 'We ran all our estimators in']","['studies presented by#AUTHOR_TAG andJohnson ( 2007 ) differed in the number of states that they used .', 'evaluated against the reduced tag set of 17 tags developed bySmith and Eisner (2005) , whileJohnson (2007) evaluated against the full Penn Treebank tag set.', '']",1
"['might be expected, our evaluation measures disagree somewhat, but the following broad tendancies seem clear.', 'On small data sets all of the Bayesian estimators strongly outperform EM ( and , to a lesser extent , VB ) with respect to all of our evaluation measures , confirming the results reported in#AUTHOR_TAG .', 'This is perhaps']","['might be expected, our evaluation measures disagree somewhat, but the following broad tendancies seem clear.', 'On small data sets all of the Bayesian estimators strongly outperform EM ( and , to a lesser extent , VB ) with respect to all of our evaluation measures , confirming the results reported in#AUTHOR_TAG .', 'This is perhaps']","['might be expected, our evaluation measures disagree somewhat, but the following broad tendancies seem clear.', 'On small data sets all of the Bayesian estimators strongly outperform EM ( and , to a lesser extent , VB ) with respect to all of our evaluation measures , confirming the results reported in#AUTHOR_TAG .', 'This is perhaps not too']","['might be expected, our evaluation measures disagree somewhat, but the following broad tendancies seem clear.', 'On small data sets all of the Bayesian estimators strongly outperform EM ( and , to a lesser extent , VB ) with respect to all of our evaluation measures , confirming the results reported in#AUTHOR_TAG .', '']",1
"['resulting training procedure is analogous to the one presented in( Brown et al. , 1993 ) and( #AUTHOR_TAG )']","['resulting training procedure is analogous to the one presented in( Brown et al. , 1993 ) and( #AUTHOR_TAG )']","['resulting training procedure is analogous to the one presented in( Brown et al. , 1993 ) and( #AUTHOR_TAG )']","['resulting training procedure is analogous to the one presented in( Brown et al. , 1993 ) and( #AUTHOR_TAG )']",1
"['m and n are the number of nodes of the two trees ( m > = n )( #AUTHOR_TAG ) .', '']","['m and n are the number of nodes of the two trees ( m > = n )( #AUTHOR_TAG ) .', '']","['m and n are the number of nodes of the two trees ( m > = n )( #AUTHOR_TAG ) .', '']",['( #AUTHOR_TAG )'],3
"['', 'Secondly , as( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .', '']","['', 'Secondly , as( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .', '']","['', 'Secondly , as( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .', '']","['', 'Firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior.', 'Secondly , as( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .', '']",3
"['good study comparing document categorization algorithms can be found in(Yang and Liu, 1999) .', 'More recently ,( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in( Joachims , 2002 ) ,( #AUTHOR_TAG ) , and( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in(Yang and Liu, 1999) .', 'More recently ,( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in( Joachims , 2002 ) ,( #AUTHOR_TAG ) , and( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in(Yang and Liu, 1999) .', 'More recently ,( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in( Joachims , 2002 ) ,( #AUTHOR_TAG ) , and( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in(Yang and Liu, 1999) .', 'More recently ,( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in( Joachims , 2002 ) ,( #AUTHOR_TAG ) , and( Lewis et al. , 2004 )']",0
"['', 'Based on this advise ( Moore and#AUTHOR_TAG ) exclude the latent segmentation variables and opt for a heuristic training procedure']","['', 'Based on this advise ( Moore and#AUTHOR_TAG ) exclude the latent segmentation variables and opt for a heuristic training procedure']","['', 'Based on this advise ( Moore and#AUTHOR_TAG ) exclude the latent segmentation variables and opt for a heuristic training procedure .', '']","['', 'Based on this advise ( Moore and#AUTHOR_TAG ) exclude the latent segmentation variables and opt for a heuristic training procedure .', '']",1
['variables we wish to consider are an increased number of word classes ; more flexible regions -- see#AUTHOR_TAG on learning a state transition diagram for acoustic regions in phone recognition -- and phonological features and syllable boundaries'],['variables we wish to consider are an increased number of word classes ; more flexible regions -- see#AUTHOR_TAG on learning a state transition diagram for acoustic regions in phone recognition -- and phonological features and syllable boundaries'],"['ent variables we wish to consider are an increased number of word classes ; more flexible regions -- see#AUTHOR_TAG on learning a state transition diagram for acoustic regions in phone recognition -- and phonological features and syllable boundaries .', '']","['', 'Latent variables we wish to consider are an increased number of word classes ; more flexible regions -- see#AUTHOR_TAG on learning a state transition diagram for acoustic regions in phone recognition -- and phonological features and syllable boundaries .', '']",0
"['future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and string-transduction tasks.', 'We would like to use features that look at wide context on the input side , which is inexpensive( #AUTHOR_TAG ) .', '']","['future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and string-transduction tasks.', 'We would like to use features that look at wide context on the input side , which is inexpensive( #AUTHOR_TAG ) .', '']","['future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and string-transduction tasks.', 'We would like to use features that look at wide context on the input side , which is inexpensive( #AUTHOR_TAG ) .', '']","['future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and string-transduction tasks.', 'We would like to use features that look at wide context on the input side , which is inexpensive( #AUTHOR_TAG ) .', '']",3
"['', '#AUTHOR_TAG for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples']","['', '#AUTHOR_TAG for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples']","['to improve the similarity estimates.', '#AUTHOR_TAG for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples']","['', '#AUTHOR_TAG for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples']",0
"['this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. ,#AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications']","['this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. ,#AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications']","['this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. ,#AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications']","['this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. ,#AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications']",0
"['already mentioned in the literature , see for example( #AUTHOR_TAG ) , knowledge about implicit predicates could be potentially useful for a variety of NLP tasks such as language generation , information extraction , question answering or machine translation .', 'Many applications of semantic relations in NLP are connected to parap']","['already mentioned in the literature , see for example( #AUTHOR_TAG ) , knowledge about implicit predicates could be potentially useful for a variety of NLP tasks such as language generation , information extraction , question answering or machine translation .', 'Many applications of semantic relations in NLP are connected to paraphrasing']","['already mentioned in the literature , see for example( #AUTHOR_TAG ) , knowledge about implicit predicates could be potentially useful for a variety of NLP tasks such as language generation , information extraction , question answering or machine translation .', 'Many applications of semantic relations in NLP are connected to parap']","['already mentioned in the literature , see for example( #AUTHOR_TAG ) , knowledge about implicit predicates could be potentially useful for a variety of NLP tasks such as language generation , information extraction , question answering or machine translation .', 'Many applications of semantic relations in NLP are connected to paraphrasing or query expansion, see for example(Voorhees, 1994) .', '']",0
"['', 'Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set( #AUTHOR_TAG ) .', '']","['', 'Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set( #AUTHOR_TAG ) .', '']","['being a single generative PCFG.', 'Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set( #AUTHOR_TAG ) .', '']","['', 'Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set( #AUTHOR_TAG ) .', '']",1
"['', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches( #AUTHOR_TAG ) and products of latent variable grammars( Petrov , 2010 ) , despite being a single generative PCFG .', '']","['', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches( #AUTHOR_TAG ) and products of latent variable grammars( Petrov , 2010 ) , despite being a single generative PCFG .', '']","['', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches( #AUTHOR_TAG ) and products of latent variable grammars( Petrov , 2010 ) , despite being a single generative PCFG .', '']","['', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches( #AUTHOR_TAG ) and products of latent variable grammars( Petrov , 2010 ) , despite being a single generative PCFG .', '']",1
"['', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches( Charniak and Johnson , 2005 ) and products of latent variable grammars( #AUTHOR_TAG ) , despite being a single generative PCFG .', 'Our most']","['', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches( Charniak and Johnson , 2005 ) and products of latent variable grammars( #AUTHOR_TAG ) , despite being a single generative PCFG .', 'Our most']","['', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches( Charniak and Johnson , 2005 ) and products of latent variable grammars( #AUTHOR_TAG ) , despite being a single generative PCFG .', 'Our most']","['', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches( Charniak and Johnson , 2005 ) and products of latent variable grammars( #AUTHOR_TAG ) , despite being a single generative PCFG .', '']",1
"[""possibly critical feature is the 'mention of names'."", ""In multi-party discussion people usually mention each other 's name for the purpose of disentanglement( #AUTHOR_TAG ) ."", ""In our corpus we found 175 instances where a participant mentions other participant's name."", 'In addition to these']","[""possibly critical feature is the 'mention of names'."", ""In multi-party discussion people usually mention each other 's name for the purpose of disentanglement( #AUTHOR_TAG ) ."", ""In our corpus we found 175 instances where a participant mentions other participant's name."", ""In addition to these, 'Subject of the""]","[""possibly critical feature is the 'mention of names'."", ""In multi-party discussion people usually mention each other 's name for the purpose of disentanglement( #AUTHOR_TAG ) ."", ""In our corpus we found 175 instances where a participant mentions other participant's name."", 'In addition to these']","[""possibly critical feature is the 'mention of names'."", ""In multi-party discussion people usually mention each other 's name for the purpose of disentanglement( #AUTHOR_TAG ) ."", ""In our corpus we found 175 instances where a participant mentions other participant's name."", ""In addition to these, 'Subject of the email', 'topic-shift cue words' can also be beneficial for a model."", '']",0
"['we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'We found that the oldest system( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .', '']","['we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'We found that the oldest system( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .', '']","['we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'We found that the oldest system( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .', '']","[', we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'We found that the oldest system( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .', '']",0
"['2', 'Future Work: Functionality is one of the several properties a relation can possess.', 'Others include selectional preferences , transitivity( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .', 'These properties are very useful in increasing our understanding about these Open IE relation strings.', 'We believe that the general principles developed in this work, for example, connecting the Open IE knowledge with an existing knowledge resource, will come in very handy in identifying these other properties']","['2', 'Future Work: Functionality is one of the several properties a relation can possess.', 'Others include selectional preferences , transitivity( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .', 'These properties are very useful in increasing our understanding about these Open IE relation strings.', 'We believe that the general principles developed in this work, for example, connecting the Open IE knowledge with an existing knowledge resource, will come in very handy in identifying these other properties']","['', 'Future Work: Functionality is one of the several properties a relation can possess.', 'Others include selectional preferences , transitivity( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .', 'These properties are very useful in increasing our understanding about these Open IE relation strings.', 'We believe that the general principles developed in this work, for example, connecting the Open IE knowledge with an existing knowledge resource, will come in very handy in identifying these other properties']","['', 'Future Work: Functionality is one of the several properties a relation can possess.', 'Others include selectional preferences , transitivity( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .', 'These properties are very useful in increasing our understanding about these Open IE relation strings.', 'We believe that the general principles developed in this work, for example, connecting the Open IE knowledge with an existing knowledge resource, will come in very handy in identifying these other properties']",0
"['isticated phrase table combination.', 'Finally , we experiment with a method for combining phrase tables proposed in( #AUTHOR_TAG ; Nakov and Ng , 2012 ) .', 'The first phrase table is extracted from word alignments for the balanced concatenation with repetitions, which are then truncated so that they are kept for only one']","['isticated phrase table combination.', 'Finally , we experiment with a method for combining phrase tables proposed in( #AUTHOR_TAG ; Nakov and Ng , 2012 ) .', 'The first phrase table is extracted from word alignments for the balanced concatenation with repetitions, which are then truncated so that they are kept for only one']","['isticated phrase table combination.', 'Finally , we experiment with a method for combining phrase tables proposed in( #AUTHOR_TAG ; Nakov and Ng , 2012 ) .', 'The first phrase table is extracted from word alignments for the balanced concatenation with repetitions, which are then truncated so that they are kept for only one copy of the Indonesian-English bi-text.', '']","['isticated phrase table combination.', 'Finally , we experiment with a method for combining phrase tables proposed in( #AUTHOR_TAG ; Nakov and Ng , 2012 ) .', 'The first phrase table is extracted from word alignments for the balanced concatenation with repetitions, which are then truncated so that they are kept for only one copy of the Indonesian-English bi-text.', '']",5
"['our previous work( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all']","['our previous work( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was']","['our previous work( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was']","['', 'For example , our previous work( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .', '']",1
"['', 'We found the same number using our previous approach( #AUTHOR_TAG ) , which is roughly equivalent to our core module .', 'Our approach']","['', 'We found the same number using our previous approach( #AUTHOR_TAG ) , which is roughly equivalent to our core module .', 'Our approach']","['', 'We found the same number using our previous approach( #AUTHOR_TAG ) , which is roughly equivalent to our core module .', 'Our approach']","['', 'We found the same number using our previous approach( #AUTHOR_TAG ) , which is roughly equivalent to our core module .', '']",2
"['take some core ideas from our previous work on mining script information( #AUTHOR_TAG ) .', '']","['take some core ideas from our previous work on mining script information( #AUTHOR_TAG ) .', '']","['take some core ideas from our previous work on mining script information( #AUTHOR_TAG ) .', '']","['take some core ideas from our previous work on mining script information( #AUTHOR_TAG ) .', '']",2
"['', 'Our own work( #AUTHOR_TAG ) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora .', '']","['', 'Our own work( #AUTHOR_TAG ) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora .', '']","['', 'Our own work( #AUTHOR_TAG ) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora .', '']","['', 'Our own work( #AUTHOR_TAG ) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora .', '']",2
"['ided with the candidate fragment elements , we previously( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .', '']","['ided with the candidate fragment elements , we previously( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .', '']","['ided with the candidate fragment elements , we previously( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .', '']","['ided with the candidate fragment elements , we previously( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .', 'We extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments (Sec.', '']",2
"['to( #AUTHOR_TAG a ) , our summarization system is , which consists of three key components : an initial sentence pre-selection module to select some important sentence candidates ; the above compression model to generate n-best compressions for each sentence ; and']","['to( #AUTHOR_TAG a ) , our summarization system is , which consists of three key components : an initial sentence pre-selection module to select some important sentence candidates ; the above compression model to generate n-best compressions for each sentence ; and']","['to( #AUTHOR_TAG a ) , our summarization system is , which consists of three key components : an initial sentence pre-selection module to select some important sentence candidates ; the above compression model to generate n-best compressions for each sentence ; and']","['to( #AUTHOR_TAG a ) , our summarization system is , which consists of three key components : an initial sentence pre-selection module to select some important sentence candidates ; the above compression model to generate n-best compressions for each sentence ; and then an ILP summarization method to select the best summary sentences from the multiple compressed sentences']",1
"['approach to the problem is more compatible with the empirical evidence we presented in our prior work( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .', 'Rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple English sentences.', 'This']","['approach to the problem is more compatible with the empirical evidence we presented in our prior work( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .', 'Rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple English sentences.', 'This']","['approach to the problem is more compatible with the empirical evidence we presented in our prior work( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .', 'Rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple English sentences.', '']","['approach to the problem is more compatible with the empirical evidence we presented in our prior work( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .', 'Rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple English sentences.', '']",1
"['the tectogrammatical parsing of Czech, the analytical tree structure is converted into the tectogrammatical one.', 'These automatic transformations are based on linguistic rules( #AUTHOR_TAG ) .', '']","['the tectogrammatical parsing of Czech, the analytical tree structure is converted into the tectogrammatical one.', 'These automatic transformations are based on linguistic rules( #AUTHOR_TAG ) .', '']","['the tectogrammatical parsing of Czech, the analytical tree structure is converted into the tectogrammatical one.', 'These automatic transformations are based on linguistic rules( #AUTHOR_TAG ) .', '']","['the tectogrammatical parsing of Czech, the analytical tree structure is converted into the tectogrammatical one.', 'These automatic transformations are based on linguistic rules( #AUTHOR_TAG ) .', 'Subsequently, tectogrammatical functors are assigned by the C4.5 classifier (2abokrtsk9 et al., 2002']",5
"['', 'We carried out two parallel experiments with two parsers available for Czech , parser I( Hajie et al. , 1998 ) and parser II( #AUTHOR_TAG ) .', '']","['analytical parsing of Czech runs in two steps: the statistical dependency parser, which creates the structure of a dependency tree, and a classifier assigning analytical functors.', 'We carried out two parallel experiments with two parsers available for Czech , parser I( Hajie et al. , 1998 ) and parser II( #AUTHOR_TAG ) .', '']","['analytical parsing of Czech runs in two steps: the statistical dependency parser, which creates the structure of a dependency tree, and a classifier assigning analytical functors.', 'We carried out two parallel experiments with two parsers available for Czech , parser I( Hajie et al. , 1998 ) and parser II( #AUTHOR_TAG ) .', '']","['analytical parsing of Czech runs in two steps: the statistical dependency parser, which creates the structure of a dependency tree, and a classifier assigning analytical functors.', 'We carried out two parallel experiments with two parsers available for Czech , parser I( Hajie et al. , 1998 ) and parser II( #AUTHOR_TAG ) .', '']",5
"['we created a probabilistic CzechEnglish dictionary by running GIZA + + training ( translation models 1-4 , see#AUTHOR_TAG ) on the training part of the English-Czech WSJ parallel corpus extended by the parallel corpus of entry/translation pairs from the manual dictionary.', '']","['we created a probabilistic CzechEnglish dictionary by running GIZA + + training ( translation models 1-4 , see#AUTHOR_TAG ) on the training part of the English-Czech WSJ parallel corpus extended by the parallel corpus of entry/translation pairs from the manual dictionary.', '']","['we created a probabilistic CzechEnglish dictionary by running GIZA + + training ( translation models 1-4 , see#AUTHOR_TAG ) on the training part of the English-Czech WSJ parallel corpus extended by the parallel corpus of entry/translation pairs from the manual dictionary.', '']","['make the dictionary more sensitive to a specific domain, which is in our case the domain of financial news, we created a probabilistic CzechEnglish dictionary by running GIZA + + training ( translation models 1-4 , see#AUTHOR_TAG ) on the training part of the English-Czech WSJ parallel corpus extended by the parallel corpus of entry/translation pairs from the manual dictionary.', 'As a result, the entry/translation pairs seen in the parallel corpus of WSJ become more probable.', '']",5
"['', 'For the evaluation of the results we use the BLEU score( #AUTHOR_TAG ) .', '']","['', 'For the evaluation of the results we use the BLEU score( #AUTHOR_TAG ) .', '']","['', 'For the evaluation of the results we use the BLEU score( #AUTHOR_TAG ) .', '']","['', 'For the evaluation of the results we use the BLEU score( #AUTHOR_TAG ) .', '']",5
"['', 'We also compare the results with the output generated by the statistical translation system GIZA + + / ISI ReWrite Decoder( AlOnaizan et al. , 1999 ; #AUTHOR_TAG ; Germann et al. , 2001 ) , trained on the same parallel corpus']","['', 'We also compare the results with the output generated by the statistical translation system GIZA + + / ISI ReWrite Decoder( AlOnaizan et al. , 1999 ; #AUTHOR_TAG ; Germann et al. , 2001 ) , trained on the same parallel corpus']","['', 'We also compare the results with the output generated by the statistical translation system GIZA + + / ISI ReWrite Decoder( AlOnaizan et al. , 1999 ; #AUTHOR_TAG ; Germann et al. , 2001 ) , trained on the same parallel corpus']","['', 'We also compare the results with the output generated by the statistical translation system GIZA + + / ISI ReWrite Decoder( AlOnaizan et al. , 1999 ; #AUTHOR_TAG ; Germann et al. , 2001 ) , trained on the same parallel corpus']",1
"[""evaluated our translations with IBM 's BLEU evaluation metric( #AUTHOR_TAG ) , using the same evaluation method and reference retranslations that were used for evaluation at HLT Workshop 2002 at CLSP ( Haji 6 et al. , 2002 ) ."", 'We used four reference retranslations of 490 sentences']","[""evaluated our translations with IBM 's BLEU evaluation metric( #AUTHOR_TAG ) , using the same evaluation method and reference retranslations that were used for evaluation at HLT Workshop 2002 at CLSP ( Haji 6 et al. , 2002 ) ."", 'We used four reference retranslations of 490 sentences']","[""evaluated our translations with IBM 's BLEU evaluation metric( #AUTHOR_TAG ) , using the same evaluation method and reference retranslations that were used for evaluation at HLT Workshop 2002 at CLSP ( Haji 6 et al. , 2002 ) ."", 'We used four reference retranslations of 490 sentences']","[""evaluated our translations with IBM 's BLEU evaluation metric( #AUTHOR_TAG ) , using the same evaluation method and reference retranslations that were used for evaluation at HLT Workshop 2002 at CLSP ( Haji 6 et al. , 2002 ) ."", '']",5
"['.', 'We performed translation experiments with an implementation of the IBM-4 translation model( #AUTHOR_TAG ) .', 'A description of the system can be found in(Tillmann and Ney, 2002) .', 'Table 5 presents an']","['distribution.', 'We performed translation experiments with an implementation of the IBM-4 translation model( #AUTHOR_TAG ) .', 'A description of the system can be found in(Tillmann and Ney, 2002) .', 'Table 5 presents an']","['', 'We performed translation experiments with an implementation of the IBM-4 translation model( #AUTHOR_TAG ) .', 'A description of the system can be found in(Tillmann and Ney, 2002) .', 'Table 5 presents an assessment']","['', 'We performed translation experiments with an implementation of the IBM-4 translation model( #AUTHOR_TAG ) .', 'A description of the system can be found in(Tillmann and Ney, 2002) .', '']",5
"['maximum entropy approach( #AUTHOR_TAG ) presents a powerful framework for the combination of several knowledge sources .', 'This principle recommends to choose the distribution which preserves as much uncertainty as possible in terms of maximizing the entropy.', 'The distribution is required to satisfy constraints, which represent facts known from the data.', 'These constraints are expressed on the basis of feature functions hu,(s,t']","['maximum entropy approach( #AUTHOR_TAG ) presents a powerful framework for the combination of several knowledge sources .', 'This principle recommends to choose the distribution which preserves as much uncertainty as possible in terms of maximizing the entropy.', 'The distribution is required to satisfy constraints, which represent facts known from the data.', 'These constraints are expressed on the basis of feature functions hu,(s,t']","['maximum entropy approach( #AUTHOR_TAG ) presents a powerful framework for the combination of several knowledge sources .', 'This principle recommends to choose the distribution which preserves as much uncertainty as possible in terms of maximizing the entropy.', 'The distribution is required to satisfy constraints, which represent facts known from the data.', 'These constraints are expressed on the basis of feature functions hu,(s,t']","['maximum entropy approach( #AUTHOR_TAG ) presents a powerful framework for the combination of several knowledge sources .', 'This principle recommends to choose the distribution which preserves as much uncertainty as possible in terms of maximizing the entropy.', 'The distribution is required to satisfy constraints, which represent facts known from the data.', 'These constraints are expressed on the basis of feature functions hu,(s,t']",5
"['input string can be preprocessed before being passed to the search algorithm.', 'If necessary, the inverse of these transformations will be applied to the generated output string.', 'In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology.', 'For descriptions of SMT systems see for example( Germann et al. , 2001 ; #AUTHOR_TAG ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']","['input string can be preprocessed before being passed to the search algorithm.', 'If necessary, the inverse of these transformations will be applied to the generated output string.', 'In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology.', 'For descriptions of SMT systems see for example( Germann et al. , 2001 ; #AUTHOR_TAG ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']","['input string can be preprocessed before being passed to the search algorithm.', 'If necessary, the inverse of these transformations will be applied to the generated output string.', 'In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology.', 'For descriptions of SMT systems see for example( Germann et al. , 2001 ; #AUTHOR_TAG ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']","['input string can be preprocessed before being passed to the search algorithm.', 'If necessary, the inverse of these transformations will be applied to the generated output string.', 'In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology.', 'For descriptions of SMT systems see for example( Germann et al. , 2001 ; #AUTHOR_TAG ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']",0
"['', 'For an introduction to maximum entropy modeling and training procedures , the reader is referred to the corresponding literature , for instance( #AUTHOR_TAG ) or( Ratnaparkhi , 1997 )']","['', 'For an introduction to maximum entropy modeling and training procedures , the reader is referred to the corresponding literature , for instance( #AUTHOR_TAG ) or( Ratnaparkhi , 1997 )']","['A = {Am } is the set of model parameters with one weight A, for each feature function hm .', 'For an introduction to maximum entropy modeling and training procedures , the reader is referred to the corresponding literature , for instance( #AUTHOR_TAG ) or( Ratnaparkhi , 1997 )']","['A = {Am } is the set of model parameters with one weight A, for each feature function hm .', 'For an introduction to maximum entropy modeling and training procedures , the reader is referred to the corresponding literature , for instance( #AUTHOR_TAG ) or( Ratnaparkhi , 1997 )']",0
"['input string can be preprocessed before being passed to the search algorithm.', 'If necessary, the inverse of these transformations will be applied to the generated output string.', 'In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology.', 'For descriptions of SMT systems see for example( #AUTHOR_TAG ; Och et al. , 1999 ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']","['input string can be preprocessed before being passed to the search algorithm.', 'If necessary, the inverse of these transformations will be applied to the generated output string.', 'In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology.', 'For descriptions of SMT systems see for example( #AUTHOR_TAG ; Och et al. , 1999 ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']","['input string can be preprocessed before being passed to the search algorithm.', 'If necessary, the inverse of these transformations will be applied to the generated output string.', 'In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology.', 'For descriptions of SMT systems see for example( #AUTHOR_TAG ; Och et al. , 1999 ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']","['input string can be preprocessed before being passed to the search algorithm.', 'If necessary, the inverse of these transformations will be applied to the generated output string.', 'In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology.', 'For descriptions of SMT systems see for example( #AUTHOR_TAG ; Och et al. , 1999 ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']",0
"['to using a global model like CRFs , our previous work in( Zhao et al. , 2006 ; #AUTHOR_TAG c ) reported the best results over the evaluated corpora of Bakeoff-2 until now7 .', 'Though those results are slightly better than the results']","['to using a global model like CRFs , our previous work in( Zhao et al. , 2006 ; #AUTHOR_TAG c ) reported the best results over the evaluated corpora of Bakeoff-2 until now7 .', 'Though those results are slightly better than the results']","['to using a global model like CRFs , our previous work in( Zhao et al. , 2006 ; #AUTHOR_TAG c ) reported the best results over the evaluated corpora of Bakeoff-2 until now7 .', 'Though those results are slightly better than the results']","['to using a global model like CRFs , our previous work in( Zhao et al. , 2006 ; #AUTHOR_TAG c ) reported the best results over the evaluated corpora of Bakeoff-2 until now7 .', '']",1
"['1 The representation in#AUTHOR_TAG is even more compact than ours for grammars that are not self-embedding .', '']","['1 The representation in#AUTHOR_TAG is even more compact than ours for grammars that are not self-embedding .', '']","['1 The representation in#AUTHOR_TAG is even more compact than ours for grammars that are not self-embedding .', '']","['1 The representation in#AUTHOR_TAG is even more compact than ours for grammars that are not self-embedding .', 'However, in this paper we use our representation as an intermediate result in approximating an unrestricted context-free grammar, with the final objective of obtaining a single minimal deterministic automaton.', ""For this purpose, Mohri and Pereira's representation offers little advantage""]",1
"['restricting the height of the stack of a pushdown automaton, one obstructs recognition of a set of strings in the context-free language, and therefore a subset approximation results.', 'This idea was proposed by Krauwer and desTombe ( 1981 ) ,Langendoen and Langsam ( 1987 ) , andPulman ( 1986 ) , and was rediscovered byBlack ( 1989 ) and recently by#AUTHOR_TAG .', '']","['restricting the height of the stack of a pushdown automaton, one obstructs recognition of a set of strings in the context-free language, and therefore a subset approximation results.', 'This idea was proposed by Krauwer and desTombe ( 1981 ) ,Langendoen and Langsam ( 1987 ) , andPulman ( 1986 ) , and was rediscovered byBlack ( 1989 ) and recently by#AUTHOR_TAG .', '']","['restricting the height of the stack of a pushdown automaton, one obstructs recognition of a set of strings in the context-free language, and therefore a subset approximation results.', 'This idea was proposed by Krauwer and desTombe ( 1981 ) ,Langendoen and Langsam ( 1987 ) , andPulman ( 1986 ) , and was rediscovered byBlack ( 1989 ) and recently by#AUTHOR_TAG .', '']","['restricting the height of the stack of a pushdown automaton, one obstructs recognition of a set of strings in the context-free language, and therefore a subset approximation results.', 'This idea was proposed by Krauwer and desTombe ( 1981 ) ,Langendoen and Langsam ( 1987 ) , andPulman ( 1986 ) , and was rediscovered byBlack ( 1989 ) and recently by#AUTHOR_TAG .', '']",0
"['rephrase the method of#AUTHOR_TAG as follows : First , we construct the approximating finite automaton according to the unparameterized RTN method above .', '']","['rephrase the method of#AUTHOR_TAG as follows : First , we construct the approximating finite automaton according to the unparameterized RTN method above .', '']","['rephrase the method of#AUTHOR_TAG as follows : First , we construct the approximating finite automaton according to the unparameterized RTN method above .', '']","['rephrase the method of#AUTHOR_TAG as follows : First , we construct the approximating finite automaton according to the unparameterized RTN method above .', '']",5
"['method can be generalized , inspired by#AUTHOR_TAG , who derive N-gram probabilities from stochastic context-free grammars .', 'By ignoring the probabilities, each N = 1, 2, 3 .... gives rise to a superset approximation that can be described as follows: The set of strings derivable from a nonterminal A is approximated by the set of strings al']","['method can be generalized , inspired by#AUTHOR_TAG , who derive N-gram probabilities from stochastic context-free grammars .', 'By ignoring the probabilities, each N = 1, 2, 3 .... gives rise to a superset approximation that can be described as follows: The set of strings derivable from a nonterminal A is approximated by the set of strings al']","['method can be generalized , inspired by#AUTHOR_TAG , who derive N-gram probabilities from stochastic context-free grammars .', 'By ignoring the probabilities, each N = 1, 2, 3 .... gives rise to a superset approximation that can be described as follows: The set of strings derivable from a nonterminal A is approximated by the set of strings al']","['method can be generalized , inspired by#AUTHOR_TAG , who derive N-gram probabilities from stochastic context-free grammars .', '']",0
['#AUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata'],['#AUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata'],['#AUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata'],['#AUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata'],0
"['The full formulation of the integrated grammar transformation and construction of the finite automaton is rather long and is therefore not given here.', 'A very similar formulation , for another grammar transformation , is given in#AUTHOR_TAG']","['full formulation of the integrated grammar transformation and construction of the finite automaton is rather long and is therefore not given here.', 'A very similar formulation , for another grammar transformation , is given in#AUTHOR_TAG']","['The full formulation of the integrated grammar transformation and construction of the finite automaton is rather long and is therefore not given here.', 'A very similar formulation , for another grammar transformation , is given in#AUTHOR_TAG']","['', 'A very similar formulation , for another grammar transformation , is given in#AUTHOR_TAG']",1
"['these rules are specified by an expert linguist, conversant with the sound and spelling systems of the language of concern.', 'Typical letter-to-sound rule sets are those described byAinsworth ( 1973 ) , McIlroy ( 1973 ) ,Elovitz et al. ( 1976 ) ,Hurmicutt ( 1976 ) , and#AUTHOR_TAG']","['these rules are specified by an expert linguist, conversant with the sound and spelling systems of the language of concern.', 'Typical letter-to-sound rule sets are those described byAinsworth ( 1973 ) , McIlroy ( 1973 ) ,Elovitz et al. ( 1976 ) ,Hurmicutt ( 1976 ) , and#AUTHOR_TAG']","['these rules are specified by an expert linguist, conversant with the sound and spelling systems of the language of concern.', 'Typical letter-to-sound rule sets are those described byAinsworth ( 1973 ) , McIlroy ( 1973 ) ,Elovitz et al. ( 1976 ) ,Hurmicutt ( 1976 ) , and#AUTHOR_TAG']","['', 'Conventionally, these rules are specified by an expert linguist, conversant with the sound and spelling systems of the language of concern.', 'Typical letter-to-sound rule sets are those described byAinsworth ( 1973 ) , McIlroy ( 1973 ) ,Elovitz et al. ( 1976 ) ,Hurmicutt ( 1976 ) , and#AUTHOR_TAG']",0
"[', this possibility is not usually given much credence.', ""For instance ,#AUTHOR_TAG recently wrote : `` To our knowledge , learning algorithms , although promising , have not ( yet ) reached the level of rule sets developed by humans '' ( p. 520 ) ."", '520).', '']","['is also conceivable that data-driven techniques can actually outperform traditional rules.', 'However, this possibility is not usually given much credence.', ""For instance ,#AUTHOR_TAG recently wrote : `` To our knowledge , learning algorithms , although promising , have not ( yet ) reached the level of rule sets developed by humans '' ( p. 520 ) ."", '520).', '']","[', this possibility is not usually given much credence.', ""For instance ,#AUTHOR_TAG recently wrote : `` To our knowledge , learning algorithms , although promising , have not ( yet ) reached the level of rule sets developed by humans '' ( p. 520 ) ."", '520).', '']","['is also conceivable that data-driven techniques can actually outperform traditional rules.', 'However, this possibility is not usually given much credence.', ""For instance ,#AUTHOR_TAG recently wrote : `` To our knowledge , learning algorithms , although promising , have not ( yet ) reached the level of rule sets developed by humans '' ( p. 520 ) ."", '520).', 'Dutoit (1997) takes this further, stating ""such training-based strategies are often assumed to exhibit much more intelligence than they do in practice, as revealed by their poor transcription scores"" (p.', '115, note 14']",0
"['', 'See also the work of#AUTHOR_TAG , which considers computer-based pronunciation by analogy but does not mention the possible application to text-to-speech synthesis .', '']","['', 'See also the work of#AUTHOR_TAG , which considers computer-based pronunciation by analogy but does not mention the possible application to text-to-speech synthesis .', '']","['', 'See also the work of#AUTHOR_TAG , which considers computer-based pronunciation by analogy but does not mention the possible application to text-to-speech synthesis .', '']","['', 'See also the work of#AUTHOR_TAG , which considers computer-based pronunciation by analogy but does not mention the possible application to text-to-speech synthesis .', '']",0
"['the above characterization is very wide ranging.', 'Consequently , fusion has been applied to a wide variety of pattern recognition and decision theoretic problems -- using a plethora of theories , techniques , and tools -- including some applications in computational linguistics ( e.g. ,#AUTHOR_TAG ; van Halteren , Zavrel , and Daelemans 1998 ) and speech technology ( e.g. , Bowles and Damper 1989 ; Romary and Pierre11989 ) .', '']","['the above characterization is very wide ranging.', 'Consequently , fusion has been applied to a wide variety of pattern recognition and decision theoretic problems -- using a plethora of theories , techniques , and tools -- including some applications in computational linguistics ( e.g. ,#AUTHOR_TAG ; van Halteren , Zavrel , and Daelemans 1998 ) and speech technology ( e.g. , Bowles and Damper 1989 ; Romary and Pierre11989 ) .', '']","['the above characterization is very wide ranging.', 'Consequently , fusion has been applied to a wide variety of pattern recognition and decision theoretic problems -- using a plethora of theories , techniques , and tools -- including some applications in computational linguistics ( e.g. ,#AUTHOR_TAG ; van Halteren , Zavrel , and Daelemans 1998 ) and speech technology ( e.g. , Bowles and Damper 1989 ; Romary and Pierre11989 ) .', '']","[', the above characterization is very wide ranging.', 'Consequently , fusion has been applied to a wide variety of pattern recognition and decision theoretic problems -- using a plethora of theories , techniques , and tools -- including some applications in computational linguistics ( e.g. ,#AUTHOR_TAG ; van Halteren , Zavrel , and Daelemans 1998 ) and speech technology ( e.g. , Bowles and Damper 1989 ; Romary and Pierre11989 ) .', '']",0
"['researchers have proposed greedy algorithms for estimating nonprobabilistic word-to-word translation models, also known as translation lexicons (e.g., Catizone, Russell, and Warwick 1989; Gale and Church 1991; Fung 1995; Kumano and Hirakawa 1994;#AUTHOR_TAG 1995; Wu and Xia 1994).', 'Most of these algorithms can be summarized as follows']","['researchers have proposed greedy algorithms for estimating nonprobabilistic word-to-word translation models, also known as translation lexicons (e.g., Catizone, Russell, and Warwick 1989; Gale and Church 1991; Fung 1995; Kumano and Hirakawa 1994;#AUTHOR_TAG 1995; Wu and Xia 1994).', 'Most of these algorithms can be summarized as follows']","['researchers have proposed greedy algorithms for estimating nonprobabilistic word-to-word translation models, also known as translation lexicons (e.g., Catizone, Russell, and Warwick 1989; Gale and Church 1991; Fung 1995; Kumano and Hirakawa 1994;#AUTHOR_TAG 1995; Wu and Xia 1994).', 'Most of these algorithms can be summarized as follows']","['researchers have proposed greedy algorithms for estimating nonprobabilistic word-to-word translation models, also known as translation lexicons (e.g., Catizone, Russell, and Warwick 1989; Gale and Church 1991; Fung 1995; Kumano and Hirakawa 1994;#AUTHOR_TAG 1995; Wu and Xia 1994).', 'Most of these algorithms can be summarized as follows']",0
"['probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by#AUTHOR_TAG b ) .', 'These models involve conditional probabilities, but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution.', '']","['probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by#AUTHOR_TAG b ) .', 'These models involve conditional probabilities, but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution.', '']","['probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by#AUTHOR_TAG b ) .', 'These models involve conditional probabilities, but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution.', '']","['probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by#AUTHOR_TAG b ) .', 'These models involve conditional probabilities, but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution.', '']",0
"['probability distribution trans (.1, ~) is a word-to-word translation model.', 'Unlike the models proposed by#AUTHOR_TAG b ) , this model is symmetric , because both word bags are generated together from a joint probability distribution .', ""Brown and his colleagues' models,""]","['probability distribution trans (.1, ~) is a word-to-word translation model.', 'Unlike the models proposed by#AUTHOR_TAG b ) , this model is symmetric , because both word bags are generated together from a joint probability distribution .', ""Brown and his colleagues' models,""]","['probability distribution trans (.1, ~) is a word-to-word translation model.', 'Unlike the models proposed by#AUTHOR_TAG b ) , this model is symmetric , because both word bags are generated together from a joint probability distribution .', ""Brown and his colleagues' models,""]","['probability distribution trans (.1, ~) is a word-to-word translation model.', 'Unlike the models proposed by#AUTHOR_TAG b ) , this model is symmetric , because both word bags are generated together from a joint probability distribution .', ""Brown and his colleagues' models, reviewed in Section 4.3, generate one half of the bitext given the other hall so they are represented by conditional probability distributions."", '']",1
"['bitext space is another kind of collocation.', ""If each word 's translation is treated as a sense tag( #AUTHOR_TAG ) , then `` translational '' collocations have the unique property that the collocate and the word sense are one and the same !"", 'Method B exploits this property under the hypothesis that ""one sense per collocation"" holds for translational collocations.', 'This hypothesis implies that if u and v are possible mutual translations, and a token u co-occurs with a token v in the bitext, then with very high probability the pair (u, v) was generated from the same concept and should be']","['bitext space is another kind of collocation.', ""If each word 's translation is treated as a sense tag( #AUTHOR_TAG ) , then `` translational '' collocations have the unique property that the collocate and the word sense are one and the same !"", 'Method B exploits this property under the hypothesis that ""one sense per collocation"" holds for translational collocations.', 'This hypothesis implies that if u and v are possible mutual translations, and a token u co-occurs with a token v in the bitext, then with very high probability the pair (u, v) was generated from the same concept and should be']","['bitext space is another kind of collocation.', ""If each word 's translation is treated as a sense tag( #AUTHOR_TAG ) , then `` translational '' collocations have the unique property that the collocate and the word sense are one and the same !"", 'Method B exploits this property under the hypothesis that ""one sense per collocation"" holds for translational collocations.', 'This hypothesis implies that if u and v are possible mutual translations, and a token u co-occurs with a token v in the bitext, then with very high probability the pair (u, v) was generated from the same concept and should be']","['bitext space is another kind of collocation.', ""If each word 's translation is treated as a sense tag( #AUTHOR_TAG ) , then `` translational '' collocations have the unique property that the collocate and the word sense are one and the same !"", 'Method B exploits this property under the hypothesis that ""one sense per collocation"" holds for translational collocations.', 'This hypothesis implies that if u and v are possible mutual translations, and a token u co-occurs with a token v in the bitext, then with very high probability the pair (u, v) was generated from the same concept and should be linked.', '']",5
"['informal experiments described elsewhere( #AUTHOR_TAG ) , I found that the G2 statistic suggested byDunning ( 1993 ) slightly outperforms 02 .', 'Let the cells of the contingency table be named as follows']","['informal experiments described elsewhere( #AUTHOR_TAG ) , I found that the G2 statistic suggested byDunning ( 1993 ) slightly outperforms 02 .', 'Let the cells of the contingency table be named as follows']","['informal experiments described elsewhere( #AUTHOR_TAG ) , I found that the G2 statistic suggested byDunning ( 1993 ) slightly outperforms 02 .', 'Let the cells of the contingency table be named as follows']","['informal experiments described elsewhere( #AUTHOR_TAG ) , I found that the G2 statistic suggested byDunning ( 1993 ) slightly outperforms 02 .', 'Let the cells of the contingency table be named as follows']",2
"['¢ cross-language information retrieval ( e.g. , McCarley 1999 ) , â\x80¢ multilingual document filtering ( e.g. , Oard 1997 ) , â\x80¢ computer-assisted language learning ( e.g. ,#AUTHOR_TAG ) , â\x80¢ certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) ,']","['cross-language information retrieval ( e.g. , McCarley 1999 ) , â\x80¢ multilingual document filtering ( e.g. , Oard 1997 ) , â\x80¢ computer-assisted language learning ( e.g. ,#AUTHOR_TAG ) , â\x80¢ certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) ,']","['¢ cross-language information retrieval ( e.g. , McCarley 1999 ) , â\x80¢ multilingual document filtering ( e.g. , Oard 1997 ) , â\x80¢ computer-assisted language learning ( e.g. ,#AUTHOR_TAG ) , â\x80¢ certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) ,']","['\x80¢ cross-language information retrieval ( e.g. , McCarley 1999 ) , â\x80¢ multilingual document filtering ( e.g. , Oard 1997 ) , â\x80¢ computer-assisted language learning ( e.g. ,#AUTHOR_TAG ) , â\x80¢ certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , â\x80¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 )']",0
"[""this situation ,#AUTHOR_TAG b , 293 ) recommend `` evaluating the expectations using only a single , probable alignment . ''"", 'The single most probable assignment Ama~ is the maximum a posteriori (MAP) assignment']","[""this situation ,#AUTHOR_TAG b , 293 ) recommend `` evaluating the expectations using only a single , probable alignment . ''"", 'The single most probable assignment Ama~ is the maximum a posteriori (MAP) assignment']","[""this situation ,#AUTHOR_TAG b , 293 ) recommend `` evaluating the expectations using only a single , probable alignment . ''"", 'The single most probable assignment Ama~ is the maximum a posteriori (MAP) assignment']","[""this situation ,#AUTHOR_TAG b , 293 ) recommend `` evaluating the expectations using only a single , probable alignment . ''"", 'The single most probable assignment Ama~ is the maximum a posteriori (MAP) assignment']",4
"['.', 'Just as easily , we can model link types that coincide with entries in an on-line bilingual dictionary separately from those that do not ( cfXXX#AUTHOR_TAG ) .', 'Brown et al. 1993).', 'When the auxiliary parameters are conditioned on different link classes, their optimization is carried out separately for each class: B (links (u, v)[cooc(u, v), A +) scorec (u, vlZ = class(u, v)) = log B(links(u, v)[cooc(u']","['texts.', 'Just as easily , we can model link types that coincide with entries in an on-line bilingual dictionary separately from those that do not ( cfXXX#AUTHOR_TAG ) .', 'Brown et al. 1993).', 'When the auxiliary parameters are conditioned on different link classes, their optimization is carried out separately for each class: B (links (u, v)[cooc(u, v), A +) scorec (u, vlZ = class(u, v)) = log B(links(u, v)[cooc(u, v), A z)"" (37)']","['', 'Just as easily , we can model link types that coincide with entries in an on-line bilingual dictionary separately from those that do not ( cfXXX#AUTHOR_TAG ) .', 'Brown et al. 1993).', 'When the auxiliary parameters are conditioned on different link classes, their optimization is carried out separately for each class: B (links (u, v)[cooc(u, v), A +) scorec (u, vlZ = class(u, v)) = log B(links(u, v)[cooc(u, v), A z)"" (37']","['', 'Just as easily , we can model link types that coincide with entries in an on-line bilingual dictionary separately from those that do not ( cfXXX#AUTHOR_TAG ) .', 'Brown et al. 1993).', 'When the auxiliary parameters are conditioned on different link classes, their optimization is carried out separately for each class: B (links (u, v)[cooc(u, v), A +) scorec (u, vlZ = class(u, v)) = log B(links(u, v)[cooc(u, v), A z)"" (37) Section 6.1.1 describes the link classes used in the experiments below']",5
"[""now , translation models have been evaluated either subjectively ( e.g. White and O'Connell 1993 ) or using relative metrics , such as perplexity with respect to other models( #AUTHOR_TAG b ) ."", 'Objective and more accurate tests can be carried out using a ""gold']","[""now , translation models have been evaluated either subjectively ( e.g. White and O'Connell 1993 ) or using relative metrics , such as perplexity with respect to other models( #AUTHOR_TAG b ) ."", 'Objective and more accurate tests can be carried out using a ""gold']","[""now , translation models have been evaluated either subjectively ( e.g. White and O'Connell 1993 ) or using relative metrics , such as perplexity with respect to other models( #AUTHOR_TAG b ) ."", 'Objective and more accurate tests can be carried out using a ""gold standard.""', '']","[""now , translation models have been evaluated either subjectively ( e.g. White and O'Connell 1993 ) or using relative metrics , such as perplexity with respect to other models( #AUTHOR_TAG b ) ."", 'Objective and more accurate tests can be carried out using a ""gold standard.""', '']",1
"['I found that the G2 statistic suggested by#AUTHOR_TAG slightly outperforms 02 .', 'Let the cells of the contingency table be named as follows']","['I found that the G2 statistic suggested by#AUTHOR_TAG slightly outperforms 02 .', 'Let the cells of the contingency table be named as follows']","['I found that the G2 statistic suggested by#AUTHOR_TAG slightly outperforms 02 .', 'Let the cells of the contingency table be named as follows']","['informal experiments described elsewhere ( Melamed 1995 ) , I found that the G2 statistic suggested by#AUTHOR_TAG slightly outperforms 02 .', 'Let the cells of the contingency table be named as follows']",0
"['above equation holds regardless of how we represent concepts.', 'There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ;#AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .', '']","['above equation holds regardless of how we represent concepts.', 'There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ;#AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .', '']","['above equation holds regardless of how we represent concepts.', 'There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ;#AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .', '']","['above equation holds regardless of how we represent concepts.', 'There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ;#AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .', '']",0
"['\x80¢ cross-language information retrieval ( e.g. ,#AUTHOR_TAG ) , â\x80¢ multilingual document filtering ( e.g. , Oard 1997 ) , â\x80¢ computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) ,']","['\x80¢ cross-language information retrieval ( e.g. ,#AUTHOR_TAG ) , â\x80¢ multilingual document filtering ( e.g. , Oard 1997 ) , â\x80¢ computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) ,']","['\x80¢ cross-language information retrieval ( e.g. ,#AUTHOR_TAG ) , â\x80¢ multilingual document filtering ( e.g. , Oard 1997 ) , â\x80¢ computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) ,']","['\x80¢ cross-language information retrieval ( e.g. ,#AUTHOR_TAG ) , â\x80¢ multilingual document filtering ( e.g. , Oard 1997 ) , â\x80¢ computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) , â\x80¢ certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , â\x80¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 )']",0
"[""other such cases are described in Danlos 's book( #AUTHOR_TAG ) ."", 'The common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints (such as unambiguous reference) or performing linguistic optimizations (such as using pronouns instead of longer referring expressions whenever possible) in cases where the constraints or optimizations depend on decisions made in multiple modules.', 'This is largely due to the fact that pipelined systems cannot perform general search over a decision space that includes decisions made in more than one module']","[""other such cases are described in Danlos 's book( #AUTHOR_TAG ) ."", 'The common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints (such as unambiguous reference) or performing linguistic optimizations (such as using pronouns instead of longer referring expressions whenever possible) in cases where the constraints or optimizations depend on decisions made in multiple modules.', 'This is largely due to the fact that pipelined systems cannot perform general search over a decision space that includes decisions made in more than one module']","[""other such cases are described in Danlos 's book( #AUTHOR_TAG ) ."", 'The common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints (such as unambiguous reference) or performing linguistic optimizations (such as using pronouns instead of longer referring expressions whenever possible) in cases where the constraints or optimizations depend on decisions made in multiple modules.', 'This is largely due to the fact that pipelined systems cannot perform general search over a decision space that includes decisions made in more than one module']","[""other such cases are described in Danlos 's book( #AUTHOR_TAG ) ."", 'The common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints (such as unambiguous reference) or performing linguistic optimizations (such as using pronouns instead of longer referring expressions whenever possible) in cases where the constraints or optimizations depend on decisions made in multiple modules.', 'This is largely due to the fact that pipelined systems cannot perform general search over a decision space that includes decisions made in more than one module']",0
"['these arguments, most applied NLG systems use a pipelined architecture; indeed, a pipeline was used in every one of the systems surveyed byReiter (1994) andPaiva (1998) .', 'This may be because pipelines have many engineering advantages , and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems( #AUTHOR_TAG )']","['these arguments, most applied NLG systems use a pipelined architecture; indeed, a pipeline was used in every one of the systems surveyed byReiter (1994) andPaiva (1998) .', 'This may be because pipelines have many engineering advantages , and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems( #AUTHOR_TAG )']","['these arguments, most applied NLG systems use a pipelined architecture; indeed, a pipeline was used in every one of the systems surveyed byReiter (1994) andPaiva (1998) .', 'This may be because pipelines have many engineering advantages , and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems( #AUTHOR_TAG )']","['these arguments, most applied NLG systems use a pipelined architecture; indeed, a pipeline was used in every one of the systems surveyed byReiter (1994) andPaiva (1998) .', 'This may be because pipelines have many engineering advantages , and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems( #AUTHOR_TAG )']",0
"['these arguments , most applied NLG systems use a pipelined architecture ; indeed , a pipeline was used in every one of the systems surveyed by#AUTHOR_TAG andPaiva ( 1998 ) .', 'This may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems (Mittal et']","['these arguments , most applied NLG systems use a pipelined architecture ; indeed , a pipeline was used in every one of the systems surveyed by#AUTHOR_TAG andPaiva ( 1998 ) .', 'This may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems (Mittal et']","['these arguments , most applied NLG systems use a pipelined architecture ; indeed , a pipeline was used in every one of the systems surveyed by#AUTHOR_TAG andPaiva ( 1998 ) .', 'This may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems (Mitt']","['these arguments , most applied NLG systems use a pipelined architecture ; indeed , a pipeline was used in every one of the systems surveyed by#AUTHOR_TAG andPaiva ( 1998 ) .', 'This may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems (Mittal et al. 1998']",0
"[""Levin's (1993) ."", 'The lexicon is used to mediate and map between a language-independent domain model and a language-dependent ontology widely used in NLG , the Upper Model( #AUTHOR_TAG ) .', 'Although the idea of a two-level representation accommodating language-neutral and language-specific requirements is not new (see for example Nirenburg and Levin [1992], Dorr and Voss [1993], and Di Eugenio [1998]), Stede is among the few who make effective use of those two levels in a complex system']","[""Levin's (1993) ."", 'The lexicon is used to mediate and map between a language-independent domain model and a language-dependent ontology widely used in NLG , the Upper Model( #AUTHOR_TAG ) .', 'Although the idea of a two-level representation accommodating language-neutral and language-specific requirements is not new (see for example Nirenburg and Levin [1992], Dorr and Voss [1993], and Di Eugenio [1998]), Stede is among the few who make effective use of those two levels in a complex system']","[""Levin's (1993) ."", 'The lexicon is used to mediate and map between a language-independent domain model and a language-dependent ontology widely used in NLG , the Upper Model( #AUTHOR_TAG ) .', 'Although the idea of a two-level representation accommodating language-neutral and language-specific requirements is not new (see for example Nirenburg and Levin [1992], Dorr and Voss [1993], and Di Eugenio [1998]), Stede is among the few who make effective use of those two levels in a complex system']","['', 'The lexicon is used to mediate and map between a language-independent domain model and a language-dependent ontology widely used in NLG , the Upper Model( #AUTHOR_TAG ) .', 'Although the idea of a two-level representation accommodating language-neutral and language-specific requirements is not new (see for example Nirenburg and Levin [1992], Dorr and Voss [1993], and Di Eugenio [1998]), Stede is among the few who make effective use of those two levels in a complex system']",0
"['be replaced by a much simpler one , based on finite-state transducers ( FSTs ) ; the same conclusion was reached independently by Kaplan and Kay , whose work remained an underground classic until it was finally published in#AUTHOR_TAG .']","['be replaced by a much simpler one , based on finite-state transducers ( FSTs ) ; the same conclusion was reached independently by Kaplan and Kay , whose work remained an underground classic until it was finally published in#AUTHOR_TAG .']","['could ] be replaced by a much simpler one , based on finite-state transducers ( FSTs ) ; the same conclusion was reached independently by Kaplan and Kay , whose work remained an underground classic until it was finally published in#AUTHOR_TAG .']",['#AUTHOR_TAG'],0
"['T therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. ,#AUTHOR_TAG b ) is of course an empirical question']","['T therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. ,#AUTHOR_TAG b ) is of course an empirical question']","['T therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. ,#AUTHOR_TAG b ) is of course an empirical question']","['T therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. ,#AUTHOR_TAG b ) is of course an empirical question']",0
"['', 'In some cases rankings may work well enough.', '#AUTHOR_TAG report excellent part-of-speech tagging results using a handcrafted approach that is close to OT .3 More speculatively , imagine an OT grammar for']","['', 'In some cases rankings may work well enough.', '#AUTHOR_TAG report excellent part-of-speech tagging results using a handcrafted approach that is close to OT .3 More speculatively , imagine an OT grammar for stylistic revision of parsed sentences']","['', 'In some cases rankings may work well enough.', '#AUTHOR_TAG report excellent part-of-speech tagging results using a handcrafted approach that is close to OT .3 More speculatively , imagine an OT grammar for']","['', 'In some cases rankings may work well enough.', '#AUTHOR_TAG report excellent part-of-speech tagging results using a handcrafted approach that is close to OT .3 More speculatively , imagine an OT grammar for stylistic revision of parsed sentences .', '']",0
['typical OT grammars offer much richer finite-state models of left context( #AUTHOR_TAG a ) than provided by the traditional HMM finite-state topologies'],['typical OT grammars offer much richer finite-state models of left context( #AUTHOR_TAG a ) than provided by the traditional HMM finite-state topologies'],"['typical OT grammars offer much richer finite-state models of left context( #AUTHOR_TAG a ) than provided by the traditional HMM finite-state topologies .', '']","['', 'But typical OT grammars offer much richer finite-state models of left context( #AUTHOR_TAG a ) than provided by the traditional HMM finite-state topologies .', '']",0
"['', '#AUTHOR_TAG use mutual information to identify collocations , a method they claim is reasonably effective for words with a frequency of not less than five']","['', ""In Smadja's collocation algorithm Xtract, the lowest-frequency words are effectively discarded as well (Smadja 1993)."", '#AUTHOR_TAG use mutual information to identify collocations , a method they claim is reasonably effective for words with a frequency of not less than five']","['', '#AUTHOR_TAG use mutual information to identify collocations , a method they claim is reasonably effective for words with a frequency of not less than five']","['', ""In Smadja's collocation algorithm Xtract, the lowest-frequency words are effectively discarded as well (Smadja 1993)."", '#AUTHOR_TAG use mutual information to identify collocations , a method they claim is reasonably effective for words with a frequency of not less than five']",0
"['s exact test leads to a slightly better recall with the same precision scores (0.31 for both tests).', ""While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure( #AUTHOR_TAG )""]","['exact test leads to a slightly better recall with the same precision scores (0.31 for both tests).', ""While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure( #AUTHOR_TAG )""]","['exact test leads to a slightly better recall with the same precision scores (0.31 for both tests).', ""While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure( #AUTHOR_TAG )""]","['', ""For the higherfrequency words, Fisher's exact test leads to a slightly better recall with the same precision scores (0.31 for both tests)."", ""While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure( #AUTHOR_TAG )""]",0
"['', 'For example ,#AUTHOR_TAG proves that Chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao , for the number 5000000000000000005000000000000005000000000005000000005000 ,']","['', 'For example ,#AUTHOR_TAG proves that Chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao , for the number 5000000000000000005000000000000005000000000005000000005000 ,']","['', 'For example ,#AUTHOR_TAG proves that Chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao , for the number 5000000000000000005000000000000005000000000005000000005000 ,']","['', 'For example ,#AUTHOR_TAG proves that Chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao , for the number 5000000000000000005000000000000005000000000005000000005000 , are not context-free , which implies that Chinese is not a context-free language and thus might parse in exponential worst-case time .', '']",0
"['some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes( #AUTHOR_TAG ) .', 'This parser produces trees to represent the structure of the sentences that compose the text.', 'However']","['some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes( #AUTHOR_TAG ) .', 'This parser produces trees to represent the structure of the sentences that compose the text.', 'However,']","['some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes( #AUTHOR_TAG ) .', 'This parser produces trees to represent the structure of the sentences that compose the text.', 'However']","['', 'For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes( #AUTHOR_TAG ) .', 'This parser produces trees to represent the structure of the sentences that compose the text.', 'However, it is set to ""skip"" or surrender attempts to parse clauses after reaching a time-out threshold.', '']",0
"['this classification method we can also derive the probability that a case belongs to a particular group (i.e., posterior probabilities), which is roughly proportional to the Mahalanobis distance from that group centroid.', 'Discriminant analysis has been employed by researchers in automatic text genre detection( #AUTHOR_TAG b ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables']","['this classification method we can also derive the probability that a case belongs to a particular group (i.e., posterior probabilities), which is roughly proportional to the Mahalanobis distance from that group centroid.', 'Discriminant analysis has been employed by researchers in automatic text genre detection( #AUTHOR_TAG b ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables']","['', 'Using this classification method we can also derive the probability that a case belongs to a particular group (i.e., posterior probabilities), which is roughly proportional to the Mahalanobis distance from that group centroid.', 'Discriminant analysis has been employed by researchers in automatic text genre detection( #AUTHOR_TAG b ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables']","['', 'Using this classification method we can also derive the probability that a case belongs to a particular group (i.e., posterior probabilities), which is roughly proportional to the Mahalanobis distance from that group centroid.', 'Discriminant analysis has been employed by researchers in automatic text genre detection( #AUTHOR_TAG b ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables']",0
"['algorithm we implemented is inspired by the work of#AUTHOR_TAG on word sense disambiguation .', 'He classified the senses of a word on the basis of other words that the given word co-occurs with.', '']","['algorithm we implemented is inspired by the work of#AUTHOR_TAG on word sense disambiguation .', 'He classified the senses of a word on the basis of other words that the given word co-occurs with.', '']","['algorithm we implemented is inspired by the work of#AUTHOR_TAG on word sense disambiguation .', 'He classified the senses of a word on the basis of other words that the given word co-occurs with.', '']","['algorithm we implemented is inspired by the work of#AUTHOR_TAG on word sense disambiguation .', 'He classified the senses of a word on the basis of other words that the given word co-occurs with.', '']",4
"['automatic annotation of nouns and verbs in the corpus has been done by matching them with the WordNet database files.', 'Before doing the annotation, though, some preprocessing of the data was required to maximize the matching between our corpus and WordNet.', 'The changes made were inspired by those described in#AUTHOR_TAG , page 75 ) .', '']","['automatic annotation of nouns and verbs in the corpus has been done by matching them with the WordNet database files.', 'Before doing the annotation, though, some preprocessing of the data was required to maximize the matching between our corpus and WordNet.', 'The changes made were inspired by those described in#AUTHOR_TAG , page 75 ) .', '']","['automatic annotation of nouns and verbs in the corpus has been done by matching them with the WordNet database files.', 'Before doing the annotation, though, some preprocessing of the data was required to maximize the matching between our corpus and WordNet.', 'The changes made were inspired by those described in#AUTHOR_TAG , page 75 ) .', '']","['automatic annotation of nouns and verbs in the corpus has been done by matching them with the WordNet database files.', 'Before doing the annotation, though, some preprocessing of the data was required to maximize the matching between our corpus and WordNet.', 'The changes made were inspired by those described in#AUTHOR_TAG , page 75 ) .', '']",4
"['and POS-tagging task , we present a novel solution using the framework in this article , and show that it gives comparable accuracies to our previous work( #AUTHOR_TAG a ) , while being more than an order of magnitude faster']","['and POS-tagging task , we present a novel solution using the framework in this article , and show that it gives comparable accuracies to our previous work( #AUTHOR_TAG a ) , while being more than an order of magnitude faster']","['and POS-tagging task , we present a novel solution using the framework in this article , and show that it gives comparable accuracies to our previous work( #AUTHOR_TAG a ) , while being more than an order of magnitude faster']","['', 'For the joint segmentation and POS-tagging task , we present a novel solution using the framework in this article , and show that it gives comparable accuracies to our previous work( #AUTHOR_TAG a ) , while being more than an order of magnitude faster']",1
"['viously( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .', 'In each test, the discriminative model was retrained and reevaluated without a particular group of features.', 'We summarize the findings of this study in this section']","['viously( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .', 'In each test, the discriminative model was retrained and reevaluated without a particular group of features.', 'We summarize the findings of this study in this section']","['viously( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .', 'In each test, the discriminative model was retrained and reevaluated without a particular group of features.', 'We summarize the findings of this study in this section']","['viously( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .', 'In each test, the discriminative model was retrained and reevaluated without a particular group of features.', 'We summarize the findings of this study in this section']",2
"['', 'This evaluation set-up is an improvement versus the one we previously reported( #AUTHOR_TAG ) , in which fixed partitions were used for training , development , and testing']","['', 'This evaluation set-up is an improvement versus the one we previously reported( #AUTHOR_TAG ) , in which fixed partitions were used for training , development , and testing']","['', 'This evaluation set-up is an improvement versus the one we previously reported( #AUTHOR_TAG ) , in which fixed partitions were used for training , development , and testing']","['', 'This evaluation set-up is an improvement versus the one we previously reported( #AUTHOR_TAG ) , in which fixed partitions were used for training , development , and testing']",2
"['now give an overview of the training algorithm, which is crucial to both the speed and accuracy of the resulting decoder.', 'CCGBank( #AUTHOR_TAG ) is used to train the model .', 'For each training sentence, the corresponding CCGBank derivation together with all its sub-derivations are treated as gold-standard hypotheses.', 'All other hypotheses that can be constructed from the same bag of words are non-gold hypotheses.', '']","['now give an overview of the training algorithm, which is crucial to both the speed and accuracy of the resulting decoder.', 'CCGBank( #AUTHOR_TAG ) is used to train the model .', 'For each training sentence, the corresponding CCGBank derivation together with all its sub-derivations are treated as gold-standard hypotheses.', 'All other hypotheses that can be constructed from the same bag of words are non-gold hypotheses.', '']","['now give an overview of the training algorithm, which is crucial to both the speed and accuracy of the resulting decoder.', 'CCGBank( #AUTHOR_TAG ) is used to train the model .', 'For each training sentence, the corresponding CCGBank derivation together with all its sub-derivations are treated as gold-standard hypotheses.', 'All other hypotheses that can be constructed from the same bag of words are non-gold hypotheses.', '']","['now give an overview of the training algorithm, which is crucial to both the speed and accuracy of the resulting decoder.', 'CCGBank( #AUTHOR_TAG ) is used to train the model .', 'For each training sentence, the corresponding CCGBank derivation together with all its sub-derivations are treated as gold-standard hypotheses.', 'All other hypotheses that can be constructed from the same bag of words are non-gold hypotheses.', '']",5
"['', 'In our previous papers( #AUTHOR_TAG ; Zhang , Blackwood , and Clark 2012 ) , we applied a set of beams to this structure , which makes it similar to the data structure used for phrase-based MT decoding ( Koehn 2010']","['', 'In our previous papers( #AUTHOR_TAG ; Zhang , Blackwood , and Clark 2012 ) , we applied a set of beams to this structure , which makes it similar to the data structure used for phrase-based MT decoding ( Koehn 2010']","['', 'In our previous papers( #AUTHOR_TAG ; Zhang , Blackwood , and Clark 2012 ) , we applied a set of beams to this structure , which makes it similar to the data structure used for phrase-based MT decoding ( Koehn 2010']","['', 'In our previous papers( #AUTHOR_TAG ; Zhang , Blackwood , and Clark 2012 ) , we applied a set of beams to this structure , which makes it similar to the data structure used for phrase-based MT decoding ( Koehn 2010 ) .', '']",1
"['', 'This was done by MERT optimization( #AUTHOR_TAG ) towards post-edits under the TER target metric .', '']","['', 'This was done by MERT optimization( #AUTHOR_TAG ) towards post-edits under the TER target metric .', '']","['', 'This was done by MERT optimization( #AUTHOR_TAG ) towards post-edits under the TER target metric .', '']","['', 'This was done by MERT optimization( #AUTHOR_TAG ) towards post-edits under the TER target metric .', '']",5
"['ization for Online-to-Batch Conversion.', 'In practice , perceptron-type algorithms are often applied in a batch learning scenario , i.e. , the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set( Freund and Schapire , 1999 ; #AUTHOR_TAG ) .', 'The difference to the online learning scenario is that we treat the multi-epoch algorithm as an empirical risk minimizer that selects a final weight vector w T,K whose expected loss on un']","['ization for Online-to-Batch Conversion.', 'In practice , perceptron-type algorithms are often applied in a batch learning scenario , i.e. , the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set( Freund and Schapire , 1999 ; #AUTHOR_TAG ) .', 'The difference to the online learning scenario is that we treat the multi-epoch algorithm as an empirical risk minimizer that selects a final weight vector w T,K whose expected loss on unseen']","['ization for Online-to-Batch Conversion.', 'In practice , perceptron-type algorithms are often applied in a batch learning scenario , i.e. , the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set( Freund and Schapire , 1999 ; #AUTHOR_TAG ) .', 'The difference to the online learning scenario is that we treat the multi-epoch algorithm as an empirical risk minimizer that selects a final weight vector w T,K whose expected loss on un']","['ization for Online-to-Batch Conversion.', 'In practice , perceptron-type algorithms are often applied in a batch learning scenario , i.e. , the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set( Freund and Schapire , 1999 ; #AUTHOR_TAG ) .', 'The difference to the online learning scenario is that we treat the multi-epoch algorithm as an empirical risk minimizer that selects a final weight vector w T,K whose expected loss on unseen data we would like to bound.', '']",1
"['corpus used their own WMT10 system( Potet et al. , 2010 ) , based on the Moses phrase-based decoder( #AUTHOR_TAG ) with dense features .', '']","['corpus used their own WMT10 system( Potet et al. , 2010 ) , based on the Moses phrase-based decoder( #AUTHOR_TAG ) with dense features .', '']","['the corpus used their own WMT10 system( Potet et al. , 2010 ) , based on the Moses phrase-based decoder( #AUTHOR_TAG ) with dense features .', '']","['', 'To prepare SMT outputs for post-editing , the creators of the corpus used their own WMT10 system( Potet et al. , 2010 ) , based on the Moses phrase-based decoder( #AUTHOR_TAG ) with dense features .', '']",5
"['contrast , a single statistical model allows one to maintain a single table( #AUTHOR_TAG )']","['contrast , a single statistical model allows one to maintain a single table( #AUTHOR_TAG )']","['contrast , a single statistical model allows one to maintain a single table( #AUTHOR_TAG )']","['contrast , a single statistical model allows one to maintain a single table( #AUTHOR_TAG )']",0
['GEN uses standard chart generation techniques( #AUTHOR_TAG ) in its base generator to efficiently produce generation candidates'],['GEN uses standard chart generation techniques( #AUTHOR_TAG ) in its base generator to efficiently produce generation candidates'],['GEN uses standard chart generation techniques( #AUTHOR_TAG ) in its base generator to efficiently produce generation candidates'],['GEN uses standard chart generation techniques( #AUTHOR_TAG ) in its base generator to efficiently produce generation candidates'],0
"[""that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen , unless we know in advance all of the properties that can be attributed to a given object , as in the case of Jordan 's work on the COCONUT domain( #AUTHOR_TAG )""]","[""that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen , unless we know in advance all of the properties that can be attributed to a given object , as in the case of Jordan 's work on the COCONUT domain( #AUTHOR_TAG )""]","[""that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen , unless we know in advance all of the properties that can be attributed to a given object , as in the case of Jordan 's work on the COCONUT domain( #AUTHOR_TAG )""]","[""that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen , unless we know in advance all of the properties that can be attributed to a given object , as in the case of Jordan 's work on the COCONUT domain( #AUTHOR_TAG )""]",0
"['has been argued that generating such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of( Grosz and Sidner , 1986 ) )( #AUTHOR_TAG a )']","['has been argued that generating such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of( Grosz and Sidner , 1986 ) )( #AUTHOR_TAG a )']","['has been argued that generating such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of( Grosz and Sidner , 1986 ) )( #AUTHOR_TAG a )']","['has been argued that generating such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of( Grosz and Sidner , 1986 ) )( #AUTHOR_TAG a )']",0
"['WordNet , annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2Some descriptions of int modifiers can be found in( #AUTHOR_TAG b )']","['WordNet , annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2Some descriptions of int modifiers can be found in( #AUTHOR_TAG b )']","['WordNet , annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2Some descriptions of int modifiers can be found in( #AUTHOR_TAG b )']","['WordNet , annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2Some descriptions of int modifiers can be found in( #AUTHOR_TAG b )']",0
"[""addition to a referring function , noun phrases ( NP ) can also serve communicative goals such as providing new information about the referent and expressing the speaker 's emotional attitude towards the referent( Appelt , 1985 ; #AUTHOR_TAG )""]","[""addition to a referring function , noun phrases ( NP ) can also serve communicative goals such as providing new information about the referent and expressing the speaker 's emotional attitude towards the referent( Appelt , 1985 ; #AUTHOR_TAG )""]","[""addition to a referring function , noun phrases ( NP ) can also serve communicative goals such as providing new information about the referent and expressing the speaker 's emotional attitude towards the referent( Appelt , 1985 ; #AUTHOR_TAG )""]","[""addition to a referring function , noun phrases ( NP ) can also serve communicative goals such as providing new information about the referent and expressing the speaker 's emotional attitude towards the referent( Appelt , 1985 ; #AUTHOR_TAG )""]",0
"['what follows we explain the properties of the model by applying it to a small number of adjective-noun combinations taken from the lexical semantics literature.', 'Table 1 gives the interpretations of eight adjective-noun combinations discussed in#AUTHOR_TAG andVendler ( 1968 ) .', 'Table 2 shows the five most likely interpretations for these combinations as derived by the model discussed in the previous sections (v is the most likely interpretation, v ']","['what follows we explain the properties of the model by applying it to a small number of adjective-noun combinations taken from the lexical semantics literature.', 'Table 1 gives the interpretations of eight adjective-noun combinations discussed in#AUTHOR_TAG andVendler ( 1968 ) .', 'Table 2 shows the five most likely interpretations for these combinations as derived by the model discussed in the previous sections (v is the most likely interpretation, v 2']","['what follows we explain the properties of the model by applying it to a small number of adjective-noun combinations taken from the lexical semantics literature.', 'Table 1 gives the interpretations of eight adjective-noun combinations discussed in#AUTHOR_TAG andVendler ( 1968 ) .', 'Table 2 shows the five most likely interpretations for these combinations as derived by the model discussed in the previous sections (v is the most likely interpretation, v 2']","['what follows we explain the properties of the model by applying it to a small number of adjective-noun combinations taken from the lexical semantics literature.', 'Table 1 gives the interpretations of eight adjective-noun combinations discussed in#AUTHOR_TAG andVendler ( 1968 ) .', 'Table 2 shows the five most likely interpretations for these combinations as derived by the model discussed in the previous sections (v is the most likely interpretation, v 2 is the second most likely interpretation, etc']",5
"['recent work in lexical semantics has been concerned with accounting for regular polysemy, i.e., the regular and predictable sense alternations certain classes of words are subject to.', 'Adjectives , more than other categories , are a striking example of regular polysemy since they are able to take on different meanings depending on their context , viz. , the noun or noun class they modify ( see#AUTHOR_TAG and the references therein )']","['recent work in lexical semantics has been concerned with accounting for regular polysemy, i.e., the regular and predictable sense alternations certain classes of words are subject to.', 'Adjectives , more than other categories , are a striking example of regular polysemy since they are able to take on different meanings depending on their context , viz. , the noun or noun class they modify ( see#AUTHOR_TAG and the references therein )']","['recent work in lexical semantics has been concerned with accounting for regular polysemy, i.e., the regular and predictable sense alternations certain classes of words are subject to.', 'Adjectives , more than other categories , are a striking example of regular polysemy since they are able to take on different meanings depending on their context , viz. , the noun or noun class they modify ( see#AUTHOR_TAG and the references therein )']","['recent work in lexical semantics has been concerned with accounting for regular polysemy, i.e., the regular and predictable sense alternations certain classes of words are subject to.', 'Adjectives , more than other categories , are a striking example of regular polysemy since they are able to take on different meanings depending on their context , viz. , the noun or noun class they modify ( see#AUTHOR_TAG and the references therein )']",0
"['chose nine adjectives according to a set of minimal criteria and paired each adjective with 10 nouns randomly selected from the BNC.', 'We chose the adjectives as follows : we first compiled a list of all the polysemous adjectives mentioned in the lexical semantics literature( Vendler , 1968 ; #AUTHOR_TAG ) .', '']","['chose nine adjectives according to a set of minimal criteria and paired each adjective with 10 nouns randomly selected from the BNC.', 'We chose the adjectives as follows : we first compiled a list of all the polysemous adjectives mentioned in the lexical semantics literature( Vendler , 1968 ; #AUTHOR_TAG ) .', '']","['chose nine adjectives according to a set of minimal criteria and paired each adjective with 10 nouns randomly selected from the BNC.', 'We chose the adjectives as follows : we first compiled a list of all the polysemous adjectives mentioned in the lexical semantics literature( Vendler , 1968 ; #AUTHOR_TAG ) .', '']","['chose nine adjectives according to a set of minimal criteria and paired each adjective with 10 nouns randomly selected from the BNC.', 'We chose the adjectives as follows : we first compiled a list of all the polysemous adjectives mentioned in the lexical semantics literature( Vendler , 1968 ; #AUTHOR_TAG ) .', '']",5
"['.', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .', '']","['eat.', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .', '']","['.', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .', '']","['', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .', '']",0
"['have presented an ensemble approach to word sense disambiguation( #AUTHOR_TAG ) where multiple Naive Bayesian classifiers , each based on co -- occurrence features from varying sized windows of context , is shown to perform well on the widely studied nouns interest and line']","['have presented an ensemble approach to word sense disambiguation( #AUTHOR_TAG ) where multiple Naive Bayesian classifiers , each based on co -- occurrence features from varying sized windows of context , is shown to perform well on the widely studied nouns interest and line']","['have presented an ensemble approach to word sense disambiguation( #AUTHOR_TAG ) where multiple Naive Bayesian classifiers , each based on co -- occurrence features from varying sized windows of context , is shown to perform well on the widely studied nouns interest and line']","['have presented an ensemble approach to word sense disambiguation( #AUTHOR_TAG ) where multiple Naive Bayesian classifiers , each based on co -- occurrence features from varying sized windows of context , is shown to perform well on the widely studied nouns interest and line']",0
['( #AUTHOR_TAG ) for a discussion'],['( #AUTHOR_TAG ) for a discussion'],['( #AUTHOR_TAG ) for a discussion'],['( #AUTHOR_TAG ) for a discussion'],0
['definitions of predicates may be found in( #AUTHOR_TAG )'],['definitions of predicates may be found in( #AUTHOR_TAG )'],['definitions of predicates may be found in( #AUTHOR_TAG )'],['definitions of predicates may be found in( #AUTHOR_TAG )'],0
"['appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in#AUTHOR_TAG']","['appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in#AUTHOR_TAG']","['appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in#AUTHOR_TAG']","['appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in#AUTHOR_TAG']",1
['task we used to compare different generalisation techniques is similar to that used by#AUTHOR_TAG andRooth et al. ( 1999 )'],['task we used to compare different generalisation techniques is similar to that used by#AUTHOR_TAG andRooth et al. ( 1999 )'],['task we used to compare different generalisation techniques is similar to that used by#AUTHOR_TAG andRooth et al. ( 1999 )'],['task we used to compare different generalisation techniques is similar to that used by#AUTHOR_TAG andRooth et al. ( 1999 )'],1
"['problem with this approach is that any threshold is , to some extent , arbitrary , and there is evidence to suggest that , for some tasks , low counts are important( #AUTHOR_TAG )']","['problem with this approach is that any threshold is , to some extent , arbitrary , and there is evidence to suggest that , for some tasks , low counts are important( #AUTHOR_TAG )']","['problem with this approach is that any threshold is , to some extent , arbitrary , and there is evidence to suggest that , for some tasks , low counts are important( #AUTHOR_TAG )']","['problem with this approach is that any threshold is , to some extent , arbitrary , and there is evidence to suggest that , for some tasks , low counts are important( #AUTHOR_TAG )']",4
"[',#AUTHOR_TAG claims that the log-likelihood chisquared statistic ( G2 ) is more appropriate for corpus-based NLP']","[',#AUTHOR_TAG claims that the log-likelihood chisquared statistic ( G2 ) is more appropriate for corpus-based NLP']","[',#AUTHOR_TAG claims that the log-likelihood chisquared statistic ( G2 ) is more appropriate for corpus-based NLP']","[',#AUTHOR_TAG claims that the log-likelihood chisquared statistic ( G2 ) is more appropriate for corpus-based NLP']",4
['task we used to compare different generalisation techniques is similar to that used byPereira et al. ( 1993 ) and#AUTHOR_TAG'],['task we used to compare different generalisation techniques is similar to that used byPereira et al. ( 1993 ) and#AUTHOR_TAG'],['task we used to compare different generalisation techniques is similar to that used byPereira et al. ( 1993 ) and#AUTHOR_TAG'],['task we used to compare different generalisation techniques is similar to that used byPereira et al. ( 1993 ) and#AUTHOR_TAG'],1
"['X2 statistic is performing at least as well as G2 , throwing doubt on the claim by#AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP']","['X2 statistic is performing at least as well as G2 , throwing doubt on the claim by#AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP']","['X2 statistic is performing at least as well as G2 , throwing doubt on the claim by#AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP']","['X2 statistic is performing at least as well as G2 , throwing doubt on the claim by#AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP']",1
"['theory of verbal argument structure can be imple- mented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing.', ""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program( #AUTHOR_TAG ; Harkema , 2000 ; Niyogi , 2001 )""]","['theory of verbal argument structure can be imple- mented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing.', ""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program( #AUTHOR_TAG ; Harkema , 2000 ; Niyogi , 2001 )""]","['theory of verbal argument structure can be imple- mented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing.', ""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program( #AUTHOR_TAG ; Harkema , 2000 ; Niyogi , 2001 )""]","['theory of verbal argument structure can be imple- mented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing.', ""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program( #AUTHOR_TAG ; Harkema , 2000 ; Niyogi , 2001 )""]",1
"['observations and this line of reasoning has not escaped the attention of theoretical linguists :#AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .', 'They describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.', 'This present framework builds on the work of Hale and Keyser, but in addition to advancing a more refined theory of verbal argument structure, I also describe a computational implementation']","['observations and this line of reasoning has not escaped the attention of theoretical linguists :#AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .', 'They describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.', 'This present framework builds on the work of Hale and Keyser, but in addition to advancing a more refined theory of verbal argument structure, I also describe a computational implementation']","['observations and this line of reasoning has not escaped the attention of theoretical linguists :#AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .', 'They describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.', 'This present framework builds on the work of Hale and Keyser, but in addition to advancing a more refined theory of verbal argument structure, I also describe a computational implementation']","['observations and this line of reasoning has not escaped the attention of theoretical linguists :#AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .', 'They describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.', 'This present framework builds on the work of Hale and Keyser, but in addition to advancing a more refined theory of verbal argument structure, I also describe a computational implementation']",0
"['light verb v DO licenses an atelic non-inchoative event, and is compatible with verbal roots expressing activity.', 'It projects a functional head , voice( #AUTHOR_TAG ) , whose specifier is the external argument .', '']","['light verb v DO licenses an atelic non-inchoative event, and is compatible with verbal roots expressing activity.', 'It projects a functional head , voice( #AUTHOR_TAG ) , whose specifier is the external argument .', '']","['light verb v DO licenses an atelic non-inchoative event, and is compatible with verbal roots expressing activity.', 'It projects a functional head , voice( #AUTHOR_TAG ) , whose specifier is the external argument .', '']","['light verb v DO licenses an atelic non-inchoative event, and is compatible with verbal roots expressing activity.', 'It projects a functional head , voice( #AUTHOR_TAG ) , whose specifier is the external argument .', '']",0
"['theory of verbal argument structure can be imple- mented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing.', ""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program( Stabler , 1997 ; #AUTHOR_TAG ; Niyogi , 2001 )""]","['theory of verbal argument structure can be imple- mented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing.', ""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program( Stabler , 1997 ; #AUTHOR_TAG ; Niyogi , 2001 )""]","['theory of verbal argument structure can be imple- mented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing.', ""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program( Stabler , 1997 ; #AUTHOR_TAG ; Niyogi , 2001 )""]","['theory of verbal argument structure can be imple- mented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing.', ""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program( Stabler , 1997 ; #AUTHOR_TAG ; Niyogi , 2001 )""]",1
"['this paper , I present a computational implementation of Distributed Morphology( #AUTHOR_TAG ) , a non-lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation .', 'This framework leads to finer-grained semantics capable of better capturing linguistic generalizations']","['this paper , I present a computational implementation of Distributed Morphology( #AUTHOR_TAG ) , a non-lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation .', 'This framework leads to finer-grained semantics capable of better capturing linguistic generalizations']","['this paper , I present a computational implementation of Distributed Morphology( #AUTHOR_TAG ) , a non-lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation .', 'This framework leads to finer-grained semantics capable of better capturing linguistic generalizations']","['this paper , I present a computational implementation of Distributed Morphology( #AUTHOR_TAG ) , a non-lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation .', 'This framework leads to finer-grained semantics capable of better capturing linguistic generalizations']",5
"['.', ""This approach has its roots in Fillmore 'sCase Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet( Baker et al. , 1998 ) and PropBank( #AUTHOR_TAG )""]","['arguments.', ""This approach has its roots in Fillmore 'sCase Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet( Baker et al. , 1998 ) and PropBank( #AUTHOR_TAG )""]","['.', ""This approach has its roots in Fillmore 'sCase Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet( Baker et al. , 1998 ) and PropBank( #AUTHOR_TAG )""]","['common lexical semantic representation in the computational linguistics literature is a frame-based model where syntactic arguments are associated with various semantic roles (essentially frame slots).', 'Verbs are viewed as simple predicates over their arguments.', ""This approach has its roots in Fillmore 'sCase Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet( Baker et al. , 1998 ) and PropBank( #AUTHOR_TAG )""]",0
"['( #AUTHOR_TAG ) , I present evidence from Mandarin Chinese that this analysis is on the right track .', 'The rest of this paper, however, will be concerned with the computational implementation of my theoretical framework']","['( #AUTHOR_TAG ) , I present evidence from Mandarin Chinese that this analysis is on the right track .', 'The rest of this paper, however, will be concerned with the computational implementation of my theoretical framework']","['( #AUTHOR_TAG ) , I present evidence from Mandarin Chinese that this analysis is on the right track .', 'The rest of this paper, however, will be concerned with the computational implementation of my theoretical framework']","['( #AUTHOR_TAG ) , I present evidence from Mandarin Chinese that this analysis is on the right track .', 'The rest of this paper, however, will be concerned with the computational implementation of my theoretical framework']",2
"['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'Due to advances in statistical syntactic parsing techniques( Collins , 1997 ; #AUTHOR_TAG ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences']","['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'Due to advances in statistical syntactic parsing techniques( Collins , 1997 ; #AUTHOR_TAG ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences']","['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'Due to advances in statistical syntactic parsing techniques( Collins , 1997 ; #AUTHOR_TAG ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences']","['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'Due to advances in statistical syntactic parsing techniques( Collins , 1997 ; #AUTHOR_TAG ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences']",0
"['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity( Dowty , 1979 ; Jackendoff , 1983 ; #AUTHOR_TAG b ;Rappaport Hovav and Levin , 1998 ) .', 'Consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity( Dowty , 1979 ; Jackendoff , 1983 ; #AUTHOR_TAG b ;Rappaport Hovav and Levin , 1998 ) .', 'Consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity( Dowty , 1979 ; Jackendoff , 1983 ; #AUTHOR_TAG b ;Rappaport Hovav and Levin , 1998 ) .', 'Consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity( Dowty , 1979 ; Jackendoff , 1983 ; #AUTHOR_TAG b ;Rappaport Hovav and Levin , 1998 ) .', 'Consider the following example']",0
"['a minimal set of features and a small number of lexical entries ,Niyogi ( 2001 ) has successfully modeled many of the argument alternations described byLevin ( 1993 ) using a#AUTHOR_TAG style analysis']","['a minimal set of features and a small number of lexical entries ,Niyogi ( 2001 ) has successfully modeled many of the argument alternations described byLevin ( 1993 ) using a#AUTHOR_TAG style analysis']","['a minimal set of features and a small number of lexical entries ,Niyogi ( 2001 ) has successfully modeled many of the argument alternations described byLevin ( 1993 ) using a#AUTHOR_TAG style analysis .', '']","['', 'With a minimal set of features and a small number of lexical entries ,Niyogi ( 2001 ) has successfully modeled many of the argument alternations described byLevin ( 1993 ) using a#AUTHOR_TAG style analysis .', '']",0
"['a period of time.', '#AUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration , which is often a question of granularity']","['a period of time.', '#AUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration , which is often a question of granularity']","['a period of time.', '#AUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration , which is often a question of granularity']","[""ities know run believe walk Accomplishments Achievements paint a picture recognize make a chair find Under Vendler's classification, activities and states both depict situations that are inherently temporally unbounded (atelic); states denote static situations, whereas activities denote on-going dynamic situations."", 'Accomplishments and achievements both express a change of state, and hence are temporally bounded (telic); achievements are punctual, whereas accomplishments extend over a period of time.', '#AUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration , which is often a question of granularity']",0
"['', 'With a minimal set of features and a small number of lexical entries ,Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by#AUTHOR_TAG using aHale and Keyser ( 1993 ) style analysis .', '']","['', 'With a minimal set of features and a small number of lexical entries ,Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by#AUTHOR_TAG using aHale and Keyser ( 1993 ) style analysis .', '']","['', 'With a minimal set of features and a small number of lexical entries ,Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by#AUTHOR_TAG using aHale and Keyser ( 1993 ) style analysis .', '']","['', 'With a minimal set of features and a small number of lexical entries ,Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by#AUTHOR_TAG using aHale and Keyser ( 1993 ) style analysis .', '']",0
"['', ""This framework , where the `` semantic load '' is spread more evenly throughout the lexicon to lexical categories not typically thought to bear semantic content , is essentially the model advocated by#AUTHOR_TAG a ) , among many others ."", 'Note that such an approach is no longer lexicalist: each lexical']","['', ""This framework , where the `` semantic load '' is spread more evenly throughout the lexicon to lexical categories not typically thought to bear semantic content , is essentially the model advocated by#AUTHOR_TAG a ) , among many others ."", 'Note that such an approach is no longer lexicalist: each lexical']","['', ""This framework , where the `` semantic load '' is spread more evenly throughout the lexicon to lexical categories not typically thought to bear semantic content , is essentially the model advocated by#AUTHOR_TAG a ) , among many others ."", 'Note that such an approach is no longer lexicalist: each lexical item']","['', ""This framework , where the `` semantic load '' is spread more evenly throughout the lexicon to lexical categories not typically thought to bear semantic content , is essentially the model advocated by#AUTHOR_TAG a ) , among many others ."", 'Note that such an approach is no longer lexicalist: each lexical item does not fully encode its associated syntactic and semantic structures.', '']",0
"['theory of verbal argument structure can be imple- mented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing.', ""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program( Stabler , 1997 ; Harkema , 2000 ; #AUTHOR_TAG )""]","['theory of verbal argument structure can be imple- mented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing.', ""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program( Stabler , 1997 ; Harkema , 2000 ; #AUTHOR_TAG )""]","['theory of verbal argument structure can be imple- mented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing.', ""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program( Stabler , 1997 ; Harkema , 2000 ; #AUTHOR_TAG )""]","['theory of verbal argument structure can be imple- mented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing.', ""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program( Stabler , 1997 ; Harkema , 2000 ; #AUTHOR_TAG )""]",1
"['the non-lexicalist tradition, these primitives are argued to occupy functional projections in the syntactic structure, as so-called light verbs.', 'Here , I adopt the model proposed by#AUTHOR_TAG and decompose lexical verbs into verbalizing heads and verbal roots .', '']","['the non-lexicalist tradition, these primitives are argued to occupy functional projections in the syntactic structure, as so-called light verbs.', 'Here , I adopt the model proposed by#AUTHOR_TAG and decompose lexical verbs into verbalizing heads and verbal roots .', '']","['the non-lexicalist tradition, these primitives are argued to occupy functional projections in the syntactic structure, as so-called light verbs.', 'Here , I adopt the model proposed by#AUTHOR_TAG and decompose lexical verbs into verbalizing heads and verbal roots .', '']","['the non-lexicalist tradition, these primitives are argued to occupy functional projections in the syntactic structure, as so-called light verbs.', 'Here , I adopt the model proposed by#AUTHOR_TAG and decompose lexical verbs into verbalizing heads and verbal roots .', 'Verbalizing heads introduce relevant eventive interpretations in the syntax, and correspond to (assumed) universal primitives of the human cognitive system.', 'On the other hand, verbal roots represent abstract (categoryless) concepts and basically correspond to open-class items drawn from encyclopedic knowledge.', '']",5
"['.', ""This approach has its roots in Fillmore 'sCase Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet( #AUTHOR_TAG ) and PropBank( Kingsbury et al. , 2002 )""]","['arguments.', ""This approach has its roots in Fillmore 'sCase Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet( #AUTHOR_TAG ) and PropBank( Kingsbury et al. , 2002 )""]","['.', ""This approach has its roots in Fillmore 'sCase Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet( #AUTHOR_TAG ) and PropBank( Kingsbury et al. , 2002 )""]","['common lexical semantic representation in the computational linguistics literature is a frame-based model where syntactic arguments are associated with various semantic roles (essentially frame slots).', 'Verbs are viewed as simple predicates over their arguments.', ""This approach has its roots in Fillmore 'sCase Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet( #AUTHOR_TAG ) and PropBank( Kingsbury et al. , 2002 )""]",0
"['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity( #AUTHOR_TAG ; Jackendoff , 1983 ; Pustejovsky , 1991 b ;Rappaport Hovav and Levin , 1998 ) .', 'Consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity( #AUTHOR_TAG ; Jackendoff , 1983 ; Pustejovsky , 1991 b ;Rappaport Hovav and Levin , 1998 ) .', 'Consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity( #AUTHOR_TAG ; Jackendoff , 1983 ; Pustejovsky , 1991 b ;Rappaport Hovav and Levin , 1998 ) .', 'Consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity( #AUTHOR_TAG ; Jackendoff , 1983 ; Pustejovsky , 1991 b ;Rappaport Hovav and Levin , 1998 ) .', 'Consider the following example']",0
"['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'Due to advances in statistical syntactic parsing techniques( #AUTHOR_TAG ; Charniak , 2001 ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences']","['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'Due to advances in statistical syntactic parsing techniques( #AUTHOR_TAG ; Charniak , 2001 ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences']","['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'Due to advances in statistical syntactic parsing techniques( #AUTHOR_TAG ; Charniak , 2001 ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences']","['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'Due to advances in statistical syntactic parsing techniques( #AUTHOR_TAG ; Charniak , 2001 ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences']",0
"['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity( Dowty , 1979 ; #AUTHOR_TAG ; Pustejovsky , 1991 b ;Rappaport Hovav and Levin , 1998 ) .', 'Consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity( Dowty , 1979 ; #AUTHOR_TAG ; Pustejovsky , 1991 b ;Rappaport Hovav and Levin , 1998 ) .', 'Consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity( Dowty , 1979 ; #AUTHOR_TAG ; Pustejovsky , 1991 b ;Rappaport Hovav and Levin , 1998 ) .', 'Consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity( Dowty , 1979 ; #AUTHOR_TAG ; Pustejovsky , 1991 b ;Rappaport Hovav and Levin , 1998 ) .', 'Consider the following example']",0
"['has developed an agenda-driven chart parser for the feature-driven formalism described above ; please refer to his paper for a description of the parsing algorithm .', 'I have adapted it for my needs and developed grammar fragments that reflect my non-lexicalist semantic framework']","['has developed an agenda-driven chart parser for the feature-driven formalism described above ; please refer to his paper for a description of the parsing algorithm .', 'I have adapted it for my needs and developed grammar fragments that reflect my non-lexicalist semantic framework.', '']","['has developed an agenda-driven chart parser for the feature-driven formalism described above ; please refer to his paper for a description of the parsing algorithm .', 'I have adapted it for my needs and developed grammar fragments that reflect my non-lexicalist semantic framework.', '']","['has developed an agenda-driven chart parser for the feature-driven formalism described above ; please refer to his paper for a description of the parsing algorithm .', 'I have adapted it for my needs and developed grammar fragments that reflect my non-lexicalist semantic framework.', 'As an example, a simplified derivation of the sentence ""The tire flattened."" is shown in Figure 1']",2
"['typical solution to the redundancy problem is to group verbs according to their argument realization patterns( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .', 'The argument structure and syntax-tosemantics mapping would then only need to be specified once for each verb class.', '']","['typical solution to the redundancy problem is to group verbs according to their argument realization patterns( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .', 'The argument structure and syntax-tosemantics mapping would then only need to be specified once for each verb class.', '']","['typical solution to the redundancy problem is to group verbs according to their argument realization patterns( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .', 'The argument structure and syntax-tosemantics mapping would then only need to be specified once for each verb class.', '']","['typical solution to the redundancy problem is to group verbs according to their argument realization patterns( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .', 'The argument structure and syntax-tosemantics mapping would then only need to be specified once for each verb class.', '']",1
"['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991 b ; Rappaport#AUTHOR_TAG ) .', 'Consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991 b ; Rappaport#AUTHOR_TAG ) .', 'Consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991 b ; Rappaport#AUTHOR_TAG ) .', 'Consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991 b ; Rappaport#AUTHOR_TAG ) .', 'Consider the following example']",0
"['ty breaks the event described by (2) into two subevents, the activity of sweeping the floor and its result, the state of the floor being clean.', ""A more recent approach , advocated byRappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment""]","['ty breaks the event described by (2) into two subevents, the activity of sweeping the floor and its result, the state of the floor being clean.', ""A more recent approach , advocated byRappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment""]","['ty breaks the event described by (2) into two subevents, the activity of sweeping the floor and its result, the state of the floor being clean.', ""A more recent approach , advocated byRappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment""]","['ty breaks the event described by (2) into two subevents, the activity of sweeping the floor and its result, the state of the floor being clean.', ""A more recent approach , advocated byRappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment""]",0
"['', '#AUTHOR_TAG b ) andTopkara et al. ( 2006 a ) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method']","['surface structure format via language generation tools.', '#AUTHOR_TAG b ) andTopkara et al. ( 2006 a ) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method']","['the surface structure format via language generation tools.', '#AUTHOR_TAG b ) andTopkara et al. ( 2006 a ) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method']","['', '#AUTHOR_TAG b ) andTopkara et al. ( 2006 a ) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method']",0
"['simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by#AUTHOR_TAG .', 'Later works, such asAtallah et al. (2001 a),Bolshakov (2004) ,Taskiran et al. (2006  andTopkara et al. (2006 b), further made use of part-ofspeech taggers and electronic dictionaries, such as WordNet and VerbNet, to increase the robustness of the method.', '']","['simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by#AUTHOR_TAG .', 'Later works, such asAtallah et al. (2001 a),Bolshakov (2004) ,Taskiran et al. (2006  andTopkara et al. (2006 b), further made use of part-ofspeech taggers and electronic dictionaries, such as WordNet and VerbNet, to increase the robustness of the method.', '']","['simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by#AUTHOR_TAG .', 'Later works, such asAtallah et al. (2001 a),Bolshakov (2004) ,Taskiran et al. (2006  andTopkara et al. (2006 b), further made use of part-ofspeech taggers and electronic dictionaries, such as WordNet and VerbNet, to increase the robustness of the method.', '']","['simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by#AUTHOR_TAG .', 'Later works, such asAtallah et al. (2001 a),Bolshakov (2004) ,Taskiran et al. (2006  andTopkara et al. (2006 b), further made use of part-ofspeech taggers and electronic dictionaries, such as WordNet and VerbNet, to increase the robustness of the method.', '']",0
"['', 'Liu et al. ( 2005 ) ,Meral et al. ( 2007 ) ,Murphy ( 2001 ) ,#AUTHOR_TAG andTopkara et al. ( 2006 a ) all belong to the syntactic transformation category .', '']","['', 'Liu et al. ( 2005 ) ,Meral et al. ( 2007 ) ,Murphy ( 2001 ) ,#AUTHOR_TAG andTopkara et al. ( 2006 a ) all belong to the syntactic transformation category .', '']","['', 'Liu et al. ( 2005 ) ,Meral et al. ( 2007 ) ,Murphy ( 2001 ) ,#AUTHOR_TAG andTopkara et al. ( 2006 a ) all belong to the syntactic transformation category .', '']","['', 'Liu et al. ( 2005 ) ,Meral et al. ( 2007 ) ,Murphy ( 2001 ) ,#AUTHOR_TAG andTopkara et al. ( 2006 a ) all belong to the syntactic transformation category .', '']",0
"['', 'The first lexical substitution method was proposed byChapman and Davida (1997) .', 'Later works , such as#AUTHOR_TAG a ) ,Bolshakov ( 2004 ) ,Taskiran et al. ( 2006 ) andTopkara et al. ( 2006 b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', '']","['', 'The first lexical substitution method was proposed byChapman and Davida (1997) .', 'Later works , such as#AUTHOR_TAG a ) ,Bolshakov ( 2004 ) ,Taskiran et al. ( 2006 ) andTopkara et al. ( 2006 b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', '']","['', 'The first lexical substitution method was proposed byChapman and Davida (1997) .', 'Later works , such as#AUTHOR_TAG a ) ,Bolshakov ( 2004 ) ,Taskiran et al. ( 2006 ) andTopkara et al. ( 2006 b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', '']","['', 'The first lexical substitution method was proposed byChapman and Davida (1997) .', 'Later works , such as#AUTHOR_TAG a ) ,Bolshakov ( 2004 ) ,Taskiran et al. ( 2006 ) andTopkara et al. ( 2006 b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', '']",0
"['', 'The first lexical substitution method was proposed byChapman and Davida (1997) .', 'Later works , such asAtallah et al. ( 2001 a ) ,Bolshakov ( 2004 ) ,Taskiran et al. ( 2006 ) and#AUTHOR_TAG b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', '']","['', 'The first lexical substitution method was proposed byChapman and Davida (1997) .', 'Later works , such asAtallah et al. ( 2001 a ) ,Bolshakov ( 2004 ) ,Taskiran et al. ( 2006 ) and#AUTHOR_TAG b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', '']","['', 'The first lexical substitution method was proposed byChapman and Davida (1997) .', 'Later works , such asAtallah et al. ( 2001 a ) ,Bolshakov ( 2004 ) ,Taskiran et al. ( 2006 ) and#AUTHOR_TAG b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', '']","['', 'The first lexical substitution method was proposed byChapman and Davida (1997) .', 'Later works , such asAtallah et al. ( 2001 a ) ,Bolshakov ( 2004 ) ,Taskiran et al. ( 2006 ) and#AUTHOR_TAG b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', '']",0
"['', 'Liu et al. ( 2005 ) ,Meral et al. ( 2007 ) ,#AUTHOR_TAG ,Murphy and Vogel ( 2007 ) andTopkara et al. ( 2006 a ) all belong to the syntactic transformation category .', '']","['', 'Liu et al. ( 2005 ) ,Meral et al. ( 2007 ) ,#AUTHOR_TAG ,Murphy and Vogel ( 2007 ) andTopkara et al. ( 2006 a ) all belong to the syntactic transformation category .', '']","['', 'Liu et al. ( 2005 ) ,Meral et al. ( 2007 ) ,#AUTHOR_TAG ,Murphy and Vogel ( 2007 ) andTopkara et al. ( 2006 a ) all belong to the syntactic transformation category .', '']","['', 'Liu et al. ( 2005 ) ,Meral et al. ( 2007 ) ,#AUTHOR_TAG ,Murphy and Vogel ( 2007 ) andTopkara et al. ( 2006 a ) all belong to the syntactic transformation category .', '']",0
"['', 'The first lexical substitution method was proposed byChapman and Davida (1997) .', 'Later works , such asAtallah et al. ( 2001 a ) ,#AUTHOR_TAG ,Taskiran et al. ( 2006 ) andTopkara et al. ( 2006 b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', '']","['', 'The first lexical substitution method was proposed byChapman and Davida (1997) .', 'Later works , such asAtallah et al. ( 2001 a ) ,#AUTHOR_TAG ,Taskiran et al. ( 2006 ) andTopkara et al. ( 2006 b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', '']","['', 'The first lexical substitution method was proposed byChapman and Davida (1997) .', 'Later works , such asAtallah et al. ( 2001 a ) ,#AUTHOR_TAG ,Taskiran et al. ( 2006 ) andTopkara et al. ( 2006 b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', '']","['', 'The first lexical substitution method was proposed byChapman and Davida (1997) .', 'Later works , such asAtallah et al. ( 2001 a ) ,#AUTHOR_TAG ,Taskiran et al. ( 2006 ) andTopkara et al. ( 2006 b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', '']",0
"['Google n-gram data was collected by Google Research for statistical language modelling , and has been used for many tasks such as lexical disambiguation( #AUTHOR_TAG ) , and contains English n-grams and their observed frequency counts , for counts of at least 40 .', 'The striking feature of the n-gram corpus is']","['Google n-gram data was collected by Google Research for statistical language modelling , and has been used for many tasks such as lexical disambiguation( #AUTHOR_TAG ) , and contains English n-grams and their observed frequency counts , for counts of at least 40 .', 'The striking feature of the n-gram corpus is']","['Google n-gram data was collected by Google Research for statistical language modelling , and has been used for many tasks such as lexical disambiguation( #AUTHOR_TAG ) , and contains English n-grams and their observed frequency counts , for counts of at least 40 .', 'The striking feature of the n-gram corpus is']","['Google n-gram data was collected by Google Research for statistical language modelling , and has been used for many tasks such as lexical disambiguation( #AUTHOR_TAG ) , and contains English n-grams and their observed frequency counts , for counts of at least 40 .', '']",0
"['ganography is concerned with hiding information in some cover medium , by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer( #AUTHOR_TAG ) .', 'The']","['ganography is concerned with hiding information in some cover medium , by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer( #AUTHOR_TAG ) .', 'The']","['ganography is concerned with hiding information in some cover medium , by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer( #AUTHOR_TAG ) .', '']","['ganography is concerned with hiding information in some cover medium , by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer( #AUTHOR_TAG ) .', '']",0
"['', 'Liu et al. ( 2005 ) ,Meral et al. ( 2007 ) ,Murphy ( 2001 ) ,Murphy and Vogel ( 2007 ) and#AUTHOR_TAG a ) all belong to the syntactic transformation category .', '']","['', 'Liu et al. ( 2005 ) ,Meral et al. ( 2007 ) ,Murphy ( 2001 ) ,Murphy and Vogel ( 2007 ) and#AUTHOR_TAG a ) all belong to the syntactic transformation category .', '']","['', 'Liu et al. ( 2005 ) ,Meral et al. ( 2007 ) ,Murphy ( 2001 ) ,Murphy and Vogel ( 2007 ) and#AUTHOR_TAG a ) all belong to the syntactic transformation category .', '']","['', 'Liu et al. ( 2005 ) ,Meral et al. ( 2007 ) ,Murphy ( 2001 ) ,Murphy and Vogel ( 2007 ) and#AUTHOR_TAG a ) all belong to the syntactic transformation category .', '']",0
"['', '#AUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition , with the idea that some presuppositional information can be removed without changing the meaning of a sentence']","['', '#AUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition , with the idea that some presuppositional information can be removed without changing the meaning of a sentence']","['', '#AUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition , with the idea that some presuppositional information can be removed without changing the meaning of a sentence']","['', '#AUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition , with the idea that some presuppositional information can be removed without changing the meaning of a sentence']",0
"['chart compares the gold price at the end of last year with the end of this year.', '#AUTHOR_TAG also note that the applicability of paraphrases is strongly influenced by context .', 'Section 4 describes']","['chart compares the gold price at the end of last year with the end of this year.', '#AUTHOR_TAG also note that the applicability of paraphrases is strongly influenced by context .', 'Section 4 describes']","['The chart compares the gold price at the end of last year with the end of this year.', '#AUTHOR_TAG also note that the applicability of paraphrases is strongly influenced by context .', 'Section 4 describes']","['', '#AUTHOR_TAG also note that the applicability of paraphrases is strongly influenced by context .', '']",0
"['', 'The paraphrase dictionary that we use was generated for us by Chris Callison-Burch , using the technique described in#AUTHOR_TAG , which exploits a parallel corpus and methods developed for statistical machine translation']","['', 'The paraphrase dictionary that we use was generated for us by Chris Callison-Burch , using the technique described in#AUTHOR_TAG , which exploits a parallel corpus and methods developed for statistical machine translation']","['', 'The paraphrase dictionary that we use was generated for us by Chris Callison-Burch , using the technique described in#AUTHOR_TAG , which exploits a parallel corpus and methods developed for statistical machine translation']","['', 'The paraphrase dictionary that we use was generated for us by Chris Callison-Burch , using the technique described in#AUTHOR_TAG , which exploits a parallel corpus and methods developed for statistical machine translation']",5
"['the secret message is embedded into syntactic parse trees of the sentences.', '#AUTHOR_TAG ,Meral et al. ( 2007 ) ,Murphy ( 2001 ) ,Murphy and Vogel ( 2007 ) andTopkara et al. ( 2006 a ) all belong to the syntactic transformation category .', '']","['the secret message is embedded into syntactic parse trees of the sentences.', '#AUTHOR_TAG ,Meral et al. ( 2007 ) ,Murphy ( 2001 ) ,Murphy and Vogel ( 2007 ) andTopkara et al. ( 2006 a ) all belong to the syntactic transformation category .', '']","['the secret message is embedded into syntactic parse trees of the sentences.', '#AUTHOR_TAG ,Meral et al. ( 2007 ) ,Murphy ( 2001 ) ,Murphy and Vogel ( 2007 ) andTopkara et al. ( 2006 a ) all belong to the syntactic transformation category .', '']","['', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', '#AUTHOR_TAG ,Meral et al. ( 2007 ) ,Murphy ( 2001 ) ,Murphy and Vogel ( 2007 ) andTopkara et al. ( 2006 a ) all belong to the syntactic transformation category .', '']",0
"['ography.', 'Note that we are concerned with transformations which are linguistic in nature , rather than dealing with superficial properties of the text , e.g. the amount of white space between words( #AUTHOR_TAG ) .', 'Our proposed']","['transformations used in Linguistic Steganography.', 'Note that we are concerned with transformations which are linguistic in nature , rather than dealing with superficial properties of the text , e.g. the amount of white space between words( #AUTHOR_TAG ) .', 'Our proposed']","['the previous transformations used in Linguistic Steganography.', 'Note that we are concerned with transformations which are linguistic in nature , rather than dealing with superficial properties of the text , e.g. the amount of white space between words( #AUTHOR_TAG ) .', 'Our proposed']","['', 'Note that we are concerned with transformations which are linguistic in nature , rather than dealing with superficial properties of the text , e.g. the amount of white space between words( #AUTHOR_TAG ) .', '']",1
"['', 'Our proposed method is based on the automatically acquired paraphrase dictionary described in#AUTHOR_TAG , in which the application of paraphrases from the dictionary encodes secret bits .', 'One advantage of the dictionary is that it has wide coverage, being automatically extracted; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts.', '']","['', 'Our proposed method is based on the automatically acquired paraphrase dictionary described in#AUTHOR_TAG , in which the application of paraphrases from the dictionary encodes secret bits .', 'One advantage of the dictionary is that it has wide coverage, being automatically extracted; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts.', '']","['', 'Our proposed method is based on the automatically acquired paraphrase dictionary described in#AUTHOR_TAG , in which the application of paraphrases from the dictionary encodes secret bits .', 'One advantage of the dictionary is that it has wide coverage, being automatically extracted; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts.', '']","['', 'Our proposed method is based on the automatically acquired paraphrase dictionary described in#AUTHOR_TAG , in which the application of paraphrases from the dictionary encodes secret bits .', 'One advantage of the dictionary is that it has wide coverage, being automatically extracted; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts.', '']",5
"['text is an obvious medium to consider.', 'However , the literature on Linguistic Steganography , in which linguistic properties of a text are modified to hide information , is small compared with other media( #AUTHOR_TAG ) .', 'The likely reason is that it is easier to make changes to images and other nonlinguistic media which are undetectable by an observer.', 'Language has the']","['text is an obvious medium to consider.', 'However , the literature on Linguistic Steganography , in which linguistic properties of a text are modified to hide information , is small compared with other media( #AUTHOR_TAG ) .', 'The likely reason is that it is easier to make changes to images and other nonlinguistic media which are undetectable by an observer.', 'Language has the']","['is an obvious medium to consider.', 'However , the literature on Linguistic Steganography , in which linguistic properties of a text are modified to hide information , is small compared with other media( #AUTHOR_TAG ) .', 'The likely reason is that it is easier to make changes to images and other nonlinguistic media which are undetectable by an observer.', 'Language has the property']","['', 'However , the literature on Linguistic Steganography , in which linguistic properties of a text are modified to hide information , is small compared with other media( #AUTHOR_TAG ) .', 'The likely reason is that it is easier to make changes to images and other nonlinguistic media which are undetectable by an observer.', '']",0
"['to improve the grammaticality checking, we use a parser as an addition to the basic Google ngram method.', 'We use the#AUTHOR_TAG CCG parser to analyse the sentence before and after paraphrasing .', '']","['to improve the grammaticality checking, we use a parser as an addition to the basic Google ngram method.', 'We use the#AUTHOR_TAG CCG parser to analyse the sentence before and after paraphrasing .', '']","['to improve the grammaticality checking, we use a parser as an addition to the basic Google ngram method.', 'We use the#AUTHOR_TAG CCG parser to analyse the sentence before and after paraphrasing .', '']","['order to improve the grammaticality checking, we use a parser as an addition to the basic Google ngram method.', 'We use the#AUTHOR_TAG CCG parser to analyse the sentence before and after paraphrasing .', '']",5
"['', 'Liu et al. ( 2005 ) ,#AUTHOR_TAG ,Murphy ( 2001 ) ,Murphy and Vogel ( 2007 ) andTopkara et al. ( 2006 a ) all belong to the syntactic transformation category .', '']","['', 'Liu et al. ( 2005 ) ,#AUTHOR_TAG ,Murphy ( 2001 ) ,Murphy and Vogel ( 2007 ) andTopkara et al. ( 2006 a ) all belong to the syntactic transformation category .', '']","['', 'Liu et al. ( 2005 ) ,#AUTHOR_TAG ,Murphy ( 2001 ) ,Murphy and Vogel ( 2007 ) andTopkara et al. ( 2006 a ) all belong to the syntactic transformation category .', '']","['', 'Liu et al. ( 2005 ) ,#AUTHOR_TAG ,Murphy ( 2001 ) ,Murphy and Vogel ( 2007 ) andTopkara et al. ( 2006 a ) all belong to the syntactic transformation category .', '']",0
"['our previous work( #AUTHOR_TAG ; Salloum and Habash , 2012 ) , we applied our approach to tokenized Arabic and our DA-MSA transfer component used feature transfer rules only .', 'We']","['our previous work( #AUTHOR_TAG ; Salloum and Habash , 2012 ) , we applied our approach to tokenized Arabic and our DA-MSA transfer component used feature transfer rules only .', 'We']","['our previous work( #AUTHOR_TAG ; Salloum and Habash , 2012 ) , we applied our approach to tokenized Arabic and our DA-MSA transfer component used feature transfer rules only .', '']","['our previous work( #AUTHOR_TAG ; Salloum and Habash , 2012 ) , we applied our approach to tokenized Arabic and our DA-MSA transfer component used feature transfer rules only .', 'We did not use a language model to pick the best path; instead we kept the ambiguity in the lattice and passed it to our SMT system.', '']",1
"['DA data.', 'Our system uses a standard phrase-based architecture.', 'The parallel corpus is word-aligned using GIZA + +( #AUTHOR_TAG ) .', 'Phrase translations']","['DA data.', 'Our system uses a standard phrase-based architecture.', 'The parallel corpus is word-aligned using GIZA + +( #AUTHOR_TAG ) .', 'Phrase translations']","['several LDC corpora including some limited DA data.', 'Our system uses a standard phrase-based architecture.', 'The parallel corpus is word-aligned using GIZA + +( #AUTHOR_TAG ) .', 'Phrase translations']","['use the open-source Moses toolkit(Koehn et al., 2007) to build a phrase-based SMT system trained on mostly MSA data (64M words on the Arabic side) obtained from several LDC corpora including some limited DA data.', 'Our system uses a standard phrase-based architecture.', 'The parallel corpus is word-aligned using GIZA + +( #AUTHOR_TAG ) .', 'Phrase translations of up to 10 words are extracted in the Moses phrase table.', '']",5
"['the English translation.', 'This is a similar conclusion to our previous work in#AUTHOR_TAG']","['the English translation.', 'This is a similar conclusion to our previous work in#AUTHOR_TAG']","['the English translation.', 'This is a similar conclusion to our previous work in#AUTHOR_TAG']","['', 'This is a similar conclusion to our previous work in#AUTHOR_TAG']",1
"['use the open-source Moses toolkit( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .', '']","['use the open-source Moses toolkit( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .', '']","['use the open-source Moses toolkit( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .', '']","['use the open-source Moses toolkit( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .', '']",5
"['.', 'Few approaches to parsing have tried to handle disfluent utterances ( notable exceptions areCore & Schubert , 1999 ; #AUTHOR_TAG ; Nakatani & Hirschberg , 1994 ;  Shriberg , Bear , &Dowding , 1992 )']","['', 'Few approaches to parsing have tried to handle disfluent utterances ( notable exceptions areCore & Schubert , 1999 ; #AUTHOR_TAG ; Nakatani & Hirschberg , 1994 ;  Shriberg , Bear , &Dowding , 1992 )']","['', 'Few approaches to parsing have tried to handle disfluent utterances ( notable exceptions areCore & Schubert , 1999 ; #AUTHOR_TAG ; Nakatani & Hirschberg , 1994 ;  Shriberg , Bear , &Dowding , 1992 )']","['', 'Few approaches to parsing have tried to handle disfluent utterances ( notable exceptions areCore & Schubert , 1999 ; #AUTHOR_TAG ; Nakatani & Hirschberg , 1994 ;  Shriberg , Bear , &Dowding , 1992 )']",0
"['to our previous work( Chan and Ng , 2005 b ) , we used the supervised WSD approach described in( #AUTHOR_TAG ) for our experiments , using the naive Bayes algorithm as our classifier .', '']","['to our previous work( Chan and Ng , 2005 b ) , we used the supervised WSD approach described in( #AUTHOR_TAG ) for our experiments , using the naive Bayes algorithm as our classifier .', '']","['to our previous work( Chan and Ng , 2005 b ) , we used the supervised WSD approach described in( #AUTHOR_TAG ) for our experiments , using the naive Bayes algorithm as our classifier .', '']","['to our previous work( Chan and Ng , 2005 b ) , we used the supervised WSD approach described in( #AUTHOR_TAG ) for our experiments , using the naive Bayes algorithm as our classifier .', '']",5
"['', 'We chose to follow#AUTHOR_TAG and split the sentences evenly to facilitate further comparison']","['', 'We chose to follow#AUTHOR_TAG and split the sentences evenly to facilitate further comparison']","['', 'We chose to follow#AUTHOR_TAG and split the sentences evenly to facilitate further comparison']","['', 'We chose to follow#AUTHOR_TAG and split the sentences evenly to facilitate further comparison']",5
"['built a two-stage baseline system , using the perceptron segmentation model from our previous work( #AUTHOR_TAG ) and the perceptron POS tagging model fromCollins ( 2002 ) .', '']","['built a two-stage baseline system , using the perceptron segmentation model from our previous work( #AUTHOR_TAG ) and the perceptron POS tagging model fromCollins ( 2002 ) .', '']","['built a two-stage baseline system , using the perceptron segmentation model from our previous work( #AUTHOR_TAG ) and the perceptron POS tagging model fromCollins ( 2002 ) .', '']","['built a two-stage baseline system , using the perceptron segmentation model from our previous work( #AUTHOR_TAG ) and the perceptron POS tagging model fromCollins ( 2002 ) .', '']",2
"['.', 'Our HDP extension is also inspired from the Bayesian model proposed by#AUTHOR_TAG .', '']","['L.', 'Our HDP extension is also inspired from the Bayesian model proposed by#AUTHOR_TAG .', 'However, their model is strictly customized for entity coreference resolution, and therefore, extending it to include additional features for each observable object is a challenging task(Ng, 2008; Poon and Domingos, 2008)']","['', 'Our HDP extension is also inspired from the Bayesian model proposed by#AUTHOR_TAG .', '']","['', 'Our HDP extension is also inspired from the Bayesian model proposed by#AUTHOR_TAG .', 'However, their model is strictly customized for entity coreference resolution, and therefore, extending it to include additional features for each observable object is a challenging task(Ng, 2008; Poon and Domingos, 2008)']",4
"['approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM.', 'These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work( He et al. , 2008 ; Gimpel and Smith , 2008 ; #AUTHOR_TAG ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']","['approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM.', 'These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work( He et al. , 2008 ; Gimpel and Smith , 2008 ; #AUTHOR_TAG ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']","['approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM.', 'These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work( He et al. , 2008 ; Gimpel and Smith , 2008 ; #AUTHOR_TAG ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']","['approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM.', 'These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work( He et al. , 2008 ; Gimpel and Smith , 2008 ; #AUTHOR_TAG ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']",4
"['approach has the merit of easily combining different features to predict the probability of each class.', 'We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM.', 'These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work( He et al. , 2008 ; #AUTHOR_TAG ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']","['approach has the merit of easily combining different features to predict the probability of each class.', 'We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM.', 'These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work( He et al. , 2008 ; #AUTHOR_TAG ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']","['approach has the merit of easily combining different features to predict the probability of each class.', 'We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM.', 'These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work( He et al. , 2008 ; #AUTHOR_TAG ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']","['approach has the merit of easily combining different features to predict the probability of each class.', 'We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM.', 'These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work( He et al. , 2008 ; #AUTHOR_TAG ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']",4
"['recently , an alignment selection approach was proposed in( #AUTHOR_TAG ) , which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold .', 'The alignments used in that work were generated from different aligners (HMM, block model, and maximum entropy model).', '']","['recently , an alignment selection approach was proposed in( #AUTHOR_TAG ) , which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold .', 'The alignments used in that work were generated from different aligners (HMM, block model, and maximum entropy model).', '']","['recently , an alignment selection approach was proposed in( #AUTHOR_TAG ) , which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold .', 'The alignments used in that work were generated from different aligners (HMM, block model, and maximum entropy model).', '']","['recently , an alignment selection approach was proposed in( #AUTHOR_TAG ) , which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold .', 'The alignments used in that work were generated from different aligners (HMM, block model, and maximum entropy model).', '']",1
"['', ""Inspired by( #AUTHOR_TAG ) , we split one phrase type into several subsymbols , which contain category information of current constituent 's parent ."", '']","['', ""Inspired by( #AUTHOR_TAG ) , we split one phrase type into several subsymbols , which contain category information of current constituent 's parent ."", '']","['', ""Inspired by( #AUTHOR_TAG ) , we split one phrase type into several subsymbols , which contain category information of current constituent 's parent ."", '']","['', ""Inspired by( #AUTHOR_TAG ) , we split one phrase type into several subsymbols , which contain category information of current constituent 's parent ."", 'For example, an NP immediately dominated by a S, will be substituted by NPˆS.', '']",4
"['feature of head word trigger which we apply to the log-linear model is motivated by the trigger-based approach( #AUTHOR_TAG ) .', '']","['feature of head word trigger which we apply to the log-linear model is motivated by the trigger-based approach( #AUTHOR_TAG ) .', '']","['feature of head word trigger which we apply to the log-linear model is motivated by the trigger-based approach( #AUTHOR_TAG ) .', '']","['feature of head word trigger which we apply to the log-linear model is motivated by the trigger-based approach( #AUTHOR_TAG ) .', '']",4
"['vious sentiment-analysis work in different domains has considered inter-document similarity(Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks( #AUTHOR_TAG )']","['vious sentiment-analysis work in different domains has considered inter-document similarity(Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks( #AUTHOR_TAG )']","['vious sentiment-analysis work in different domains has considered inter-document similarity(Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks( #AUTHOR_TAG )']","['vious sentiment-analysis work in different domains has considered inter-document similarity(Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks( #AUTHOR_TAG )']",0
"['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit(Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts)( #AUTHOR_TAG ; Efron , 2004 ; Mullen and Malouf , 2006 ) .', '']","['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit(Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts)( #AUTHOR_TAG ; Efron , 2004 ; Mullen and Malouf , 2006 ) .', '']","['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit(Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts)( #AUTHOR_TAG ; Efron , 2004 ; Mullen and Malouf , 2006 ) .', '']","['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit(Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts)( #AUTHOR_TAG ; Efron , 2004 ; Mullen and Malouf , 2006 ) .', '']",0
"[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit(Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts)( Laver et al. , 2003 ; #AUTHOR_TAG ; Mullen and Malouf , 2006 ) .', '']","['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit(Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts)( Laver et al. , 2003 ; #AUTHOR_TAG ; Mullen and Malouf , 2006 ) .', '']","['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit(Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts)( Laver et al. , 2003 ; #AUTHOR_TAG ; Mullen and Malouf , 2006 ) .', '']","['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit(Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts)( Laver et al. , 2003 ; #AUTHOR_TAG ; Mullen and Malouf , 2006 ) .', '']",0
"['able early papers on graph-based semisupervised learning includeBlum and Chawla ( 2001 ) ,Bansal et al. ( 2002 ) ,Kondor and Lafferty ( 2002 ) , and#AUTHOR_TAG .', 'Zhu (2005) maintains a survey of this area']","['able early papers on graph-based semisupervised learning includeBlum and Chawla ( 2001 ) ,Bansal et al. ( 2002 ) ,Kondor and Lafferty ( 2002 ) , and#AUTHOR_TAG .', 'Zhu (2005) maintains a survey of this area']","['able early papers on graph-based semisupervised learning includeBlum and Chawla ( 2001 ) ,Bansal et al. ( 2002 ) ,Kondor and Lafferty ( 2002 ) , and#AUTHOR_TAG .', 'Zhu (2005) maintains a survey of this area']","['able early papers on graph-based semisupervised learning includeBlum and Chawla ( 2001 ) ,Bansal et al. ( 2002 ) ,Kondor and Lafferty ( 2002 ) , and#AUTHOR_TAG .', 'Zhu (2005) maintains a survey of this area']",0
"[', several alternative , often quite sophisticated approaches to collective classification have been proposed( #AUTHOR_TAG ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be inter- esting to investigate the application of such meth- ods to our problem.', 'However, we also believe that our approach has important advantages, in- cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative , often quite sophisticated approaches to collective classification have been proposed( #AUTHOR_TAG ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be inter- esting to investigate the application of such meth- ods to our problem.', 'However, we also believe that our approach has important advantages, in- cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative , often quite sophisticated approaches to collective classification have been proposed( #AUTHOR_TAG ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be inter- esting to investigate the application of such meth- ods to our problem.', 'However, we also believe that our approach has important advantages, in- cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative , often quite sophisticated approaches to collective classification have been proposed( #AUTHOR_TAG ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be inter- esting to investigate the application of such meth- ods to our problem.', 'However, we also believe that our approach has important advantages, in- cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"['sophisticated approaches have been proposed( #AUTHOR_TAG ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments( Galley et al. , 2004 ) .', '']","['sophisticated approaches have been proposed( #AUTHOR_TAG ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments( Galley et al. , 2004 ) .', '']","['sophisticated approaches have been proposed( #AUTHOR_TAG ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments( Galley et al. , 2004 ) .', '']","['sophisticated approaches have been proposed( #AUTHOR_TAG ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments( Galley et al. , 2004 ) .', '']",0
"['', 'Also relevant is work on the general problems of dialog-act tagging( Stolcke et al. , 2000 ) , citation analysis( Lehnert et al. , 1990 ) , and computational rhetorical analysis( #AUTHOR_TAG ; Teufel and Moens , 2002 )']","['', 'Also relevant is work on the general problems of dialog-act tagging( Stolcke et al. , 2000 ) , citation analysis( Lehnert et al. , 1990 ) , and computational rhetorical analysis( #AUTHOR_TAG ; Teufel and Moens , 2002 )']","['', 'Also relevant is work on the general problems of dialog-act tagging( Stolcke et al. , 2000 ) , citation analysis( Lehnert et al. , 1990 ) , and computational rhetorical analysis( #AUTHOR_TAG ; Teufel and Moens , 2002 )']","['', 'Also relevant is work on the general problems of dialog-act tagging( Stolcke et al. , 2000 ) , citation analysis( Lehnert et al. , 1990 ) , and computational rhetorical analysis( #AUTHOR_TAG ; Teufel and Moens , 2002 )']",0
"['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit( Shulman et al. , 2005 ; Cardie et al. , 2006 ; #AUTHOR_TAG ) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or']","['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit( Shulman et al. , 2005 ; Cardie et al. , 2006 ; #AUTHOR_TAG ) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or']","['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit( Shulman et al. , 2005 ; Cardie et al. , 2006 ; #AUTHOR_TAG ) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or']","['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit( Shulman et al. , 2005 ; Cardie et al. , 2006 ; #AUTHOR_TAG ) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts)(Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006) .', '']",0
"['', 'For example, we may find textual4 evidence of a high likelihood of agreement be-tween two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings (seeMullen and Malouf (2006) but cfXXX#AUTHOR_TAG .', '']","['', 'For example, we may find textual4 evidence of a high likelihood of agreement be-tween two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings (seeMullen and Malouf (2006) but cfXXX#AUTHOR_TAG .', '']","['', 'For example, we may find textual4 evidence of a high likelihood of agreement be-tween two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings (seeMullen and Malouf (2006) but cfXXX#AUTHOR_TAG .', '']","['', 'For example, we may find textual4 evidence of a high likelihood of agreement be-tween two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings (seeMullen and Malouf (2006) but cfXXX#AUTHOR_TAG .', '']",0
"['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit(Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts)( Laver et al. , 2003 ; Efron , 2004 ; #AUTHOR_TAG ) .', '']","['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit(Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts)( Laver et al. , 2003 ; Efron , 2004 ; #AUTHOR_TAG ) .', '']","['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit(Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts)( Laver et al. , 2003 ; Efron , 2004 ; #AUTHOR_TAG ) .', '']","['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit(Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts)( Laver et al. , 2003 ; Efron , 2004 ; #AUTHOR_TAG ) .', '']",0
"['our experiments, we employed the well-known classifier SVMlight to obtain individual-document classification scores, treating Y as the positive class and using plain unigrams as features.5', 'Fol- lowing standard practice in sentiment analysis( #AUTHOR_TAG ) , the input to SVMlight con- sisted of normalized presence-of-feature (rather than frequency-of-feature) vectors.', 'The ind value for each speech segment s was based on the signed distanceds fromthevectorrepresentingstothe trained SVM decision plane: \x0e � ds ']","['our experiments, we employed the well-known classifier SVMlight to obtain individual-document classification scores, treating Y as the positive class and using plain unigrams as features.5', 'Fol- lowing standard practice in sentiment analysis( #AUTHOR_TAG ) , the input to SVMlight con- sisted of normalized presence-of-feature (rather than frequency-of-feature) vectors.', 'The ind value for each speech segment s was based on the signed distanceds fromthevectorrepresentingstothe trained SVM decision plane: \x0e � ds \x0e23�4s� d s �23�4s�']","['our experiments, we employed the well-known classifier SVMlight to obtain individual-document classification scores, treating Y as the positive class and using plain unigrams as features.5', 'Fol- lowing standard practice in sentiment analysis( #AUTHOR_TAG ) , the input to SVMlight con- sisted of normalized presence-of-feature (rather than frequency-of-feature) vectors.', 'The ind value for each speech segment s was based on the signed distanceds fromthevectorrepresentingstothe trained SVM decision plane: \x0e � ds \x0e23�4s� d s �23�4s� ds ��23�4s def ds = ']","['our experiments, we employed the well-known classifier SVMlight to obtain individual-document classification scores, treating Y as the positive class and using plain unigrams as features.5', 'Fol- lowing standard practice in sentiment analysis( #AUTHOR_TAG ) , the input to SVMlight con- sisted of normalized presence-of-feature (rather than frequency-of-feature) vectors.', '']",5
"['have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text( Yang and Callan , 2005 ; #AUTHOR_TAG )']","['have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text( Yang and Callan , 2005 ; #AUTHOR_TAG )']","['have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text( Yang and Callan , 2005 ; #AUTHOR_TAG )']","['have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text( Yang and Callan , 2005 ; #AUTHOR_TAG )']",0
"[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; #AUTHOR_TAG ) .', 'It would be inter- esting to investigate the application of such meth- ods to our problem.', 'However, we also believe that our approach has important advantages, in- cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; #AUTHOR_TAG ) .', 'It would be inter- esting to investigate the application of such meth- ods to our problem.', 'However, we also believe that our approach has important advantages, in- cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; #AUTHOR_TAG ) .', 'It would be inter- esting to investigate the application of such meth- ods to our problem.', 'However, we also believe that our approach has important advantages, in- cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; #AUTHOR_TAG ) .', 'It would be inter- esting to investigate the application of such meth- ods to our problem.', 'However, we also believe that our approach has important advantages, in- cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"[', we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents( Das and Chen , 2001 ; Pang et al. , 2002 ; #AUTHOR_TAG ; Dave et al. , 2003 )']","[', we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents( Das and Chen , 2001 ; Pang et al. , 2002 ; #AUTHOR_TAG ; Dave et al. , 2003 )']","[', we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents( Das and Chen , 2001 ; Pang et al. , 2002 ; #AUTHOR_TAG ; Dave et al. , 2003 )']","['', ""In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents( Das and Chen , 2001 ; Pang et al. , 2002 ; #AUTHOR_TAG ; Dave et al. , 2003 )""]",0
"['vious sentiment-analysis work in different domains has considered inter-document similarity( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyper- links(Agrawal et al., 2003)']","['vious sentiment-analysis work in different domains has considered inter-document similarity( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyper- links(Agrawal et al., 2003)']","['vious sentiment-analysis work in different domains has considered inter-document similarity( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyper- links(Agrawal et al., 2003)']","['vious sentiment-analysis work in different domains has considered inter-document similarity( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyper- links(Agrawal et al., 2003)']",0
"[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; #AUTHOR_TAG ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be inter- esting to investigate the application of such meth- ods to our problem.', 'However, we also believe that our approach has important advantages, in- cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; #AUTHOR_TAG ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be inter- esting to investigate the application of such meth- ods to our problem.', 'However, we also believe that our approach has important advantages, in- cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; #AUTHOR_TAG ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be inter- esting to investigate the application of such meth- ods to our problem.', 'However, we also believe that our approach has important advantages, in- cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; #AUTHOR_TAG ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be inter- esting to investigate the application of such meth- ods to our problem.', 'However, we also believe that our approach has important advantages, in- cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"['VMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work( #AUTHOR_TAG ; Munson et al. , 2005 )']","['VMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work( #AUTHOR_TAG ; Munson et al. , 2005 )']","['VMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work( #AUTHOR_TAG ; Munson et al. , 2005 )']","['VMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work( #AUTHOR_TAG ; Munson et al. , 2005 )']",3
"['', 'Also relevant is work on the general problems of dialog-act tagging( Stolcke et al. , 2000 ) , citation analysis( #AUTHOR_TAG ) , and computational rhetorical analysis( Marcu , 2000 ; Teufel and Moens , 2002 )']","['', 'Also relevant is work on the general problems of dialog-act tagging( Stolcke et al. , 2000 ) , citation analysis( #AUTHOR_TAG ) , and computational rhetorical analysis( Marcu , 2000 ; Teufel and Moens , 2002 )']","['', 'Also relevant is work on the general problems of dialog-act tagging( Stolcke et al. , 2000 ) , citation analysis( #AUTHOR_TAG ) , and computational rhetorical analysis( Marcu , 2000 ; Teufel and Moens , 2002 )']","['', 'More sophisticated approaches have been proposed(Hillard et al., 2003) , including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments(Galley et al., 2004) .', 'Also relevant is work on the general problems of dialog-act tagging( Stolcke et al. , 2000 ) , citation analysis( #AUTHOR_TAG ) , and computational rhetorical analysis( Marcu , 2000 ; Teufel and Moens , 2002 )']",0
"['sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) .', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evalu']","['sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) .', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative']","['sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) .', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evalu']","['sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) .', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', '']",0
"['sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) .', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evalu']","['sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) .', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative']","['sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) .', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evalu']","['sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) .', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', '']",0
"['sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) .', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evalu']","['sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) .', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative']","['sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) .', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evalu']","['sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) .', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', '']",0
"[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; #AUTHOR_TAG ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; #AUTHOR_TAG ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; #AUTHOR_TAG ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; #AUTHOR_TAG ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"['used a simple method to learn to identify cross-speaker references indicating agreement.', 'More sophisticated approaches have been proposed( Hillard et al. , 2003 ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments( #AUTHOR_TAG ) .', '']","['used a simple method to learn to identify cross-speaker references indicating agreement.', 'More sophisticated approaches have been proposed( Hillard et al. , 2003 ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments( #AUTHOR_TAG ) .', '']","['used a simple method to learn to identify cross-speaker references indicating agreement.', 'More sophisticated approaches have been proposed( Hillard et al. , 2003 ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments( #AUTHOR_TAG ) .', '']","['used a simple method to learn to identify cross-speaker references indicating agreement.', 'More sophisticated approaches have been proposed( Hillard et al. , 2003 ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments( #AUTHOR_TAG ) .', 'Also relevant is work on the general problems of dialog-act tagging(Stolcke et al., 2000) , citation analysis(Lehnert et al., 1990) , and computational rhetorical analysis(Marcu, 2000;Teufel and Moens, 2002)']",0
"['has been previously observed and exploited in the NLP literature( #AUTHOR_TAG ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', '']","['has been previously observed and exploited in the NLP literature( #AUTHOR_TAG ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', '']","['has been previously observed and exploited in the NLP literature( #AUTHOR_TAG ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', '']","['has been previously observed and exploited in the NLP literature( #AUTHOR_TAG ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', '']",1
"[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"[""the United States , for example , governmental bodies are providing and soliciting political documents via the Internet , with lofty goals in mind : electronic rulemaking ( eRulemaking ) initiatives involving the `` electronic collection , distribution , synthesis , and analysis of public commentary in the regulatory rulemaking process '' , may `` [ alter ] the citizen-government relationship ''( #AUTHOR_TAG ) ."", '']","[""the United States , for example , governmental bodies are providing and soliciting political documents via the Internet , with lofty goals in mind : electronic rulemaking ( eRulemaking ) initiatives involving the `` electronic collection , distribution , synthesis , and analysis of public commentary in the regulatory rulemaking process '' , may `` [ alter ] the citizen-government relationship ''( #AUTHOR_TAG ) ."", '']","[""the United States , for example , governmental bodies are providing and soliciting political documents via the Internet , with lofty goals in mind : electronic rulemaking ( eRulemaking ) initiatives involving the `` electronic collection , distribution , synthesis , and analysis of public commentary in the regulatory rulemaking process '' , may `` [ alter ] the citizen-government relationship ''( #AUTHOR_TAG ) ."", '']","[""the United States , for example , governmental bodies are providing and soliciting political documents via the Internet , with lofty goals in mind : electronic rulemaking ( eRulemaking ) initiatives involving the `` electronic collection , distribution , synthesis , and analysis of public commentary in the regulatory rulemaking process '' , may `` [ alter ] the citizen-government relationship ''( #AUTHOR_TAG ) ."", 'Additionally, much media attention has been focused recently on the potential impact that Internet sites may have on politics 2 , or at least on political journalism 3 .', '']",0
"['since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents( #AUTHOR_TAG ; Pang et al. , 2002 ; Turney , 2002 ; Dave et al. , 2003 )']","['since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents( #AUTHOR_TAG ; Pang et al. , 2002 ; Turney , 2002 ; Dave et al. , 2003 )']","['since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents( #AUTHOR_TAG ; Pang et al. , 2002 ; Turney , 2002 ; Dave et al. , 2003 )']","['', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents( #AUTHOR_TAG ; Pang et al. , 2002 ; Turney , 2002 ; Dave et al. , 2003 )']",0
"['able early papers on graph-based semisupervised learning includeBlum and Chawla (2001) ,Bansal et al. (2002) , Kondor andLafferty (2002) , andJoachims (2003) .', '#AUTHOR_TAG maintains a survey of this area']","['able early papers on graph-based semisupervised learning includeBlum and Chawla (2001) ,Bansal et al. (2002) , Kondor andLafferty (2002) , andJoachims (2003) .', '#AUTHOR_TAG maintains a survey of this area']","['able early papers on graph-based semisupervised learning includeBlum and Chawla (2001) ,Bansal et al. (2002) , Kondor andLafferty (2002) , andJoachims (2003) .', '#AUTHOR_TAG maintains a survey of this area']","['able early papers on graph-based semisupervised learning includeBlum and Chawla (2001) ,Bansal et al. (2002) , Kondor andLafferty (2002) , andJoachims (2003) .', '#AUTHOR_TAG maintains a survey of this area']",0
"['vious sentiment-analysis work in different domains has considered inter-document similarity( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyperlinks(Agrawal et al., 2003)']","['vious sentiment-analysis work in different domains has considered inter-document similarity( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyperlinks(Agrawal et al., 2003)']","['vious sentiment-analysis work in different domains has considered inter-document similarity( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyperlinks(Agrawal et al., 2003)']","['vious sentiment-analysis work in different domains has considered inter-document similarity( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyperlinks(Agrawal et al., 2003)']",0
"['properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includesWiebe and Rapaport ( 1988 ) ,Hearst ( 1992 ) ,#AUTHOR_TAG , andWiebe ( 1994 ) ; seeEsuli ( 2006 ) for an active bibliography ) .', '']","['properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includesWiebe and Rapaport ( 1988 ) ,Hearst ( 1992 ) ,#AUTHOR_TAG , andWiebe ( 1994 ) ; seeEsuli ( 2006 ) for an active bibliography ) .', '']","['properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includesWiebe and Rapaport ( 1988 ) ,Hearst ( 1992 ) ,#AUTHOR_TAG , andWiebe ( 1994 ) ; seeEsuli ( 2006 ) for an active bibliography ) .', '']","['properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includesWiebe and Rapaport ( 1988 ) ,Hearst ( 1992 ) ,#AUTHOR_TAG , andWiebe ( 1994 ) ; seeEsuli ( 2006 ) for an active bibliography ) .', '']",0
"['has been previously observed and exploited in the NLP literature( Pang and Lee , 2004 ; #AUTHOR_TAG ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', '']","['has been previously observed and exploited in the NLP literature( Pang and Lee , 2004 ; #AUTHOR_TAG ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', '']","['has been previously observed and exploited in the NLP literature( Pang and Lee , 2004 ; #AUTHOR_TAG ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', '']","['has been previously observed and exploited in the NLP literature( Pang and Lee , 2004 ; #AUTHOR_TAG ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', '']",1
"['', 'Our classification framework , directly inspired by#AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .', '']","['', 'Our classification framework , directly inspired by#AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .', '']","['', 'Our classification framework , directly inspired by#AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .', '']","['', 'Our classification framework , directly inspired by#AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .', '']",5
"['have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text( #AUTHOR_TAG ; Purpura and Hillard , 2006 )']","['have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text( #AUTHOR_TAG ; Purpura and Hillard , 2006 )']","['have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text( #AUTHOR_TAG ; Purpura and Hillard , 2006 )']","['have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text( #AUTHOR_TAG ; Purpura and Hillard , 2006 )']",0
"['properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includesWiebe and Rapaport ( 1988 ) ,Hearst ( 1992 ) ,Sack ( 1994 ) , and#AUTHOR_TAG ; seeEsuli ( 2006 ) for an active bibliography ) .', '']","['properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includesWiebe and Rapaport ( 1988 ) ,Hearst ( 1992 ) ,Sack ( 1994 ) , and#AUTHOR_TAG ; seeEsuli ( 2006 ) for an active bibliography ) .', '']","['properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includesWiebe and Rapaport ( 1988 ) ,Hearst ( 1992 ) ,Sack ( 1994 ) , and#AUTHOR_TAG ; seeEsuli ( 2006 ) for an active bibliography ) .', '']","['properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includesWiebe and Rapaport ( 1988 ) ,Hearst ( 1992 ) ,Sack ( 1994 ) , and#AUTHOR_TAG ; seeEsuli ( 2006 ) for an active bibliography ) .', '']",0
"['able early papers on graph-based semisupervised learning include#AUTHOR_TAG ,Bansal et al. ( 2002 ) ,Kondor and Lafferty ( 2002 ) , andJoachims ( 2003 ) .', 'Zhu (2005) maintains a survey of this area']","['able early papers on graph-based semisupervised learning include#AUTHOR_TAG ,Bansal et al. ( 2002 ) ,Kondor and Lafferty ( 2002 ) , andJoachims ( 2003 ) .', 'Zhu (2005) maintains a survey of this area']","['able early papers on graph-based semisupervised learning include#AUTHOR_TAG ,Bansal et al. ( 2002 ) ,Kondor and Lafferty ( 2002 ) , andJoachims ( 2003 ) .', 'Zhu (2005) maintains a survey of this area']","['able early papers on graph-based semisupervised learning include#AUTHOR_TAG ,Bansal et al. ( 2002 ) ,Kondor and Lafferty ( 2002 ) , andJoachims ( 2003 ) .', 'Zhu (2005) maintains a survey of this area']",0
"['.', 'Relationships between the unlabeled items#AUTHOR_TAG consider sequential relations between different types of emails ( e.g. , between requests and satisfactions thereof ) to classify messages , and thus also explicitly exploit the structure of conversations']","['such information in future work.', 'Relationships between the unlabeled items#AUTHOR_TAG consider sequential relations between different types of emails ( e.g. , between requests and satisfactions thereof ) to classify messages , and thus also explicitly exploit the structure of conversations']","['such information in future work.', 'Relationships between the unlabeled items#AUTHOR_TAG consider sequential relations between different types of emails ( e.g. , between requests and satisfactions thereof ) to classify messages , and thus also explicitly exploit the structure of conversations']","['currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work.', 'Relationships between the unlabeled items#AUTHOR_TAG consider sequential relations between different types of emails ( e.g. , between requests and satisfactions thereof ) to classify messages , and thus also explicitly exploit the structure of conversations']",0
"['language and the fact that ( U.S. ) bills often reach several hundred pages in length( #AUTHOR_TAG ) .', '']","['language and the fact that ( U.S. ) bills often reach several hundred pages in length( #AUTHOR_TAG ) .', '']","['ative language and the fact that ( U.S. ) bills often reach several hundred pages in length( #AUTHOR_TAG ) .', '']",['( #AUTHOR_TAG )'],0
"['has been previously observed and exploited in the NLP literature( Pang and Lee , 2004 ; Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', '']","['has been previously observed and exploited in the NLP literature( Pang and Lee , 2004 ; Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', '']","['has been previously observed and exploited in the NLP literature( Pang and Lee , 2004 ; Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', '']","['has been previously observed and exploited in the NLP literature( Pang and Lee , 2004 ; Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', '']",1
"[', we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents( Das and Chen , 2001 ; #AUTHOR_TAG ; Turney , 2002 ; Dave et al. , 2003 )']","[', we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents( Das and Chen , 2001 ; #AUTHOR_TAG ; Turney , 2002 ; Dave et al. , 2003 )']","[', we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents( Das and Chen , 2001 ; #AUTHOR_TAG ; Turney , 2002 ; Dave et al. , 2003 )']","['', ""In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents( Das and Chen , 2001 ; #AUTHOR_TAG ; Turney , 2002 ; Dave et al. , 2003 )""]",0
"['able early papers on graph-based semisupervised learning includeBlum and Chawla ( 2001 ) ,#AUTHOR_TAG ,Kondor and Lafferty ( 2002 ) , andJoachims ( 2003 ) .', 'Zhu (2005) maintains a survey of this area']","['able early papers on graph-based semisupervised learning includeBlum and Chawla ( 2001 ) ,#AUTHOR_TAG ,Kondor and Lafferty ( 2002 ) , andJoachims ( 2003 ) .', 'Zhu (2005) maintains a survey of this area']","['able early papers on graph-based semisupervised learning includeBlum and Chawla ( 2001 ) ,#AUTHOR_TAG ,Kondor and Lafferty ( 2002 ) , andJoachims ( 2003 ) .', 'Zhu (2005) maintains a survey of this area']","['able early papers on graph-based semisupervised learning includeBlum and Chawla ( 2001 ) ,#AUTHOR_TAG ,Kondor and Lafferty ( 2002 ) , andJoachims ( 2003 ) .', 'Zhu (2005) maintains a survey of this area']",0
"[', we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents( Das and Chen , 2001 ; Pang et al. , 2002 ; Turney , 2002 ; #AUTHOR_TAG )']","[', we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents( Das and Chen , 2001 ; Pang et al. , 2002 ; Turney , 2002 ; #AUTHOR_TAG )']","[', we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents( Das and Chen , 2001 ; Pang et al. , 2002 ; Turney , 2002 ; #AUTHOR_TAG )']","['', ""In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents( Das and Chen , 2001 ; Pang et al. , 2002 ; Turney , 2002 ; #AUTHOR_TAG )""]",0
"['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit( Shulman et al. , 2005 ; #AUTHOR_TAG ; Kwon et al. , 2006 ) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or']","['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit( Shulman et al. , 2005 ; #AUTHOR_TAG ; Kwon et al. , 2006 ) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or']","['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit( Shulman et al. , 2005 ; #AUTHOR_TAG ; Kwon et al. , 2006 ) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or']","['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit( Shulman et al. , 2005 ; #AUTHOR_TAG ; Kwon et al. , 2006 ) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts)(Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006) .', '']",0
"['properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes#AUTHOR_TAG ,Hearst ( 1992 ) ,Sack ( 1994 ) , andWiebe ( 1994 ) ; seeEsuli ( 2006 ) for an active bibliography ) .', '']","['properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes#AUTHOR_TAG ,Hearst ( 1992 ) ,Sack ( 1994 ) , andWiebe ( 1994 ) ; seeEsuli ( 2006 ) for an active bibliography ) .', '']","['properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes#AUTHOR_TAG ,Hearst ( 1992 ) ,Sack ( 1994 ) , andWiebe ( 1994 ) ; seeEsuli ( 2006 ) for an active bibliography ) .', '']","['properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes#AUTHOR_TAG ,Hearst ( 1992 ) ,Sack ( 1994 ) , andWiebe ( 1994 ) ; seeEsuli ( 2006 ) for an active bibliography ) .', '']",0
"[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; #AUTHOR_TAG ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; #AUTHOR_TAG ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; #AUTHOR_TAG ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative , often quite sophisticated approaches to collective classification have been proposed( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; #AUTHOR_TAG ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"['able early papers on graph-based semisupervised learning includeBlum and Chawla ( 2001 ) ,Bansal et al. ( 2002 ) ,#AUTHOR_TAG , andJoachims ( 2003 ) .', 'Zhu (2005) maintains a survey of this area']","['able early papers on graph-based semisupervised learning includeBlum and Chawla ( 2001 ) ,Bansal et al. ( 2002 ) ,#AUTHOR_TAG , andJoachims ( 2003 ) .', 'Zhu (2005) maintains a survey of this area']","['able early papers on graph-based semisupervised learning includeBlum and Chawla ( 2001 ) ,Bansal et al. ( 2002 ) ,#AUTHOR_TAG , andJoachims ( 2003 ) .', 'Zhu (2005) maintains a survey of this area']","['able early papers on graph-based semisupervised learning includeBlum and Chawla ( 2001 ) ,Bansal et al. ( 2002 ) ,#AUTHOR_TAG , andJoachims ( 2003 ) .', 'Zhu (2005) maintains a survey of this area']",0
"['', 'Also relevant is work on the general problems of dialog-act tagging( Stolcke et al. , 2000 ) , citation analysis( Lehnert et al. , 1990 ) , and computational rhetorical analysis( Marcu , 2000 ; #AUTHOR_TAG )']","['', 'Also relevant is work on the general problems of dialog-act tagging( Stolcke et al. , 2000 ) , citation analysis( Lehnert et al. , 1990 ) , and computational rhetorical analysis( Marcu , 2000 ; #AUTHOR_TAG )']","['', 'Also relevant is work on the general problems of dialog-act tagging( Stolcke et al. , 2000 ) , citation analysis( Lehnert et al. , 1990 ) , and computational rhetorical analysis( Marcu , 2000 ; #AUTHOR_TAG )']","['', 'Also relevant is work on the general problems of dialog-act tagging( Stolcke et al. , 2000 ) , citation analysis( Lehnert et al. , 1990 ) , and computational rhetorical analysis( Marcu , 2000 ; #AUTHOR_TAG )']",0
"['', 'An exception is#AUTHOR_TAG , who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']","['', 'An exception is#AUTHOR_TAG , who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']","['', 'An exception is#AUTHOR_TAG , who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']","['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit(Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts)(Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006) .', 'An exception is#AUTHOR_TAG , who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']",0
"['VMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work( Daelemans and Hoste , 2002 ; #AUTHOR_TAG )']","['VMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work( Daelemans and Hoste , 2002 ; #AUTHOR_TAG )']","['VMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work( Daelemans and Hoste , 2002 ; #AUTHOR_TAG )']","['VMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work( Daelemans and Hoste , 2002 ; #AUTHOR_TAG )']",3
"['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit( #AUTHOR_TAG ; Cardie et al. , 2006 ; Kwon et al. , 2006 ) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or']","['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit( #AUTHOR_TAG ; Cardie et al. , 2006 ; Kwon et al. , 2006 ) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or']","['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit( #AUTHOR_TAG ; Cardie et al. , 2006 ; Kwon et al. , 2006 ) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or']","['ically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit( #AUTHOR_TAG ; Cardie et al. , 2006 ; Kwon et al. , 2006 ) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts)(Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006) .', '']",0
"['', 'For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings ( see#AUTHOR_TAG but cfXXXAgrawal et al. ( 2003 ) ) .', '']","['', 'For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings ( see#AUTHOR_TAG but cfXXXAgrawal et al. ( 2003 ) ) .', '']","['', 'For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings ( see#AUTHOR_TAG but cfXXXAgrawal et al. ( 2003 ) ) .', '']","['', 'For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings ( see#AUTHOR_TAG but cfXXXAgrawal et al. ( 2003 ) ) .', '']",0
"['vious sentiment-analysis work in different domains has considered inter-document similarity( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks(Agrawal et al., 2003)']","['vious sentiment-analysis work in different domains has considered inter-document similarity( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks(Agrawal et al., 2003)']","['vious sentiment-analysis work in different domains has considered inter-document similarity( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks(Agrawal et al., 2003)']","['vious sentiment-analysis work in different domains has considered inter-document similarity( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks(Agrawal et al., 2003)']",0
"['properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includesWiebe and Rapaport ( 1988 ) ,#AUTHOR_TAG ,Sack ( 1994 ) , andWiebe ( 1994 ) ; seeEsuli ( 2006 ) for an active bibliography ) .', '']","['properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includesWiebe and Rapaport ( 1988 ) ,#AUTHOR_TAG ,Sack ( 1994 ) , andWiebe ( 1994 ) ; seeEsuli ( 2006 ) for an active bibliography ) .', '']","['properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includesWiebe and Rapaport ( 1988 ) ,#AUTHOR_TAG ,Sack ( 1994 ) , andWiebe ( 1994 ) ; seeEsuli ( 2006 ) for an active bibliography ) .', '']","['properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includesWiebe and Rapaport ( 1988 ) ,#AUTHOR_TAG ,Sack ( 1994 ) , andWiebe ( 1994 ) ; seeEsuli ( 2006 ) for an active bibliography ) .', '']",0
"['', 'Our plan is to implement a windowed or moving-average version of BLEU as in( #AUTHOR_TAG )']","['', 'Our plan is to implement a windowed or moving-average version of BLEU as in( #AUTHOR_TAG )']","['', 'Our plan is to implement a windowed or moving-average version of BLEU as in( #AUTHOR_TAG )']","['', 'Our plan is to implement a windowed or moving-average version of BLEU as in( #AUTHOR_TAG )']",3
"['flexible architecture we have presented enables interesting future research : ( i ) a straightforward improvement is the use of lexical similarity to reduce data sparseness , e.g.( #AUTHOR_TAG ; Basili et al. , 2006 ; Bloehdorn et al. , 2006 ) .', '']","['flexible architecture we have presented enables interesting future research : ( i ) a straightforward improvement is the use of lexical similarity to reduce data sparseness , e.g.( #AUTHOR_TAG ; Basili et al. , 2006 ; Bloehdorn et al. , 2006 ) .', '']","['flexible architecture we have presented enables interesting future research : ( i ) a straightforward improvement is the use of lexical similarity to reduce data sparseness , e.g.( #AUTHOR_TAG ; Basili et al. , 2006 ; Bloehdorn et al. , 2006 ) .', '']","['flexible architecture we have presented enables interesting future research : ( i ) a straightforward improvement is the use of lexical similarity to reduce data sparseness , e.g.( #AUTHOR_TAG ; Basili et al. , 2006 ; Bloehdorn et al. , 2006 ) .', '']",0
"['an average score of 51.8 the improvement over the English only variant (50.6) is minimal.', 'This contrasts with the findings described in#AUTHOR_TAG where significant improvements could be achieved by increasing the number of source languages .', 'So this casts some doubt on these.', 'However, as English was not considered as a source language there, the performance levels were mostly between 10 and 20, leaving much room for improvement.', 'This']","['an average score of 51.8 the improvement over the English only variant (50.6) is minimal.', 'This contrasts with the findings described in#AUTHOR_TAG where significant improvements could be achieved by increasing the number of source languages .', 'So this casts some doubt on these.', 'However, as English was not considered as a source language there, the performance levels were mostly between 10 and 20, leaving much room for improvement.', 'This']","['an average score of 51.8 the improvement over the English only variant (50.6) is minimal.', 'This contrasts with the findings described in#AUTHOR_TAG where significant improvements could be achieved by increasing the number of source languages .', 'So this casts some doubt on these.', 'However, as English was not considered as a source language there, the performance levels were mostly between 10 and 20, leaving much room for improvement.', 'This']","['right column in Table 1 shows the scores if (using the product-of-ranks algorithm) four source languages are taken into account in parallel.', 'As can be seen, with an average score of 51.8 the improvement over the English only variant (50.6) is minimal.', 'This contrasts with the findings described in#AUTHOR_TAG where significant improvements could be achieved by increasing the number of source languages .', 'So this casts some doubt on these.', 'However, as English was not considered as a source language there, the performance levels were mostly between 10 and 20, leaving much room for improvement.', '']",1
"['and all we want is to compute the translations into a fifth language, then we can simply extend our approach to what we call the product-of-ranks algorithm.', 'As suggested in#AUTHOR_TAG this can be done by looking up the ranks of each of the four given words ( i.e. the words occurring in a particular word equation ) within the association vector of a translation candidate , and by multiplying these ranks .', 'So for each candidate we obtain a product of ranks.', 'We then assume that the candidate with the smallest product will be the best translation.']","['and all we want is to compute the translations into a fifth language, then we can simply extend our approach to what we call the product-of-ranks algorithm.', 'As suggested in#AUTHOR_TAG this can be done by looking up the ranks of each of the four given words ( i.e. the words occurring in a particular word equation ) within the association vector of a translation candidate , and by multiplying these ranks .', 'So for each candidate we obtain a product of ranks.', 'We then assume that the candidate with the smallest product will be the best translation.']","['and all we want is to compute the translations into a fifth language, then we can simply extend our approach to what we call the product-of-ranks algorithm.', 'As suggested in#AUTHOR_TAG this can be done by looking up the ranks of each of the four given words ( i.e. the words occurring in a particular word equation ) within the association vector of a translation candidate , and by multiplying these ranks .', 'So for each candidate we obtain a product of ranks.', 'We then assume that the candidate with the smallest product will be the best translation. 3', '']","['far, we always computed translations to single source words.', 'However, if we assume, for example, that we already have word equations for four languages, and all we want is to compute the translations into a fifth language, then we can simply extend our approach to what we call the product-of-ranks algorithm.', 'As suggested in#AUTHOR_TAG this can be done by looking up the ranks of each of the four given words ( i.e. the words occurring in a particular word equation ) within the association vector of a translation candidate , and by multiplying these ranks .', 'So for each candidate we obtain a product of ranks.', 'We then assume that the candidate with the smallest product will be the best translation. 3', '']",4
"['as#AUTHOR_TAG dealt only with an English corpus , the current work shows that this methodology is applicable to a wide range of languages and corpora .', 'We were able to']","['as#AUTHOR_TAG dealt only with an English corpus , the current work shows that this methodology is applicable to a wide range of languages and corpora .', 'We were able to']","['as#AUTHOR_TAG dealt only with an English corpus , the current work shows that this methodology is applicable to a wide range of languages and corpora .', 'We were able to']","['as#AUTHOR_TAG dealt only with an English corpus , the current work shows that this methodology is applicable to a wide range of languages and corpora .', '']",1
"['', 'For automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in#AUTHOR_TAG .', '']","['', 'For automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in#AUTHOR_TAG .', '']","['', 'For automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in#AUTHOR_TAG .', '']","['', 'For automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in#AUTHOR_TAG .', '']",3
"['', 'A possible future direction would be to compare the query string to retrieved results using a method similar to that of#AUTHOR_TAG .', 'This would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry']","['', 'A possible future direction would be to compare the query string to retrieved results using a method similar to that of#AUTHOR_TAG .', 'This would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry']","['', 'A possible future direction would be to compare the query string to retrieved results using a method similar to that of#AUTHOR_TAG .', 'This would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry']","['', 'A possible future direction would be to compare the query string to retrieved results using a method similar to that of#AUTHOR_TAG .', 'This would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry']",3
"['article classifier is a discriminative model that draws on the state-of-the-art approach described in#AUTHOR_TAG .', 'The']","['article classifier is a discriminative model that draws on the state-of-the-art approach described in#AUTHOR_TAG .', 'The']","['article classifier is a discriminative model that draws on the state-of-the-art approach described in#AUTHOR_TAG .', 'The model']","['article classifier is a discriminative model that draws on the state-of-the-art approach described in#AUTHOR_TAG .', '']",5
"['choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks( #AUTHOR_TAG ) .', 'Thus, the classifiers trained on the learner data make use of a discriminative model.', '']","['choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks( #AUTHOR_TAG ) .', 'Thus, the classifiers trained on the learner data make use of a discriminative model.', '']","['choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks( #AUTHOR_TAG ) .', 'Thus, the classifiers trained on the learner data make use of a discriminative model.', '']","['choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks( #AUTHOR_TAG ) .', 'Thus, the classifiers trained on the learner data make use of a discriminative model.', '']",4
"['line of research that is correlated with ours is recognition of agreement/disagreement( Misra and Walker , 2013 ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; #AUTHOR_TAG ; Hillard et al. , 2003 ) and classification of stances( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .', '']","['line of research that is correlated with ours is recognition of agreement/disagreement( Misra and Walker , 2013 ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; #AUTHOR_TAG ; Hillard et al. , 2003 ) and classification of stances( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .', '']","['line of research that is correlated with ours is recognition of agreement/disagreement( Misra and Walker , 2013 ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; #AUTHOR_TAG ; Hillard et al. , 2003 ) and classification of stances( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .', '']","['line of research that is correlated with ours is recognition of agreement/disagreement( Misra and Walker , 2013 ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; #AUTHOR_TAG ; Hillard et al. , 2003 ) and classification of stances( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .', '']",1
"['', 'In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement( Galley et al. , 2004 ; #AUTHOR_TAG )']","['categories and these words turn to be useful features for classification.', 'In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement( Galley et al. , 2004 ; #AUTHOR_TAG )']","['', 'In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement( Galley et al. , 2004 ; #AUTHOR_TAG )']","['', 'The MI approach discovers the words that are highly associated with Agree/Disagree categories and these words turn to be useful features for classification.', 'In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement( Galley et al. , 2004 ; #AUTHOR_TAG )']",4
"['line of research that is correlated with ours is recognition of agreement/disagreement( #AUTHOR_TAG ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; Galley et al. , 2004 ; Hillard et al. , 2003 ) and classification of stances( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .', '']","['line of research that is correlated with ours is recognition of agreement/disagreement( #AUTHOR_TAG ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; Galley et al. , 2004 ; Hillard et al. , 2003 ) and classification of stances( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .', '']","['line of research that is correlated with ours is recognition of agreement/disagreement( #AUTHOR_TAG ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; Galley et al. , 2004 ; Hillard et al. , 2003 ) and classification of stances( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .', '']","['line of research that is correlated with ours is recognition of agreement/disagreement( #AUTHOR_TAG ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; Galley et al. , 2004 ; Hillard et al. , 2003 ) and classification of stances( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .', '']",1
"['', 'In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement( #AUTHOR_TAG ; Misra and Walker , 2013 )']","['categories and these words turn to be useful features for classification.', 'In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement( #AUTHOR_TAG ; Misra and Walker , 2013 )']","['', 'In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement( #AUTHOR_TAG ; Misra and Walker , 2013 )']","['', 'The MI approach discovers the words that are highly associated with Agree/Disagree categories and these words turn to be useful features for classification.', 'In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement( #AUTHOR_TAG ; Misra and Walker , 2013 )']",4
