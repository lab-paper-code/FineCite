unique_id	citing_id	citing_title	cited_title	cited_authors	section_title	cited_abstract	citation_context	cite_context_paragraph	citation_class_label	dynamic_contexts_combined
CC1317	W02-0309	Biomedical text retrieval in languages with a complex morphology	how effective is suffixing	['D Harman']	conclusion	s and titles from the Cranfield collection (with 225 queries and 1400 documents), comprised the major test collection for this study. The Medlars collection (30 queries and 1033 documents), and the CACM collection (64 queries and 3204 documents) were used to provide information about the variation of stemming performance across different subject areas and test collections. In addition to the standard recall/precision measures, with SMART system averaging (Salton, 1971), several methods more suited to an interactive retrieval environment were adopted. The interactive environment returns lists of the top ranked documents, and allows the users to scan titles of a group of documents a screenful at a time, so that the ranking of individual documents within the screenful is not as important as the total number of relevant titles within a screen. Furthermore, the number of relevant documents in the first few screens is far more important for the user than the number of relevant in the last screenfuls. Three measures were selected which evaluate performance at given rank cutoff points, such as those corresponding to a screenful of document titles. The first measure, the E measure (Van Rijsbergen, 1979), is a weighted combination of recall and precision that evaluates a set of retrieved documents at a given cutoff, ignoring the ranking within that set. The measure may have weights of 0.5, 1.0, and 2.0 which correspond, respectively, to attaching half the importance to recall as to precision, equal importance to both, and double importance to recall. A lower E value indicates a more effective performance. A second measure, the total number of relevant documents retrieved by a given cutoff, was also calculated. Cutoffs of 10 and 30 documents were used, with ten reflecting a minimum number a user might be expected to TABLE 2. Retrieval performance for Cranfteld 225. scan, and 30 being an assumed upper limit of what a user would scan before query modification. The third measure applicable to the interactive environment is the number of queries that retrieve no relevant documents by the given cutoff. This measure is important because many types of query modification techniques, such as relevance feedback, require relevant documents to be in the retrieved set to work well. These measures were all used in Croft (1983) as complementary measures to the standard recall/precision evaluation.	There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .	['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']	0	['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .']
CC472	J05-3003	Gaussian coordinates and the large scale universe	identifying verb arguments and their syntactic function in the penn treebank	['Alexandra Kinyon', 'Carlos Prolo']	related work	In this paper, we present a tool that allows one to automatically extract verb argument-structure from the Penn Treebank as well as from other corpora annotated with the Penn Treebank release 2 conventions. More specifically, we examine each possible sequence of tags, both functional and categorial and determine whether such a sequence indicates an obligatory argument, an optional argument or a modifier. We argue that this approach is more fine-grained and thus more satisfactory than the existing approaches which have aimed at determining argumenthood in the Penn Treebank. The goal of this work is to provide a set of sufficiently general and fine-grained rules as well as an implementation which will be reusable and freely available to the research community. 1	#AUTHOR_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank .	"['Approaches using treebank-based data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data.', '#AUTHOR_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank .', 'This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'Each of these sequences was categorized as a modifier or argument.', 'Arguments were then mapped to traditional syntactic functions.', 'For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic function is subject.', 'In general, argumenthood was preferred over adjuncthoood.', 'As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is.', 'Sarkar and Zeman (2000) present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank (Hajic 1998).', 'Czech is a language with a freer word order than English and so configurational information cannot be relied upon.', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'Finding subcategorization frames involves filtering adjuncts from the observed frame.', 'This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score.', 'The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more).', 'Marinov and Hemming (2004) present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank (Simov, Popova, and Osenova 2002).', ""In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming's system collects both arguments and adjuncts."", 'It then uses the binomial log-likelihood ratio to filter incorrect frames.', 'The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees.', 'The work done for Bulgarian is small-scale, however, as Marinov and Hemming are working with a preliminary version of the treebank with 580 sentences.']"	0	['#AUTHOR_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank .']
CC1164	P97-1063	A word-to-word model of translational equivalence	a program for aligning sentences in bilingual corporaquot	['W Gale', 'K W Church']			This imbalance foils thresholding strategies , clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .	['Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']	0	['Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'This imbalance foils thresholding strategies , clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .']
CC905	J97-4003	On Expressing Lexical Generalizations in HPSG	ale—the attribute logic engine users guide version 201	['Bob Carpenter', 'Gerald Penn']	related work	ale 3.0 is completely compatible with ale 2.0 grammars, and adds the following new features: * A semantic-head-driven generator, based on the algorithm presented in Shieber et al. (1990). The generator was adapted to the logic of typed feature structures by Octav Popescu in his Carnegie Mellon Master's Thesis, Popescu (1996). Octav also wrote most of the generation code for this release. Grammars can be compiled for parsing only, generation only, or both. Some glue-code is also available from the ale homepage, to parse and generate with different grammars through a unix pipe. * A source-level debugger with a graphical XEmacs interface. This debugger works only with SICStus Prolog 3.0.6 and higher. A debugger with reduced functionality will be made available to SWI Prolog users in a later release. This debugger builds on, and incorporates the functionality of the code for the SICStus source-level debugger, written by Per Mildner at Uppsala University. * a /1 atoms. There are now an infinite number of atoms (types with no appropriate features), implicitly declared in every signature. These atoms can be arbitrary Prolog terms, including unbound variables, and can be used wherever normal ale types can, e.g., f:(a p(3.7)). a /1 atoms are extensional as Prolog terms, i.e., are taken to be identical according to the Prolog predicate , ==/2. In particular, this means that ground atoms behave exactly as ale extensional types. * Optional edge subsumption checking. For completeness of parsing, one only needs to ensure that, for every pair of nodes in the chart, the most general feature structure spanning those nodes is stored in the chart. This can reduce the number of edges in many domains. * An autonomous intro/2 operator. Features can now be declared on their own in a separate part of the grammar. * Default specifications for types. These are NOT default types. If a type appears on the right-hand side of a sub/2 or intro/2 specification, but not on iii iv CONTENTS the left-hand side of one, ale will assume this type is maximal, i.e., assume the specification, T ype sub []. Similarly, if it occurs on a left-hand side, but not on a right-hand side, ale will assume the type is immediately subsumed by bot, the most general type. In both cases, ale will announce these assumptions during compilation. * Several bug corrections and more compile-time warning and error messages. * An SWI Prolog 2.9.7 ...	A common computational treatment of lexical rules adopted , for example , in the ALE system ( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time .	['A common computational treatment of lexical rules adopted , for example , in the ALE system ( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time .', 'While this provides a front-end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation.', 'We mentioned in Section 2.2 that eliminating lexical rules in a precompilation step makes it impossible to process lexical rules or lexical entries that impose constraints that can only be properly executed once information from syntactic processing is available.', 'A related problem is that for analyses resulting in infinite lexica, the number of lexical rule applications needs to be limited.', 'In the ALE system, for example, a depth bound can be specified for this purpose.', 'Finally, as shown in Section 6, using an expanded out lexicon can be less time and space efficient than using a lexicon encoding that makes computational use of generalizations over lexical information, as, for example, the covariation encoding.']	1	['A common computational treatment of lexical rules adopted , for example , in the ALE system ( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time .', 'While this provides a front-end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation.', 'We mentioned in Section 2.2 that eliminating lexical rules in a precompilation step makes it impossible to process lexical rules or lexical entries that impose constraints that can only be properly executed once information from syntactic processing is available.', 'A related problem is that for analyses resulting in infinite lexica, the number of lexical rule applications needs to be limited.']
CC924	J97-4003	On Expressing Lexical Generalizations in HPSG	lexical polymorphism and word disambiguation	['Antonio Sanfilippo']	related work	We present an approach to lexical ambiguity where regularities about sense/u~ge extensibillty are represented by underepecifying word entries through lexic~d polymorphism. Word diumbiguation is carried out using contextual information gathered during language processing to ground polymorphic lexical entries.	In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; #AUTHOR_TAG ) .	['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; #AUTHOR_TAG ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']	1	['In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; #AUTHOR_TAG ) .']
CC680	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	algorithms for deterministic incremental dependency parsing	['Joakim Nivre']	related work	Abstract Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.	For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #AUTHOR_TAG .	"['All results are reported mainly in terms of labeled attachment accuracy score (the parent word and the type of dependency relation to it, abbreviated as LAS), which is also used for greedy (hill-climbing) decisions for feature combination.', 'Unlabeled attachment accuracy score (UAS) and label accuracy (dependency relation regardless of parent, LS) are also given.', ""For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #AUTHOR_TAG ."", 'We denote p < 0.05 and p < 0.01 with + and ++ , respectively.']"	5	"[""For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #AUTHOR_TAG .""]"
CC126	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	guiding semisupervision with constraintdriven learning	['M W Chang', 'L Ratinov', 'D Roth']	introduction		This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #AUTHOR_TAG ; Chang et al. , 2010 ) .	['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #AUTHOR_TAG ; Chang et al. , 2010 ) .', 'The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', 'For our setting this would mean using weak application specific signals to improve dependency parsing.', 'Though we explore such ideas in our experiments, in particular for semi-supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training.']	1	['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #AUTHOR_TAG ; Chang et al. , 2010 ) .', 'The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.']
CC1238	W00-1017	WIT	understanding unsegmented user utterances in realtime spoken dialogue systems	['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']		This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries.	The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .	['The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs).', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence.', 'ISSS also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time.']	5	['The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .']
CC1419	W04-1610	Automatic Arabic document categorization based on the Naïve Bayes algorithm	modern information retrieval	['R B Yates', 'B R Neto']		Digital technologies can help scholars to navigate the vast quantities of musical data and source materials now available to them, but an imaginative leap is needed in order to conceptualize the kinds of musicological research questions we might ask of electronic corpora. In particular, our data-rich digital world offers enormous potential for the exploration of musical transmission and relatedness.  In this article, we explore the 16th- and 17th-century instrumental battaglia (battle piece), a genre with a very distinctive collective identity arising from the use of numerous shared ingredients (including melodic, motivic, textural, harmonic and rhythmic features). However, a battaglia is not defined by the presence of a core set of essential features, and exact concordance between these pieces is often remarkably low. This kind of musical 'family resemblance' (formulated after Wittgenstein) poses a serious challenge to both traditional musicological apparatus (for example, finding aids such as thematic catalogues) and Music Information Retrieval (which has often privileged melodic similarity at the expense of other kinds of musical relatedness).  This case study provides a stimulus for rethinking the complex nature of musical similarity. In doing this, we outline a set of requirements for digital tools that could support the discovery, exploration and representation of these kinds of relationships	It compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .	"['The root extraction process is concerned with the transformation of all Arabic word derivatives to their single common root or canonical form.', 'This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root.', 'In this work, we use the Arabic root extraction technique in (El Kourdi, 2004).', 'It compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', 'In the remainder of this paper, we will use the term ""root"" and ""term"" interchangeably to refer to canonical forms obtained through this root extraction process.']"	4	"['This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root.', 'It compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', 'In the remainder of this paper, we will use the term ""root"" and ""term"" interchangeably to refer to canonical forms obtained through this root extraction process.']"
CC1349	W02-1601	A synchronization structure of SSTC and its applications in machine translation	nonisomorphic synchronous tags	['K Harbusch', 'P Poller']		"Synchronous tree{adjoining grammars (S{TAGs) combine two standard tree{adjoining grammars (TAGs), e.g., for language transduction in Machine Translation (MT). Recent advances show that the restriction to isomorphic derivation trees (IS{TAGs) ensures eecient transduction because only tree{adjoining languages can be formed in each component. As a result IS{TAGs only allow for  triv-ial"" transfer rules, due to the fact that only isomorphic derivations can be synchronized. This means that only very similar constructions in the two languages can be translated into each other. To overcome these limitations and provide a way of realizing more complex translation phenomena, this paper introduces a new formalism, the dynamic link synchronous tree{adjoining grammars or DLS{TAGs. This formalism allows for the synchronization of non{isomorphic derivation trees by introducing the new concept of dynamic links. DLS{TAGs are more powerful than IS-TAGs. More precisely speaking, DLS{TAGs allow for the formulation of a non{tree{adjoining language in one of the two components. This makes the translation problem more diicult but not untractable as outlined in this paper. However , there remain non{isomorphic translation phenomena which cannot be handled by DLS-TAGs as we also show in this paper."	It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #AUTHOR_TAG ) .	['The idea of parallelized formalisms is widely used one, and one which has been applied in many different ways.', 'The use of synchronous formalisms is motivated by the desire to describe two languages that are closely related to each other but that do not have the same structures.', 'For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeillé et al.,1990), (Harbusch & Poller,1996), or for relating a syntactic TAG and semantic one for the same language (Shieber & Schabes,1990).', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #AUTHOR_TAG ) .', 'As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.', 'In this case only TAL can be formed in each component.', 'This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'Also contrastive well-known translation phenomena exist in different languages, which cannot be expressed by IS-TAG, Figure 3 illustrates some examples (Shieber, 1994).']	0	['The use of synchronous formalisms is motivated by the desire to describe two languages that are closely related to each other but that do not have the same structures.', 'For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeille et al.,1990), (Harbusch & Poller,1996), or for relating a syntactic TAG and semantic one for the same language (Shieber & Schabes,1990).', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #AUTHOR_TAG ) .', 'Also contrastive well-known translation phenomena exist in different languages, which cannot be expressed by IS-TAG, Figure 3 illustrates some examples (Shieber, 1994).']
CC1146	P13-3018	DNA, Words and Models, Statistics of Exceptional Words	morphological decomposition and the reverse base frequency effect	['M Taft']	related work	If recognition of a polymorphemic word always takes place via its decomposition into stem and affix, then the higher the frequency of its stem (i.e., base frequency) the easier the lexical decision response should be when frequency of the word itself (i.e., surface frequency) is controlled. Past experiments have demonstrated such a base frequency effect, but not under all circumstances. Thus, a dual pathway notion has become dominant as an account of morphological processing whereby both decomposition and whole-word access is possible. Two experiments are reported here that demonstrate how an obligatory decomposition account can handle the absence of base frequency effects. In particular, it is shown that the later stage of recombining the stem and affix is harder for high base frequency words than for lower base frequency words when matched on surface frequency, and that this can counterbalance the advantage of easier access to the higher frequency stem. When the combination stage is crucial for discriminating the word items from the nonword items, a reverse base frequency effect emerges, revealing the disadvantage at this stage for high base frequency words. Such an effect is hard for the dual-pathway account to explain, but follows naturally from the idea of obligatory decomposition.	Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; #AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose .	['It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; #AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose .', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'If it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts.']	0	['It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; #AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose .', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'If it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts.']
CC1008	P02-1001	Parameter estimation for probabilistic finite-state transducers	a rational design for a weighted finitestate transducer library	['M Mohri', 'F Pereira', 'M Riley']	introduction		The availability of toolkits for this weighted case ( #AUTHOR_TAG ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical NLP .	['The availability of toolkits for this weighted case ( #AUTHOR_TAG ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical NLP .', 'Such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy-channel decoding, 1 including classic models for speech recognition (Pereira and Riley, 1997) and machine translation (Knight and Al-Onaizan, 1998).', 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']	0	['The availability of toolkits for this weighted case ( #AUTHOR_TAG ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical NLP .']
CC1275	W01-0706	Exploring evidence for shallow parsing	a new statistical parser based on bigram lexical dependencies	['M Collins']	experiments	This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.  1 Introduction  Lexical information has been shown to be crucial for many parsing decisions, such as prepositional-phrase attachment (for example (Hindle and Rooth 93)). However, early approaches to probabilistic parsing (Pereira and Schabes 92; Magerman and Marcus 91; Briscoe and Carroll 93) conditioned probabil..	For the full parser , we use the one developed by Michael Collins ( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .	['We perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins ( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'After that, it will choose the candidate parse tree with the highest probability as output.', 'The experiments use the version that was trained (by Collins) on sections 02-21 of the Penn Treebank.', 'The reported results for the full parse tree (on section 23) are recall/precision of 88.1/87.5 (Collins, 1997).']	5	['For the full parser , we use the one developed by Michael Collins ( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .']
CC22	D08-1010	Maximum entropy based rule selection model for syntax-based statistical machine translation	treetostring alignment template for statistical machine translation	['Yang Liu', 'Qun Liu', 'Shouxun Lin']	experiments	We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models.	The features can be easily obtained by modifying the TAT extraction algorithm described in ( #AUTHOR_TAG ) .	['The features can be easily obtained by modifying the TAT extraction algorithm described in ( #AUTHOR_TAG ) .', 'When a TAT is extracted from a word-aligned, source-parsed parallel sentence, we just record the contextual features and the features of the sub-trees.', 'Then we use the toolkit implemented by Zhang (2004) to train MERS models for the ambiguous source syntactic trees separately.', 'We set the iteration number to 100 and Gaussian prior to 1.']	2	['The features can be easily obtained by modifying the TAT extraction algorithm described in ( #AUTHOR_TAG ) .', 'When a TAT is extracted from a word-aligned, source-parsed parallel sentence, we just record the contextual features and the features of the sub-trees.']
CC540	J06-2002	Generating Referring Expressions that Involve Gradable Properties	textual economy through close coupling of syntax and semantics	['Matthew Stone', 'Bonnie Webber']		We focus on the production of efficient descriptions of objects, actions and events. We define a type of  efficiency, textual economy, that exploits the hearer&apos;s recognition of inferential links to material elsewhere  within a sentence. Textual economy leads to efficient descriptions because the material that supports such  inferences has been included to satisfy independent communicative goals, and is therefore overloaded  in the sense of Pollack [18]. We argue that achieving textual economy imposes strong requirements  on the representation and reasoning used in generating sentences. The representation must support the  generator&apos;s simultaneous consideration of syntax and semantics. Reasoningmust enable the generator  to assess quickly and reliably at any stage how the hearer will interpret the current sentence, with its  &apos;-(inc6mplete)syntax and&apos;semantics. We show that these representational and reasoning requirements are  met in the SPUD system for sentence planning and realization	Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #AUTHOR_TAG ; Krahmer and Theune 2002 ) .	['Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #AUTHOR_TAG ; Krahmer and Theune 2002 ) .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', 'Consider, for instance, the list of properties L = size > 3 cm, size < 9 cm .', 'If interleaving forced us to realize the two properties in L one by one, then it would no longer be possible to combine them into, for example, the largest mouse but one (if the facts in the KB support it), or even into the mice between 3 and 9 cm (since size > 3 cm is realized before size < 9 cm).', 'Clearly, sophisticated use of gradable adjectives requires a separation between CD and linguistic realization, unless one is willing to complicate linguistic realization considerably.']	1	['Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #AUTHOR_TAG ; Krahmer and Theune 2002 ) .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.']
CC600	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	expert systems a technology before its time ai expert available at wwwstanfordedugroup scipavsgtexpertsystemsaiexperthtml	['A Barr', 'S Tessler']			The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #AUTHOR_TAG ) and case-based reasoning ( Watson 1997 ) .	['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #AUTHOR_TAG ) and case-based reasoning ( Watson 1997 ) .', 'Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998).', 'In contrast, the techniques examined in this article are corpus-based and data-driven.', 'The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus.']	1	['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #AUTHOR_TAG ) and case-based reasoning ( Watson 1997 ) .', 'Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998).', 'In contrast, the techniques examined in this article are corpus-based and data-driven.']
CC278	E12-1068	Modeling Inflection and Word-Formation in SMT	enriching morphologically poor languages for statistical machine translation	['Eleftherios Avramidis', 'Philipp Koehn']	related work	We address the problem of translating from morphologically poor to morphologically rich languages by adding per-word linguistic information to the source language. We use the syntax of the source sentence to extract information for noun cases and verb persons and annotate the corresponding words accordingly. In experiments, we show improved performance for translating from English into Greek and Czech. For English-Greek, we reduce the error on the verb conjugation from 19% to 5.4% and noun case agreement from 9% to 6%.	Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #AUTHOR_TAG , Yeniterzi and Oflazer ( 2010 ) and others .	"['Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #AUTHOR_TAG , Yeniterzi and Oflazer ( 2010 ) and others .', 'Toutanova et.', ""al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information."", 'Using additional source side information beyond the markup did not produce a gain in performance.']"	1	['Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #AUTHOR_TAG , Yeniterzi and Oflazer ( 2010 ) and others .', 'Toutanova et.', 'Using additional source side information beyond the markup did not produce a gain in performance.']
CC709	J14-2004	Unsupervised Event Coreference Resolution	unsupervised event coreference resolution with rich linguistic features	['Cosmin Adrian Bejan', 'Sanda Harabagiu']	related work	This paper examines how a new class of nonparametric Bayesian models can be effectively applied to an open-domain event coreference task. Designed with the purpose of clustering complex linguistic objects, these models consider a potentially infinite number of features and categorical outcomes. The evaluation performed for solving both within- and cross-document event coreference shows significant improvements of the models when compared against two baselines for this task.	This article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ; #AUTHOR_TAG ) .	['This article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ; #AUTHOR_TAG ) .', 'In this work, we present more details on the problem of solving both within-and cross-document event coreference as well as describe a generic framework for solving this type of problem in an unsupervised way.', 'As data sets, we consider three different resources, including our own corpus (which is the only corpus available that encodes event coreference annotations across and within documents).', 'In the next section, we provide additional information on how we performed the annotation of this corpus.', 'Another major contribution of this article is an extended description of the unsupervised models for solving event coreference.', 'In particular, we focused on providing further explanations about the implementation of the mIBP framework as well as its integration into the HDP and iHMM models.', 'Finally, in this work, we significantly extended the experimental results section, which also includes a novel set of experiments performed over the OntoNotes English corpus (LDC-ON 2007).']	2	['This article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ; #AUTHOR_TAG ) .', 'In this work, we present more details on the problem of solving both within-and cross-document event coreference as well as describe a generic framework for solving this type of problem in an unsupervised way.', 'As data sets, we consider three different resources, including our own corpus (which is the only corpus available that encodes event coreference annotations across and within documents).', 'Another major contribution of this article is an extended description of the unsupervised models for solving event coreference.']
CC1610	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	semantic interpretation of nominalizations	['Richard D Hull', 'Fernando Gomez']	related work	A computational approach to the semantic interpretation of nominalizations is described. Interpretation of nominalizations involves three tasks: deciding whether the nominalization is being used in a verbal or non-verbal sense; disambiguating the nominalized verb when a verbal sense is used; and determining the fillers of the thematic roles of the verbal concept or predicate of the nominalization. A verbal sense can be recognized by the presence of modifiers that represent the arguments of the verbal concept. It is these same modifiers which provide the semantic clues to disambiguate the nominalized verb. In the absence of explicit modifiers, heuristics are used to discriminate between verbal and nonverbal senses. A correspondence between verbs and their nominalizations is exploited so that only a small amount of additional knowledge is needed to handle the nominal form. These methods are tested in the domain of encyclopedic texts and the results are shown.	Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .	['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']	0	['Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .']
CC1483	W06-1104	Automatically creating datasets for measures of semantic relatedness	computing semantic relatedness across parts of speech	['Iryna Gurevych']	experiments		#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .	"['We developed a web-based interface to obtain human judgments of semantic relatedness for each automatically generated concept pair.', 'Test subjects were invited via email to participate in the experiment.', 'Thus, they were not supervised during the experiment.', '#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .', 'Their results differed particularly in cases of antonymy or distributionally related pairs.', 'We created a manual with a detailed introduction to SR stressing the crucial points.', 'The manual was presented to the subjects before the experiment and could be re-accessed at any time.', 'During the experiment, one concept pair at a time was presented to the test subjects in random ordering.', 'Subjects had to assign a discrete relatedness value {0,1,2,3,4} to each pair.', ""Figure 2 shows the system's GUI.""]"	4	"['Test subjects were invited via email to participate in the experiment.', 'Thus, they were not supervised during the experiment.', '#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .', 'Their results differed particularly in cases of antonymy or distributionally related pairs.', 'We created a manual with a detailed introduction to SR stressing the crucial points.', 'The manual was presented to the subjects before the experiment and could be re-accessed at any time.', 'Subjects had to assign a discrete relatedness value {0,1,2,3,4} to each pair.', ""Figure 2 shows the system's GUI.""]"
CC452	J05-3003	Gaussian coordinates and the large scale universe	automated extraction of tags from the penn treebank	['John Chen', 'K Vijay-Shanker']	introduction	The accuracy of statistical parsing models can be improved with the use of lexical information. Statistical parsing using Lexicalized tree adjoining grammar (LTAG), a kind of lexicalized grammar, has remained relatively unexplored. We believe that is largely in part due to the absence of large corpora accurately bracketed in terms of a perspicuous yet broad coverage LTAG. Our work attempts to alleviate this difficulty. We extract different LTAGs from the Penn Treebank. We show that certain strategies yield an improved extracted LTAG in terms of compactness, broad coverage, and supertagging accuracy. Furthermore, we perform a preliminary investigation in smoothing these grammars by means of an external linguistic resource, namely, the tree families of an XTAG grammar, a hand built grammar of English.	Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( #AUTHOR_TAG ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .	['Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( #AUTHOR_TAG ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).', 'However, our approach also generalizes to CFG category-based approaches.', 'In LFG, subcategorization requirements are enforced through semantic forms specifying which grammatical functions are required by a particular predicate.', 'Our approach is based on earlier work on LFG semantic form extraction (van Genabith, Sadler, and Way 1999) and recent progress in automatically annotating the Penn-II and Penn-III Treebanks with LFG f-structures (Cahill et al. 2002;Cahill, McCarthy, et al. 2004).', 'Our technique requires a treebank annotated with LFG functional schemata.', 'In the early approach of van Genabith, Sadler, and Way (1999), this was provided by manually annotating the rules extracted from the publicly available subset of the AP Treebank to automatically produce corresponding f-structures.', 'If the f-structures are of high quality, reliable LFG semantic forms can be generated quite simply by recursively reading off the subcategorizable grammatical functions for each local PRED value at each level of embedding in the f-structures.', 'The work reported in van Genabith, Sadler, and Way (1999) was small scale (100 trees) and proof of concept and required considerable manual annotation work.', 'It did not associate frames with probabilities, discriminate between frames for active and passive constructions, properly reflect the effects of long-distance dependencies (LDDs), or include CFG category information.', 'In this article we show how the extraction process can be scaled to the complete Wall Street Journal (WSJ) section of the Penn-II Treebank, with about one million words in 50,000 sentences, based on the automatic LFG f-structure annotation algorithm described in Cahill et al. (2002) and Cahill, McCarthy, et al. (2004).', 'More recently we have extended the extraction approach to the larger, domain-diverse Penn-III Treebank.', 'Aside from the parsed WSJ section, this version of the treebank contains parses for a subsection of the Brown corpus (almost 385,000 words in 24,000 trees) taken from a variety of text genres. 1 In addition to extracting grammatical-function-For the remainder of this work, when we refer to the Penn-II Treebank, we mean the parse-annotated WSJ, and when we refer to the Penn-III Treebank, we mean the parse-annotated WSJ and Brown corpus combined.']	0	['Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( #AUTHOR_TAG ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'Aside from the parsed WSJ section, this version of the treebank contains parses for a subsection of the Brown corpus (almost 385,000 words in 24,000 trees) taken from a variety of text genres. 1 In addition to extracting grammatical-function-For the remainder of this work, when we refer to the Penn-II Treebank, we mean the parse-annotated WSJ, and when we refer to the Penn-III Treebank, we mean the parse-annotated WSJ and Brown corpus combined.']
CC843	J91-2003	On compositional semantics	the representation and use of focus in a system for understanding dialogsquot	['B J Grosz']	introduction	As a dialog progresses the objects and actions that are most relevant to the conversation, and hence in the focus of attention of the dialog participants, change. This paper describes a representation of focus for language understanding systems, emphasizing its use in understanding task-oriented dialogs. The representation highlights that part of the knowledge base relevant at a given point in a dialog. A model of the task is used both to structure the focus representation and to provide an index into potentially relevant concepts in the knowledge base The use of the focus representation to make retrieval of items from the knowledge base more efficient is described.	Other factors , such as the role of focus ( #AUTHOR_TAG , 1978 ; Sidner 1983 ) or quantifier scoping ( Webber 1983 ) must play a role , too .	['We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus ( #AUTHOR_TAG , 1978 ; Sidner 1983 ) or quantifier scoping ( Webber 1983 ) must play a role , too .', 'Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.']	0	['Other factors , such as the role of focus ( #AUTHOR_TAG , 1978 ; Sidner 1983 ) or quantifier scoping ( Webber 1983 ) must play a role , too .', 'Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.']
CC1290	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	headdriven phrase structure grammar	['Carl Pollard', 'Ivan A Sag']	introduction		This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #AUTHOR_TAG ) by a method of grammar conversion .	['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #AUTHOR_TAG ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']	0	['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #AUTHOR_TAG ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']
CC323	J00-4002	Bidirectional Contextual Resolution	categorial semantics and scoping	['Fernando C N Pereira']		Certain restrictions on possible scopings of quantified noun phrases in natural language are usually expressed in terms of formal constraints on binding at a level of logical form. Such reliance on the form rather than the content of semantic interpretations goes against the spirit of compositionality. I will show that those scoping restrictions follow from simple and fundamental facts about functional application and abstraction, and can be expressed as constraints on the derivation of possible meanings for sentences rather than constraints of the alleged forms of those meanings.	The version proposed here combines a basic insight from Lewin ( 1990 ) with higher-order unification to give an analysis that has a strong resemblance to that proposed in #AUTHOR_TAG , 1991 ) , with some differences that are commented on below .	"['We can implement a deductive theory of quantifier scope using the conditional equivalence mechanism.', 'The version proposed here combines a basic insight from Lewin ( 1990 ) with higher-order unification to give an analysis that has a strong resemblance to that proposed in #AUTHOR_TAG , 1991 ) , with some differences that are commented on below .', ""Like Pereira's approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by Lewin."", 'We analyze quantified NPs at the QLF level as illustrated in the QLF for: We assume that every determiner has its own equivalence, which resolves it as a quantifier: sometimes this can be quite a complicated matter, as with any (Alshawi 1990), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity.', '6']"	1	['The version proposed here combines a basic insight from Lewin ( 1990 ) with higher-order unification to give an analysis that has a strong resemblance to that proposed in #AUTHOR_TAG , 1991 ) , with some differences that are commented on below .']
CC95	D10-1074	What do we know about conversation participants	what do we know about conversation participants experiments on conversation entailment	['Chen Zhang', 'Joyce Chai']	introduction	Given the increasing amount of conversation data, techniques to automatically acquire information about conversation participants have become more important. Towards this goal, we investigate the problem of conversation entailment, a task that determines whether a given conversation discourse entails a hypothesis about the participants. This paper describes the challenges related to conversation entailment based on our collected data and presents a probabilistic framework that incorporates conversation context in entailment prediction. Our preliminary experimental results have shown that conversation context, in particular dialogue act, plays an important role in conversation entailment.	To address this limitation , our previous work ( #AUTHOR_TAG ) has initiated an investigation on the problem of conversation entailment .	['To address this limitation , our previous work ( #AUTHOR_TAG ) has initiated an investigation on the problem of conversation entailment .', 'The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H.', 'For instance, as in Example 1, the first hypothesis can be entailed from the conversation segment while the second hypothesis cannot.', 'While our previous work has provided some interesting preliminary observations, it mostly focused on data collection and initial experiments and analysis using a small set of development data.', 'It is not clear whether the previous results are generally applicable, how different components in the entailment framework interact with each other, and how different representations may influence the entailment outcome.']	2	['To address this limitation , our previous work ( #AUTHOR_TAG ) has initiated an investigation on the problem of conversation entailment .']
CC814	J91-2003	On compositional semantics	abductive inferencequot	['J A Reggia']	introduction		Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #AUTHOR_TAG , the `` diagnosis from first principles '' of Reiter ( 1987 ) , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) .	"['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure assumption"" (ibid.),', '""domain circumscription"" (cf.', 'Etherington and Mercer 1987), and their kin.', ""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #AUTHOR_TAG , the `` diagnosis from first principles '' of Reiter ( 1987 ) , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) ."", 'But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'These connections are being examined elsewhere (Zadrozny forthcoming).']"	1	"['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure assumption"" (ibid.),', 'Etherington and Mercer 1987), and their kin.', ""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #AUTHOR_TAG , the `` diagnosis from first principles '' of Reiter ( 1987 ) , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) ."", 'These connections are being examined elsewhere (Zadrozny forthcoming).']"
CC1112	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	interpretation and generation in a knowledgebased tutorial system	['Myroslava O Dzikovska', 'Charles B Callaway', 'Elaine Farrow']	experiments		The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .	['The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .', 'At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits.']	5	['The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .']
CC857	J91-2003	On compositional semantics	lectures on contemporary syntactic theories csli lecture notes	['P Sells']	introduction		For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''	"['A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings.', ""For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''"", 'The pairing of these two sentences may be said to create a small paragraph.', 'Our point is that an acceptable structure can be assigned to the utterance ""Reagan thinks bananas"" only within the paragraph in which this utterance occurs.', 'We believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two (or one) neighboring sentences, will be sufficient for this task.', ""That is, we can ask in the first sentence of a paragraph about Kissinger's favorite fruit, elaborate the question and the circumstances in the next few sentences, and give the above answer at the end."", 'We do not claim that a paragraph is necessarily described by a set of grammar rules in some grammar formalism (although it may be); rather, it has the grammatical role of providing functional structures that can be assigned to strings.']"	4	"[""For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''"", 'The pairing of these two sentences may be said to create a small paragraph.']"
CC1500	W06-1104	Automatically creating datasets for measures of semantic relatedness	using measures of semantic relatedness for word sense disambiguation	['Siddharth Patwardhan', 'Satanjeev Banerjee', 'Ted Pedersen']		This paper generalizes the Adapted Lesk Algorithm of Banerjee and Pedersen (2002) to a method of word sense disambiguation based on semantic relatedness. This is possible since Lesk&apos;s original algorithm (1986) is based on gloss overlaps which can be viewed as a measure of semantic relatedness. We evaluate a variety of measures of semantic relatedness when applied to word sense disambiguation by carrying out experiments using the English lexical sample data of Senseval-2. We find that the gloss overlaps of Adapted Lesk and the semantic distance measure of Jiang and Conrath (1997) result in the highest accuracy	The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task.	['The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task.', 'But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'A certain measure may work well in one application, but not in another.', 'Application-based evaluation can only state the fact, but give little explanation about the reasons.']	0	['The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task.', 'But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'A certain measure may work well in one application, but not in another.', 'Application-based evaluation can only state the fact, but give little explanation about the reasons.']
CC1359	W02-1601	A synchronization structure of SSTC and its applications in machine translation	natural language analysis in machine translation mt based on the stringtree correspondence grammar stcg	['E K Tang']		The formalism is argued to be a totally declarative grammar formalism that can associate, to strings in a language, arbitrary tree structures as desired by the grammar writer to be the linguistic representation structures of the strings. More importantly is the facility to specify the correspondence between the string and the associated tree in a very natural manner. These features are very much desired in grammar writing, in particular for the treatment of certain linguistic phenomena which are 'non-standard', namely featurisation, lexicalisation and crossed dependencies [2,3]. Furthermore, a grammar written in this way naturally inherits the desired property of bi-directionality (in fact non-directionality [4]) such that the same grammar can be interpreted for both analysis and generation.	A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ) .	"['Figure 2 illustrates the sentence ""John picks the box up"" with its corresponding SSTC.', 'It contains a nonprojective correspondence.', 'An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ) .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']"	5	['A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ) .']
CC770	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	the design of a computer language for linguistic information	['S Shieber']	introduction	A considerable body of accumulated knowledge about the design of languages for communicating information to computers has been derived from the subfields of programming language design and semantics. It has been the goal of the PATR group at SRI to utilize a relevant portion of this knowledge in implementing tools to facilitate communication of linguistic information to computers. The PATR-II formalism is our current computer language for encoding linguistic information. This paper, a brief overview of that formalism, attempts to explicate our design decisions in terms of a set of properties that effective computer languages should incorporate.Engineering and Applied Science	Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( Kaplan and Bresnan , 1982 ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( #AUTHOR_TAG ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .	['Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( Kaplan and Bresnan , 1982 ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( #AUTHOR_TAG ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'The research described below is taking place in the context of three collaborative projects (Boguraev, 1987;Russell et al., 1986;Phillips and Thompson, 1986) to develop a general-purpose, wide coverage morphological and syntactic analyser for English.', 'One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.']	0	['Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( Kaplan and Bresnan , 1982 ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( #AUTHOR_TAG ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .']
CC705	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	introduction to arabic natural language processing	['Nizar Habash']	experiments	"Abstract This book provides system developers and researchers in natural language processing and computational linguistics with the necessary background information for working with the Arabic language. The goal is to introduce Arabic linguistic phenomena and review the state-of-the-art in Arabic processing. The book discusses Arabic script, phonology, orthography, morphology, syntax and semantics, with a final chapter on machine translation issues. The chapter sizes correspond more or less to what is linguistically distinctive about Arabic, with morphology getting the lion's share, followed by Arabic script. No previous knowledge of Arabic is needed. This book is designed for computer scientists and linguists alike. The focus of the book is on Modern Standard Arabic; however, notes on practical issues related to Arabic dialects and languages written in the Arabic script are presented in different chapters. Table of Contents: What is ""Arabic""? / Arabic Script / Arabic Phonology and Orthography / Arabic Mo..."	"7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG , p. 102 ) ."	"['7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG , p. 102 ) .']"	0	"['7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG , p. 102 ) .']"
CC1646	W14-3902	Code Mixing: A Challenge for Language Identification in the Language of Social Media	dcu aspectbased polarity classification for semeval task 4	['Joachim Wagner', 'Piyush Arora', 'Santiago Cortes', 'Utsab Barman', 'Dasha Bogdanova', 'Jennifer Foster', 'Lamia Tounsi']	experiments	We describe the work carried out by DCU on the Aspect Based Sentiment Analysis task at SemEval 2014. Our team submitted one constrained run for the restaurant domain and one for the laptop domain for sub-task B (aspect term polarity prediction), ranking highest out of 36 systems on the restaurant test set and joint highest out of 32 systems on the laptop test set.	raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #AUTHOR_TAG ) and create multiple features for length using a decision tree ( J48 ) .	['1. Char-n-grams (G): We start with a character n-gram-based approach (Cavnar and Trenkle, 1994), which is most common and followed by many language identification researchers.', 'Following the work of King and Abney (2013), we select character n-grams (n=1 to 5) and the word as the features in our experiments.', '2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments.', 'raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #AUTHOR_TAG ) and create multiple features for length using a decision tree ( J48 ) .', 'We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.', '4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized.']	2	['1. Char-n-grams (G): We start with a character n-gram-based approach (Cavnar and Trenkle, 1994), which is most common and followed by many language identification researchers.', 'Following the work of King and Abney (2013), we select character n-grams (n=1 to 5) and the word as the features in our experiments.', 'raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #AUTHOR_TAG ) and create multiple features for length using a decision tree ( J48 ) .', 'We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.', '4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized.']
CC198	D13-1115	Integrating Theory and Practice: A Daunting Task	distinctive image features from scaleinvariant keypoints	['David G Lowe']	related work	This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are ...	The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']	0	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']
CC1252	W00-1312	Cross-lingual information retrieval using hidden Markov models	finding terminology translations from nonparallel corporaquot	['P Fung', 'K Mckeown']		We present a statistical word feature, the Word Relation Matrix, which can be used to find translated pairs of words and terms from non-parallel corpora, across language groups. Online dictionary entries are used as seed words to generate Word Relation Matrices for the unknown words according to correlation measures. Word Relation Matrices are then mapped across the corpora to find translation pairs. Translation accuracies are around 30% when only the top candidate is counted. Nevertheless, top 20 candidate output give a 50.9% average increase in accuracy on human translator performance.	Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .	['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.']	0	['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.']
CC495	J05-3003	Gaussian coordinates and the large scale universe	the automatic acquisition of frequencies of verb subcategorization frames from tagged corpora	['Akira Ushioda', 'David Evans', 'Ted Gibson', 'Alex Waibel']	related work	We describe a mechanism for automatically acquiring verb subcategorization frames and their frequencies in a large corpus. A tagged corpus is first partially parsed to identify noun phrases and then a finear grammar is used to estimate the appropriate subcategorization frame for each verb token in the corpus. In an experiment involving the identification of six fixed subcategorization frames, our current system showed more than 80% accuracy. In addition, a new statistical approach substantially improves the accuracy of the frequency estimation.	#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .	"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', '#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']"	0	"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', '#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']"
CC1457	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	automatic content extraction	['ACE']	conclusion	This paper presents an integrated approach to automatically provide an overview of content on Thai websites based on tag cloud. This approach is intended to address the information overload issue by presenting the overview to users in order that they could assess whether the information meets their needs. The approach has incorporated Web content extraction, Thai word segmentation, and information presentation to generate a tag cloud in Thai language as an overview of the key content in the webpage. From the experimental study, the generated Thai Tag clouds are able to provide an overview of the tags which frequently appear in the title and body of the content. Moreover, the first few lines in the tag cloud offer an improved readability	These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #AUTHOR_TAG Arabic data .	['These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #AUTHOR_TAG Arabic data .', 'The experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'In addition, we also report results on the official test data.']	5	['These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #AUTHOR_TAG Arabic data .', 'The experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'In addition, we also report results on the official test data.']
CC1286	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	translating the xtag english grammar to hpsg	['Yuka Tateisi', 'Kentaro Torisawa', 'Yusuke Miyao', 'Jun’ichi Tsujii']	introduction		Tateisi et al. also translated LTAG into HPSG ( #AUTHOR_TAG ) .	['Tateisi et al. also translated LTAG into HPSG ( #AUTHOR_TAG ) .', 'However, their method depended on translator�s intuitive analy- sis of the original grammar.', 'Thus the transla- tion was manual and grammar dependent.', 'The manual translation demanded considerable efforts from the translator, and obscures the equiva- lence between the original and obtained gram- mars.', 'Other works (Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars.', 'However, given the greater ex- pressive power of HPSG, it is impossible to con- vert an arbitrary HPSG grammar into an LTAG grammar.', 'Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capac- ity.', 'Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above ad- vantages.']	1	['Tateisi et al. also translated LTAG into HPSG ( #AUTHOR_TAG ) .', 'Thus the transla- tion was manual and grammar dependent.', 'Other works (Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars.', 'Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capac- ity.', 'Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above ad- vantages.']
CC370	J02-3002	Periods, Capitalized Words, etc.	overview of muc7”	['Nancy Chinchor']			For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( #AUTHOR_TAG ) .	['• abbreviation (as opposed to regular word) These four lists can be acquired completely automatically from raw (unlabeled) texts.', 'For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( #AUTHOR_TAG ) .', 'We used these texts because the approach described in this article was initially designed to be part of a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed for MUC-7.', 'Although the corpus size of 300,000 words can be seen as large, the fact that this corpus does not have to be annotated in any way and that a corpus of similar size can be easily collected from on-line sources (including the Internet) makes this resource cheap to obtain.']	5	['* abbreviation (as opposed to regular word) These four lists can be acquired completely automatically from raw (unlabeled) texts.', 'For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( #AUTHOR_TAG ) .', 'We used these texts because the approach described in this article was initially designed to be part of a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed for MUC-7.', 'Although the corpus size of 300,000 words can be seen as large, the fact that this corpus does not have to be annotated in any way and that a corpus of similar size can be easily collected from on-line sources (including the Internet) makes this resource cheap to obtain.']
CC1566	W06-2933	Labeled pseudo-projective dependency parsing with support vector machines	the tiger treebank	['S Brants', 'S Dipper', 'S Hansen', 'W Lezius', 'G Smith']	experiments	Proceedings of the 16th Nordic Conference   of Computational Linguistics NODALIDA-2007.  Editors: Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek and Mare Koit.  University of Tartu, Tartu, 2007.  ISBN 978-9985-4-0513-0 (online)  ISBN 978-9985-4-0514-7 (CD-ROM)  pp. 81-88	This is noticeable for German ( #AUTHOR_TAG ) and Portuguese ( Afonso et al. , 2002 ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( DËzeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .	['A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'This is noticeable for German ( #AUTHOR_TAG ) and Portuguese ( Afonso et al. , 2002 ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( DË\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .', 'On the other hand, all three languages behave like high-accuracy languages with respect to attachment score.', 'A very similar pattern is found for Spanish (Civit Torruella and Martí Antonín, 2002), although this cannot be explained by a high proportion of non-projective structures.', 'One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features.']	1	['This is noticeable for German ( #AUTHOR_TAG ) and Portuguese ( Afonso et al. , 2002 ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( DË\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .', 'On the other hand, all three languages behave like high-accuracy languages with respect to attachment score.', 'One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features.']
CC646	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	numerical optimization	['Jorge Nocedal', 'Stephen J Wright']		Numerical Optimization presents a comprehensive and up-to-date description of the most effective methods in continuous optimization. It responds to the growing interest in optimization in engineering, science, and business by focusing on the methods that are best suited to practical problems. For this new edition the book has been thoroughly updated throughout. There are new chapters on nonlinear interior methods and derivative-free methods for optimization, both of which are used widely in practice and the focus of much current research. Because of the emphasis on practical methods, as well as the extensive illustrations and exercises, the book is accessible to a wide audience. It can be used as a graduate text in engineering, operations research, mathematics, computer science, and business. It also serves as a handbook for researchers and practitioners in the field. The authors have strived to produce a text that is pleasant to read, informative, and rigorous - one that reveals both the beautiful nature of the discipline and its practical side.	Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule ( #AUTHOR_TAG ) .	"['Hence the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ.', 'We optimize the dual objective using the gradient based methods shown in Algorithm 1.', ""Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule ( #AUTHOR_TAG ) ."", 'Here, β∇(λ) represents an ascent direction chosen as follows: For inequality constraints, it is the projected gradient (Bertsekas 1999); for equality constraints with slack, we use conjugate gradient (Nocedal and Wright 1999), noting that when λ = 0, the objective is not differentiable.', 'In practice this only happens at the start of optimization and we use a sub-gradient for the first direction.']"	5	"[""Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule ( #AUTHOR_TAG ) ."", 'Here, b(l) represents an ascent direction chosen as follows: For inequality constraints, it is the projected gradient (Bertsekas 1999); for equality constraints with slack, we use conjugate gradient (Nocedal and Wright 1999), noting that when l = 0, the objective is not differentiable.']"
CC930	J97-4003	On Expressing Lexical Generalizations in HPSG	featurebased inheritance networks for computational lexicons	['Hans-Ulrich Krieger', 'John Nerbonne']	related work	The virtues of viewing the lexicon as an inheritance network are its succinctness and its tendency to highlight significant clusters of linguistic properties. From its succinctness follow two practical advantages, namely its ease of maintenance and modification. In this paper we present a feature-based foundation for lexical inheritance. We argue that the feature-based foundation is both more economical and expressively more powerful than non-feature-based systems. It is more economical because it employs only mechanisms already assumed to be present elsewhere in the grammar (viz., in the feature system), and it is more expressive because feature systems are more expressive than other mechanisms used in expressing lexical inheritance (cf. DATR). The lexicon furthermore allows the use of default unification, based on the ideas of default unification, defined by Bouma. These claims are buttressed in sections sketching the opportunities for lexical description in feature-based lexicons in two central lexical topics, inflection and derivation. Briefly, we argue that the central notion of paradigm may be defined in feature structures, and that it may be more satisfactorily (in fact, immediately) linked to the syntactic information in this fashion. Our discussion of derivation is more programmatic; but here, too, we argue that feature structures of a suitably rich sort provide a foundation for the definition of lexical rules. We illustrate theoretical claims in application to German lexis. This work is currently under implementation in a natural language understanding effort (DISCO) at the German Artiffical Intelligence Center (Deutsches Forschungszentrum fur Kunstliche Intelligenz).	In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).	['Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']	1	['In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']
CC1066	P07-1068	Advanced Machine Learning Models for Coreference Resolution	using semantic relations to refine coreference decisions	['H Ji', 'D Westbrook', 'R Grishman']	introduction	We present a novel mechanism for improving reference resolution by using the output of a relation tagger to rescore coreference hypotheses. Experiments show that this new framework can improve performance on two quite different languages - English and Chinese.	As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .	['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	0	['As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']
CC1636	W11-2020	Novel Approaches to Pattern-based Interaction Quality Modeling	on nomatchs noinputs and bargeins do nonacoustic features support anger detection	['Alexander Schmitt', 'Tobias Heinroth', 'Jackson Liscombe']		Most studies on speech-based emotion recognition are based on prosodic and acoustic features, only employing artificial acted corpora where the results cannot be generalized to telephone-based speech applications. In contrast, we present an approach based on utterances from 1,911 calls from a deployed telephone-based speech application, taking advantage of additional dialogue features, NLU features and ASR features that are incorporated into the emotion recognition process. Depending on the task, non-acoustic features add 2.3% in classification accuracy compared to using only acoustic features.	The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( #AUTHOR_TAG ) .	['The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( #AUTHOR_TAG ) .', '(Schmitt et al., 2009).', 'From all 4,832 user turns, 68.5% were non-angry, 14.3% slightly angry, 5.0% very angry and 12.2% contained garbage, i.e. nonspeech events.', 'In total, the number of interaction parameters servings as input variables for the model amounts to 52.']	2	['The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( #AUTHOR_TAG ) .', 'From all 4,832 user turns, 68.5% were non-angry, 14.3% slightly angry, 5.0% very angry and 12.2% contained garbage, i.e. nonspeech events.']
CC210	D13-1115	Integrating Theory and Practice: A Daunting Task	the story picturing engine—a system for automatic text illustration	['Dhiraj Joshi', 'James Z Wang', 'Jia Li']	introduction	We present an unsupervised approach to automated story picturing. Semantic keywords are extracted from the story, an annotated image database is searched. Thereafter, a novel image ranking scheme automatically determines the importance of each image. Both lexical annotations and visual content play a role in determining the ranks. Annotations are processed using the Wordnet. A mutual reinforcement-based rank is calculated for each image. We have implemented the methods in our Story Picturing Engine (SPE) system. Experiments on large-scale image databases are reported. A user study has been performed and statistical analysis of the results has been presented.	Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( #AUTHOR_TAG ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .	['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( #AUTHOR_TAG ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .']	0	['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( #AUTHOR_TAG ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .']
CC1368	W03-0806	Blueprint for a high performance NLP infrastructure	a distributed architecture for robust automatic speech recognition	['Kadri Hacioglu', 'Bryan Pellom']		In this paper, we attempt to decompose a state-of-the-art speech recognition system into its components and define an infrastructure that allows a flexible, efficient and effective interaction among the components. Motivated by the success of DARPA Communicator program, we select the open source Galaxy architecture as our development test bed. It consists of a hub that allows communication among servers connected to it by message passing and supports the plug-and-play paradigm. In addition to message passing it supports high bandwidth data (binary or audio) transfer between servers via a brokering scheme. For several reasons, we believe that it is the right time to start developing a distributed framework for speech recognition along with data and protocol standards supporting interoperability. We present our work towards that goal using the Colorado University (CU) Sonic recognizer. We divide Sonic into a number of components and structure it around the Hub. We describe the system in some detail and report on its present status with some possibilities for future development. 1	There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ) .	['The final interface we intend to implement is a collection of web services for NLP.', 'A web service provides a remote procedure that can be called using XML based encodings (XMLRPC or SOAP) of function names, arguments and results transmitted via internet protocols such as HTTP.', 'Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI.', 'This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ) .', 'Web services will allow components developed by different researchers in different locations to be composed to build larger systems.']	0	['The final interface we intend to implement is a collection of web services for NLP.', 'A web service provides a remote procedure that can be called using XML based encodings (XMLRPC or SOAP) of function names, arguments and results transmitted via internet protocols such as HTTP.', 'Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI.', 'This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ) .', 'Web services will allow components developed by different researchers in different locations to be composed to build larger systems.']
CC1068	P07-1068	Advanced Machine Learning Models for Coreference Resolution	automatic acquisition of hyponyms from large text corpora	['M Hearst']	introduction	We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested. 1 Introduction  Currently there is much interest in the automatic acquisition of lexical syntax and semantics, with the goal of building up large lexicons for natural language processing. Projects..	These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .	"['(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', 'Again, this represents our attempt to coarsely model subcategorization.', ""(4) NE: We use BBN's IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value."", 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .', 'Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.', 'An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun.', 'We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.', '(7) NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs (see Lin (1998a)).', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP."", ""To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.""]"	4	"['(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', ""(4) NE: We use BBN's IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value."", 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .', 'Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.', 'An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun.', 'We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.', '(7) NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs (see Lin (1998a)).', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP.""]"
CC31	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	question answering based on semantic structures	['Srini Narayanan', 'Sanda Harabagiu']	introduction	The ability to answer complex questions posed in Natural Language depends on (1) the depth of the available semantic representations and (2) the inferential mechanisms they support. In this paper we describe a QA architecture where questions are analyzed and candidate answers generated by 1) identifying predicate argument structures and semantic frames from the input and 2) performing structured probabilistic inference using the extracted relations in the context of a domain and scenario model. A novel aspect of our system is a scalable and expressive representation of actions and events based on Coordinated Probabilistic Relational Models (CPRM). In this paper we report on the ability of the implemented system to perform several forms of probabilistic and temporal inferences to extract answers to complex questions. The results indicate enhanced accuracy over current state-of-the-art Q/A systems.	Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( #AUTHOR_TAG ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) .	['Semantic Role labeling (SRL) was first defined in Gildea and Jurafsky (2002).', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.', 'Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( #AUTHOR_TAG ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) .', 'With the efforts of many researchers (Carreras and Màrquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.']	0	['Semantic Role labeling (SRL) was first defined in Gildea and Jurafsky (2002).', 'The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.', 'Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( #AUTHOR_TAG ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) .', 'With the efforts of many researchers (Carreras and Marquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.']
CC245	E03-1005	An efficient implementation of a new DOP model	new ranking algorithms for parsing and tagging kernels over discrete structures and the voted perceptron	['M Collins', 'N Duffy']	introduction	"This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm. We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the ""all subtrees"" (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence. We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data."	#AUTHOR_TAG showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1 's subtrees , reporting a 5.1 % relative reduction in error rate over the model in Collins ( 1999 ) on the WSJ .	"['One instantiation of DOP which has received considerable interest is the model known as DOP1 2 (Bod 1992).', 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).', 'Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.', ""However, the problem of computing the most probable parse turns out to be NP-hard (Sima'an 1996), mainly because the same parse tree can be generated by exponentially many derivations."", ""Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998;Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999;Chappelier et al. 2002)."", ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", 'Goodman (1996Goodman ( , 1998 developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.', 'While Goodman\'s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the ""maximum constituents parse"", i.e. the parse tree which is most likely to have the largest number of correct constituents.', ""Johnson (1998b showed that DOP1's subtree estimation method is statistically biased and inconsistent."", 'Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.', 'This resulted in a statistically consistent model dubbed ML-DOP.', 'However, ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.', 'Cross-validation is needed to avoid this problem.', 'But even with cross-validation, ML-DOP is outperformed by the much simpler DOP1 model on both the ATIS and OVIS treebanks (Bod 2000b).', ""Bonnema et al. (1999) observed that another problem with DOP1's subtree-estimation method is that it provides more probability to nodes with more subtrees, and therefore more probability to larger subtrees."", 'As an alternative, Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.', 'Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.', ""Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003)."", ""#AUTHOR_TAG showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1 's subtrees , reporting a 5.1 % relative reduction in error rate over the model in Collins ( 1999 ) on the WSJ ."", ""Goodman (2002) furthermore showed how Bonnema et al.'s (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction, but did not report any experiments with these reductions.""]"	0	"[""However, the problem of computing the most probable parse turns out to be NP-hard (Sima'an 1996), mainly because the same parse tree can be generated by exponentially many derivations."", ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", 'Goodman (1996Goodman ( , 1998 developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.', ""#AUTHOR_TAG showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1 's subtrees , reporting a 5.1 % relative reduction in error rate over the model in Collins ( 1999 ) on the WSJ .""]"
CC672	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	a statistical parser for czech	['Michael Collins', 'Jan Hajic', 'Lance Ramshaw', 'Christoph Tillmann']	introduction	This paper considers statistical parsing of Czech, which differs radically from English in at least two respects: (1) it is a highly inflected language, and (2) it has relatively free word order. These differences are likely to pose new problems for techniques that have been developed on English. We describe our experience in building on the parsing model of (Collins 97). Our final results- 80% dependency accuracy - represent good progress towards the 91% accuracy of the parser on English (Wall Street Journal) text.	For example , modeling CASE in Czech improves Czech parsing ( #AUTHOR_TAG ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .	['Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors.', 'In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.', 'For example , modeling CASE in Czech improves Czech parsing ( #AUTHOR_TAG ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .', 'It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;.']	4	['Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors.', 'In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.', 'For example , modeling CASE in Czech improves Czech parsing ( #AUTHOR_TAG ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .', 'It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;.']
CC946	K15-1002	A Joint Framework for Coreference Resolution and Mention Head Detection	a constrained latent variable model for coreference resolution	['K-W Chang', 'R Samdani', 'D Roth']		Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.	More details can be found in #AUTHOR_TAG et al. (2013).	['More details can be found in #AUTHOR_TAG et al. (2013).', 'The difference here is that we also consider the validity of mention heads using �(u),�(m)']	0	['More details can be found in #AUTHOR_TAG et al. (2013).']
CC1640	W14-1619	Probabilistic Modeling of Joint-context in Distributional Similarity	learning syntactic categories using paradigmatic representations of word context	['Mehmet Ali Yatbaz', 'Enis Sert', 'Deniz Yuret']	introduction	We investigate paradigmatic representations of word context in the domain of unsupervised syntactic category acquisition. Paradigmatic representations of word context are based on potential substitutes of a word in contrast to syntagmatic representations based on properties of neighboring words. We compare a bigram based baseline model with several paradigmatic models and demonstrate significant gains in accuracy. Our best model based on Euclidean co-occurrence embedding combines the paradigmatic context representation with morphological and orthographic features and achieves 80% many-to-one accuracy on a 45-tag 1M word corpus.	This choice is inspired by recent work on learning syntactic categories ( #AUTHOR_TAG ) , which successfully utilized such language models to represent word window contexts of target words .	['We hypothesize that an underlying model that could capture syntagmatic patterns in large word contexts, yet is flexible enough to deal with data sparseness, is desired.', 'It is generally accepted that the semantics of verbs in particular are correlated with their syntagmatic properties (Levin, 1993;Hanks, 2013).', 'This provides grounds to expect that such model has the potential to excel for verbs.', 'To capture syntagmatic patterns, we choose in this work standard n-gram language models as the basis for a concrete model implementing our scheme.', 'This choice is inspired by recent work on learning syntactic categories ( #AUTHOR_TAG ) , which successfully utilized such language models to represent word window contexts of target words .', 'However, we note that other richer types of language models, such as class-based (Brown et al., 1992) or hybrid (Tan et al., 2012), can be seamlessly integrated into our scheme.']	4	['This choice is inspired by recent work on learning syntactic categories ( #AUTHOR_TAG ) , which successfully utilized such language models to represent word window contexts of target words .', 'However, we note that other richer types of language models, such as class-based (Brown et al., 1992) or hybrid (Tan et al., 2012), can be seamlessly integrated into our scheme.']
CC879	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	the collection and preliminary analysis of a spontaneous speech databasequot darpa speech and natural language workshop	['V Zue', 'N Daly', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff', 'M Soclof']			The recognizer for these systems is the SUMMIT system ( #AUTHOR_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing .	['At present, we have available at MIT two systems, VOYAGER and ATIS, involving specific application domains in which a person can carry on a dialog with the computer, either through spoken speech or through text input.', 'In both of these systems, TINA provides the interface between the recognizer and the application back-end.', 'In this section, I will describe our current interfaces between TINA and the recognizer and our future plans in this area.', 'In addition, I will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation.', 'This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'The recognizer for these systems is the SUMMIT system ( #AUTHOR_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing .', 'The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'The search algorithm is the standard Viterbi search (Viterbi 1967), except that the match involves a network-to-network alignment problem rather than sequence-to-sequence.']	5	['At present, we have available at MIT two systems, VOYAGER and ATIS, involving specific application domains in which a person can carry on a dialog with the computer, either through spoken speech or through text input.', 'In both of these systems, TINA provides the interface between the recognizer and the application back-end.', 'In this section, I will describe our current interfaces between TINA and the recognizer and our future plans in this area.', 'In addition, I will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation.', 'This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'The recognizer for these systems is the SUMMIT system ( #AUTHOR_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing .', 'The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.']
CC120	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	unsupervised methods for determining object and relation synonyms on the web	['A Yates', 'O Etzioni']	experiments	The task of identifying synonymous relations and objects, or synonym resolution, is critical for high-quality information extraction. This paper investigates synonym resolution in the context of unsupervised information extraction, where neither hand-tagged training examples nor domain knowledge is available. The paper presents a scalable, fully-implemented system that runs in O(KN log N) time in the number of extractions, N, and the maximum number of synonyms per word, K. The system, called RESOLVER, introduces a probabilistic relational model for predicting whether two strings are co-referential based on the similarity of the assertions containing them. On a set of two million assertions extracted from the Web, RESOLVER resolves objects with 78% precision and 68% recall, and resolves relations with 90% precision and 35% recall. Several variations of RESOLVER's probabilistic model are explored, and experiments demonstrate that under appropriate conditions these variations can improve F1 by 5%. An extension to the basic RESOLVER system allows it to handle polysemous names with 97% precision and 95% recall on a data set from the TREC corpus.	Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction ( #AUTHOR_TAG ) and textual entailment ( Berant et al. , 2010 ) .	['The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well.', 'Labeled attachment score is used as our base model loss function.', 'In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_�,_)(i__) For each word of the sentence we compute the dis- tance between the word�s position i and the posi- tion of the words head _i.', 'The arc-length score is the summed length of all those with correct head assignments (_(_�i,_i) is 1 if the predicted head and the correct head match, 0 otherwise).', 'The score is normalized by the summed arc lengths for the sentence.', 'The labeled version of this score requires that the labels of the arc are also correct.', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction ( #AUTHOR_TAG ) and textual entailment ( Berant et al. , 2010 ) .']	0	['The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well.', 'In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_,_)(i__) For each word of the sentence we compute the dis- tance between the words position i and the posi- tion of the words head _i.', 'The arc-length score is the summed length of all those with correct head assignments (_(_i,_i) is 1 if the predicted head and the correct head match, 0 otherwise).', 'The score is normalized by the summed arc lengths for the sentence.', 'The labeled version of this score requires that the labels of the arc are also correct.', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction ( #AUTHOR_TAG ) and textual entailment ( Berant et al. , 2010 ) .']
CC1478	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	a mentionsynchronous coreference resolution algorithm based on the bell tree	['Xiaoqiang Luo', 'Abe Ittycheriah', 'Hongyan Jing', 'Nanda Kambhatla', 'Salim Roukos']	experiments	This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.	ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #AUTHOR_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .	"['In this section, we present the coreference results on the devtest defined earlier.', 'First, to see the effect of stem matching features, we compare two coreference systems: one with the stem features, the other with- out.', 'We test the two systems on both ""true"" and system mentions of the devtest set.', 'True mentions mean that input to the coreference system are mentions marked by human, while system mentions are output from the mention detection system.', 'We report results with two metrics: ECM-F and ACE- Value.', 'ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #AUTHOR_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .', 'The result is shown in Table 4: the baseline numbers without stem features are listed under \\Base,"" and the results of the coreference system with stem features are listed under \\Base+Stem.""']"	5	"['In this section, we present the coreference results on the devtest defined earlier.', 'First, to see the effect of stem matching features, we compare two coreference systems: one with the stem features, the other with- out.', 'We test the two systems on both ""true"" and system mentions of the devtest set.', 'True mentions mean that input to the coreference system are mentions marked by human, while system mentions are output from the mention detection system.', 'We report results with two metrics: ECM-F and ACE- Value.', 'ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #AUTHOR_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .', 'The result is shown in Table 4: the baseline numbers without stem features are listed under \\Base,"" and the results of the coreference system with stem features are listed under \\Base+Stem.""']"
CC574	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	firstyear medical students’ information needs and resource selection responses to a clinical scenario	['Keith W Cogdill', 'Margaret E Moore']	introduction	Etude ayant pour but une meilleure comprehension des besoins en information des etudiants en medecine de premiere annee, et de leurs perceptions des ressources appropriees( ressources telles que livres , MEDLINE ... )	MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians ' questions , and is commonly used in that capacity ( #AUTHOR_TAG ; De Groote and Dorsch 2003 ) .	"['Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985;Gorman, Ash, and Wykoff 1994;Ely et al. 1999Ely et al. , 2005.', ""MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians ' questions , and is commonly used in that capacity ( #AUTHOR_TAG ; De Groote and Dorsch 2003 ) ."", 'However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994;Chambliss and Conley 1996).', 'Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts.', 'Not only is clinical question answering interesting from a research perspective, it also represents a potentially high-impact, real-world application of language processing and information retrieval technology-better information systems to provide decision support for physicians have the potential to improve the quality of health care.']"	0	"['Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985;Gorman, Ash, and Wykoff 1994;Ely et al. 1999Ely et al. , 2005.', ""MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians ' questions , and is commonly used in that capacity ( #AUTHOR_TAG ; De Groote and Dorsch 2003 ) .""]"
CC562	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	effective mapping of biomedical text to the umls metathesaurus the metamap program	['Alan R Aronson']	introduction	The UMLS (r) Metathesaurus(r), the largest thesaurus in the biomedical domain, provides a representation of biomedical knowledge consisting of concepts classi-fied by semantic type and both hierarchical and non-hierarchical relationships among the concepts. This knowledge has proved useful for many applications including decision support systems, management of patient records, information retrieval (IR) and data mining. Gaining effective access to the knowledge is critical to the success of these applications. This paper describes MetaMap, a program developed at the National Library of Medicine (NLM) to map bio-medical text to the Metathesaurus or, equivalently, to discover Metathesaurus concepts referred to in text. MetaMap uses a knowledge intensive approach based on symbolic, natural language processing (NLP) and computational linguistic techniques. Besides being applied for both IR and data mining applications, MetaMap is one of the foundations of NLM's Indexing Initiative System which is being applied to both semi-automatic and fully automatic indexing of the biomed-ical literature at the library	Second , software for utilizing this ontology already exists : MetaMap ( #AUTHOR_TAG ) identifies concepts in free text , and SemRep ( Rindflesch and Fiszman 2003 ) extracts relations between the concepts .	['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'This domain is well-suited for exploring the posed research questions for several reasons.', 'First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'Second , software for utilizing this ontology already exists : MetaMap ( #AUTHOR_TAG ) identifies concepts in free text , and SemRep ( Rindflesch and Fiszman 2003 ) extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', 'The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.', 'Third, the paradigm of evidence-based medicine (Sackett et al. 2000) provides a task-based model of the clinical information-seeking process.', 'The PICO framework (Richardson et al. 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system.', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']	0	['First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'Second , software for utilizing this ontology already exists : MetaMap ( #AUTHOR_TAG ) identifies concepts in free text , and SemRep ( Rindflesch and Fiszman 2003 ) extracts relations between the concepts .', 'The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'The PICO framework (Richardson et al. 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system.']
CC387	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	three heads are better than one	['Robert Frederking', 'Sergei Nirenburg']	conclusion	With the stress of ongoing budget cuts librarians are tempted to hunker down and focus exclusively on their clients, their college, department or assigned area. But collaboration across campus, within new areas, with different faculty, and different students, can be beneficial to both student and faculty learning. Students often have research needs which cannot be answered by one faculty member or librarian. Cross disciplinary collaboration between multiple librarians and faculty is key to providing the best service to these students. In this case study a team of agribusiness students need help in preparing for a competition on food distribution. During the contest the students play the role of consultants, listen to a client's problem, research the industry and possible solutions, and then present a solution to the client. This competition requires research on commodities, government policies for food safety, food distribution, economics, management, marketing, and merchandising. A team was formed of an agriculture librarian, business librarian, and an agribusiness faculty advisor in order to cover all the elements required for student success. Each person played a specific role in preparing the students for the competition. The business librarian taught a selection of databases and online resources, the agriculture librarian taught agriculture resources and created a LibGuide specific to the contest, and the faculty advisor gave real world examples about the competition and best practices for their presentations. Outcomes of this collaboration included the sharing of knowledge about the research process, building bonds between faculty and librarians, knowledge transfer between the librarians, and successfully preparing the team of students for their competition	These translations gave rise to a number of automatically constructed linguistic resources : ( 1 ) the original ( source , target ) phrasal translation pairs , ( 2 ) the marker lexicon , ( 3 ) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #AUTHOR_TAG , Frederking et al. ( 1994 ) , and Hogan and Frederking ( 1998 ) .	['These translations gave rise to a number of automatically constructed linguistic resources : ( 1 ) the original ( source , target ) phrasal translation pairs , ( 2 ) the marker lexicon , ( 3 ) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #AUTHOR_TAG , Frederking et al. ( 1994 ) , and Hogan and Frederking ( 1998 ) .']	1	['These translations gave rise to a number of automatically constructed linguistic resources : ( 1 ) the original ( source , target ) phrasal translation pairs , ( 2 ) the marker lexicon , ( 3 ) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #AUTHOR_TAG , Frederking et al. ( 1994 ) , and Hogan and Frederking ( 1998 ) .']
CC1448	W04-1805	Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus	query expansion using lexicalsemantic relations	['Ellen M Voorhees']	introduction	Applications such as office automation, news filtering, help facilities in complex systems, and the like require the ability to retrieve documents from full-text databases where vocabulary problems can be particularly severe. Experiments performed on small collections with single-domain thesauri suggest that expanding query vectors with words that are lexically related to the original query words can ameliorate some of the problems of mismatched vocabularies. This paper examines the utility of lexical query expansion in the large, diverse TREC collection. Concepts are represented by WordNet synonym sets and are expanded by following the typed links included in WordNet. Experimental results show this query expansion technique makes little difference in retrieval effectiveness if the original queries are relatively complete descriptions of the information being sought even when the concepts to be expanded are selected by hand. Less well developed queries can be significantly improved by expansion of hand-chosen concepts. However, an automatic procedure that can approximate the set of hand picked synonym sets has yet to be devised, and expanding by the synonym sets that are automatically generated can degrade retrieval performance.	Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with WoRDNET relations ) .	['Even though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval.', 'Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with WoRDNET relations ) .']	1	['Even though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval.', 'Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with WoRDNET relations ) .']
CC1020	P02-1001	Parameter estimation for probabilistic finite-state transducers	speech recognition by composition of weighted finite automata	['Fernando C N Pereira', 'Michael Riley']	introduction	We present a general framework based on weighted finite automata and weighted finite-state transducers for describing and implementing speech recognizers. The framework allows us to represent uniformly the information sources and data structures used in recognition, including context-dependent units, pronunciation dictionaries, language models and lattices. Furthermore, general but efficient algorithms can used for combining information sources in actual recognizers and for optimizing their application. In particular, a single composition algorithm is used both to combine in advance information sources such as language models and dictionaries, and to combine acoustic observations and information sources dynamically during recognition.	Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( #AUTHOR_TAG ) and machine translation ( Knight and Al-Onaizan , 1998 ) .	"['The availability of toolkits for this weighted case (Mohri et al., 1998;van Noord and Gerdemann, 2001) promises to unify much of statistical NLP.', ""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( #AUTHOR_TAG ) and machine translation ( Knight and Al-Onaizan , 1998 ) ."", 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']"	0	"['The availability of toolkits for this weighted case (Mohri et al., 1998;van Noord and Gerdemann, 2001) promises to unify much of statistical NLP.', ""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( #AUTHOR_TAG ) and machine translation ( Knight and Al-Onaizan , 1998 ) ."", 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']"
CC437	J04-3001	Sample Selection for Statistical Parsing	learning probabilistic lexicalized grammars for natural language processing	['Rebecca Hwa']		A good representation of language is essential to building natural language processing (NLP) applications. In recent years, the growing availability of machine-readable text corpora has popularized the use of corpus-trained probabilistic grammars to represent languages in NLP systems. Although automatically inducing grammars from large corpora is an appealing idea, it faces several challenges. First, the trained grammar must capture the complexity and ambiguities inherent in human languages. Second, to be useful in practical applications, the grammar must be computationally tractable. Third, although there exists an abundance of raw text, the induction of high-quality grammars depends on manually annotated training data, which are scarce; therefore, the learning algorithm must be able to generalize well. Finally, there are inherent trade-offs in attempting to meet all three challenges; a meta-level challenge is to find a good compromise between the competing factors.  To address these challenges, this thesis presents a partially supervised induction algorithm based on the Expectation-Maximization principle for the Probabilistic Lexicalized Tree Insertion Grammar (PLTIG) formalism. Using the lexical properties of the PLTIG formalism in the learning algorithm, we show that it is possible to automatically induce a grammar for a natural language that adequately resolves ambiguities and manages domain complexity at a tractable computational cost. By augmenting the basic learning algorithm with training techniques such as grammar adaptation and sample-selection, we show that the induction's dependency on annotated training data can be significantly reduced. Our empirical studies indicate that even with a 36% reduction in annotated training data, the learning algorithm can nonetheless induce grammars without degrading their quality.	In the first experiment , we use an induction algorithm ( #AUTHOR_TAGa ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs .	"['In the first experiment , we use an induction algorithm ( #AUTHOR_TAGa ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs .', ""The algorithm performs heuristic search through an iterative reestimation procedure to find local optima: sets of values for the grammar parameters that maximizes the grammar's likelihood of generating the training data."", 'In principle, the algorithm supports unsupervised learning; however, because the search space has too many local optima, the algorithm tends to converge on a model that is unsuitable for parsing.', 'Here, we consider a partially supervised variant in which we assume that the learner is given the phrasal boundaries of the training sentences but not the label of the constituent units.', 'For example, the sentence Several fund managers expect a rough market this morning before prices stabilize.', 'would be labeled as ""((Several fund managers) (expect ((a rough market) (this morning)) (before (prices stabilize))).)""', 'Our algorithm is similar to the approach taken by Pereira and Schabes (1992) for inducing PCFG parsers.']"	5	['In the first experiment , we use an induction algorithm ( #AUTHOR_TAGa ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs .']
CC1417	W04-0910	Paraphrastic grammars	an algebra for semantic construction in constraintbased grammars	['A Copestake', 'A Lascarides', 'D Flickinger']			The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .	['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']	1	['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .']
CC1569	W06-2933	Labeled pseudo-projective dependency parsing with support vector machines	design and implementation of the bulgarian hpsgbased treebank	['K Simov', 'P Osenova', 'A Simov', 'M Kouylekov']	experiments		Typical examples are Bulgarian ( #AUTHOR_TAG ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .	['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian ( #AUTHOR_TAG ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']	0	['Typical examples are Bulgarian ( #AUTHOR_TAG ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .']
CC1188	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	a wordclass approach to labeling pscfg rules for machine translation	['Andreas Zollmann', 'Stephan Vogel']	experiments	In this work we propose methods to label probabilistic synchronous context-free grammar (PSCFG) rules using only word tags, generated by either part-of-speech analysis or unsupervised word class induction. The proposals range from simple tag-combination schemes to a phrase clustering model that can incorporate an arbitrary number of features.    Our models improve translation quality over the single generic label approach of Chiang (2005) and perform on par with the syntactically motivated approach from Zollmann and Venugopal (2006) on the NIST large Chinese-to-English translation task. These results persist when using automatically learned word tags, suggesting broad applicability of our technique across diverse language pairs for which syntactic resources are not available.	11 From ( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .	['For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus.', 'The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine.', 'For the hyperparameters, we set Į to 0.1 and p expand = 1/3 to give a preference to the rules with small fragments.', 'We built an s2t translation system with the achieved U-trees after the 1000th iteration.', 'We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 From ( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .', 'Thus, to be convenient, we only conduct experiments with the SAMT system.']	4	['For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus.', 'The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine.', 'For the hyperparameters, we set I to 0.1 and p expand = 1/3 to give a preference to the rules with small fragments.', 'We built an s2t translation system with the achieved U-trees after the 1000th iteration.', 'We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 From ( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .', 'Thus, to be convenient, we only conduct experiments with the SAMT system.']
CC1257	W01-0706	Exploring evidence for shallow parsing	statistical parsing with a contextfree grammar and word statistics	['E Charniak']	introduction		Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"	0	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1550	W06-2807	Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein	free culture how big media uses technology and the law to lock down culture and control creativity	['Lawrence Lessig']		espanolEl presente texto publicado en la Zona REMIX de la revista Communiars se corresponde con la introduccion del afamado libro Cultura Libre, del profesor Lawrence Lessig, presidente de la organizacion Creative Commons, dedicada a promover el acceso e intercambio culturales. El texto > (en espanol Cultura Libre. Como los grandes medios usan la tecnologia y la ley para bloquear la cultura y controlar la creatividad) es un libro publicado en 2004 y centrado en presentar otra manera de organizar la cultura y el conocimiento, abriendo las restricciones del obsoleto paradigma del copyright, y apoyandose en el modelo copyleft promovido desde el software libre. La introduccion que aqui se presenta traduce el espiritu abierto de un texto clave para la comprension y evolucion de la actualidad cultural. La version que se publica procede la version PDF de Free Culture, licenciada bajo Creative Commons en su variante BY-NC 1.0. EnglishThe present text published in the REMIX Zone of the Communiars Journal corresponds to the introduction of the famous book Free Culture, by Professor Lawrence Lessig, president of the Creative Commons organization, dedicated to promoting cultural access and exchange. The text Free Culture. How big media uses technology and the law to lock down culture and control creativity is a book published in 2004 and focused on presenting another way of organizing culture and knowledge, opening the restrictions of the obsolete paradigm of copyright, and relying on the copyleft model promoted by free software. The introduction presented here translates the open spirit of a key text for the understanding and evolution of current cultural reality. The version that is published is part of PDF version of Free Culture, licensed under Creative Commons in its variant BY-NC 1.0.	Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #AUTHOR_TAG ) .	"['If a user lets others edit some lexias, he has the right to retain or refuse the attribution when other users have edited it.', 'In the first instance, the edited version simply moves ahead the document history.', 'In the second one, the last user, who has edited the lexia, may claim the attribution for himself.', 'The lexia will be marked as a derivative work from the original one, and a new document history timeline will start (see Figure 2).', 'Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #AUTHOR_TAG ) .', 'If nobody claims the document for himself, it will fall in the public domain.', 'The set of lexias in the public domain will form a special document, owned by a special user, called Public Domain.', ""If the author refuses the permission to create derivative works, i.e. to edit his own lexias, users still have the right to comment the author's work."", 'So as to come to terms with this idea, we need a concept invented by Nelson (1992), i.e. transclusion.', ""Rather than copy-and-paste contents from a lexia, a user may recall a quotation of the author's lexia and write a comment in the surroundings."", ""In doing so, the link list of the author's lexia will be updated with a special citation link marker, called quotation link (see later for details)."", ""Usually, the quotation will be 'frozen', as in the moment where it was transcluded (see Figure 3)."", 'Consequently the transclusion resembles a copiedand-pasted text chunk, but the link to the original document will always be consistent, i.e. neither it expires nor it returns an error.', 'Otherwise the user who has transcluded the quotation may choose to keep updated the links to the original document.', 'This choice has to be made when the transclusion is done.', 'If so, the transcluded quotation will update automatically, following the history timeline of the original document.', 'For example, if the original document changes topic from stars to pentagons, the quotation transcluded will change topic too (see Figure 4).']"	0	"['Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #AUTHOR_TAG ) .', ""If the author refuses the permission to create derivative works, i.e. to edit his own lexias, users still have the right to comment the author's work.""]"
CC1379	W03-0806	Blueprint for a high performance NLP infrastructure	generative programming methods tools and applications	['Krzysztof Czarnecki', 'Ulrich W Eisenecker']	introduction	"1. What Is This Book About? From Handcrafting to Automated Assembly Lines. Generative Programming. Benefits and Applicability. I. ANALYSIS AND DESIGN METHODS AND TECHNIQUES. 2. Domain Engineering. Why Is This Chapter Worth Reading? What Is Domain Engineering? Domain Analysis. Domain Design and Domain Implementation. Application Engineering. Product-Line Practices. Key Domain Engineering Concepts. Domain. Domain Scope and Scoping. Relationships between Domains. Features and Feature Models. Method Tailoring and Specialization. Survey of Domain Analysis and Domain Engineering Methods. Feature-Oriented Domain Analysis (FODA). Organization Domain Modeling (ODM). Draco. Capture. Domain Analysis and Reuse Environment (DARE). Domain-Specific Software Architecture (DSSA) Approach. Algebraic Approach. Other Approaches. Domain Engineering and Related Approaches. Historical Notes. Summary. 3. Domain Engineering and Object-Oriented Analysis and Design. Why Is This Chapter Worth Reading? OO Technology and Reuse. Solution Space. Problem Space. Relationship between Domain Engineering and Object-Oriented Analysis and Design (OOA/D) Methods. Aspects of Integrating Domain Engineering and OOA/D Methods. Horizontal versus Vertical Methods. Selected Methods. Rational Unified Process. 00ram. Reuse-Driven Software Engineering Business (RSEB). FeatuRSEB. Domain Engineering Method for Reusable Algorithmic Libraries (DEMRAL). 4. Feature Modeling. Why Is This Chapter Worth Reading? Features Revisited. Feature Modeling. Feature Models. Feature Diagrams. Other Infon-Nation Associated with Feature Diagrams in a Feature Model. Assigning Priorities to Variable Features. Availability Sites, Binding Sites, and Binding Modes. Relationship between Feature Diagrams and Other Modeling Notations and Implementation Techniques. Single Inheritance. Multiple Inheritance. Parameterized Inheritance. Static Parameterization. Dynamic Parameterization. Implementing Constraints. Tool Support for Feature Models. Frequently Asked Questions about Feature Diagrams. Feature Modeling Process. How to Find Features. Role of Variability in Modeling. 5. The Process of Generative Programming. Why Is This Chapter Worth Reading? Generative Domain Models. Main Development Steps in Generative Programming. Adapting Domain Engineering for Generative Programming. Domain-Specific Languages. DEMRAL: Example of a Domain Engineering Method for Generative Programming. Outline of DEMRAL. Domain Analysis. Domain Definition. Domain Modeling. Domain Design. Scope Domain Model for Implementation. Identify Packages. Develop Target Architectures and Identify the Implementation Components. Identify User DSLs. Identify Interactions between DSLs. Specify DSLs and Their Translation. Configuration DSLs. Expression DSLs. Domain Implementation. II. IMPLEMENTATION TECHNOLOGIES. 6. Generic Programming. Why Is This Chapter Worth Reading? What Is Generic Programming? Generic versus Generative Programming. Generic Parameters. Parametric versus Subtype Polymorphism. Genericity in Java. Bounded versus Unbounded Polymorphism. A Fresh Look at Polymorphism. Parameterized Components. Parameterized Programming. Types, Interfaces, and Specifications. Adapters. Vertical and Horizontal Parameters. Module Expressions. C++ Standard Template Library. Iterators. Freestanding Functions versus Member Functions. Generic Methodology. Historical Notes. 7. Component-Oriented Template-Based C++ Programming Techniques. Why Is This Chapter Worth Reading? Types of System Configuration. C++ Support for Dynamic Configuration. C++ Support for Static Configuration. Static Typing. Static Binding. Inlining. Templates. Parameterized Inheritance. typedefs. Member Types. Nested Classes. Prohibiting Certain Template Instantiations. Static versus Dynamic Parameterization. Wrappers Based on Parameterized Inheritance. Template Method Based on Parameterized Inheritance. Parameterizing Binding Mode. Consistent Parameterization of Multiple Components. Static Interactions between Components. Components with Influence. Components under Influence. Structured Configurations. Recursive Components. Intelligent Configuration. 8. Aspect-Oriented Decomposition and Composition. Why Is This Chapter Worth Reading? What Is Aspect-Oriented Programming? Aspect-Oriented Decomposition Approaches. Subject-Oriented Programming. Composition Filters. Demeter / Adaptive Programming. Aspect-Oriented Decomposition and Domain Engineering. How Aspects Arise. Composition Mechanisms. Requirements on Composition Mechanisms. Example: Synchronizing a Bounded Buffer. ""Tangled"" Synchronized Stack. Separating Synchronization Using Design Patterns. Separating Synchronization Using SOP. Some Problems with Design Patterns and Some Solutions. Implementing Noninvasive, Dynamic Composition in Smalltalk. Kinds of Crosscutting. How to Express Aspects in Programming Languages. Separating Synchronization Using AspectJ Cool. Implementing Dynamic Cool in Smalltalk. Implementation Technologies for Aspect-Oriented Programming. Technologies for Implementing Aspect-Specific Abstractions. Technologies for Implementing Weaving. AOP and Specialized Language Extensions. AOP and Active Libraries. Final Remarks. 9. Generators. Why Is This Chapter Worth Reading? What Are Generators? Transformational Model of Software Development. Technologies for Building Generators. Compositional versus Transformational Generators. Kinds of Transformations. Compiler Transformations. Source-to-Source Transformations. Transformation Systems. Scheduling Transformations. Existing Transformation Systems and Their Applications. Selected Approaches to Generation. Draco. GenVoca. Approaches Based on Algebraic Specifications. 10. Static Metaprogramming in C++. Why Is This Chapter Worth Reading? What Is Metaprogramming? A Quick Tour of Metaprogramming. Static Metaprogramming. C++ as a Two-Level Language. Functional Flavor of the Static Level. Class Templates as Functions. Integers and Types as Data. Symbolic Names Instead of Variables. Constant Initialization and typedef-Statements Instead of Assignment. Template Recursion Instead of Loops. Conditional Operator and Template Specialization as Conditional Constructs. Template Metaprogramming. Template Metafunctions. Metafinctions as Arguments and Return Values of Other Metafinctions. Representing Metainformation. Member Traits. Traits Classes. Traits Templates. Example: Using Template Metafunctions and Traits Templates to Implement Type Promotions. Compile-Time Lists and Trees as Nested Templates. Compile-Time Control Structures. Explicit Selection Constructs. Template Recursion as a Looping Construct. Explicit Looping Constructs. Code Generation. Simple Code Selection. Composing Templates. Generators Based on Expression Templates. Recursive Code Expansion. Explicit Loops for Generating Code. Example: Using Static Execute Loops to Test Metafunctions. Partial Evaluation in C++. Workarounds for Partial Template Specialization. Problems of Template Metaprogramming. Historical Notes. 11. Intentional Programming. Why Is This Chapter Worth Reading? What Is Intentional Programming? Technology behind IP. System Architecture. Representing Programs in IP: The Source Graph. Source Graph + Methods = Active Source. Working with the IP Programming Environment. Editing. Further Capabilities of the IP Editor. Extending the IP System with New Intentions. Advanced Topics. Questions, Methods, and a Frameworklike Organization. Source-Pattem-Based Polymorphism. Methods as Visitors. Asking Questions Synchronously and Asynchronously. Reduction. The Philosophy behind IP. Why Do We Need Extendible Programming Environments? or What Is the Problem with Fixed Programming Languages? Moving Focus from Fixed Languages to Language Features and the Emergence of an Intention Market. Intentional Programming and Component-Based Development. Frequently Asked Questions. Summary. III. APPLICATION EXAMPLES. 12. List Container. Why Is This Chapter Worth Reading? Overview. Domain Analysis. Domain Design. Implementation Components. Manual Assembly. Specifying Lists. The Generator. Extensions. 13. Bank Account. Why Is This Chapter Worth Reading? The Successful Programming Shop. Design Pattems, Frameworks, and Components. Domain Engineering and Generative Programming. Feature Modeling. Architecture Design. Implementation Components. Configurable Class Hierarchies. Designing a Domain-Specific Language. Bank Account Generator. Testing Generators and Their Products. 14. Generative Matrix Computation Library (GMCL). Why Is This Chapter Worth Reading? Why Matrix Computations? Domain Analysis. Domain Definition. Domain Modeling. Domain Design and Implementation. Matrix Type Generation. Generating Code for Matrix Expressions. Implementing the Matrix Component in IP. APPENDICES. Appendix A: Conceptual Modeling. What Are Concepts? Theories of Concepts. Basic Terminology. The Classical View. The Probabilistic View. The Exemplar View. Summary of the Three Views. Important Issues Concerning Concepts. Stability of Concepts. Concept Core. Informational Contents of Features. Feature Composition and Relationships between Features. Quality of Features. Abstraction and Generalization. Conceptual Modeling, Object-Orientation, and Software Reuse. Appendix B: Instance-Specific Extension Protocol for Smalltalk. Appendix C: Protocol for Attaching Listener Objects in Smalltalk. Appendix D: Glossary of Matrix Computation Terms. Appendix E: Metafunction for Evaluating Dependency Tables. Glossary of Generative Programming Terms. References. Index. 020130977T04062001"	Software engineering research on Generative Programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .	['Software engineering research on Generative Programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .', 'Our infrastructure for NLP will provide high performance 1 components inspired by Generative Programming principles.']	0	['Software engineering research on Generative Programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .']
CC649	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	building a multilingual parallel subtitle corpus	['J¨org Tiedemann']		In this paper on-going work of creating an extensive multilingual parallel corpus of movie subtitles is presented. The corpus currently contains roughly 23,000 pairs of aligned subtitles covering about 2,700 movies in 29 languages. Subtitles mainly consist of transcribed speech, sometimes in a very condensed way. Insertions, deletions and paraphrases are very frequent which makes them a challenging data set to work with especially when applying automatic sentence alignment. Standard alignment approaches rely on translation consistency either in terms of length or term translations or a combination of both. In the paper, we show that these approaches are not applicable for subtitles and we propose a new alignment approach based on time overlaps specifically designed for subtitles. In our experiments we obtain a significant improvement of alignment accuracy compared to standard length-based	results are based on a corpus of movie subtitles ( #AUTHOR_TAG ) , and are consequently shorter sentences , whereas the En â Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) .	['In this section, we compare the different alignments produced with and without PR based on how well they can be used for transfer of linguistic resources across languages.', 'We used the system proposed by Ganchev, Gillenwater, and Taskar (2009).', 'This system uses a word-aligned corpus and a parser for a resource-rich language (source language) in order to create a parser for a resource-poor language (target language).', 'We consider a parse tree on the source language as a set of dependency edges to be transferred.', 'For each such edge, if both end points are aligned to words in the target language, then the edge is transferred.', 'These edges are then used as weak supervision when training a generative or discriminative dependency parser.', 'In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised parse trees on the target side.', 'By changing the threshold in MBR decoding of alignments, we can trade off accuracy of the transferred edges vs. transferring more edges.', 'We generated supervised parses using the first-order model from the MST parser (McDonald, Crammer, and Pereira 2005) trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and Spanish.', 'Following Ganchev, Gillenwater, and Taskar (2009), we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En→Bg) and from English to Spanish (En→Es).', 'results are based on a corpus of movie subtitles ( #AUTHOR_TAG ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) .', 'We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM.']	5	['results are based on a corpus of movie subtitles ( #AUTHOR_TAG ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) .']
CC1433	W04-1610	Automatic Arabic document categorization based on the Naïve Bayes algorithm	modern information retrieval	['R B Yates', 'B R Neto']	experiments	Digital technologies can help scholars to navigate the vast quantities of musical data and source materials now available to them, but an imaginative leap is needed in order to conceptualize the kinds of musicological research questions we might ask of electronic corpora. In particular, our data-rich digital world offers enormous potential for the exploration of musical transmission and relatedness.  In this article, we explore the 16th- and 17th-century instrumental battaglia (battle piece), a genre with a very distinctive collective identity arising from the use of numerous shared ingredients (including melodic, motivic, textural, harmonic and rhythmic features). However, a battaglia is not defined by the presence of a core set of essential features, and exact concordance between these pieces is often remarkably low. This kind of musical 'family resemblance' (formulated after Wittgenstein) poses a serious challenge to both traditional musicological apparatus (for example, finding aids such as thematic catalogues) and Music Information Retrieval (which has often privileged melodic similarity at the expense of other kinds of musical relatedness).  This case study provides a stimulus for rethinking the complex nature of musical similarity. In doing this, we outline a set of requirements for digital tools that could support the discovery, exploration and representation of these kinds of relationships	TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ) .	['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the χ 2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the χ 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the χ 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ) .', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']	0	['(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the kh 2 statistic for a term.', 'TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ) .']
CC491	J05-3003	Gaussian coordinates and the large scale universe	automated extraction of tags from the penn treebank	['John Chen', 'K Vijay-Shanker']	related work	The accuracy of statistical parsing models can be improved with the use of lexical information. Statistical parsing using Lexicalized tree adjoining grammar (LTAG), a kind of lexicalized grammar, has remained relatively unexplored. We believe that is largely in part due to the absence of large corpora accurately bracketed in terms of a perspicuous yet broad coverage LTAG. Our work attempts to alleviate this difficulty. We extract different LTAGs from the Penn Treebank. We show that certain strategies yield an improved extracted LTAG in terms of compactness, broad coverage, and supertagging accuracy. Furthermore, we perform a preliminary investigation in smoothing these grammars by means of an external linguistic resource, namely, the tree families of an XTAG grammar, a hand built grammar of English.	#AUTHOR_TAG explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing .	"['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', '#AUTHOR_TAG explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing .', ""The extraction procedure utilizes a head percolation table as introduced by Magerman (1995) in combination with a variation of Collins's (1997) approach to the differentiation between complement and adjunct."", 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.', 'The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman (1994) and Collins (1997).', 'Each node is subsequently assigned the relevant category based on its constituent type and surface configuration.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Unlike our approach, those of Xia (1999) and Hockenmaier, Bierner, and Baldridge (2004) include a substantial initial correction and clean-up of the Penn-II trees.', 'Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of ""inverse schemata.""']"	0	['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', '#AUTHOR_TAG explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing .']
CC1570	W06-2933	Labeled pseudo-projective dependency parsing with support vector machines	sinica treebank design criteria representational issues and implementation	['K Chen', 'C Luo', 'M Chang', 'F Chen', 'C Chen', 'C Huang', 'Z Gao']	experiments	The disclosed apparatus measures the mass rate of flow of gas through an orifice of a flow nozzle under critical flow conditions and comprises a reservoir having an inlet connected to a source of high pressure through a first valve and an outlet connected to a flow nozzle adapted to conduct pressurized gas from said reservoir under critical flow conditions to a region of low pressure downstream of the flow nozzle. The critical flow conditions are established by a second valve mounted downstream of the nozzle orifice. Density and pressure measuring devices are coupled to the reservoir and provide signals representative of the density and pressure, respectively, of the gas flowing through the flow nozzle. The density and pressure signals are applied to a device which calculates the square root of the product of density and pressure to provide a signal proportional to the mass rate of gas flow.	Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( #AUTHOR_TAG ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .	['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( #AUTHOR_TAG ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']	0	['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( #AUTHOR_TAG ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .']
CC142	D12-1037	Discriminative Training for Log-Linear Based SMT	srilm  an extensible language modeling toolkit	['Andreas Stolcke']	experiments	SRILM is a collection of C++ libraries, executable programs, and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications. SRILM is freely available for noncommercial purposes. The toolkit supports creation and evaluation of a variety of language model types based on N-gram statistics, as well as several related tasks, such as statistical tagging and manipulation of N-best lists and word lattices. This paper summarizes the functionality of the toolkit and discusses its design and implementation, highlighting ease of rapid prototyping, reusability, and combinability of tools. 1	We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( #AUTHOR_TAG ) with modified Kneser-Ney smoothing ( Chen and Goodman , 1998 ) .	['We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( #AUTHOR_TAG ) with modified Kneser-Ney smoothing ( Chen and Goodman , 1998 ) .', 'In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl', 'as the evaluation tool.', 'The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004).']	5	['We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( #AUTHOR_TAG ) with modified Kneser-Ney smoothing ( Chen and Goodman , 1998 ) .']
CC917	J97-4003	On Expressing Lexical Generalizations in HPSG	word formation in lexical type hierarchies a case study of baradjectives in german masters thesis	['Susanne Riehemann']	related work		In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; #AUTHOR_TAG ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .	['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; #AUTHOR_TAG ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']	1	['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; #AUTHOR_TAG ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .']
CC863	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	grammaticallybased automatic word class formationquot	['L Hirschman', 'R Grishman', 'N Sager']			This approach resembles the work by Grishman et al. ( 1986 ) and #AUTHOR_TAG on selectional restrictions .	['Thus far, we have added all semantic filters by hand, and they are implemented in a hard-fail mode, i.e., if the semantic restrictions fail, the node dies.', 'This strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains.', 'In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.', 'Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals.', 'This approach resembles the work by Grishman et al. ( 1986 ) and #AUTHOR_TAG on selectional restrictions .', 'The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', 'There is obviously a great deal more work to be done in this important area.']	1	['This strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains.', 'This approach resembles the work by Grishman et al. ( 1986 ) and #AUTHOR_TAG on selectional restrictions .', 'The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', 'There is obviously a great deal more work to be done in this important area.']
CC197	D13-1115	Integrating Theory and Practice: A Daunting Task	grounded models of semantic representation	['Carina Silberer', 'Mirella Lapata']	introduction	A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evidence suggesting that language is grounded in perception and action. In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data. Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms (i.e., attributes native speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two.	Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	0	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"
CC1021	P02-1001	Parameter estimation for probabilistic finite-state transducers	generic epsilonremoval and input epsilonnormalization algorithms for weighted transducers	['M Mohri']			It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #AUTHOR_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version .	"['• Computing t i is an instance of the well-known algebraic path problem (Lehmann, 1977;Tarjan, 1981a).', 'Let T i = x i •f •y i .', 'Then t i is the total semiring weight w 0n of paths in T i from initial state 0 to final state n (assumed WLOG to be unique and unweighted).', 'It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #AUTHOR_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version .', 'If n and m are the number of states and edges, 19 then both problems are O(n 3 ) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b).', 'For a general graph T i , Tarjan (1981b) shows how to partition into ""hard"" subgraphs that localize the cyclicity or irreducibility, then run the O(n 3 ) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results.', 'The overhead of partitioning and recombining is essentially only O(m).']"	0	['* Computing t i is an instance of the well-known algebraic path problem (Lehmann, 1977;Tarjan, 1981a).', 'Let T i = x i *f *y i .', 'It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #AUTHOR_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version .', 'The overhead of partitioning and recombining is essentially only O(m).']
CC1320	W02-0309	Biomedical text retrieval in languages with a complex morphology	stemming algorithms a case study for detailed evaluation	['D A Hull']	conclusion	The majority of information retrieval experiments are evaluated by measures such as average precision and average recall. Fundamental decisions about the superiority of one retrieval technique over another are made solely on the basis of these measures. We claim that average performance figures need to be validated with a careful statistical analysis and that there is a great deal of additional information that can be uncovered by looking closely at the results of individual queries. This article is a case study of stemming algorithms which describes a number of novel approaches to evaluation and demonstrates their value. (c) 1996 John Wiley & Sons, Inc.	There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .	['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']	0	['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .']
CC294	J00-1004	Learning Dependency Translation Models as Collections of Finite-State Head Transducers	englishtomandarin speech translation with head transducers	['Hiyan Alshawi', 'Fei Xia']		"We describe the head transducer model used in an experimental English-toMandarin speech translation system. Head transduction is a translation method in which weighted finite state transducers are associated with sourcetarget word pairs. The method is suitable for speech translation because it allows efficient bottom up processing. The head transducers in the experimental system have a wider range of output positions than input positions. This asymmetry is motivated by a tradeoff between model complexity and search efficiency. 1 I n t r o d u c t i o n In this paper we describe the head transducer model used for translation in an experimental English-to-Mandarin speech translation system. Head transducer models consist of collections of weighted finite state transducers associated with pairs of lexical items in a bilingual lexicon. Head transducers operate ""outwards"" from the heads of phrases; they convert the left and right dependents of a source word into the left and right dependents of a corresponding target word. The transducer model can be characterized as a statistical translation model, but unlike the models proposed by Brown et al. (1990, 1993), these models have non-uniform linguistically motivated structure, at present coded by hand. The underlying linguistic structure of these models is similar to dependency grammar (Hudson 1984), although dependency representations are not explicitly constructed in our approach to translation. The original motivation for the head transducer models was Fei Xia D e p a r t m e n t of C o m p u t e r and I n f o r m a t i o n Science Univers i ty of P e n n s y l v a n i a Ph i l ade lph ia , PA 19104, USA fx i aQc i s .upenn . edu that they are simpler and more amenable to automatic model structure acquisition as compared with earlier transfer models. We first describe the head transduction approach in general in Section 2. In Section 3 we explain properties of the particular head transducers used in the experimental English-to-Mandarin speech translator. In Section 4, we explain how head transducers help satisfy the requirements of the speech translation application, and we conclude in Section 5. 2 B i l i n g u a l H e a d T r a n s d u c t i o n 2.1 Bilingual Head Transducers A head transducer M is a finite state machine associated with a pair of words, a source word w and a target word v. In fact, w is taken from the set V1 consisting of the source language vocabulary augmented by the ""empty word"" e, and v is taken from V~, the target language vocabulary augmented with e. A head transducer reads from a pair of source sequences, a left source sequence L1 and a right source sequence Rt; it writes to a pair of target sequences, a left target sequence L2 and a right target sequence R2 (Figure 1). Head transducers were introduced in Alshawi 1996b, where the symbols in the source and target sequences are source and target words respectively. In the model described in this paper, the symbols written are dependency relation symbols, or the empty symbol e. The use of relation symbols here is a result of the historical development of the system from an earlier transfer model. A conceptually simpler translator can be built using head transducer models with only lexical items, in which case the distinction between different dependents is implicit in the state of a transducer. In head transducer models, the use of relations corresponds to a type of class-based model (cf Je-"	In the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers ( #AUTHOR_TAG ) and automatically trained transducers ( Alshawi and Douglas 2000 ) with a larger range of positions .	['In the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers ( #AUTHOR_TAG ) and automatically trained transducers ( Alshawi and Douglas 2000 ) with a larger range of positions .']	5	['In the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers ( #AUTHOR_TAG ) and automatically trained transducers ( Alshawi and Douglas 2000 ) with a larger range of positions .']
CC444	J05-3003	Gaussian coordinates and the large scale universe	duden—das stilworterbuch duden—the style dictionary	['editor Dudenredaktion']			She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden ( #AUTHOR_TAG ) .	['Carroll and Rooth (1998) achieve an F-score of 77% against the OALD when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each.', 'Their system recognizes 15 frames, and these do not contain details of subcategorizedfor prepositions.', 'Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3. Sarkar and Zeman (2000) evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88%.', 'However, their evaluation does not examine the extracted subcategorization frames but rather the argument-adjunct distinctions posited by their system.', 'The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German.', 'She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden ( #AUTHOR_TAG ) .', 'We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3.']	0	['Carroll and Rooth (1998) achieve an F-score of 77% against the OALD when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each.', 'She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden ( #AUTHOR_TAG ) .', 'We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3.']
CC1372	W03-0806	Blueprint for a high performance NLP infrastructure	maximum entropy models for natural language ambiguity resolution	['Adwait Ratnaparkhi']	experiments	"This thesis demonstrates that several important kinds of natural language ambiguities can be resolved to state-of-the-art accuracies using a single statistical modeling technique based on the principle of maximum entropy.  We discuss the problems of sentence boundary detection, part-of-speech tagging, prepositional phrase attachment, natural language parsing, and text categorization under the maximum entropy framework. In practice, we have found that maximum entropy models offer the following advantages:  State-of-the-art accuracy. The probability models for all of the tasks discussed perform at or near state-of-the-art accuracies, or outperform competing learning algorithms when trained and tested under similar conditions. Methods which outperform those presented here require much more supervision in the form of additional human involvement or additional supporting resources.  Knowledge-poor features. The facts used to model the data, or features, are linguistically very simple, or ""knowledge-poor"", but yet succeed in approximating complex linguistic relationships.  Reusable software technology. The mathematics of the maximum entropy framework are essentially independent of any particular task, and a single software implementation can be used for all of the probability models in this thesis.  The experiments in this thesis suggest that experimenters can obtain state-of-the-art accuracies on a wide range of natural language tasks, with little task-specific effort, by using maximum entropy probability models."	An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) .	['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']	0	['An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).']
CC959	K15-1003	A Supertag-Context Model for Weakly-Supervised CCG Parser Learning	typesupervised hidden markov models for partofspeech tagging with incomplete tag dictionaries	['Dan Garrette', 'Jason Baldridge']	method	Past work on learning part-of-speech taggers from tag dictionaries and raw data has reported good results, but the assumptions made about those dictionaries are often unrealistic: due to historical precedents, they assume access to information about labels in the raw and test sets. Here, we demonstrate ways to learn hidden Markov model taggers from incomplete tag dictionaries. Taking the min-greedy algorithm (Ravi et al., 2010) as a starting point, we improve it with several intuitive heuristics. We also define a simple HMM emission initialization that takes advantage of the tag dictionary and raw data to capture both the openness of a given tag and its estimated prevalence in the raw data. Altogether, our augmentations produce improvements to performance over the original min-greedy algorithm for both English and Italian data.	We employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus ( #AUTHOR_TAG ; Garrette et al. , 2015 ) .4	['We employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus ( #AUTHOR_TAG ; Garrette et al. , 2015 ) .4', 'This procedure attempts to automatically estimate the frequency of each word/tag combination by dividing the number of raw-corpus occurrences of each word in the dictionary evenly across all of its associated tags.', 'These counts are then combined with estimates of the �openness� of each tag in order to assess its likelihood of appearing with new words.']	5	['We employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus ( #AUTHOR_TAG ; Garrette et al. , 2015 ) .4', 'These counts are then combined with estimates of the openness of each tag in order to assess its likelihood of appearing with new words.']
CC836	J91-2003	On compositional semantics	intended models circumscription and commonsense reasoningquot	['W Zadrozny']	introduction	"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations."	This semantics was constructed ( #AUTHOR_TAGa , 1987b ) as a formal framework for default and commonsense reasoning .	"['We adopt the three-level semantics as a formal tool for the analysis of paragraphs.', 'This semantics was constructed ( #AUTHOR_TAGa , 1987b ) as a formal framework for default and commonsense reasoning .', 'It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.', 'For instance, relating ""they"" to ""apples"" in the sentence (cf.', 'Haugeland 1985 p. 195;Zadrozny 1987a):']"	5	['This semantics was constructed ( #AUTHOR_TAGa , 1987b ) as a formal framework for default and commonsense reasoning .']
CC267	E12-1068	Modeling Inflection and Word-Formation in SMT	factored translation models	['Philipp Koehn', 'Hieu Hoang']	introduction	We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level -- may it be linguistic markup or automatically generated word classes. In a number of experiments we show that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence.	#AUTHOR_TAG showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models .	['We use an additional target factor to obtain the coarse POS for each stem, applying a 7-gram POS model.', '#AUTHOR_TAG showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models .']	0	['We use an additional target factor to obtain the coarse POS for each stem, applying a 7-gram POS model.', '#AUTHOR_TAG showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models .']
CC564	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	automatically identifying health outcome information in medline records	['Dina Demner-Fushman', 'Barbara Few', 'Susan E Hauser', 'George Thoma']		Objective: Understanding the effect of a given intervention on the patient's health outcome is one of the key elements in providing optimal patient care. This study presents a methodology for automatic identification of outcomes-related information in medical text and evaluates its potential in satisfying clinical information needs related to health care outcomes. Design: An annotation scheme based on an evidence-based medicine model for critical appraisal of evidence was developed and used to annotate 633 MEDLINE citations. Textual, structural, and meta-information features essential to outcome identification were learned from the created collection and used to develop an automatic system. Accuracy of automatic outcome identification was assessed in an intrinsic evaluation and in an extrinsic evaluation, in which ranking of MEDLINE search results obtained using PubMed Clinical Queries relied on identified outcome statements. Measurements: The accuracy and positive predictive value of outcome identification were calculated. Effectiveness of the outcome-based ranking was measured using mean average precision and precision at rank 10. Results: Automatic outcome identification achieved 88% to 93% accuracy. The positive predictive value of individual sentences identified as outcomes ranged from 30% to 37%. Outcome-based ranking improved retrieval accuracy, tripling mean average precision and achieving 389% improvement in precision at rank 10. Conclusion: Preliminary results in outcome-based document ranking show potential validity of the evidence-based medicine-model approach in timely delivery of information critical to clinical decision support at the point of service	The extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine ( #AUTHOR_TAG ) .	['The extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine ( #AUTHOR_TAG ) .', 'As will be described herein, the population, problem, and the intervention extractors are based largely on recognition of semantic types and a few manually constructed rules; the outcome extrac-tor, in contrast, is implemented as an ensemble of classifiers trained using supervised machine learning techniques ).', 'These two very different approaches can be attributed to differences in the nature of the frame elements: Whereas problems and interventions can be directly mapped to UMLS concepts, and populations easily mapped to patterns that include UMLS concepts, outcome statements follow no predictable pattern.', 'The initial goal of the annotation effort was to identify outcome statements in abstract text.', 'A physician, two registered nurses, and an engineering researcher manually identified sentences that describe outcomes in 633 MEDLINE abstracts; a post hoc analysis demonstrates good agreement (κ = 0.77).', 'The annotated abstracts were retrieved using PubMed and attempted to model different user behaviors ranging from naive to expert (where advanced search features were employed).', 'With the exception of 50 citations retrieved to answer a question about childhood immunization, the rest of the results were retrieved by querying on a disease, for example, diabetes.', 'Of the 633 citations, 100 abstracts were also fully annotated with population, problems, and interventions.', 'These 100 abstracts were set aside as a held-out test set.', 'Of the remaining citations, 275 were used for training and rule derivation, as described in the following sections.']	5	['The extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine ( #AUTHOR_TAG ) .', 'The initial goal of the annotation effort was to identify outcome statements in abstract text.', 'A physician, two registered nurses, and an engineering researcher manually identified sentences that describe outcomes in 633 MEDLINE abstracts; a post hoc analysis demonstrates good agreement (k = 0.77).', 'The annotated abstracts were retrieved using PubMed and attempted to model different user behaviors ranging from naive to expert (where advanced search features were employed).', 'With the exception of 50 citations retrieved to answer a question about childhood immunization, the rest of the results were retrieved by querying on a disease, for example, diabetes.']
CC1351	W02-1601	A synchronization structure of SSTC and its applications in machine translation	towards memorybased translation	['S Sato', 'M Nagao']			For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #AUTHOR_TAG ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .	['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #AUTHOR_TAG ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']	0	['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #AUTHOR_TAG ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']
CC852	J91-2003	On compositional semantics	intended models circumscription and commonsense reasoningquot	['W Zadrozny']	introduction	"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations."	We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .	['We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.']	2	['We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.']
CC261	E03-1005	An efficient implementation of a new DOP model	Motivation	new ranking algorithms for parsing and tagging kernels over discrete structures and the voted perceptron	['M Collins', 'N Duffy']	"This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm. We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the ""all subtrees"" (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence. We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data."	And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .	"['The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992;Chiang 2000) that do not include nonlexicalized fragments.', 'However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997;Johnson 1998a).', 'The importance of including nonheadwords has become uncontroversial (e.g.', 'Collins 1999;Charniak 2000;Goodman 1998).', ""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]"	4	"['This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992;Chiang 2000) that do not include nonlexicalized fragments.', ""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]"
CC984	P00-1006	A Maximum Entropy/Minimum Divergence Translation Model	a maximum entropy approach to natural language processing	['Adam L Berger', 'Stephen A Della Pietra', 'Vincent J Della Pietra']	introduction	The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.	A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling ( #AUTHOR_TAG ) .	['A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling ( #AUTHOR_TAG ) .']	5	['A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling ( #AUTHOR_TAG ) .']
CC222	D13-1115	Integrating Theory and Practice: A Daunting Task	integrating experiential and distributional data to learn semantic representations	['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']	introduction	The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.	This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .	['In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.', 'The model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .', 'While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning.', 'We also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features.', 'Finally, we describe two ways to extend the model by incorporating three or more modalities.', 'We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning.', 'We release both our code and data to the community for future research. 1']	2	['In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .', 'While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning.', 'We also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features.', 'Finally, we describe two ways to extend the model by incorporating three or more modalities.', 'We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning.']
CC271	E12-1068	Modeling Inflection and Word-Formation in SMT	agreement constraints for statistical machine translation into german	['Philip Williams', 'Philipp Koehn']	related work	Languages with rich inflectional morphology pose a difficult challenge for statistical machine translation. To address the problem of morphologically inconsistent output, we add unification-based constraints to the target-side of a string-to-tree model. By integrating constraint evaluation into the decoding process, implausible hypotheses can be penalised or filtered out during search. We use a simple heuristic process to extract agreement constraints for German and test our approach on an English-German system trained on WMT data, achieving a small improvement in translation accuracy as measured by BLEU.	#AUTHOR_TAG used unification in an SMT system to model some of the agreement phenomena that we model.	['Given a stem such as brother, Toutanova et. al�s system might generate the �stem and inflection� corresponding to and his brother.', 'Viewing and and his as inflection is problematic since a map- ping from the English phrase and his brother to the Arabic stem for brother is required.', 'The situation is worse if there are English words (e.g., ad- jectives) separating his and brother.', 'This required mapping is a significant problem for generaliza- tion.', 'We view this issue as a different sort of prob- lem entirely, one of word-formation (rather than inflection).', 'We apply a �split in preprocessing and resynthesize in postprocessing� approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al.', 'The only work that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', 'Koehn and Hoang (2007) introduced factored SMT.', 'We use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to in- flected forms.', 'Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system).', 'Both efforts were ineffective on large data sets.', '#AUTHOR_TAG used unification in an SMT system to model some of the agreement phenomena that we model.', 'Our CRF framework allows us to use more complex con- text features.']	1	['Given a stem such as brother, Toutanova et. als system might generate the stem and inflection corresponding to and his brother.', 'Viewing and and his as inflection is problematic since a map- ping from the English phrase and his brother to the Arabic stem for brother is required.', 'The situation is worse if there are English words (e.g., ad- jectives) separating his and brother.', 'This required mapping is a significant problem for generaliza- tion.', 'We view this issue as a different sort of prob- lem entirely, one of word-formation (rather than inflection).', 'We apply a split in preprocessing and resynthesize in postprocessing approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al.', 'The only work that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', 'Koehn and Hoang (2007) introduced factored SMT.', 'We use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to in- flected forms.', 'Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system).', 'Both efforts were ineffective on large data sets.', '#AUTHOR_TAG used unification in an SMT system to model some of the agreement phenomena that we model.', 'Our CRF framework allows us to use more complex con- text features.']
CC1232	W00-1017	WIT	understanding unsegmented user utterances in realtime spoken dialogue systems	['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']		This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries.	Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAGa ) .	['Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAGa ) .', 'Thus the system can respond immediately after user pauses when the user has the initiative.', 'When the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way (Dohsaka and Shimazu, 1997).']	0	['Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAGa ) .', 'Thus the system can respond immediately after user pauses when the user has the initiative.', 'When the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way (Dohsaka and Shimazu, 1997).']
CC1094	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	a natural language tutorial dialogue system for physics	['Pamela Jordan', 'Maxim Makatchev', 'Umarani Pappuswamy', 'Kurt VanLehn', 'Patricia Albacete']	introduction	Abstract : We describe the WHY2-ATLAS intelligent tutoring system for qualitative physics that interacts with students via natural language dialogue. We focus on the issue of analyzing and responding to multi-sentential explanations. We explore approaches for achieving a deeper understanding of these explanations and dialogue management approaches and strategies for providing appropriate feedback on them.	Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .	['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']	0	['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.']
CC4	A00-1009	A framework for MT and multilingual NLG systems based on uniform lexico-structural processing	applied text generation	['T Korelsky']	introduction	While we were able to exploit existing research for many of the design issues, it turned out that we needed to develop our own approach to text planning (Rambow 1990). This paper will present the system and a t tempt to show how these design objectives led to particular design decisions. The structure of the paper is as follows. In Section 2, we will present the underlying application and give examples of the output of the System. In Section 3, we will discuss the overall s tructure of Joyce. We then discuss the three main components in turn: the text planner in Section 4, the sentence planner in Section 5 and the realizer in Section 6. We will discuss the text planner in some detail since it represents a new approach to the problem. Section 7 traces the generation of a short text. In Section 8, we address the problem of portability, and wind up by discussing some shortcomings of Joyce in the conclusion.	Our work extends directions taken in systems such as Ariane ( Vauquois and Boitet , 1985 ) , FoG ( Kittredge and Polguere , 1991 ) , JOYCE ( Rambow and #AUTHOR_TAG ) , and LFS ( Iordanskaja et al. , 1992 ) .	['In this paper we present a linguistically motivated framework for uniform lexicostructural processing.', 'It has been used for transformations of conceptual and syntactic structures during generation in monolingual and multilingual natural language generation (NLG) and for transfer in machine translation (MT).', 'Our work extends directions taken in systems such as Ariane ( Vauquois and Boitet , 1985 ) , FoG ( Kittredge and Polguere , 1991 ) , JOYCE ( Rambow and #AUTHOR_TAG ) , and LFS ( Iordanskaja et al. , 1992 ) .', 'Although it adopts the general principles found in the abovementioned systems, the approach presented in this paper is more practical, and we believe, would eventually integrate better with emerging statistics-based approaches to MT.']	2	['Our work extends directions taken in systems such as Ariane ( Vauquois and Boitet , 1985 ) , FoG ( Kittredge and Polguere , 1991 ) , JOYCE ( Rambow and #AUTHOR_TAG ) , and LFS ( Iordanskaja et al. , 1992 ) .']
CC1074	P10-2059	Classification of Feedback Expressions in Multimodal Data	coefficient kappa some uses misuses and alternatives	['Robert L Brennan', 'Dale J Prediger']	introduction	"This paper considers some appropriate and inappropriate uses of coefficient kappa and alternative kappa-like statistics. Discussion is restricted to the descriptive characteristics of these statistics for measuring agreement with categorical data in studies of reliability and validity. Special consideration is given to assumptions about whether marginals are fixed a priori, or free to vary. In reliability studies, when marginals are fixed, coefficient kappa is found to be appropriate. When either or both of the marginals are free to vary, however, it is suggested that the ""chance"" term in kappa be replaced by 1/n, where n is the number of categories. In validity studies, we suggest considering whether one wants an index of improvement beyond ""chance"" or beyond the best a priori strategy employing base rates. In the former case, considerations are similar to those in reliability studies with the marginals for the criterion measure considered as fixed. In the latter case, it is suggested that the largest marginal proportion for the criterion measure be used in place of the ""chance"" term in kappa. Similarities and differences among these statistics are discussed and illustrated with synthetic data."	Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( Cohen , 1960 ) 1 and corrected kappa ( #AUTHOR_TAG ) 2 .	"['In general, dialogue act, agreement and turn anno- tations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement.', 'A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL.', ""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( Cohen , 1960 ) 1 and corrected kappa ( #AUTHOR_TAG ) 2 ."", 'Anvil divides the annotations in slices and compares each slice.', 'We used slices of 0.04 seconds.', 'The inter-coder agreement figures obtained for the three types of annotation are given in Table 2.']"	5	"[""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( Cohen , 1960 ) 1 and corrected kappa ( #AUTHOR_TAG ) 2 .""]"
CC1247	W00-1312	Cross-lingual information retrieval using hidden Markov models	the effects of query structure and dictionary setups in dictionarybased crosslanguage information retrievalquot	['An Pirkola']	method		There are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .	"['A second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .', 'One such method is to treat different translations of the same term as synonyms.', 'Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms.', 'However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'By contrast, our HMM approach supports translation probabilities.', ""The synonym approach is equivalent to changing all non-zero translation probabilities P(W~[ Wy)'s to 1 in our retrieyal function."", 'Even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations.']"	1	['There are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .']
CC1175	P97-1063	A word-to-word model of translational equivalence	how to compile a bilingual collocational lexicon automaticallyquot	['F Smadja']	conclusion		Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #AUTHOR_TAG ) .	"['Even better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy .', ""Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #AUTHOR_TAG ) ."", 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.', 'In this manner, the model can account for a wider range of translation phenomena.']"	3	"['Even better accuracy can be achieved with a more fine-grained link class structure.', ""Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #AUTHOR_TAG ) ."", 'In this manner, the model can account for a wider range of translation phenomena.']"
CC200	D13-1115	Integrating Theory and Practice: A Daunting Task	combining feature norms and text data with topic models	['Mark Steyvers']	introduction	Many psychological theories of semantic cognition assume that concepts are represented by features. The empirical procedures used to elicit features from humans rely on explicit human judgments which limit the scope of such representations. An alternative computational framework for semantic cognition that does not rely on explicit human judgment is based on the statistical analysis of large text collections. In the topic modeling approach, documents are represented as a mixture of learned topics where each topic is represented as a probability distribution over words. We propose feature-topic models, where each document is represented by a mixture of learned topics as well as predefined topics that are derived from feature norms. Results indicate that this model leads to systematic improvements in generalization tasks. We show that the learned topics in the model play in an important role in the generalization performance by including words that are not part of current feature norms.2009 Elsevier B.V. All rights reserved.	Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	0	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"
CC189	D13-1115	Integrating Theory and Practice: A Daunting Task	modeling the shape of the scene a holistic representation of the spatial envelope	['Aude Oliva', 'Antonio Torralba']	experiments	In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category.	We also compute GIST vectors ( #AUTHOR_TAG ) for every image using LearGIST ( Douze et al. , 2009 ) .	"['We also compute GIST vectors ( #AUTHOR_TAG ) for every image using LearGIST ( Douze et al. , 2009 ) .', 'Unlike SURF descriptors, GIST produces a single vector representation for an image.', 'The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall ""gist"" of the whole image.', 'It is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in GIST space correlates well with semantic distance in WordNet.', 'After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality.']"	5	"['We also compute GIST vectors ( #AUTHOR_TAG ) for every image using LearGIST ( Douze et al. , 2009 ) .', 'Unlike SURF descriptors, GIST produces a single vector representation for an image.', 'The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall ""gist"" of the whole image.', 'It is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in GIST space correlates well with semantic distance in WordNet.']"
CC1149	P13-3018	DNA, Words and Models, Statistics of Exceptional Words	conscious choice and some light verbs in urduquot	['M Butt']	related work		#AUTHOR_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure .	"['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries.', '#AUTHOR_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure .', 'Bashir (1993) tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']"	0	"['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries.', '#AUTHOR_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure .', 'Bashir (1993) tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']"
CC896	J97-4003	On Expressing Lexical Generalizations in HPSG	on implementing an hpsg theory aspects of the logical architecture the formalization and the implementation of headdriven phrase structure grammars	['Detmar Meurers']	introduction	The paper presents some aspects involved in the formalization and implementation of HPSG theories. As basis, the logical setups of Carpenter (1992) and King (1989, 1994) are briefly compared regarding their usefulness as basis for HPSGII (Pollard and Sag 1994). The possibilities for expressing HPSG theories in the HPSGII architecture and in various computational systems (ALE, Troll, CUF, and TFS) are discussed. Beside a formal characterization of the possibilities, the paper investigates the specific choices for constraints with certain linguistic motivations, i.e. the lexicon, structure licencing, and grammatical principles. An ALE implementation of a theory for German proposed by Hinrichs and Nakazawa (1994) is used as example and the ALE grammar is included in the appendix.	As shown in #AUTHOR_TAG this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .	['A computational treatment expanding out the lexicon cannot be used for the increasing number of HPSG analyses that propose lexical rules that would result in an infinite lexicon.', 'Most current HPSG analyses of Dutch, German, Italian, and French fall into that category.', '1 Furthermore, since lexical rules in such an approach only serve in a precompilation step, the generalizations captured by the lexical rules cannot be used at run-time.', 'Finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry.', 'This conflicts with the standard assumption made in HPSG that only the properties changed by a lexical rule need be mentioned.', 'As shown in #AUTHOR_TAG this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .']	4	['A computational treatment expanding out the lexicon cannot be used for the increasing number of HPSG analyses that propose lexical rules that would result in an infinite lexicon.', 'Most current HPSG analyses of Dutch, German, Italian, and French fall into that category.', 'Finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry.', 'This conflicts with the standard assumption made in HPSG that only the properties changed by a lexical rule need be mentioned.', 'As shown in #AUTHOR_TAG this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .']
CC521	J06-2002	Generating Referring Expressions that Involve Gradable Properties	two theories about adjectives	['Hans Kamp']	experiments		Similar things hold for multifaceted properties like intelligence ( #AUTHOR_TAG ) .	['). Multidimensionality can also slip in through the backdoor.', 'Consider big, for example, when applied to 3D shapes.', 'If there exists a formula for mapping three dimensions into one (e.g., length × width × height) then the result is one dimension (overall-size), and the algorithm of Section 4 can be applied verbatim.', 'But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situation.', 'Similar things hold for multifaceted properties like intelligence ( #AUTHOR_TAG ) .']	0	['). Multidimensionality can also slip in through the backdoor.', 'Similar things hold for multifaceted properties like intelligence ( #AUTHOR_TAG ) .']
CC86	D10-1052	Does Destination Image and Perceived Destination Quality Influence Tourist Satisfaction and Word of Mouth of Culinary Tourism	ordering phrases with function words	['Hendra Setiawan', 'Min-Yen Kan', 'Haizhou Li']		This paper presents a Function Word centered, Syntax-based (FWS) solution to address phrase ordering in the context of statistical machine translation (SMT). Motivated by the observation that function words often encode grammatical relationship among phrases within a sentence, we propose a probabilistic synchronous grammar to model the ordering of function words and their left and right arguments. We improve phrase ordering performance by lexicalizing the resulting rules in a small number of cases corresponding to function words. The experiments show that the FWS approach consistently outperforms the baseline system in ordering function words' arguments and improving translation quality in both perfect and noisy word alignment scenarios.	The reordering models we describe follow our previous work using function word models for translation ( #AUTHOR_TAG ; Setiawan et al. , 2009 ) .	['The reordering models we describe follow our previous work using function word models for translation ( #AUTHOR_TAG ; Setiawan et al. , 2009 ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases.', 'This section provides a high level overview of our reordering model, which attempts to leverage this information.']	2	['The reordering models we describe follow our previous work using function word models for translation ( #AUTHOR_TAG ; Setiawan et al. , 2009 ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.']
CC272	E12-1068	Modeling Inflection and Word-Formation in SMT	efficient parsing of highly ambiguous contextfree grammars with bit vectors	['Helmut Schmid']	introduction	An efficient bit-vector-based CKY-style parser for context-free parsing is presented. The parser computes a compact parse forest representation of the complete set of possible analyses for large treebank grammars and long input sentences. The parser uses bit-vector operations to parallelise the basic parsing operations. The parser is particularly useful when all analyses are needed rather than just the most probable one.	The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German ( Schmid et al. , 2004 ) and the BitPar parser , which is a state-of-the-art parser of German ( #AUTHOR_TAG ) .	['The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German ( Schmid et al. , 2004 ) and the BitPar parser , which is a state-of-the-art parser of German ( #AUTHOR_TAG ) .']	5	['The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German ( Schmid et al. , 2004 ) and the BitPar parser , which is a state-of-the-art parser of German ( #AUTHOR_TAG ) .']
CC32	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	the specification of the semantic knowledgebase of contemporary chinese	['Hui Wang', 'Weidong Zhan', 'Shiwen Yu']			The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese ( #AUTHOR_TAG ) .	['SemCat (semantic category) of predicate, Sem-Cat of first word, SemCat of head word, SemCat of last word, SemCat of predicate + SemCat of first word, SemCat of predicate + SemCat of last word, predicate + SemCat of head word, SemCat of predicate + head word.', 'The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese ( #AUTHOR_TAG ) .']	5	['The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese ( #AUTHOR_TAG ) .']
CC744	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	machinereadable dictionaries lexical data bases and the lexical system	['Nicoletta Calzolari']			However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #AUTHOR_TAG for further discussion ) .	"['The lispified LDOCE file retains the broad structure of the typesetting tape and divides each entry into a number of fields --head word, pronunciation, grammar codes, definitions, examples, and so forth.', 'However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #AUTHOR_TAG for further discussion ) .', 'For this purpose the formatting codes on the typesetting tape are crucial since they provide clues to the correct structure of this information.', 'For example, word senses are largely defined in terms of the 2000 word core vocabulary, however, in some cases other words (themselves defined elsewhere in terms of this vocabulary) are used.', 'These words always appear in small capitals and can therefore be recognised because they will be preceded by a font change control character.', 'In Figure 1 above the definition of rivet as verb includes the noun definition of ""RIVET 1\'\', as signalled by the font change and the numerical superscript which indicates that it is the first (i.e.', 'noun entry) homograph; additional notation exists for word senses within homographs.', 'On the typesetting tape, font control characters are indicated by hexadecimal numbers within curly brackets.', 'In addition, there is a further complication because this sense is used in the plural and the plural morpheme must be removed before RIVET can be associated with a dictionary entry.', 'However, the restructuring program can achieve this because such morphology is always italicised, so the program knows that, in the context of non-core vocabulary items, the italic font control character signals the Figure 3 occurrence of a morphological variant of a LDOCE head entry.']"	0	"['The lispified LDOCE file retains the broad structure of the typesetting tape and divides each entry into a number of fields --head word, pronunciation, grammar codes, definitions, examples, and so forth.', 'However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #AUTHOR_TAG for further discussion ) .', 'For this purpose the formatting codes on the typesetting tape are crucial since they provide clues to the correct structure of this information.', 'For example, word senses are largely defined in terms of the 2000 word core vocabulary, however, in some cases other words (themselves defined elsewhere in terms of this vocabulary) are used.', 'These words always appear in small capitals and can therefore be recognised because they will be preceded by a font change control character.', 'In Figure 1 above the definition of rivet as verb includes the noun definition of ""RIVET 1\'\', as signalled by the font change and the numerical superscript which indicates that it is the first (i.e.', 'noun entry) homograph; additional notation exists for word senses within homographs.', 'On the typesetting tape, font control characters are indicated by hexadecimal numbers within curly brackets.', 'In addition, there is a further complication because this sense is used in the plural and the plural morpheme must be removed before RIVET can be associated with a dictionary entry.']"
CC1256	W01-0706	Exploring evidence for shallow parsing	cooccurrence and transformation in linguistic structure	['Z S Harris']	introduction		Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997;Charniak, 1997a;Charniak, 1997b;Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns -syntactic phrases or words that participate in a syntactic relationship (Church, 1988;Ramshaw and Marcus, 1995;Argamon et al., 1998;Cardie and Pierce, 1998;Munoz et al., 1999;Punyakanok and Roth, 2001;Buchholz et al., 1999;Tjong Kim Sang and Buchholz, 2000).']"	0	['Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .']
CC1125	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	verbocean mining the web for finegrained semantic verb relations	['Timothy Chklovski', 'Patrick Pantel']	introduction	Broad-coverage repositories of semantic relations between verbs could benefit many NLP tasks. We present a semi-automatic method for extracting fine-grained semantic relations between verbs. We detect similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexicosyntactic patterns over the Web. On a set of 29,165 strongly associated verb pairs, our extraction algorithm yielded 65.5% accuracy. Analysis of error types shows that on the relation strength we achieved 75% accuracy. We provide the resource, called VERBOCEAN, for download at http://semantics.isi.edu/ocean/.	These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( #AUTHOR_TAG ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .	['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( #AUTHOR_TAG ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similar- ity/relatedness scores.']	0	['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( #AUTHOR_TAG ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similar- ity/relatedness scores.']
CC937	K15-1002	A Joint Framework for Coreference Resolution and Mention Head Detection	a joint model for entity analysis coreference typing and linking	['G Durrett', 'D Klein']	experiments	We present a joint model of three core tasks in the entity analysis stack: coreference res-olution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia en-tities). Our model is formally a structured con-ditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.	Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #AUTHOR_TAG ) and HOTCoref system ( Bj Â¨ orkelund and Kuhn , 2014 ) .	['The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0', 'dataset only has mention annotations.', 'Therefore, we preprocess Ontonote-5.0 to derive mention heads using Collins head rules (Collins, 1999) with gold constituency parsing information and gold named entity information.', 'The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec.', '3.1.1.', 'The nonoverlapping mention head assumption in Sec.', '3.1.1', 'can be verified empirically on both ACE-2004 and OntoNotes-5.0', 'datasets.', 'Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #AUTHOR_TAG ) and HOTCoref system ( Bj Â¨ orkelund and Kuhn , 2014 ) .', 'Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 .', 'When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.', 'The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD.', 'We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10', 'We use Gurobi v5.0.1 as our ILP solver.', '3: Performance of coreference resolution for all systems on the CoNLL-2012 dataset.', 'Subscripts ( M , H ) indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.9, λ 1 = 0.25 and λ 2 = 0.2.', 'erence model that we implemented, resulting in a traditional pipelined end-to-end coreference system, namely H-M-Coref.', 'We name our new proposed end-to-end coreference resolution system incorporating both the mention head candidate generation module and the joint framework as H-Joint-M.', 'Evaluation Metrics We compare all systems using three popular metrics for coreference resolution: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAF e ) (Luo, 2005).', 'We use the average F1 scores (AVG) of these three metrics as the main metric for comparison.', 'We use the v7.0 scorer provided by CoNLL-2012 Shared Task 11 .', 'We also evaluate the mention detection performance based on precision, recall and F1 score.', 'As mention heads are important for both mention detection and coreference resolution, we also report results evaluated on mention heads.']	1	['The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0', 'The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'The nonoverlapping mention head assumption in Sec.', 'Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #AUTHOR_TAG ) and HOTCoref system ( Bj Â¨ orkelund and Kuhn , 2014 ) .', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'We use the v7.0 scorer provided by CoNLL-2012 Shared Task 11 .']
CC1449	W04-1805	Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus	using partofspeech and semantic tagging for the corpusbased learning of qualia structure elements	['Pierrette Bouillon', 'Vincent Claveau', 'Cecile Fabre', 'Pascale Sebillot']	method	This paper describes the im plementation and results of a machine learning method de veloped within the inductive logic programming ILP frame work Muggleton and De Raedt to automatically extract from a corpus tagged with parts of speech POS and semantic classes noun verb pairs whose components are bound by one of the relations de ned in the qualia structure in the Genera tive Lexicon Pustejovsky We demonstrate that the seman tic tagging of the corpus improves the quality of the learning both on a theoretical and an empiri cal point of view We also show that a set of the rules learnt by our ILP method have a linguistic signi cance regarding the detec tion of the clues that distinguish in terms of POS and seman tic surrounding context noun verb pairs that are linked by one qualia role from others that are not semantically related	ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ) .	['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.', 'However, the N-V combinations sought are more specific than those that were identified in these previous experiments.']	0	['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ) .']
CC883	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	integration of speech recognition and natural language processing in the mit voyager systemquot	['V Zue', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff']		The MIT VOYAGER speech understanding system is an urban exploration and navigation system that interacts with the user through spoken dialogue, text, and graphics. The authors describe recent attempts at improving the integration between the speech recognition and natural language components. They used the generation capability of the natural language component to produce a word-pair language model to constrain the recognizer's search space, thus improving the coverage of the overall system. They also implemented a strategy in which the recognizer generates the top N word strings and passes them along to the natural language component for filtering. Results on performance evaluation are presented.>	Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .	"['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""We have not yet made use of TINA'Sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort N-best outputs, giving a significant improvement in performance (Goodine et al. 1991)."", ""Ultimately we want to incorporate TINA'Sprobabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model.""]"	5	"['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""We have not yet made use of TINA'Sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort N-best outputs, giving a significant improvement in performance (Goodine et al. 1991)."", ""Ultimately we want to incorporate TINA'Sprobabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model.""]"
CC1358	W02-1601	A synchronization structure of SSTC and its applications in machine translation	representation trees and stringtree correspondences	['C Boitet', 'Y Zaharin']			For more details on the proprieties of SSTC , see #AUTHOR_TAG .	"['The case depicted in Figure 2, describes how the SSTC structure treats some non-standard linguistic phenomena.', 'The particle ""up"" is featurised into the verb ""pick"" and in discontinuous manner (e.g.', '""up"" (4-5) in ""pick-up"" (1-2+4-5)) in the sentence ""He picks the box up"".', 'For more details on the proprieties of SSTC , see #AUTHOR_TAG .']"	0	"['""up"" (4-5) in ""pick-up"" (1-2+4-5)) in the sentence ""He picks the box up"".', 'For more details on the proprieties of SSTC , see #AUTHOR_TAG .']"
CC309	J00-3002	Incremental Processing and Acceptability	parsing and derivational equivalence	['Mark Hepple', 'Glyn Morrill']		It is a tacit assumption of much linguistic inquiry that all distinct derivations of a string should assign distinct meanings. But despite the tidiness of such derlvational uniqueness, there seems to be no a priori reason to assume that a gramma r must have this property. If a grammar exhibits derivational equivalence, whereby distinct derivations of a string assign the same meanings, naive exhaustive search for all derivations will be redundant, and quite possibly intractable. In this paper we show how notions of derivation-reduction and normal form can be used to avoid unnecessary work while parsing with grammars exhibiting derivational equivalence. With grammar regarded as analogous to logic, derivations are proofs; what we are advocating is proof-reduction, and normal form proof; the invocation of these logical techniques adds a further paragraph to the story of parsing-as-deduction	An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in #AUTHOR_TAG but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction .	['(17) By a result of Zielonka (1981), the Lambek calculus is not axiomatizable by any finite set of combinatory schemata, so no such combinatory presentation can constitute the logic of concatenation in the sense of Lambek calculus.', 'Combinatory categorial grammar does not concern itself with the capture of all (or only) the concatenatively valid combinatory schemata, but rather with incrementality, for example, on a shiftreduce design.', 'An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in #AUTHOR_TAG but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction .']	0	['An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in #AUTHOR_TAG but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction .']
CC1366	W03-0806	Blueprint for a high performance NLP infrastructure	combining labeled and unlabeled data with cotraining	['Avrim Blum', 'Tom Mitchell']			Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( #AUTHOR_TAG ) .	"['As discussed earlier, there are two main requirements of the system that are covered by ""high performance"": speed and state of the art accuracy.', 'Efficiency is required both in training and processing.', 'Efficient training is required because the amount of data available for training will increase significantly.', 'Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( #AUTHOR_TAG ) .', 'Processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly.']"	0	"['As discussed earlier, there are two main requirements of the system that are covered by ""high performance"": speed and state of the art accuracy.', 'Efficiency is required both in training and processing.', 'Efficient training is required because the amount of data available for training will increase significantly.', 'Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( #AUTHOR_TAG ) .']"
CC803	J91-2003	On compositional semantics	a grammar of contemporary english	['R Quirk', 'S Greenbaum', 'G Leech', 'J Svartvik']	introduction	"The publication of this important volume fills the need for an up-to-date survey of the entire scope of English syntax. Though it falls short of a perfectly balanced treatment of the whole system, it touches upon all the essential topics and treats in depth a number of crucial problems of current interest such as case, ellipsis, and information focus. Even the publishers' claims are vindicated to a surprising degree. The statement that it ""constitutes a standard reference grammar"" is reasonably well justified. Recent investigations, including the authors' own research, are integrated into the ""accumulated grammatical tradition"" quite effectively. But whether it is ""the fullest and most comprehensive synchronic description of English grammar ever written"" is arguable. No one acquainted with Poutsma's work would agree with that. Very advanced foreign students o r native speakers of English who want to learn about basic grammar will find some of thel sections suitable for their needs, such as the lesson about restrictive and nonrestrictive relative clauses, though even here some of the explanations require very intensive study. Most of the chapters are rather like an advanced textbook for teachers or linguists. The organization and viewpoint give the impression of a carefully planned university lecture supplemented by diagrams, charts, and lists. A good example is the lesson on auxiliaries and verb phrases, which starts with a set of sample sentences demonstrating that ""should see"" and ""happen to see"" behave differently under various transformations and expansions. After the essential concepts are explained and exemplified-lexical verb, semi-auxiliary, operator, and the like-lists and paradigms are given as in the usual reference work. A particularly useful feature of this chapter is the outline of modal auxiliaries with examples of their divergent meanings."	"Opposition (called ""adversative"" or ""contrary-to-expectation"" by Halliday and Hasan 1976;cf. also #AUTHOR_TAG , p. 672 ) ."	"['Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '""And,"" ""or,"" and ""but"" are the three main coordinating connectives in English.', 'However, ""but"" does not behave quite like the other two--semantically, ""but"" signals a contradiction, and in this role it seems to have three subfunctions: . .', 'Opposition (called ""adversative"" or ""contrary-to-expectation"" by Halliday and Hasan 1976;cf. also #AUTHOR_TAG , p. 672 ) .']"	0	"['""And,"" ""or,"" and ""but"" are the three main coordinating connectives in English.', 'However, ""but"" does not behave quite like the other two--semantically, ""but"" signals a contradiction, and in this role it seems to have three subfunctions: . .', 'Opposition (called ""adversative"" or ""contrary-to-expectation"" by Halliday and Hasan 1976;cf. also #AUTHOR_TAG , p. 672 ) .']"
CC761	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	the automatic grammatical tagging of the lob corpus	['Geoffrey Leech', 'Roger Garside', 'Erik Atwell']	conclusion	In collaboration with the English Department, University of Oslo, and the Nowegian Computing Centre for the Humanities, Bergen we have been engaged in the automatic grammatical tagging of the LOB (LancasterOslo/Bergen) Corpus of British English. The computer programs for this task are running at a success rate of approximately 96.7% and a substantial part of the 1,000,000-word corpus has already been tagged. The purpose of this paper is to give an account of the project, with special reference to the methods of tagging we have adopted.	In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #AUTHOR_TAG ) .	['In the longer term, it is clear that neither the contents nor form of any existing published dictionary meet all the requirements of a natural language processing system.', 'A substantial component of the research reported above has been devoted to restructuring LDOCE to make it more suitable for automatic analysis.', 'However, even after this process much of the information in LDOCE remains difficult to access, essentially because it is aimed at a human reader, as opposed to a computer system.', 'This suggests that the automatic construction of dictionaries from published sources intended for other purposes will have a limited life unless lexicography is heavily influenced by the requirements of automated natural language analysis.', 'In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #AUTHOR_TAG ) .', 'However, in the short term, the approach outlined in this paper will allow us to produce a relatively sophisticated and useful dictionary rapidly.']	3	['In the longer term, it is clear that neither the contents nor form of any existing published dictionary meet all the requirements of a natural language processing system.', 'A substantial component of the research reported above has been devoted to restructuring LDOCE to make it more suitable for automatic analysis.', 'However, even after this process much of the information in LDOCE remains difficult to access, essentially because it is aimed at a human reader, as opposed to a computer system.', 'In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #AUTHOR_TAG ) .', 'However, in the short term, the approach outlined in this paper will allow us to produce a relatively sophisticated and useful dictionary rapidly.']
CC1139	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	moses open source toolkit for statistical machine translation	['Philipp Koehn', 'Hieu Hoang', 'Alexandra Birch', 'Chris Callison-Burch', 'Marcello Federico', 'Nicola Bertoldi', 'Brooke Cowan', 'Wade Shen']		We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.	Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .	['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003).', 'There are several methods to build phrase tables.', 'The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.', 'In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .', 'We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .', 'Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.', 'The resulting 1 http://www.statmt.org/wmt10/', 'phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.']	5	['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003).', 'There are several methods to build phrase tables.', 'The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.', 'We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .', 'Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'The resulting 1 http://www.statmt.org/wmt10/', 'phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.']
CC1145	P13-3018	DNA, Words and Models, Statistics of Exceptional Words	causal chains and compound verbsquot	['E Bashir']	related work		"#AUTHOR_TAG tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind""."	"['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure.', '#AUTHOR_TAG tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']"	0	"['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', '#AUTHOR_TAG tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.']"
CC153	D12-1037	Discriminative Training for Log-Linear Based SMT	minimum error rate training in statistical machine translation	['Franz Josef Och']	related work	Minimum Error Rate Training (MERT) is an effective means to estimate the feature func-tion weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature func-tion its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, candidates in MERT are represented as N-best lists which contain the N most probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm that allows for efficiently constructing and repre-senting the exact error surface of all trans-lations that are encoded in a phrase lattice. Compared to N-best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experi-ments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N-best MERT.	( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .	['Several works have proposed discriminative techniques to train log-linear model for SMT.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .', '(Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions.']	1	['Several works have proposed discriminative techniques to train log-linear model for SMT.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .', '(Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions.']
CC1209	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	what’s in a translation rule	['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']	introduction	Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.	Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .	['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']	0	['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']
CC756	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	and forthcoming machine readable dictionaries and research in computational linguistics	['Branimir Boguraev']	introduction		In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #AUTHOR_TAG for details ) .	['The idea of using the machine readable source of a published dictionary has occurred to a wide range of researchers, for spelling correction, lexical analysis, thesaurus construction, and machine translation, to name but a few applications.', 'Most of the work on automated dictionaries has concentrated on extracting lexical or other information, essentially by batch processing (eg. Amsler, 1981 ;Walker and Amsler, 1986), or Copyright 1987 by the Association for Computational Linguistics.', 'Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.', 'To copy otherwise, or to republish, requires a fee and/or specific permission.', '0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b).', 'Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.', 'Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand.', 'Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources.', 'In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #AUTHOR_TAG for details ) .', 'However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.', 'In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.']	0	['Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.', 'To copy otherwise, or to republish, requires a fee and/or specific permission.', 'In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #AUTHOR_TAG for details ) .', 'However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.']
CC1267	W01-0706	Exploring evidence for shallow parsing	introduction to the conll2000 shared task chunking	['E F Tjong Kim Sang', 'S Buchholz']	experiments		Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) to compare it to other shallow parsers .	['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']	5	['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']
CC221	D13-1115	Integrating Theory and Practice: A Daunting Task	how many words is a picture worth automatic caption generation for news images	['Yansong Feng', 'Mirella Lapata']	related work	In this paper we tackle the problem of automatic caption generation for news images. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned. Inspired by recent work in summarization, we propose extractive and abstractive caption generation models. They both operate over the output of a probabilistic image annotation model that pre-processes the pictures and suggests keywords to describe their content. Experimental results show that an abstractive model defined over phrases is superior to extractive methods.	The first work to do this with topic models is #AUTHOR_TAGb ) .	['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #AUTHOR_TAGb ) .', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013).']	0	['The first work to do this with topic models is #AUTHOR_TAGb ) .']
CC1284	W01-0706	Exploring evidence for shallow parsing	the use of classifiers in sequential inference	['V Punyakanok', 'D Roth']	experiments	We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.	Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .	['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']	2	['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .']
CC1087	P10-2059	Classification of Feedback Expressions in Multimodal Data	hidden naive bayes	['Harry Zhang', 'Liangxiao Jiang', 'Jiang Su']			The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( #AUTHOR_TAG ) .	['The multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4.', 'These two sets of data were used for automatic dialogue act classification, which was run in the Weka system (Witten and Frank, 2005).', 'We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table .', 'The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( #AUTHOR_TAG ) .', 'Therefore, here we show the results of this classifier.', 'Ten-folds crossvalidation was applied throughout.']	5	['The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( #AUTHOR_TAG ) .', 'Therefore, here we show the results of this classifier.']
CC123	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	corpus variation and parser performance	['D Gildea']	introduction	Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank. While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might a ect parser performance, and how portable parsing models are across corpora. We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained. This leads us to a technique for pruning parameters to reduce the size of the parsing model.	In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .	['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']	0	['In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.']
CC1489	W06-1104	Automatically creating datasets for measures of semantic relatedness	using information content to evaluate semantic similarity	['Philip Resnik']	related work	Evaluating Semantic similarity has a widely application areas range from Psychology, Linguistics, Cognitive Science to Artificial Intelligence. This paper proposes the merely use of HowNet to evaluate Information Content (IC) as the semantic similarity of two terms or word senses. While the conventional ways of measuring the IC of word senses must depend on both an ontology like WordNet and a large corpus, experiments of this paper prove that the semantic similarity measured in this method is easy to calculate and more approach to human judgments.	This experiment was again replicated by #AUTHOR_TAG with 10 subjects .	"['In the seminal work by Rubenstein and Goodenough (1965), similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.', 'Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'This experiment was again replicated by #AUTHOR_TAG with 10 subjects .', 'Table 1']"	0	['This experiment was again replicated by #AUTHOR_TAG with 10 subjects .', 'Table 1']
CC815	J91-2003	On compositional semantics	the interpretation of tense in discoursequot	['B Webber']	introduction		The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #AUTHOR_TAG ) to see what a formal interpretation of events in time might look like .	"['The text concerns events happening in time.', 'Naturally, we will use a logical notation in which formulas may have temporal and event compo- nents.', 'We assume that any formal interpretation of time will agree with the intuitive one.', 'So it is not necessary now to present a formal semantics here.', 'The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #AUTHOR_TAG ) to see what a formal interpretation of events in time might look like .', 'Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator.', ""Extending and revising Jackendoff's (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram- matical constraint (that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon---ibid.).""]"	0	['The text concerns events happening in time.', 'We assume that any formal interpretation of time will agree with the intuitive one.', 'So it is not necessary now to present a formal semantics here.', 'The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #AUTHOR_TAG ) to see what a formal interpretation of events in time might look like .']
CC1226	W00-1017	WIT	asj continuous speech corpus for research	['Tetsunori Kobayashi', 'Shuichi Itahashi', 'Satoru Hayamizu', 'Toshiyuld Takezawa']			Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #AUTHOR_TAG ) .	"['word hypotheses.', 'As the recogn/fion engine, either VoiceRex, developed by NTI"" (Noda et al., 1998), or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #AUTHOR_TAG ) .', 'This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search (Hirasawa et al., 1998) using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses.', 'This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible.', 'This module continuously runs and outputs recognition results when it detects a speech interval.', 'This enables the language generation module to react immediately to user interruptions while the system is speaking.', 'The language model for speech recognition is a network (regular) grammar, and it allows each speech interval to be an arbitrary number of phrases.', 'A phrase is a sequence of words, which is to be defined in a domain-dependent way.', 'Sentences can be decomposed into a couple of phrases.', 'The reason we use a repetition of phrases instead of a sentence grammar for the language model is that the speech recognition module of a robust spoken dialogue system sometimes has to recognize spontaneously spoken utterances, which include self-repairs and repetition.', 'In Japanese, bunsetsu is appropriate for defining phrases.', 'A bunsetsu consists of one content word and a number (possibly zero) of function words.', 'In the meeting room reservation system we have developed, examples of defined phrases are bunsetsu to specify the room to be reserved and the time of the reservation and bunsetsu to express affirmation and negation.']"	5	['word hypotheses.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #AUTHOR_TAG ) .', 'This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible.', 'This module continuously runs and outputs recognition results when it detects a speech interval.', 'This enables the language generation module to react immediately to user interruptions while the system is speaking.', 'A phrase is a sequence of words, which is to be defined in a domain-dependent way.', 'Sentences can be decomposed into a couple of phrases.', 'In Japanese, bunsetsu is appropriate for defining phrases.', 'A bunsetsu consists of one content word and a number (possibly zero) of function words.']
CC304	J00-2001	Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator	generating natural language linder pragmatic constraints lawrence erlbaum	['Eduard H Hovy']			These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( #AUTHOR_TAGa , 1988c ) .	"['There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( #AUTHOR_TAGa , 1988c ) ."", 'All of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'The text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.', '5']"	0	"['There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( #AUTHOR_TAGa , 1988c ) ."", 'All of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'The text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.', '5']"
CC645	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	a systematic comparison of various statistical alignment models	['Franz Josef Och', 'Hermann Ney']	introduction	We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.	Many researchers use the GIZA + + software package ( #AUTHOR_TAG ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency .	['IBM Models 1 and 2 and the HMM are simple and tractable probabilistic models, which produce the target sentence one target word at a time by choosing a source word and generating its translation.', 'IBM Models 3, 4, and 5 attempt to capture fertility (the tendency of each source word to generate several target words), resulting in probabilistically deficient, intractable models that require local heuristic search and are difficult to implement and extend.', 'Many researchers use the GIZA + + software package ( #AUTHOR_TAG ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency .', 'All of the models are asymmetric (switching target and source languages produces drastically different results) and the simpler models (IBM Models 1, 2, and HMM) do not enforce bijectivity (the majority of words translating as a single word).', 'Although there are systematic translation phenomena where one cannot hope to obtain 1-to-1 alignments, we observe that in over 6 different European language pairs the majority of alignments are in fact 1-to-1 (86-98%).', 'This leads to the common practice of post-processing heuristics for intersecting directional alignments to produce nearly bijective and symmetric results (Koehn, Och, and Marcu 2003).']	0	['IBM Models 1 and 2 and the HMM are simple and tractable probabilistic models, which produce the target sentence one target word at a time by choosing a source word and generating its translation.', 'IBM Models 3, 4, and 5 attempt to capture fertility (the tendency of each source word to generate several target words), resulting in probabilistically deficient, intractable models that require local heuristic search and are difficult to implement and extend.', 'Many researchers use the GIZA + + software package ( #AUTHOR_TAG ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency .', 'All of the models are asymmetric (switching target and source languages produces drastically different results) and the simpler models (IBM Models 1, 2, and HMM) do not enforce bijectivity (the majority of words translating as a single word).', 'Although there are systematic translation phenomena where one cannot hope to obtain 1-to-1 alignments, we observe that in over 6 different European language pairs the majority of alignments are in fact 1-to-1 (86-98%).']
CC821	J91-2003	On compositional semantics	38 examples of elusive antecedents from published texts	['J R Hobbs']	introduction		Adding selectional restrictions ( semantic feature information , #AUTHOR_TAG ) does not solve the problem , because isolated features offer only part of the background knowledge necessary for reference disambiguation .	"['Adding selectional restrictions ( semantic feature information , #AUTHOR_TAG ) does not solve the problem , because isolated features offer only part of the background knowledge necessary for reference disambiguation .', 'Later, Hobbs (1979Hobbs ( , 1982 proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using ""salience"" in choosing facts from this knowledge base.']"	0	"['Adding selectional restrictions ( semantic feature information , #AUTHOR_TAG ) does not solve the problem , because isolated features offer only part of the background knowledge necessary for reference disambiguation .', 'Later, Hobbs (1979Hobbs ( , 1982 proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using ""salience"" in choosing facts from this knowledge base.']"
CC1158	P13-3018	DNA, Words and Models, Statistics of Exceptional Words	lexical representation of derivational relation	['D Bradley']	related work		Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .	['It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'If it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts.']	0	['It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'If it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts.']
CC406	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	examplebased incremental synchronous interpretation	['Hans-Ulrich Block']	introduction	"This article describes a new approach to example based incremental translation for automatic interpretation systems developed in Verbmobil. The translation module is completely learned from a bilingual corpus. The training phase combines statistical word alignment with precomputation of translation ""chunks"" and contextual clustering of syntactic equivalence classes (word classes). The system gives incremental output for every piece of input being it words or sequences of words. It thus tries to mimic the behaviour of a human synchronous interpreter. If a larger context leads to the need for reformulation the system utters a correction marker like I mean, and restarts the output from the starting position of the reformulation. The system is currently effective for German = English. German = Chinese and German a Japanese are under construction. In the Verbmobil evaluation, this approach reached 79% of approximately correct translations on speech recognition output."	In a final processing stage , we generalize over the marker lexicon following a process found in #AUTHOR_TAG .	"['In a final processing stage , we generalize over the marker lexicon following a process found in #AUTHOR_TAG .', ""In Block's approach, word alignments are assigned probabilities by means of a statistical word alignment tool."", 'In a subsequent stage, chunk pairs are extracted, which are then generalized to produce a set of translation templates for each source, target segment.']"	5	"['In a final processing stage , we generalize over the marker lexicon following a process found in #AUTHOR_TAG .', ""In Block's approach, word alignments are assigned probabilities by means of a statistical word alignment tool."", 'In a subsequent stage, chunk pairs are extracted, which are then generalized to produce a set of translation templates for each source, target segment.']"
CC707	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	parsing indian languages with maltparser	['Joakim Nivre']	related work	This paper describes the application of MaltParser, a transition-based dependency parser, to three Indian languages - Bangla, Hindi and Telugu - in the context of the NLP Tools Contest at ICON 2009. In the final evaluation, MaltParser was ranked second among the participating systems and achieved an unlabeled attachment score close to 90% for Bangla and Hindi, and over 85% for Telugu, while the labeled attachment score was 15-25 percentage points lower. It is likely that the high unlabeled accuracy is achieved thanks to a relatively low syntactic complexity in the data sets, while the low labeled accuracy is due to the limited amounts of training data.	Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .	"['Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', 'We go beyond previous work, however, and explore additional lexical and inflectional features.', 'Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .', 'Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task.', 'Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6).', 'Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '9 We expect this kind of feature to yield lower gains for Arabic, unless: r one uses functional feature values (such as those used here for the first time in Arabic NLP), r one uses yet another representation level to account for the otherwise non-identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as VS (where the verb precedes its subject), and r one adequately represents the otherwise ""inverse"" number agreement (a phenomenon common to other Semitic languages, such as Hebrew, too).']"	1	['Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .']
CC183	D13-1115	Integrating Theory and Practice: A Daunting Task	how many words is a picture worth automatic caption generation for news images	['Yansong Feng', 'Mirella Lapata']	introduction	In this paper we tackle the problem of automatic caption generation for news images. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned. Inspired by recent work in summarization, we propose extractive and abstractive caption generation models. They both operate over the output of a probabilistic image annotation model that pre-processes the pictures and suggests keywords to describe their content. Experimental results show that an abstractive model defined over phrases is superior to extractive methods.	Some efforts have tackled tasks such as automatic image caption generation ( #AUTHOR_TAGa ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .	['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'Some efforts have tackled tasks such as automatic image caption generation ( #AUTHOR_TAGa ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .']	0	['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Some efforts have tackled tasks such as automatic image caption generation ( #AUTHOR_TAGa ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .']
CC661	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	a systematic comparison of various statistical alignment models	['Franz Josef Och', 'Hermann Ney']	introduction	We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.	Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #AUTHOR_TAG ) .	['A word alignment for a parallel sentence pair represents the correspondence between words in a source language and their translations in a target language (Brown et al. 1993b).', 'There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, auxiliary verbs used in one language but not the other (e.g., English He walked and French Il est allé), articles required in one language but optional in the other (e.g., English Cars use gas and Portuguese Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expressions translated indirectly.', 'Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #AUTHOR_TAG ) .', 'The top row of Figure 1 shows two word alignments between an English-French sentence pair.', 'We use the following notation: the alignment on the left (right) will be referenced as source-target (target-source) and contains source (target) words as rows and target (source) words as columns.', 'Each entry in the matrix corresponds to a source-target word pair, and is the candidate for an alignment link.', 'Sure links are represented as squares with borders, and possible links']	0	['There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, auxiliary verbs used in one language but not the other (e.g., English He walked and French Il est alle), articles required in one language but optional in the other (e.g., English Cars use gas and Portuguese Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expressions translated indirectly.', 'Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #AUTHOR_TAG ) .', 'The top row of Figure 1 shows two word alignments between an English-French sentence pair.', 'Each entry in the matrix corresponds to a source-target word pair, and is the candidate for an alignment link.']
CC473	J05-3003	Gaussian coordinates and the large scale universe	automatic extraction of subcategorization frames from the bulgarian tree bank	['Svetoslav Marinov', 'Cecilia Hemming']	related work	(1) a. Teodora opened the door. b. *Arto looked the door. In (1-a) the verb open takes as an obligatory argument an NP and therefore differs from look in (1-b), which is an ill-formed sentence, because look require	#AUTHOR_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) .	"['Approaches using treebank-based data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data.', 'Kinyon and Prolo (2002) describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank.', 'This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'Each of these sequences was categorized as a modifier or argument.', 'Arguments were then mapped to traditional syntactic functions.', 'For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic function is subject.', 'In general, argumenthood was preferred over adjuncthoood.', 'As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is.', 'Sarkar and Zeman (2000) present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank (Hajic 1998).', 'Czech is a language with a freer word order than English and so configurational information cannot be relied upon.', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'Finding subcategorization frames involves filtering adjuncts from the observed frame.', 'This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score.', 'The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more).', '#AUTHOR_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) .', ""In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming's system collects both arguments and adjuncts."", 'It then uses the binomial log-likelihood ratio to filter incorrect frames.', 'The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees.', 'The work done for Bulgarian is small-scale, however, as Marinov and Hemming are working with a preliminary version of the treebank with 580 sentences.']"	0	"['Kinyon and Prolo (2002) describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank.', 'This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'Arguments were then mapped to traditional syntactic functions.', 'For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic function is subject.', 'In general, argumenthood was preferred over adjuncthoood.', 'As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is.', 'Czech is a language with a freer word order than English and so configurational information cannot be relied upon.', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'Finding subcategorization frames involves filtering adjuncts from the observed frame.', '#AUTHOR_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) .', ""In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming's system collects both arguments and adjuncts."", 'The work done for Bulgarian is small-scale, however, as Marinov and Hemming are working with a preliminary version of the treebank with 580 sentences.']"
CC1073	P10-2059	Classification of Feedback Expressions in Multimodal Data	linguistic functions of head movements in the context of speech	['Evelyn McClave']	introduction		Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #AUTHOR_TAG for an overview ) .	['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #AUTHOR_TAG for an overview ) .', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example, Jokinen and Ragni (2007) and Jokinen et al. (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'Related are also the studies by Rieks op den Akker and Schulz (2008) and Murray and Renals (2008): both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus.']	0	['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #AUTHOR_TAG for an overview ) .']
CC203	D13-1115	Integrating Theory and Practice: A Daunting Task	latent dirichlet allocation	['David M Blei', 'Andrew Y Ng', 'Michael I Jordan']	method	Topic models (e.g., pLSA, LDA, sLDA) have been widely used for segmenting imagery. However, these models are confined to crisp segmentation, forcing a visual word (i.e., an image patch) to belong to one and only one topic. Yet, there are many images in which some regions cannot be assigned a crisp categorical label (e.g., transition regions between a foggy sky and the ground or between sand and water at a beach). In these cases, a visual word is best represented with partial memberships across multiple topics. To address this, we present a partial membership latent Dirichlet allocation (PM-LDA) model and an associated parameter estimation algorithm. This model can be useful for imagery where a visual word may be a mixture of multiple topics. Experimental results on visual and sonar imagery show that PM-LDA can produce both crisp and soft semantic image segmentations; a capability previous topic modeling methods do not have.Comment: Version 1, Sent for Review. arXiv admin note: substantial text   overlap with arXiv:1511.0282	Latent Dirichlet Allocation ( #AUTHOR_TAG ) , or LDA , is an unsupervised Bayesian probabilistic model of text documents .	['Latent Dirichlet Allocation ( #AUTHOR_TAG ) , or LDA , is an unsupervised Bayesian probabilistic model of text documents .', 'It assumes that all documents are probabilistically generated from a shared set of K common topics, where each topic is a multinomial distribution over the vocabulary (notated as β), and documents are modeled as mixtures of these shared topics (notated as θ).', 'LDA assumes every document in the corpus is generated using the fol-lowing generative process:']	0	['Latent Dirichlet Allocation ( #AUTHOR_TAG ) , or LDA , is an unsupervised Bayesian probabilistic model of text documents .', 'It assumes that all documents are probabilistically generated from a shared set of K common topics, where each topic is a multinomial distribution over the vocabulary (notated as b), and documents are modeled as mixtures of these shared topics (notated as th).', 'LDA assumes every document in the corpus is generated using the fol-lowing generative process:']
CC56	D09-1056	The role of named entities in web people search	named entity disambiguation a hybrid statistical and rulebased incremental approach	['Hien T Nguyen', 'Tru H Cao']	related work	The rapidly increasing use of large-scale data on the Web makes named entity disambiguation become one of the main challenges to research in Information Extraction and development of Semantic Web. This paper presents a novel method for detecting proper names in a text and linking them to the right entities in Wikipedia. The method is hybrid, containing two phases of which the first one utilizes some heuristics and patterns to narrow down the candidates, and the second one employs the vector space model to rank the ambiguous cases to choose the right candidate. The novelty is that the disambiguation process is incremental and includes several rounds that filter the candidates, by exploiting previously identified entities and extending the text by those entity attributes every time they are successfully resolved in a round. We test the performance of the proposed method in disambiguation of names of people, locations and organizations in texts of the news domain. The experiment results show that our approach achieves high accuracy and can be used to construct a robust named entity disambiguation system.	Some researchers ( Cucerzan , 2007 ; #AUTHOR_TAG ) have explored the use of Wikipedia information to improve the disambiguation process .	['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers ( Cucerzan , 2007 ; #AUTHOR_TAG ) have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']	0	['Some researchers ( Cucerzan , 2007 ; #AUTHOR_TAG ) have explored the use of Wikipedia information to improve the disambiguation process .']
CC354	J02-3002	Periods, Capitalized Words, etc.	some applications of treebased modeling to speech and language indexing”	['Michael D Riley']			Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #AUTHOR_TAG ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .	['Automatically trainable software is generally seen as a way of producing sys- tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #AUTHOR_TAG ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Ma- chine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con- text of a potential sentence-terminating punctuation sign.', 'Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.', 'This implies an investment in the data annotation phase.']	0	['Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #AUTHOR_TAG ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.', 'This implies an investment in the data annotation phase.']
CC716	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	toward natural language computation	['A Biermann', 'B Ballard']	experiments	"A computer programming system called the ""Natural Language Computer"" (NLC) is described which allows a user to type English commands while watching them executed on sample data appearing on a display screen. Direct visual feedback enables the user to detect most misinterpretation errors as they are made so that incorrect or ambiguous commands can be retyped or clarified immediately. A sequence of correctly executed commands may be given a name and used as a subroutine, thus extending the set of available operations and allowing larger English-language programs to be constructed hierarchically. In addition to discussing the transition network syntax and procedural semantics of the system, special attention is devoted to the following topics: the nature of imperative sentences in the matrix domain; the processing of non-trivial noun phrases; conjunction; pronominals; and programming constructs such as ""if"", ""repeat"", and procedure definition."	An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #AUTHOR_TAG ) .	['This section has given an overview of the approach to history-based expectation processing.', 'The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented.', 'The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve.', 'The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.', 'An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #AUTHOR_TAG ) .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).', '[The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.]']	0	['An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #AUTHOR_TAG ) .', '[The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.]']
CC1283	W01-0706	Exploring evidence for shallow parsing	a linear observed time statistical parser based on maximum entropy models	['A Ratnaparkhi']	introduction	This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.	Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"	0	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC343	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	multilingual coreference resolution	['Sanda Harabagiu', 'Steven Maiorano']		The current work investigates the problems that occur when coreference resolution is considered as a multilingual task. We assess the issues that arise when a framework using the mention-pair coreference resolution model and memory-based learning for the resolution process are used. Along the way, we revise three essential subtasks of coreference resolution: mention detection, mention head detection and feature selection. For each of these aspects we propose various multilingual solutions including both heuristic, rule-based and machine learning methods. We carry out a detailed analysis that includes eight different languages (Arabic, Catalan, Chinese, Dutch, English, German, Italian and Spanish) for which datasets were provided by the only two multilingual shared tasks on coreference resolution held so far: SemEval-2 and CoNLL-2012. Our investigation shows that, although complex, the coreference resolution task can be targeted in a multilingual and even language independent way. We proposed machine learning methods for each of the subtasks that are affected by the transition, evaluated and compared them to the performance of rule-based and heuristic approaches. Our results confirmed that machine learning provides the needed flexibility for the multilingual task and that the minimal requirement for a language independent system is a part-of-speech annotation layer provided for each of the approached languages. We also showed that the performance of the system can be improved by introducing other layers of linguistic annotations, such as syntactic parses (in the form of either constituency or dependency parses), named entity information, predicate argument structure, etc. Additionally, we discuss the problems occurring in the proposed approaches and suggest possibilities for their improvement	Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; #AUTHOR_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .	['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; #AUTHOR_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']	0	['Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; #AUTHOR_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.']
CC378	J02-3002	Periods, Capitalized Words, etc.	mitre description of the alembic system used for muc6”	['John S Aberdeen', 'John D Burger', 'David S Day', 'Lynette Hirschman', 'Patricia Robinson', 'Marc Vilain']			The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system ( #AUTHOR_TAG ) : a 0.5 % error rate .	['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system ( #AUTHOR_TAG ) : a 0.5 % error rate .', 'The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus.', 'In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000).', 'We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.']	1	['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system ( #AUTHOR_TAG ) : a 0.5 % error rate .', 'The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus.']
CC1520	W06-1705	Annotated web as corpus	introduction to the special issue on the web as corpus	['A Kilgarriff', 'G Grefenstette']	introduction	The Web, teeming as it is with language data, of all manner of varieties and languages, in vast quantity and freely available, is a fabulous linguists' playground. This special issue of Computational Linguistics explores ways in which this dream is being explored.	In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ) .	['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']	0	['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ) .', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).']
CC1152	P13-3018	DNA, Words and Models, Statistics of Exceptional Words	what can we learn from the morphology of hebrew a maskedpriming investigation of morphological representation	['R Frost', 'K I Forster', 'A Deutsch']	introduction	All Hebrew words are composed of 2 interwoven morphemes: a triconsonantal root and a phonological word pattern. the lexical representations of these morphemic units were examined using masked priming. When primes and targets shared an identical word pattern, neither lexical decision nor naming of targets was facilitated. In contrast root primes facilitated both lexical decisions and naming of target words that were derived from these roots. This priming effect proved to be independent of meaning similarity because no priming effects were found when primes and targets were semantically but not morphologically related. These results suggest that Hebrew roots are lexical units whereas word patterns are not. A working model of lexical organization in Hebrew is offered on the basis of these results.	There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; Grainger , et al. , 1991 ; Drews and Zwitserlood , 1995 ) .	['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; Grainger , et al. , 1991 ; Drews and Zwitserlood , 1995 ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.']	0	['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; Grainger , et al. , 1991 ; Drews and Zwitserlood , 1995 ) .']
CC671	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	catib the columbia arabic treebank	['Nizar Habash', 'Ryan Roth']	experiments	The Columbia Arabic Treebank (CATiB) is a resource for Arabic parsing. CATiB contrasts with previous efforts on Arabic treebanking and treebanking of morphologically rich languages in that it encodes less linguistic information in the interest of speedier annotation of large amounts of text. This paper describes CATiB's representation and annotation procedure, and reports on achieved inter-annotator agreement and annotation speed	We use the Columbia Arabic Treebank ( CATiB ) ( #AUTHOR_TAG ) .	"['We use the Columbia Arabic Treebank ( CATiB ) ( #AUTHOR_TAG ) .', 'Specifically, we use the portion converted from Part 3 of the PATB to the CATiB format, which enriches the CATiB dependency trees with full PATB morphological information.', ""CATiB's dependency representation is based on traditional Arabic grammar and emphasizes syntactic case relations."", 'It has a reduced POS tag set consisting of six tags only (henceforth CATIB6).', 'The tags are: NOM (non-proper nominals including nouns, pronouns, adjectives, and adverbs), PROP (proper nouns), VRB (active-voice verbs), VRB-PASS (passive-voice verbs), PRT (particles such as prepositions or conjunctions), and PNX (punctuation).', 'CATiB uses a standard set of eight dependency relations: SBJ and OBJ for subject and (direct or indirect) object, respectively (whether they appear pre-or postverbally); IDF for the idafa (possessive) relation; MOD for most other modifications; and other less common relations that we will not discuss here.', 'For other PATB-based POS tag sets, see Sections 2.6 and 2.7.']"	5	['We use the Columbia Arabic Treebank ( CATiB ) ( #AUTHOR_TAG ) .']
CC851	J91-2003	On compositional semantics	episodes as chunks in narrative memoryquot	['J B Black', 'G H Bower']	introduction		Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .	['An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .']	0	['An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .']
CC938	K15-1002	A Joint Framework for Coreference Resolution and Mention Head Detection	understanding the value of features for coreference resolution	['E Bengtson', 'D Roth']	experiments	In recent years there has been substantial work on the important problem of coreference resolution, most of which has concentrated on the development of new models and algorithmic techniques. These works often show that complex models improve over a weak pairwise baseline. However, less attention has been given to the importance of selecting strong features to support learning a coreference model.    This paper describes a rather simple pairwise classification model for coreference resolution, developed with a well-designed set of features. We show that this produces a state-of-the-art system that outperforms systems built with complex models. We suggest that our system can be used as a baseline for the development of more complex models -- which may have less impact when a more robust set of features is used. The paper also presents an ablation study and discusses the relative contributions of various features.	We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; #AUTHOR_TAG ) .	['The ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; #AUTHOR_TAG ) .', 'The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents.', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'We report results on the test documents for both datasets.', ') indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.8, λ 1 = 0.2 and λ 2 = 0.3.']	5	['The ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; #AUTHOR_TAG ) .', 'The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents.', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'We report results on the test documents for both datasets.', ') indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.']
CC328	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	cogniac high precision coreference with limited knowledge and linguistic resources	['Breck Baldwin']		This paper presents a high precision pronoun resolution system that is capable of greater than 90% precision with 60% and better recall for some pronouns. It is suggested that the system is resolving a sub-set of anaphors that do not require general world knowledge or sophisticated linguistic processing for successful resolution. The system does this by being very sensitive to ambiguity, and only resolving pronouns when very high confidence rules have been satisfied. The system is capable of 'noticing' ambiguity because it requires that there be a unique antecedent within a salience ranking, and the salience rankings are not total orders, i.e. two or more antecedents can be equally salient. Given the nature of the systems rules, it is very likely that they are largely domain independent and that they reflect processing strategies used by humans for general language comprehension. The system has been evaluated in two distinct experiments which support the overall validity of the approach.	A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; #AUTHOR_TAG ; Mitkov 1996 , 1998b ) .	['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input.', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; #AUTHOR_TAG ; Mitkov 1996 , 1998b ) .']	0	['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input.', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; #AUTHOR_TAG ; Mitkov 1996 , 1998b ) .']
CC1050	P07-1068	Advanced Machine Learning Models for Coreference Resolution	libsvm a library for support vector machines software available at httpwwwcsientuedutw∼cjlinlibsvm	['C-C Chang', 'C-J Lin']	introduction		Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.�s method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #AUTHOR_TAG )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features.	['In addition, we train an SVM classifier for SC determination by combining the output of five clas- sification methods: DL, 1-NN, ME, NB, and Soon et al.�s method as described in the introduction,8 with the goal of examining whether SC classifica- tion accuracy can be improved by combining the output of individual classifiers in a supervised man- ner.', 'Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.�s method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #AUTHOR_TAG )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features.', 'More specifically, let L = i li\x0e ; li2 ; li3 ; li4 ; li5 be the set of predictions that we obtained for i in step (2).', 'To represent i, we generate one feature from each non-empty subset of Li.']	5	['In addition, we train an SVM classifier for SC determination by combining the output of five clas- sification methods: DL, 1-NN, ME, NB, and Soon et al.s method as described in the introduction,8 with the goal of examining whether SC classifica- tion accuracy can be improved by combining the output of individual classifiers in a supervised man- ner.', 'Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.�s method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #AUTHOR_TAG )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features.', 'More specifically, let L = i li\x0e ; li2 ; li3 ; li4 ; li5 be the set of predictions that we obtained for i in step (2).', 'To represent i, we generate one feature from each non-empty subset of Li.']
CC1033	P05-3005	Dynamically generating a protein entity dictionary using online resources	sgd saccharomyces genome database nucleic acids res	['Cherry JM', 'C Adler', 'C Ball', 'Chervitz SA', 'Dwight SS', 'Hester ET', 'Y Jia', 'G Juvik', 'T Roe', 'M Schroeder']			Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) #AUTHOR_TAG , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .	['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) #AUTHOR_TAG , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']	5	['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) #AUTHOR_TAG , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']
CC1031	P02-1001	Parameter estimation for probabilistic finite-state transducers	speech recognition by composition of weighted finite automata	['Fernando C N Pereira', 'Michael Riley']	introduction	We present a general framework based on weighted finite automata and weighted finite-state transducers for describing and implementing speech recognizers. The framework allows us to represent uniformly the information sources and data structures used in recognition, including context-dependent units, pronunciation dictionaries, language models and lattices. Furthermore, general but efficient algorithms can used for combining information sources in actual recognizers and for optimizing their application. In particular, a single composition algorithm is used both to combine in advance information sources such as language models and dictionaries, and to combine acoustic observations and information sources dynamically during recognition.	A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .	['A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .', 'The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (λ); reading another b (ν); transducing b to p rather than q (µ); starting to transduce p to rather than x (ρ). 4 To prove (1)⇒(3), express f as an FST and apply the well-known Kleene-Schützenberger construction (Berstel and Reutenauer, 1988), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'A full proof is straightforward, as are proofs of (3)⇒( 2), ( 2)⇒(1).']	0	['A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .']
CC901	J97-4003	On Expressing Lexical Generalizations in HPSG	lexical rules in hpsg what are they	['Mike Calcagno', 'Carl Pollard']	introduction		Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; Calcagno 1995 ; #AUTHOR_TAG ) and the .	['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; Calcagno 1995 ; #AUTHOR_TAG ) and the .', 'lexical rules (DLRs; Meurers 1995).', '5']	0	['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; Calcagno 1995 ; #AUTHOR_TAG ) and the .', 'lexical rules (DLRs; Meurers 1995).', '5']
CC1597	W06-3309	Generative content models for structural analysis of medical abstracts	categorization of sentence types in medical abstracts	['Larry McKnight', 'Padmini Srinivasan']	introduction	"This study evaluated the use of machine learning techniques in the classification of sentence type. 7253 structured abstracts and 204 unstructured abstracts of Randomized Controlled Trials from MedLINE were parsed into sentences and each sentence was labeled as one of four types (Introduction, Method, Result, or Conclusion). Support Vector Machine (SVM) and Linear Classifier models were generated and evaluated on cross-validated data. Treating sentences as a simple ""bag of words"", the SVM model had an average ROC area of 0.92. Adding a feature of relative sentence location improved performance markedly for some models and overall increasing the average ROC to 0.95. Linear classifier performance was significantly worse than the SVM in all datasets. Using the SVM model trained on structured abstracts to predict unstructured abstracts yielded performance similar to that of models trained with unstructured abstracts in 3 of the 4 types. We conclude that classification of sentence type seems feasible within the domain of RCT's. Identification of sentence types may be helpful for providing context to end users or other text summarization techniques."	Table (b) again reproduces the results from #AUTHOR_TAG (2003) for a comparable task on a different subset of 206 unstructured abstracts.	['Table (b) again reproduces the results from #AUTHOR_TAG (2003) for a comparable task on a different subset of 206 unstructured abstracts.']	1	['Table (b) again reproduces the results from #AUTHOR_TAG (2003) for a comparable task on a different subset of 206 unstructured abstracts.']
CC1029	P02-1001	Parameter estimation for probabilistic finite-state transducers	expectation semirings flexible em for finitestate transducers	['Jason Eisner']	introduction		Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .	"['o:e −→, and a:ae −→ share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .']"	0	['Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .']
CC1396	W04-0910	Paraphrastic grammars	an open source grammar development environment and broadcoverage english grammar using hpsg	['Ann Copestake', 'Dan Flickinger']		The LinGO (Linguistic Grammars Online) project's English Resource Grammar and the LKB grammar development environment are language resources which are freely available for download for any purpose, including commercial use (see http://lingo.stanford.edu). Executable programs and source code are both included. In this paper, we give an outline of the LinGO English grammar and LKB system, and discuss the ways in which they are currently being used. The grammar and processing system can be used independently or combined to give a central component which can be exploited in a variety of ways. Our intention in writing this paper is to encourage more people to use the technology, which supports collaborative development on many levels.	Thus for instance , ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .	"['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']"	0	"['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']"
CC458	J05-3003	Gaussian coordinates and the large scale universe	lexical functional grammar volume 34 of syntax and semantics	['Mary Dalrymple']			According to #AUTHOR_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ .	['The value of the PRED attribute in an f-structure is a semantic form Π gf 1 , gf 2 , . . .', ', gf n , where Π is a lemma and gf a grammatical function.', 'The semantic form provides an argument list gf 1 ,gf 2 , . . .', ',gf n specifying the governable grammatical functions (or arguments) required by the predicate to form a grammatical construction.', 'In Figure 1 the verb FOCUS requires a subject and an oblique object introduced by the preposition on: FOCUS (↑ SUBJ)(↑ OBL on ) .', 'The argument list can be empty, as in the PRED value for judge in Figure 1.', 'According to #AUTHOR_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ .', 'OBJ θ and OBL θ represent families of grammatical functions indexed by their semantic role, represented by the theta subscript.', 'This list of grammatical functions is divided into governable (subcategorizable) grammatical functions (arguments) and nongovernable (nonsubcategorizable) grammatical functions (modifiers/adjuncts), as summarized in Table 1.']	0	['According to #AUTHOR_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ .']
CC1517	W06-1104	Automatically creating datasets for measures of semantic relatedness	semantic similarity based on corpus statistics and lexical taxonomy	['Jay J Jiang', 'David W Conrath']		This paper presents a new approach for measuring semantic similarity/distance between words and concepts. It combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data. Specifically, the proposed measure is a combined approach that inherits the edge-based approach of the edge counting scheme, which is then enhanced by the node-based approach of the information content calculation. When tested on a common data set of word pair similarity ratings, the proposed approach outperforms other computational models. It gives the highest correlation value (r = 0.828) with a benchmark based on human similarity judgements, whereas an upper bound (r = 0.885) is observed when human subjects replicate the same task.	Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	0	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .']
CC326	J00-4002	Bidirectional Contextual Resolution	on reasoning with ambiguities	['Uwe Reyle']		The paper adresses the problem of reasoning with ambiguities. Semantic representations are presented that leave scope relations between quantifiers and/or other operators unspecified. Truth conditions are provided for these representations and different consequence relations are judged on the basis of intuitive correctness. Finally inference patterns are presented that operate directly on these underspecified structures, i.e. do not rely on any translation into the set of their disambiguations.Comment: EACL 199	But the general outlines are reasonably clear , and we can adapt some of the UDRS ( #AUTHOR_TAG ) work to our own framework .	['Developing a calculus for reasoning with QLFs is too large a task to be undertaken here.', 'But the general outlines are reasonably clear , and we can adapt some of the UDRS ( #AUTHOR_TAG ) work to our own framework .', 'Reyle points out that many of the inferences involving underspecified representations that we would like to capture rely on the assumption that whatever context disambiguates the premise also disambiguates the conclusion, even if we do not know what that context or disambiguation is.', 'His example is:']	5	['But the general outlines are reasonably clear , and we can adapt some of the UDRS ( #AUTHOR_TAG ) work to our own framework .', 'Reyle points out that many of the inferences involving underspecified representations that we would like to capture rely on the assumption that whatever context disambiguates the premise also disambiguates the conclusion, even if we do not know what that context or disambiguation is.']
CC1604	W06-3309	Generative content models for structural analysis of medical abstracts	discoursal movements in medical english abstracts and their linguistic exponents a genre analysis study	['Franc¸oise Salanger-Meyer']	introduction		This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #AUTHOR_TAG ) .	"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #AUTHOR_TAG ) .', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']"	0	"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orasan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #AUTHOR_TAG ) .', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']"
CC147	D12-1037	Discriminative Training for Log-Linear Based SMT	statistical significance tests for machine translation evaluation	['Philipp Koehn']	experiments	If two translation systems differ differ in perfor-mance on a test set, can we trust that this indicates a difference in true system quality? To answer this question, we describe bootstrap resampling meth-ods to compute statistical significance of test results, and validate them on the concrete example of the BLEU score. Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real.	The significance testing is performed by paired bootstrap re-sampling ( #AUTHOR_TAG ) .	['We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998).', 'In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl', 'as the evaluation tool.', 'The significance testing is performed by paired bootstrap re-sampling ( #AUTHOR_TAG ) .']	5	['The significance testing is performed by paired bootstrap re-sampling ( #AUTHOR_TAG ) .']
CC1126	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	multiwordnet developing and aligned multilingual database	['Emanuele Pianta', 'Luisa Bentivogli', 'Christian Girardi']	introduction		Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #AUTHOR_TAG ) ) have been created for several languages , with different degrees of coverage .	['Addressing CLTE we have to face additional and more problematic issues related to: i) the stronger need of lexical knowledge, and ii) the limited availability of multilingual lexical resources.', 'As regards the first issue, it�s worth noting that in the monolingual scenario simple �bag of words� (or �bag of n- grams�) approaches are per se sufficient to achieve results above baseline.', 'In contrast, their application in the cross-lingual setting is not a viable solution due to the impossibility to perform direct lex- ical matches between texts and hypotheses in different languages.', 'This situation makes the availability of multilingual lexical knowledge a necessary condition to bridge the language gap.', 'However, with the only exceptions represented by WordNet and Wikipedia, most of the aforementioned resources are available only for English.', 'Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #AUTHOR_TAG ) ) have been created for several languages , with different degrees of coverage .', 'As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet�s synsets, thus making the coverage issue even more problematic than for TE.', 'As regards Wikipedia, the cross- lingual links between pages in different languages offer a possibility to extract lexical knowledge useful for CLTE.', 'However, due to their relatively small number (especially for some languages), bilingual lexicons extracted from Wikipedia are still inadequate to provide acceptable coverage.', 'In addition, featuring a bias towards named entities, the information acquired through cross-lingual links can at most complement the lexical knowledge extracted from more generic multilingual resources (e.g bilingual dictionaries).']	0	['This situation makes the availability of multilingual lexical knowledge a necessary condition to bridge the language gap.', 'However, with the only exceptions represented by WordNet and Wikipedia, most of the aforementioned resources are available only for English.', 'Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #AUTHOR_TAG ) ) have been created for several languages , with different degrees of coverage .']
CC1250	W00-1312	Cross-lingual information retrieval using hidden Markov models	a language modeling approach to information retrievalquot	['J Ponte', 'W B Croft']	related work		Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .	['Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']	1	['Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']
CC1522	W06-1705	Annotated web as corpus	the linguists search engine getting started guide	['P Resnik', 'A Elkiss']	related work		Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; #AUTHOR_TAG ) .	"['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; #AUTHOR_TAG ) .""]"	0	"['restricted access or document size).', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; #AUTHOR_TAG ) .""]"
CC1223	W00-1017	WIT	the philips automatic train timetable information system	['Harald Aust', 'Martin Oerder', 'Frank Seide', 'Volker Steinbiss']	introduction		The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .	['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']	0	['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']
CC1355	W02-1601	A synchronization structure of SSTC and its applications in machine translation	pilot implementation of a bilingual knowledge bank	['V Sadler', 'R Vendelmans']		A Bilingual Knowledge Bank is a syntactically and referentially structured pair of corpora, one being a  translation of the other, in which translation units are  cross-codexl between the corpora. A pilot implementation  is described for a corpus of some 20,000 words  each in English, French and Esperanto which has been cross-coded between English and Esperanto and &apos;between Esperanto and French. The aim is to develop a corpus-based general-purpose knowledge sontee for applicatious in machine translation and computer-  aided translation	For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus	"[') are governed by the following constraints:  .', 'This means allowing one-to-one, one-to-many and many-to-many, but the mappings do not overlap.', 'Note that these constraints can be used to license only the linguistically meaningful synchronous correspondences between the two SSTCs of the S-SSTC (i.e. between the two languages).', ""For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus""]"	0	"['Note that these constraints can be used to license only the linguistically meaningful synchronous correspondences between the two SSTCs of the S-SSTC (i.e. between the two languages).', ""For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus""]"
CC1639	W14-1609	Lexicon Infused Phrase Embeddings for Named Entity Resolution	design challenges and misconceptions in named entity recognition	['Lev Ratinov', 'Dan Roth']	introduction	We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.	It is inspired by the system described in #AUTHOR_TAG .	"['In this section we describe in detail the baseline NER system we use.', 'It is inspired by the system described in #AUTHOR_TAG .', 'Because NER annotations are commonly not nested (for example, in the text ""the US Army"", ""US Army"" is treated as a single entity, instead of the location ""US"" and the organization ""US Army"") it is possible to treat NER as a sequence labeling problem, where each token in the sentence receives a label which depends on which entity type it belongs to and its position in the entity.', 'Following Ratinov and Roth (2009) we use the BILOU encoding, where each token can either BEGIN an entity, be INSIDE an entity, be the LAST token in an entity, be OUTSIDE an entity, or be the single UNIQUE token in an entity.']"	4	['In this section we describe in detail the baseline NER system we use.', 'It is inspired by the system described in #AUTHOR_TAG .']
CC1318	W02-0309	Biomedical text retrieval in languages with a complex morphology	morphosemantic parsing of medical expressions	['R Baud', 'C Lovis', 'A-M Rassinoux', 'J-R Scherrer']	introduction		From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG ) .	"['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG ) .']"	0	['This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG ) .']
CC1062	P07-1068	Advanced Machine Learning Models for Coreference Resolution	a machine learning approach to coreference resolution of noun phrases	['W M Soon', 'H T Ng', 'D Lim']	introduction	this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of &quot;organization,&quot; &quot;person,&quot; or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data set	However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , #AUTHOR_TAG , Markert and Nissim ( 2005 ) ) .	"['Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs.', 'However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , #AUTHOR_TAG , Markert and Nissim ( 2005 ) ) .', ""It is not easy to measure the accuracy of this heuristic, but the fact that the SC agreement feature is not used by Soon et al.'s decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this first-sense heuristic.""]"	0	"['Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs.', 'However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , #AUTHOR_TAG , Markert and Nissim ( 2005 ) ) .', ""It is not easy to measure the accuracy of this heuristic, but the fact that the SC agreement feature is not used by Soon et al.'s decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this first-sense heuristic.""]"
CC1195	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	spmt statistical machine translation with syntactified target language phrases	['Daniel Marcu', 'Wei Wang', 'Abdessamad Echihabi', 'Kevin Knight']	introduction	We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a human-based quality metric that ranks translations on a scale from 1 to 5.	Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .	['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']	0	['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']
CC940	K15-1002	A Joint Framework for Coreference Resolution and Mention Head Detection	a constrained latent variable model for coreference resolution	['K-W Chang', 'R Samdani', 'D Roth']	introduction	Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.	Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj Â¨ orkelund and Kuhn , 2014 ) .	"['Mention detection is rarely studied as a stand-alone research problem (Recasens et al. (2013) is one key exception).', 'Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj Â¨ orkelund and Kuhn , 2014 ) .', 'However, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty.', 'Indeed, many papers report results in terms of gold mentions versus system generated mentions, as shown in Table 1.', 'Current state-of-the-art systems show a very significant drop in performance when running on system generated mentions.', 'These performance gaps are worrisome, since the real goal of NLP systems is to process raw data.', '1: Performance gaps between using gold mentions and predicted mentions for three state-of-the-art coreference resolution systems.', 'Performance gaps are always larger than 10%.', ""Illinois's system (Chang et al., 2013) is evaluated on CoNLL (2012CoNLL ( , 2011) Shared Task and ACE-2004 datasets."", 'It reports an average F1 score of MUC, B and CEAF e metrics using CoNLL v7.0 scorer.', ""Berkeley's system (Durrett and Klein, 2013) reports the same average score on the CoNLL-2011 Shared Task dataset."", ""Results of Stanford's system (Lee et al., 2011) are for B 3 metric on ACE-2004 dataset."", 'This paper focuses on improving end-to-end coreference performance.', 'We do this by: 1) Developing a new ILP-based joint learning and inference formulation for coreference and mention head detection.', '2) Developing a better mention head candidate generation algorithm.', 'Importantly, we focus on heads rather than mention boundaries since those can be identified more robustly and used effectively in an end-to-end system.', 'As we show, this results in a dramatic improvement in the quality of the MD component and, consequently, a significant reduction in the performance gap between coreference on gold mentions and coreference on raw data.']"	0	"['Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj Â¨ orkelund and Kuhn , 2014 ) .', 'Indeed, many papers report results in terms of gold mentions versus system generated mentions, as shown in Table 1.', '1: Performance gaps between using gold mentions and predicted mentions for three state-of-the-art coreference resolution systems.', ""Results of Stanford's system (Lee et al., 2011) are for B 3 metric on ACE-2004 dataset."", 'This paper focuses on improving end-to-end coreference performance.', 'We do this by: 1) Developing a new ILP-based joint learning and inference formulation for coreference and mention head detection.', 'As we show, this results in a dramatic improvement in the quality of the MD component and, consequently, a significant reduction in the performance gap between coreference on gold mentions and coreference on raw data.']"
CC39	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	labeling chinese predicates with semantic roles	['Nianwen Xue']	experiments	Driven by the availability of semantically annotated corpora such as the FrameNet and the Proposition Bank, recent years have seen a revived interst in semantic parsing by applying statistical and machine-learning methods to human annotated corpora. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this article we report work on Chinese Semantic Role Labeling (SRL), taking advantage of two recently completed corpora, the Chinese Proposition Bank, a semantically annotated corpus of Chinese verbs, and the Chinese Nombank, a companion corpus that annotates the predicateargument structure of nominalized predicates. Since the semantic role labels are assigned to the constituents in a parse tree, we first report experiments in which Semantic Role Labels are automatically assigned to hand-crafted parses in the Chinese Treebank. This gives us a measure of the extent to which Semantic Role Labels can be bootstrapped from the syntactic annotation provided in the treebank. We then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. These experiments gauge how successful Semantic Role Labeling for Chinese can be done in realistic situations. Our results show that when hand-crafted parses are used, SRL accuracy for Chinese is comparable to what has been reported for the state-of-the-art English SRL systems trained and tested on the English Proposition Bank, even though the Chinese Proposition Bank is significantly smaller in size. When an automatic parser is used, however, the accuracy of our system is significantly lower than the English state-of-the-art. This indicates that the improvement in Chinese parsing is critical to high-performance Semantic Role Labeling for Chinese. Our results also show that, in general, SRL accuracy is significantly higher for verbs than for nominalized predicates across all experimental conditions. We believe that this is due to the fact that the mapping from the syntactic structure to the predicate-argument structure is less transparent for nominalized predicates than for verbs. 1	To prove that our method is effective , we also make a comparison between the performances of our system and Xue and Palmer ( 2005 ) , #AUTHOR_TAG .	['To prove that our method is effective , we also make a comparison between the performances of our system and Xue and Palmer ( 2005 ) , #AUTHOR_TAG .', 'Xue (2008) is the best SRL system until now and it has the same data setting with ours.', 'The results are presented in Table 6  We have to point out that all the three systems are based on Gold standard parsing.', 'From the table 6, we can find that our system is better than both of the related systems.', 'Our system has outperformed Xue (2008) with a relative error reduction rate of 9.8%.']	1	['To prove that our method is effective , we also make a comparison between the performances of our system and Xue and Palmer ( 2005 ) , #AUTHOR_TAG .', 'The results are presented in Table 6  We have to point out that all the three systems are based on Gold standard parsing.', 'Our system has outperformed Xue (2008) with a relative error reduction rate of 9.8%.']
CC902	J97-4003	On Expressing Lexical Generalizations in HPSG	an expanded logical formalism for headdriven phrase structure grammar	['Paul King']		Though Pollard and Sag 1994] assumes that an unspeciied variant of the formal logic of Carpenter 1992] will provide a formalism for HPSG, a precise formulation of the envisaged formalism is not immediately obvious, primarily because a principal tenet of Carpenter 1992], that feature structures represent partial information, seems to connict with a principal tenet of Pollard and Sag 1994], that feature structures represent abstract linguistic entities. This has caused many HPSGians to be mistakenly concerned with partial-information speciic notions, such as subsumption, that are appropriate for the Carpenter 1992] logic but inappropriate for the formalism Pollard and Sag 1994] envisages. This paper hopes to allay this concern and the confusion it engenders by substituting King 1989] for Carpenter 1992] as the basis of the envisaged formalism. It demonstrates that the formal logic of King 1989] provides a formalism for HPSG that meets all Pollard and Sag 1994] asks of the envisaged formalism. It further shows that the most credible variant of the Carpenter 1992] logic consistent with the aims of Pollard and Sag 1994] is not only incompatible with the tenet that feature structures represent partial information, but also an instance of the King 1989] logic.	The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and #AUTHOR_TAG ; Gotz and Meurers 1997a ) .	['The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and #AUTHOR_TAG ; Gotz and Meurers 1997a ) .', 'We tested the covariation approach with a complex grammar implementing an HPSG analysis covering the so-called aux-flip phenomenon, and partial-VP topicalization in the three clause types of German (Hinrichs, Meurers, and Nakazawa 1994).', 'This test grammar includes eight lexical rules; some serve syntactic purposes, like the Partial-VP Topicalization Lexical Rule, others are of morphological nature as, for example, an inflectional lexical rule that relates nonfinite verbs to their finite form.', 'Our compiler distinguished seven word classes.', 'Some nouns and most verbal lexical entries fed lexical rules, and a single base lexical entry resulted in up to 12 derivations.']	5	['The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and #AUTHOR_TAG ; Gotz and Meurers 1997a ) .', 'We tested the covariation approach with a complex grammar implementing an HPSG analysis covering the so-called aux-flip phenomenon, and partial-VP topicalization in the three clause types of German (Hinrichs, Meurers, and Nakazawa 1994).', 'This test grammar includes eight lexical rules; some serve syntactic purposes, like the Partial-VP Topicalization Lexical Rule, others are of morphological nature as, for example, an inflectional lexical rule that relates nonfinite verbs to their finite form.']
CC1623	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	the descent of hierarchy and selection in relational semantics	['Barbara Rosario', 'Marti Hearst', 'Charles Fillmore']	related work	In many types of technical texts, meaning is embedded in noun compounds. A language understanding program needs to be able to interpret these in order to ascertain sentence meaning. We explore the possibility of using an existing lexical hierarchy for the purpose of placing words from a noun compound into categories, and then using this category membership to determine the relation that holds between the nouns. In this paper we present the results of an analysis of this method on two-word noun compounds from the biomedical domain, obtaining classification accuracy of approximately 90%. Since lexical hierarchies are not necessarily ideally suited for this task, we also pose the question: how far down the hierarchy must the algorithm descend before all the terms within the subhierarchy behave uniformly with respect to the semantic relation in question? We find that the topmost levels of the hierarchy yield an accurate classification, thus providing an economic way of assigning relations to noun compounds.	Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; #AUTHOR_TAG ) ) .	['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; #AUTHOR_TAG ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']	0	['Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; #AUTHOR_TAG ) ) .']
CC657	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	hmm word and phrase alignment for statistical machine translation	['Yonggang Deng', 'William Byrne']	related work	Estimation and alignment procedures for word and phrase alignment hidden Markov models (HMMs) are developed for the alignment of parallel text. The development of these models is motivated by an analysis of the desirable features of IBM Model 4, one of the original and most effective models for word alignment. These models are formulated to capture the desirable aspects of Model 4 in an HMM alignment formalism. Alignment behavior is analyzed and compared to human-generated reference alignments, and the ability of these models to capture different types of alignment phenomena is evaluated. In analyzing alignment performance, Chinese-English word alignments are shown to be comparable to those of IBM Model 4 even when models are trained over large parallel texts. In translation performance, phrase-based statistical machine translation systems based on these HMM alignments can equal and exceed systems based on Model 4 alignments, and this is shown in Arabic-English and Chinese-English translation. These alignment models can also be used to generate posterior statistics over collections of parallel text, and this is used to refine and extend phrase translation tables with a resulting improvement in translation quality.	In the context of word alignment , #AUTHOR_TAG use a state-duration HMM in order to model word-to-phrase translations .	"['The idea of introducing constraints over a model to better guide the learning process has appeared before.', 'In the context of word alignment , #AUTHOR_TAG use a state-duration HMM in order to model word-to-phrase translations .', 'The fertility of each source word is implicitly encoded in the durations of the HMM states.', 'Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant η > 1.', 'This encourages more transitions and hence shorter phrases.', 'For the task of unsupervised dependency parsing, Smith and Eisner (2006) add a constraint of the form ""the average length of dependencies should be X"" to capture the locality of syntax (at least half of the dependencies are between adjacent words), using a scheme they call structural annealing.', ""They modify the model's distribution over trees p θ (y) by a penalty term as: p θ (y) ∝ p θ (y)e (δ e∈y length(e)) , where length(e) is the surface length of edge e."", 'The factor δ changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training.', 'These two approaches also have the goal of controlling unsupervised learning, and the form of the modified distributions is reminiscent of the form that the projected posteriors take.', 'However, the approaches differ substantially from PR. Smith and Eisner (2006) make a statement of the form ""scale the total length of edges"", which depending on the value of δ will prefer to have more shorter/longer edges.', 'Such statements are not data dependent.', 'Depending on the value of δ, for instance if δ ≤ 0, even if the data is such that the model already uses too many short edges on average, this value of δ will push for more short edges.', 'By contrast the statements we can make in PR are of the form ""there should be more short edges than long edges"".', 'Such a statement is data-dependent in the sense that if the model satisfies the constraints then we do not need to change it; if it is far from satisfying it we might need to make very dramatic changes.']"	0	"['The idea of introducing constraints over a model to better guide the learning process has appeared before.', 'In the context of word alignment , #AUTHOR_TAG use a state-duration HMM in order to model word-to-phrase translations .', 'The fertility of each source word is implicitly encoded in the durations of the HMM states.', 'Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant e > 1.', 'This encourages more transitions and hence shorter phrases.', 'For the task of unsupervised dependency parsing, Smith and Eisner (2006) add a constraint of the form ""the average length of dependencies should be X"" to capture the locality of syntax (at least half of the dependencies are between adjacent words), using a scheme they call structural annealing.', ""They modify the model's distribution over trees p th (y) by a penalty term as: p th (y)  p th (y)e (d ey length(e)) , where length(e) is the surface length of edge e."", 'The factor d changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training.', 'These two approaches also have the goal of controlling unsupervised learning, and the form of the modified distributions is reminiscent of the form that the projected posteriors take.', 'However, the approaches differ substantially from PR. Smith and Eisner (2006) make a statement of the form ""scale the total length of edges"", which depending on the value of d will prefer to have more shorter/longer edges.', 'Such statements are not data dependent.', 'Depending on the value of d, for instance if d <= 0, even if the data is such that the model already uses too many short edges on average, this value of d will push for more short edges.', 'By contrast the statements we can make in PR are of the form ""there should be more short edges than long edges"".', 'Such a statement is data-dependent in the sense that if the model satisfies the constraints then we do not need to change it; if it is far from satisfying it we might need to make very dramatic changes.']"
CC1180	P97-1063	A word-to-word model of translational equivalence	measuring semantic entropyquot	['I D Melamed']	conclusion		Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #AUTHOR_TAG ) .	"['Even better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #AUTHOR_TAG ) .', 'Another interesting extension is to broaden the definition of a ""word"" to include multi-word lexical units (Smadja, 1992).', 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.', 'In this manner, the model can account for a wider range of translation phenomena.']"	3	"['Even better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #AUTHOR_TAG ) .', 'Another interesting extension is to broaden the definition of a ""word"" to include multi-word lexical units (Smadja, 1992).', 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.']"
CC1037	P05-3005	Dynamically generating a protein entity dictionary using online resources	mining the biomedical literature in the genomic era an overview	['H Shatkay', 'R Feldman']	introduction	The past decade has seen a tremendous growth in the amount of experimental and computational biomedical data, specifically in the areas of genomics and proteomics. This growth is accompanied by an accelerated increase in the number of biomedical publications discussing the findings. In the last few years, there has been a lot of interest within the scientific community in literature-mining tools to help sort through this abundance of literature and find the nuggets of information most relevant and useful for specific analysis tasks. This paper provides a road map to the various literature-mining methods, both in general and within bioinformatics. It surveys the disciplines involved in unstructured-text analysis, categorizes current work in biomedical literature mining with respect to these disciplines, and provides examples of text analysis methods applied towards meeting some of the current challenges in bioinformatics.	With the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible #AUTHOR_TAG .	['With the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible #AUTHOR_TAG .', 'One requirement for NLP is the ability to accurately recognize terms that represent biological entities in free text.', 'Another requirement is the ability to associate these terms with corresponding biological entities (i.e., records in biological databases) in order to be used by other automated systems for literature mining.', 'Such task is called biological entity tagging.', 'Biological entity tagging is not a trivial task because of several characteristics associated with biological entity names, namely: synonymy (i.e., different terms refer to the same entity), ambiguity (i.e., one term is associated with different entities), and coverage (i.e., entity terms or entities are not present in databases or knowledge bases).']	0	['With the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible #AUTHOR_TAG .']
CC957	K15-1003	A Supertag-Context Model for Weakly-Supervised CCG Parser Learning	online learning of relaxed ccg grammars for parsing to logical form	['Luke S Zettlemoyer', 'Michael Collins']	experiments	We consider the problem of learning to parse sentences to lambda-calculus representations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG). A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammar--for example allowing flexible word order, or insertion of lexical items-- with learned costs. We also present a new, online algorithm for inducing a weighted CCG. Results for the approach on ATIS data show 86% F-measure in recovering fully correct semantic analyses and 95.9% F-measure by a partial-match criterion, a more than 5% improvement over the 90.3% partial-match figure reported by He and Young (2006).	"This is similar to the ""deletion"" strategy employed by #AUTHOR_TAG , but we do it directly in the grammar ."	"['When evaluating on test set sentences, if the model is unable to find a parse given the constraints of the tag dictionary, then we would have to take a score of zero for that sentence: every dependency would be ""wrong"".', 'Thus, it is important that we make a best effort to find a parse.', 'To accomplish this, we implemented a parsing backoff strategy.', 'The parser first tries to find a valid parse that has either s dcl or np at its root.', 'If that fails, then it searches for a parse with any root.', 'If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without).', 'This is similar to the ""deletion"" strategy employed by #AUTHOR_TAG , but we do it directly in the grammar .', 'We add unary rules of the form D →u for every potential supertag u in the tree.', 'Then, at each node spanning exactly two tokens (but no higher in the tree), we allow rules t→ D , v and t→ v, D .', 'Recall that in §3.1, we stated that D is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary.', 'Additionally, since u will still remain as the preterminal, it will be the category examined as the context by adjacent constituents.']"	1	"['When evaluating on test set sentences, if the model is unable to find a parse given the constraints of the tag dictionary, then we would have to take a score of zero for that sentence: every dependency would be ""wrong"".', 'Thus, it is important that we make a best effort to find a parse.', 'The parser first tries to find a valid parse that has either s dcl or np at its root.', 'If that fails, then it searches for a parse with any root.', 'If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without).', 'This is similar to the ""deletion"" strategy employed by #AUTHOR_TAG , but we do it directly in the grammar .', 'We add unary rules of the form D -u for every potential supertag u in the tree.', 'Then, at each node spanning exactly two tokens (but no higher in the tree), we allow rules t- D , v and t- v, D .']"
CC73	D09-1056	The role of named entities in web people search	weps 2 evaluation campaign overview of the web people search clustering task	['Javier Artiles', 'Julio Gonzalo', 'Satoshi Sekine']	related work	The second WePS (Web People Search) Evaluation cam-paign took place in 2008-2009 with the participation of 19 re-search groups from Europe, Asia and North America. Given the output of a Web Search Engine for a (usually ambiguous) person name as query, two tasks were addressed: a clustering task, which consists of grouping together web pages referring to the same person, and an extraction task, which consists of extracting salient attributes for each of the persons shar-ing the same name. This paper presents the definition, re-sources, methodology and evaluation metrics, participation and comparative results for the clustering task	In 2009 , the second WePS campaign showed similar trends regarding the use of NE features ( #AUTHOR_TAG ) .	['Among the 16 teams that submitted results for the first WePS campaign, 10 of them 1 used NEs in their document representation.', 'This makes NEs the second most common type of feature; only the BoW feature was more popular.', 'Other features used by the systems include noun phrases (Chen and Martin, 2007), word n-grams (Popescu and Magnini, 2007), emails and URLs (del Valle-Agudo et al., 2007), etc.', 'In 2009 , the second WePS campaign showed similar trends regarding the use of NE features ( #AUTHOR_TAG ) .', 'Due to the complexity of systems, the results of the WePS evaluation do not provide a direct answer regarding the advantages of using NEs over other computationally lighter features such as BoW or word n-grams.', 'But the WePS campaigns did provide a useful, standardised resource to perform the type of studies that were not possible before.', 'In the next Section we describe this dataset and how it has been adapted for our purposes.']	0	['Among the 16 teams that submitted results for the first WePS campaign, 10 of them 1 used NEs in their document representation.', 'This makes NEs the second most common type of feature; only the BoW feature was more popular.', 'Other features used by the systems include noun phrases (Chen and Martin, 2007), word n-grams (Popescu and Magnini, 2007), emails and URLs (del Valle-Agudo et al., 2007), etc.', 'In 2009 , the second WePS campaign showed similar trends regarding the use of NE features ( #AUTHOR_TAG ) .', 'Due to the complexity of systems, the results of the WePS evaluation do not provide a direct answer regarding the advantages of using NEs over other computationally lighter features such as BoW or word n-grams.', 'But the WePS campaigns did provide a useful, standardised resource to perform the type of studies that were not possible before.', 'In the next Section we describe this dataset and how it has been adapted for our purposes.']
CC966	N01-1003	SPoT	discriminative reranking for natural language parsing	['Michael Collins']		This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation	Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #AUTHOR_TAG .	['Rankboost, like other machine learning programs of the boosting family, can handle a very large number of features.', 'Therefore, instead of carefully choosing a small number of features by hand which may be useful, we generated a very large number of features and let Rank-Boost choose the relevant ones.', 'In total, we used 3,291 features in training the SPR.', 'Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #AUTHOR_TAG .', 'The motivation for the features was to capture declaratively decisions made by the randomized SPG.', 'We avoided features specific to particular text plans by discarding those that occurred fewer than 10 times.']	1	['Rankboost, like other machine learning programs of the boosting family, can handle a very large number of features.', 'Therefore, instead of carefully choosing a small number of features by hand which may be useful, we generated a very large number of features and let Rank-Boost choose the relevant ones.', 'Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #AUTHOR_TAG .']
CC1272	W01-0706	Exploring evidence for shallow parsing	text chunking using transformationbased learning	['L A Ramshaw', 'M P Marcus']	introduction	"Transformation-based learning, a technique introduced by Eric Brill (1993b), has been shown to do part-of-speech tagging with fairly high accuracy. This same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive ""baseNP"" chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 93% for baseNP chunks (trained on 950K words) and 88% for somewhat more complex chunks that partition the sentence (trained on 200K words). Working in this new application and with larger template and training sets has also required some interesting adaptations to the transformation-based learning approach."	Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"	0	['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']
CC260	E03-1005	An efficient implementation of a new DOP model	a maximumentropyinspired parser	['E Charniak']	introduction		The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ; #AUTHOR_TAG ; Goodman 1998 ) .	"['The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include non- lexicalized fragments.', 'However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997; Johnson 1998a).', 'The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ; #AUTHOR_TAG ; Goodman 1998 ) .', 'And Collins (2000) argues for ""keeping track of counts of arbitrary fragments within parse trees"", which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of (all) tree fragments as proposed in Bod (1992).']"	0	['The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', 'However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ; #AUTHOR_TAG ; Goodman 1998 ) .']
CC1237	W00-1017	WIT	europa a generic framework for developing spoken dialogue systems	['Munehiko Sasajima', 'Yakehide Yano', 'Yasuyuld Kono']	introduction	Voice interfaces are not popular since they are neither useful nor user-friendly for non-specialist users. In this paper, EUROPA, a new framework for developing spoken dialogue systems, is introduced. In developing EUROPA, the authors focused on three points : (1) acceptance of spoken language, (2) portability in terms of domain and task, and (3) practical performance of the applied system. The framework is applied to prototyping a car navigation system called MINOS. MINOS is built on a portable PC, can process over 700 words of recognition vocabulary, and is able to respond to a user's question within a few seconds.	To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .	['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model.', 'It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain.', 'However, it limits system functions; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics.', 'Another is GALAXY-II (Seneffet al., 1998), *Mikio Nakano is currently a visiting scientist at MIT Laboratory for Computer Science. which enables modules in a dialogue system to communicate with each other.', 'It consists of the hub and several servers, such as the speech recognition server and the natural language server, and the hub communicates with these servers.', 'Although it requires more specifications than finitestate-model-based toolkits, it places less limitations on system functions.']	0	['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain.', 'It consists of the hub and several servers, such as the speech recognition server and the natural language server, and the hub communicates with these servers.']
CC482	J05-3003	Gaussian coordinates and the large scale universe	headdriven phrase structure grammar	['Carl Pollard', 'Ivan Sag']	introduction		In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ #AUTHOR_TAG ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .	['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ #AUTHOR_TAG ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']	0	['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ #AUTHOR_TAG ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']
CC464	J05-3003	Gaussian coordinates and the large scale universe	a comparison of evaluation metrics for a broad coverage parser	['Richard Crouch', 'Ron Kaplan', 'Tracy King', 'Stefan Riezler']	method		Both use the evaluation software and triple encoding presented in #AUTHOR_TAG .	['In order to ensure the quality of the semantic forms extracted by our method, we must first ensure the quality of the f-structure annotations.', 'The results of two different evaluations of the automatically generated f-structures are presented in Table 2.', 'Both use the evaluation software and triple encoding presented in #AUTHOR_TAG .', 'The first of these is against the DCU 105, a gold-standard set of 105 hand-coded f-structures']	5	['Both use the evaluation software and triple encoding presented in #AUTHOR_TAG .', 'The first of these is against the DCU 105, a gold-standard set of 105 hand-coded f-structures']
CC1631	W10-3814	New Parameterizations and Features for PSCFG-Based Machine Translation	a discriminative latent variable model for statistical machine translation	['Phil Blunsom', 'Trevor Cohn', 'Miles Osborne']	conclusion	Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems.We argue that a principle reason for this failure is not dealing with multiple, equivalent translations. We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised. Results show that accounting for multiple derivations does indeed improve performance. Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions. (c) 2008 Association for Computational Linguistics	Future research should apply the work of #AUTHOR_TAG and Blunsom and Osborne ( 2008 ) , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .	['Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.', 'This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension.', 'Future research should apply the work of #AUTHOR_TAG and Blunsom and Osborne ( 2008 ) , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']	3	['Future research should apply the work of #AUTHOR_TAG and Blunsom and Osborne ( 2008 ) , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']
CC1547	W06-2807	Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein	understanding comics	['Scott McCloud']	introduction	During the spring semester of 2010, as part of my graduate program in English Education, I took a class titled American Comic Book. I took what I learned there, turned around, and immediately applied it to my own teaching. I teach 7th grade language arts and developed a unit on understanding and creating comics, pulling from what I was learning in the class at the University of Iowa, and utilized some other resources including Great Source u27s Daybook of Critical Reading and Writing, and ideas from other books on using graphic novels as a teaching tool. The unit was taught during April and May of this year. I have collected my lesson plans, examples of student work, and much, much more on a website, http://sites.google.com/site/7thgradecomicsunit/	For example , a ` web page ' is more similar to an infinite canvas than a written page ( #AUTHOR_TAG ) .	"['1.1 Hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""Terms as 'chapter', 'page' or 'footnote' simply become meaningless in the new texts, or they highly change their meaning."", 'When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', ""For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e."", 'literary criticism -unlike in the previous times (Eisenstein, 1983).', 'Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', ""For example , a ` web page ' is more similar to an infinite canvas than a written page ( #AUTHOR_TAG ) ."", 'Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an 'opera aperta' (open work), as Eco would define it (1962)."", 'From a more pessimistic one, an author may feel to have lost power in this openness.', 'Henceforth the collaborative traits of blogs and wikis (McNeill, 2005) emphasize annotation, comment, and strong editing.', 'They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.', 'This situation could make new problems rise up: Who owns the text?', 'Which role is suitable for authors?', 'We have to analyse them before presenting the architecture of Novelle.']"	0	"['When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', ""For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e."", ""For example , a ` web page ' is more similar to an infinite canvas than a written page ( #AUTHOR_TAG ) .""]"
CC1642	W14-1815	Natural Language Generation with Vocabulary Constraints	generating diagnostic multiple choice comprehension cloze questions	['Jack Mostow', 'Hyeju Jang']	related work	This paper describes and evaluates DQGen, which automatically generates multiple choice cloze questions to test a child's comprehension while reading a given text. Unlike previous methods, it generates different types of distracters designed to diagnose different types of comprehension failure, and tests comprehension not only of an individual sentence but of the context that precedes it. We evaluate the quality of the overall questions and the individual distracters, according to 8 human judges blind to the correct answers and intended distracter types. The results, errors, and judges' comments reveal limitations and suggest how to address some of them.	Our motivation for generation of material for language education exists in work such as Sumita et al. ( 2005 ) and #AUTHOR_TAG , which deal with automatic generation of classic fill in the blank questions .	['Our motivation for generation of material for language education exists in work such as Sumita et al. ( 2005 ) and #AUTHOR_TAG , which deal with automatic generation of classic fill in the blank questions .', 'Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences.']	4	['Our motivation for generation of material for language education exists in work such as Sumita et al. ( 2005 ) and #AUTHOR_TAG , which deal with automatic generation of classic fill in the blank questions .', 'Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences.']
CC1304	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	a study of tree adjoining grammars	['K Vijay-Shanker']	introduction	Constrained grammatical system have been the object of study in computational linguistics over the last few years, both with respect to their linguistic adequacy and their computational properties. A Tree Adjoining Grammar (TAG) is a tree rewriting system whose linguistic relevance has been extensively studied. A key property of these systems is that a TAG factors recursion from the co-occurrence restrictions. In this thesis, we study some mathematical properties of TAG u27s. We show that TAG u27s have several interesting properties and are a natural generalization of Context Free Grammars. We show the equivalence of the classes of languages generated by TAG u27s with those generated by Head Grammars and a linear version of Indexed Grammars, which have been studied for their linguistic applicability. We define the embedded pushdown automaton, an extention of the pushdown automaton, and prove that they are equivalent to TAG u27s. We show that the class of Tree Adjoining Languages form a substitution closed abstract family of languages, and that each Tree Adjoining Language is a semilinear language. We show that a TAG can be parsed in polynomial time by adapting the Cocke-Kasami-Younger algorithm for CFL u27s. Feature structures, essentially a set of attribute value pairs, have been used in computational linguistics to make statements of equality to capture some linguistic phenomena such as subcategorization and agreement. We embed TAG u27s in a feature structure based framework. We show that the resulting system has several advantages over TAG u27s. We give a mathematical model of this system based on the logical calculus developed by Rounds, Kasper, and Manaster-Ramer. Finally, we propose a restriction of this system and show how parsing of such a system can be done efficiently	FBLTAG ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .	['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with ¥).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with £).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).', 'FBLTAG ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeillé and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']	0	['An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with Y=).', 'FBLTAG ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).']
CC12	A00-1024	Categorizing unknown words	a stochastic parts program and noun phrase parser for unrestricted text	['K Church']	experiments	A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.>	We use an in-house statistical tagger ( based on ( #AUTHOR_TAG ) ) to tag the text in which the unknown word occurs .	['The first feature represents the part of speech of the word.', 'We use an in-house statistical tagger ( based on ( #AUTHOR_TAG ) ) to tag the text in which the unknown word occurs .', 'The tag set used is a simplified version of the tags used in the machinereadable version of the Oxford Advanced Learners Dictionary (OALD).', 'The tag set contains just one tag to identify nouns.']	5	['The first feature represents the part of speech of the word.', 'We use an in-house statistical tagger ( based on ( #AUTHOR_TAG ) ) to tag the text in which the unknown word occurs .', 'The tag set contains just one tag to identify nouns.']
CC19	D08-1006	Refining generative language models using discriminative learning	a discriminative language model with pseudonegative samples	"['Daisuke Okanohara', ""Jun'ichi Tsujii""]"			Unfortunately , as shown in ( #AUTHOR_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) .	"['Self-supervised boosting was presented as a general method for density estimation, and was not tested in the context of language modeling.', 'Rather, Welling at al. demonstrated its effectiveness in modeling hand-written digits and on synthetic data.', 'Đn both cases essentially linear classifiers were used as features.', 'As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples.', 'Unfortunately , as shown in ( #AUTHOR_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) .', 'While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data.', 'This means that MCMC techniques like Gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications.', 'For this reason we use a different sampling scheme which we refer to as rejection sampling.', ""This allows us to sample from the true model distribution while requiring a drastically smaller number of classifications, as long as the current model isn't too far removed from the baseline.""]"	4	"['Rather, Welling at al. demonstrated its effectiveness in modeling hand-written digits and on synthetic data.', 'Dn both cases essentially linear classifiers were used as features.', 'Unfortunately , as shown in ( #AUTHOR_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) .', ""This allows us to sample from the true model distribution while requiring a drastically smaller number of classifications, as long as the current model isn't too far removed from the baseline.""]"
CC548	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	modern information retrieval	['Ricardo Baeza-Yates', 'Berthier Ribeiro-Neto']		Digital technologies can help scholars to navigate the vast quantities of musical data and source materials now available to them, but an imaginative leap is needed in order to conceptualize the kinds of musicological research questions we might ask of electronic corpora. In particular, our data-rich digital world offers enormous potential for the exploration of musical transmission and relatedness.  In this article, we explore the 16th- and 17th-century instrumental battaglia (battle piece), a genre with a very distinctive collective identity arising from the use of numerous shared ingredients (including melodic, motivic, textural, harmonic and rhythmic features). However, a battaglia is not defined by the presence of a core set of essential features, and exact concordance between these pieces is often remarkably low. This kind of musical 'family resemblance' (formulated after Wittgenstein) poses a serious challenge to both traditional musicological apparatus (for example, finding aids such as thematic catalogues) and Music Information Retrieval (which has often privileged melodic similarity at the expense of other kinds of musical relatedness).  This case study provides a stimulus for rethinking the complex nature of musical similarity. In doing this, we outline a set of requirements for digital tools that could support the discovery, exploration and representation of these kinds of relationships	Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved ( #AUTHOR_TAG ).	['Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved ( #AUTHOR_TAG ).', 'It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall.']	5	['Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved ( #AUTHOR_TAG ).', 'It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall.']
CC1423	W04-1610	Automatic Arabic document categorization based on the Naïve Bayes algorithm	toward an arabic web page classifierquot master project	['M Yahyaoui']	conclusion		To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #AUTHOR_TAG ) .	"['To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #AUTHOR_TAG ) .', 'In this work, the average accuracy over all categories is: 68.78% in cross validation and 62% in evaluation set experiments.', 'The corresponding performances in (Yahyaoui, 2001) are 75.6% and 50%, respectively.', 'Thus, the overall performance (including cross validation and evaluation set experiments) in this work is comparable to that in (Yahyaoui, 2001).', 'This offers some indication that the performance of NB algorithm in classifying Arabic documents is not sensitive to the Arabic root extraction algorithm.', 'Future work will be directed at experimenting with other root extraction algorithms.', ""Further improvement of NB's performance may be effected by using unlabeled documents; e.g., EM has been used successfully for this purpose in (Nigam et al.,200), where EM has increased the classification accuracy by 30% for classifying English documents.""]"	1	"['To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #AUTHOR_TAG ) .', 'In this work, the average accuracy over all categories is: 68.78% in cross validation and 62% in evaluation set experiments.', 'The corresponding performances in (Yahyaoui, 2001) are 75.6% and 50%, respectively.', 'Thus, the overall performance (including cross validation and evaluation set experiments) in this work is comparable to that in (Yahyaoui, 2001).', 'This offers some indication that the performance of NB algorithm in classifying Arabic documents is not sensitive to the Arabic root extraction algorithm.', ""Further improvement of NB's performance may be effected by using unlabeled documents; e.g., EM has been used successfully for this purpose in (Nigam et al.,200), where EM has increased the classification accuracy by 30% for classifying English documents.""]"
CC643	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	nonlinear programming 2nd edition athena scientific	['Dimitri P Bertsekas']			Here , PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient ( #AUTHOR_TAG ) ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable .	"['Hence the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ.', 'We optimize the dual objective using the gradient based methods shown in Algorithm 1.', ""Here η is an optimization precision, α is a step size chosen with the strong Wolfe's rule (Nocedal and Wright 1999)."", 'Here , PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient ( #AUTHOR_TAG ) ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable .', 'In practice this only happens at the start of optimization and we use a sub-gradient for the first direction.']"	5	"[""Here e is an optimization precision, a is a step size chosen with the strong Wolfe's rule (Nocedal and Wright 1999)."", 'Here , PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient ( #AUTHOR_TAG ) ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable .', 'In practice this only happens at the start of optimization and we use a sub-gradient for the first direction.']"
CC129	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	conllx shared task on multilingual dependency parsing	['S Buchholz', 'E Marsi']	experiments	Each year the Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their systems on exactly the same data sets, in order to better compare systems. The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing. In this paper, we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured. We also give an overview of the parsing approaches that participants took and the results that they achieved. Finally, we try to draw general conclusions about multi-lingual parsing: What makes a particular language, treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser?	For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) ( #AUTHOR_TAG ) .	['In the next section, we present a set of scoring functions that can be used in the inline reranker loss framework, resulting in a new augmented-loss for each one.', 'Augmented-loss learning is then applied to target a downstream task using the loss functions to measure gains.', 'We show empirical results for two extrinsic loss-functions (optimizing for the downstream task): machine translation and domain adaptation; and for one intrinsic loss-function: an arclength parsing score.', 'For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) ( #AUTHOR_TAG ) .']	5	['We show empirical results for two extrinsic loss-functions (optimizing for the downstream task): machine translation and domain adaptation; and for one intrinsic loss-function: an arclength parsing score.', 'For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) ( #AUTHOR_TAG ) .']
CC402	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	gaijin a bootstrapping templatedriven approach to examplebased machine translation	['Tony Veale', 'Andy Way']	introduction		Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; #AUTHOR_TAG ; Carl 1999 ) .7	['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; #AUTHOR_TAG ; Carl 1999 ) .7']	0	['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; #AUTHOR_TAG ; Carl 1999 ) .7']
CC752	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	designing a computerised lexicon for linguistic purposes	['Erik Akkerman', 'Pieter Masereeuw', 'Willem Meijs']			One approach to this problem is that taken by the ASCOT project ( #AUTHOR_TAG ; Akkerman , 1986 ) .	['This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see Michiels, 1982, for further comment).', 'One approach to this problem is that taken by the ASCOT project ( #AUTHOR_TAG ; Akkerman , 1986 ) .', 'In this project, a new lexicon is being manually derived from LDOCE.', 'The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power.', 'More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis.', 'In the medium term this approach, though time consuming, will be of some utility for producing more reliable lexicons for natural language processing.']	0	['One approach to this problem is that taken by the ASCOT project ( #AUTHOR_TAG ; Akkerman , 1986 ) .', 'In this project, a new lexicon is being manually derived from LDOCE.', 'In the medium term this approach, though time consuming, will be of some utility for producing more reliable lexicons for natural language processing.']
CC391	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	generating language with a phrasal lexicon	['Edward Hovy']	introduction	"In this paper, we ask: How should language be represented in a generator program? In particular, how do the concepts the generator must express, the grammar it is to use, and the words and phrases with which it must express them, relate? The answer presented here is that all linguistic knowledge -- all language -- should be contained in the lexicon. The argument is the following: A generator performs three types of task to produce text (deciding what material to include; ordering the parts within paragraphs and sentences; and expressing the parts as appropriate phrases and parts of speech). It gets the information it requires to do these tasks from three sources: from the grammar, from partially frozen phrases (including multi-predicate phrasal patterns), and from certain words. In a functionally organized system, there is no reason why an a priori distinction should be made between the contents of the lexicon and the contents of the grammar. From the generator's perspective, the difference between these sources is not important. Rules of grammar, multi-predicate phrases, and phrasal and verb predicate patterns can all be viewed as phrases, frozen to a greater or lesser degree, and should all be part of the lexicon. Some such ""phrases"" can be quite complex, prescribing a series of actions and tests to perform the three tasks: these can be thought of as specialist procedures. Others can be very simple: templates. This paper also describes the elements that constitute the lexicon of a phrasal generator program and the way the elements are used."	â¢ Learnability ( Zernik and Dyer 1987 ) â¢ Text generation ( #AUTHOR_TAG ; Milosavljevic , Tulloch , and Dale 1996 ) â¢ Speech generation ( Rayner and Carter 1997 ) â¢ Localization ( Sch Â¨ aler 1996 )	['â\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation ( #AUTHOR_TAG ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation ( Rayner and Carter 1997 ) â\x80¢ Localization ( Sch Â¨ aler 1996 )']	0	['â\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation ( #AUTHOR_TAG ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation ( Rayner and Carter 1997 ) â\x80¢ Localization ( Sch Â¨ aler 1996 )']
CC961	K15-1003	A Supertag-Context Model for Weakly-Supervised CCG Parser Learning	widecoverage efficient statistical parsing with ccg and loglinear models	['Stephen Clark', 'James R Curran']			We further add rules for combining with punctuation to the left and right and allow for the merge rule X â X X of #AUTHOR_TAG .	['Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'The direction of the slash operator gives the behavior of the function.', 'A category (s\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\) with a noun phrase (np) that serves as its subject.', 'We follow Lewis and Steedman (2014) in allowing a small set of generic, linguistically-plausible unary and binary grammar rules.', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X â\x86\x92 X X of #AUTHOR_TAG .']	5	['The direction of the slash operator gives the behavior of the function.', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X â\x86\x92 X X of #AUTHOR_TAG .']
CC181	D13-1115	Integrating Theory and Practice: A Daunting Task	visual and semantic similarity in imagenet	['Thomas Deselaers', 'Vittorio Ferrari']	experiments	"Many computer vision approaches take for granted positive answers to questions such as ""Are semantic categories visually separable?"" and ""Is visual similarity correlated to semantic similarity?"". In this paper, we study experimentally whether these assumptions hold and show parallels to questions investigated in cognitive science about the human visual system. The insights gained from our analysis enable building a novel distance function between images assessing whether they are from the same basic-level category. This function goes beyond direct visual distance as it also exploits semantic similarity measured through ImageNet. We demonstrate experimentally that it outperforms purely visual distances."	It is frequently used in tasks like scene identification , and #AUTHOR_TAG shows that distance in GIST space correlates well with semantic distance in WordNet .	"['We also compute GIST vectors (Oliva and Torralba, 2001) for every image using LearGIST (Douze et al., 2009).', 'Unlike SURF descriptors, GIST produces a single vector representation for an image.', 'The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall ""gist"" of the whole image.', 'It is frequently used in tasks like scene identification , and #AUTHOR_TAG shows that distance in GIST space correlates well with semantic distance in WordNet .', 'After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality.']"	4	['It is frequently used in tasks like scene identification , and #AUTHOR_TAG shows that distance in GIST space correlates well with semantic distance in WordNet .']
CC977	N01-1010	Tree-cut and a lexicon based on systematic polysemy	automatic extraction of systematic polysemy using treecut	['N Tomuro']	introduction	This paper describes an automatic method for extracting systematic polysemy from a hierarchically organized semantic lexicon (WordNet). Systematic polysemy is a set of word senses that are related in systematic and predictable ways. Our method uses a modification of a tree generalization technique used in (Li and Abe, 1998), and generates a tree-cut, which is a list of clusters that partition a tree. We compare the systematic relations extracted by our automatic method to manually extracted WordNet cousins.	In our previous work ( #AUTHOR_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability .	['In this paper, we describes a lexicon organized around systematic polysemy.', 'The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998).', 'In our previous work ( #AUTHOR_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability .', 'In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy.', 'We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996).', 'The results are quite promising: our extraction method discovered 89% of the WordNet cousins, and the sense partitions in our lexicon yielded better values (Carletta, 1996) than arbitrary sense groupings on the agreement data.']	2	['In this paper, we describes a lexicon organized around systematic polysemy.', 'The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998).', 'In our previous work ( #AUTHOR_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability .', 'In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy.', 'We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996).', 'The results are quite promising: our extraction method discovered 89% of the WordNet cousins, and the sense partitions in our lexicon yielded better values (Carletta, 1996) than arbitrary sense groupings on the agreement data.']
CC163	D13-1115	Integrating Theory and Practice: A Daunting Task	distinctive image features from scaleinvariant keypoints	['David G Lowe']	related work	This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are ...	They use a Bag of Visual Words ( BoVW ) model ( #AUTHOR_TAG ) to create a bimodal vocabulary describing documents .	['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is Feng and Lapata (2010b).', 'They use a Bag of Visual Words ( BoVW ) model ( #AUTHOR_TAG ) to create a bimodal vocabulary describing documents .', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013).']	0	['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is Feng and Lapata (2010b).', 'They use a Bag of Visual Words ( BoVW ) model ( #AUTHOR_TAG ) to create a bimodal vocabulary describing documents .', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.']
CC470	J05-3003	Gaussian coordinates and the large scale universe	compacting the penn treebank grammar	['Alexander Krotov', 'Mark Hepple', 'Robert Gaizauskas', 'Yorick Wilks']		Treebanks, such as the Penn Treebank (PTB), offer a simple approach to obtaining a broad coverage grammar: one can simply read the grammar off the parse trees in the treebank. While such a grammar is easy to obtain, a square-root rate of growth of the rule set with corpus size suggests that the derived grammar is far from complete and that much more treebanked text would be required to obtain a complete grammar, if one exists at some limit. However, we offer an alternative explanation in terms of the underspecification of structures within the treebank. This hypothesis is explored by applying an algorithm to compact the derived grammar by eliminating redundant rules - rules whose right hand sides can be parsed by other rules. The size of the resulting compacted grammar, which is significantly less than that of the full treebank grammar, is shown to approach a limit. However, such a compacted grammar does not yield very good performance figures. A version of the compaction algorithm taking rule probabilities into account is proposed, which is argued to be more linguistically motivated. Combined with simple thresholding, this method can be used to give a 58% reduction in grammar size without significant change in parsing performance, and can produce a 69% reduction with some gain in recall, but a loss in precision.	In Charniak ( 1996 ) and #AUTHOR_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .	['The rate of accession may also be represented graphically.', 'In Charniak ( 1996 ) and #AUTHOR_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', 'We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity.', 'Figure 8 graphs the rate of induction of semantic form and CFG rule types from Penn-III (the WSJ and parse-annotated Brown corpus combined).', 'Because of the variation in the size of sections between the Brown and the WSJ, we plotted accession against word count.', 'The first part of the graph (up to 1,004,414 words)']	0	['In Charniak ( 1996 ) and #AUTHOR_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', 'We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity.', 'Figure 8 graphs the rate of induction of semantic form and CFG rule types from Penn-III (the WSJ and parse-annotated Brown corpus combined).', 'The first part of the graph (up to 1,004,414 words)']
CC772	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	a comprehensive grammar of english longman group limited	['Randolph Quirk', 'Sidney Greenbaum', 'Geoffrey Leech', 'Jan Svartvik']			The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #AUTHOR_TAG ) .	['In what follows we will discuss the format of the grammar codes in some detail as they are the focus of the current paper, however, the reader should bear in mind that they represent only one comparatively constrained field of an LDOCE entry and therefore, a small proportion of the overall restructuring task.', 'Figure 4 illustrates the grammar code field for the third word sense of the verb believe as it appears in the published dictionary, on the typesetting tape and after restructuring.', 'Figure 4 LDOCE provides considerably more syntactic information than a traditional dictionary.', 'The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #AUTHOR_TAG ) .', 'A grammar code describes a particular pattern of behaviour of a word.', 'Patterns are descriptive, and are used to convey a range of information: eg.', 'distinctions between count and mass nouns (dog vs. desire), predicative, postpositive and attributive adjectives (asleep vs. elect vs. jokular), noun complementation (fondness, fact) and, most importantly, verb complementation and valency.']	0	['In what follows we will discuss the format of the grammar codes in some detail as they are the focus of the current paper, however, the reader should bear in mind that they represent only one comparatively constrained field of an LDOCE entry and therefore, a small proportion of the overall restructuring task.', 'Figure 4 illustrates the grammar code field for the third word sense of the verb believe as it appears in the published dictionary, on the typesetting tape and after restructuring.', 'Figure 4 LDOCE provides considerably more syntactic information than a traditional dictionary.', 'The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #AUTHOR_TAG ) .', 'A grammar code describes a particular pattern of behaviour of a word.', 'Patterns are descriptive, and are used to convey a range of information: eg.']
CC1644	W14-1815	Natural Language Generation with Vocabulary Constraints	automatic generation of tamil lyrics for melodies	['Ananth Ramakrishnan A', 'Sankar Kuppan', 'Sobha Lalitha Devi']	related work	This paper presents our on-going work to automatically generate lyrics for a given melody, for phonetic languages such as Tamil. We approach the task of identifying the required syllable pattern for the lyric as a sequence labeling problem and hence use the popular CRF++ toolkit for learning. A corpus comprising of 10 melodies was used to train the system to understand the syllable patterns. The trained model is then used to guess the syllabic pattern for a new melody to produce an optimal sequence of syllables. This sequence is presented to the Sentence Generation module which uses the Dijkstra's shortest path algorithm to come up with a meaningful phrase matching the syllabic pattern.	Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan #AUTHOR_TAG ) , where specified meter or rhyme schemes are enforced .	['The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan #AUTHOR_TAG ) , where specified meter or rhyme schemes are enforced .', 'In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric.']	1	['The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan #AUTHOR_TAG ) , where specified meter or rhyme schemes are enforced .', 'In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric.']
CC779	J90-3003	Prosodic phrasing for speech synthesis of written telecommunications by the deaf	texttospeechan overview	['S P Olive', 'M Y Liberman']	introduction		In previous work ( Bachenko et al. 1986 ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .	['In previous work ( Bachenko et al. 1986 ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .', 'The system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'This paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'Like the earlier version, it has been implemented in a text-to-speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries.']	0	['In previous work ( Bachenko et al. 1986 ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .', 'The system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'This paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'Like the earlier version, it has been implemented in a text-to-speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries.']
CC1082	P10-2059	Classification of Feedback Expressions in Multimodal Data	clustering experiments on the communicative prop erties of gaze and gestures	['Kristiina Jokinen', 'Anton Ragni']	introduction		For example , #AUTHOR_TAG and Jokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .	['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example , #AUTHOR_TAG and Jokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .', 'Related are also the studies by Rieks op den Akker and Schulz (2008) and Murray and Renals (2008): both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus.']	0	['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example , #AUTHOR_TAG and Jokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .']
CC873	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	on whmovementquot in formal syntax edited by	['Noam Chomsky']			To a first approximation , a CURRENT-FOCUS reaches only nodes that are c-commanded ( #AUTHOR_TAG ) by its generator .	['2.5.1 Gaps.', 'The mechanism to deal with gaps resembles in certain respects the Hold register idea of ATNs, but with an important difference, reflecting the design philoso-phy that no node can have access to information outside of its immediate domain.', 'The mechanism involves two slots that are available in the feature vector of each parse node.', 'These are called the CURRENT-FOCUS and the FLOAT-OBJECT, respectively.', 'The CURRENT-FOCUS slot contains, at any given time, a pointer to the most recently mentioned content phrase in the sentence.', 'If the FLOAT-OBJECT slot is occupied, it means that there is a gap somewhere in the future that will ultimately be filled by the partial parse contained in the FLOAT-OBJECT.', 'The process of getting into the FLOAT-OBJECT slot (which is analogous to the Hold register) requires two steps, executed independently by two different nodes.', 'The first node, the generator, fills the CURRENT-FOCUS slot with the subparse returned to it by its children.', 'The second node, the activator, moves the CURRENT-FOCUS into the FLOAT-OBJECT position, for its children, during the top-down cycle.', 'It also requires that the FLOAT-OBJECT be absorbed somewhere among its descendants by a designated absorber node, a condition that is checked during the bottom-up cycle.', 'The CURRENT-FOCUS only gets passed along to siblings and their descendants, and hence is unavailable to activators at higher levels of the parse tree.', 'That is to say, the CURRENT-FOCUS is a feature, like verb-mode, that is blocked when an [end] node is encountered.', 'To a first approximation , a CURRENT-FOCUS reaches only nodes that are c-commanded ( #AUTHOR_TAG ) by its generator .', 'Finally, certain blocker nodes block the transfer of the FLOAT-OBJECT to their children.']	0	['2.5.1 Gaps.', 'The CURRENT-FOCUS slot contains, at any given time, a pointer to the most recently mentioned content phrase in the sentence.', 'If the FLOAT-OBJECT slot is occupied, it means that there is a gap somewhere in the future that will ultimately be filled by the partial parse contained in the FLOAT-OBJECT.', 'To a first approximation , a CURRENT-FOCUS reaches only nodes that are c-commanded ( #AUTHOR_TAG ) by its generator .']
CC1202	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	statistical significance tests for machine translation evaluation	['Philipp Koehn']	experiments	If two translation systems differ differ in perfor-mance on a test set, can we trust that this indicates a difference in true system quality? To answer this question, we describe bootstrap resampling meth-ods to compute statistical significance of test results, and validate them on the concrete example of the BLEU score. Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real.	The statistical significance test is performed by the re-sampling approach ( #AUTHOR_TAG ) .	['The experiments are conducted on Chinese-to-English translation.', 'The training data are the FBIS corpus with approximately 7.1 million Chinese words and 9.2 million English words.', 'We obtain the bidirectional word alignment with GIZA++, and then adopt the grow-diag-final-and strategy to obtain the final symmetric alignment.', 'We train a 5gram language model on the Xinhua portion of the English Gigaword corpus and the English part of the training data.', 'For tuning and testing, we use the NIST MT 2003 evaluation data as the development set, and use the NIST MT04 and MT05 data as the test set.', 'We use MERT (Och, 2004) to tune parameters.', 'Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set.', 'The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty.', 'The statistical significance test is performed by the re-sampling approach ( #AUTHOR_TAG ) .']	5	['The statistical significance test is performed by the re-sampling approach ( #AUTHOR_TAG ) .']
CC1040	P05-3005	Dynamically generating a protein entity dictionary using online resources	kwitek a et al rat genome database rgd mapping disease onto the genome nucleic acids res	['S Twigger', 'J Lu', 'M Shimoyama', 'D Chen', 'D Pasko', 'H Long', 'J Ginster', 'Chen CF', 'R Nigam']			Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) #AUTHOR_TAG , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .	['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) #AUTHOR_TAG , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']	5	['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) #AUTHOR_TAG , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']
CC68	D09-1056	The role of named entities in web people search	large scale named entity disambiguation based on wikipedia data	['Silviu Cucerzan']	related work	This paper presents a large-scale system for the recognition and semantic disambiguation of named entities based on information extracted from a large encyclopedic collection and Web search results. It describes in detail the disambiguation paradigm employed and the information extraction process from Wikipedia. Through a process of maximizing the agreement between the contextual information extracted from Wikipedia and the context of a document, as well as the agreement among the category tags associated with the candidate entities, the implemented system shows high disambiguation accuracy on both news stories and Wikipedia articles.	Some researchers ( #AUTHOR_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process .	['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers ( #AUTHOR_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']	0	['Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.', 'Some researchers ( #AUTHOR_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process .']
CC1269	W01-0706	Exploring evidence for shallow parsing	a linear observed time statistical parser based on maximum entropy models	['A Ratnaparkhi']	introduction	This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.	However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; #AUTHOR_TAG ) .	['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; #AUTHOR_TAG ) .']	0	['However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; #AUTHOR_TAG ) .']
CC1418	W04-0910	Paraphrastic grammars	alternations and verb semantic classes for french analysis and class formation chapter 5	['P Saint-Dizier']		In this paper, we show how alternations (called here contexts) can be defined for French, what their semantic properties are and how verb semantic classes can be constructed from syntactic criteria following (Levin 93). We then analyze the global quality of the results in terms of overlap with classes formed from the same verb-senses using WordNetlike classification criteria. Finally, we propose a method which combines these two approaches to form verb semantic classes better suited for natural language processing.	For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .	['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']	0	['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']
CC1555	W06-2807	Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein	the printing revolution in early modern europe	['Elizabeth L Eisenstein']	introduction	What difference did printing make? Although the importance of the advent of printing for the Western world has long been recognized, it was Elizabeth Eisenstein in her monumental, two-volume work, The Printing Press as an Agent of Change, who provided the first full-scale treatment of the subject. This illustrated and abridged edition provides a stimulating survey of the communications revolution of the fifteenth century. After summarizing the initial changes, and introducing the establishment of printing shops, it considers how printing effected three major cultural movements: the Renaissance, the Reformation, and the rise of modern science. First Edition Hb (1984) 0-521-25858-8 First Edition Pb (1984) 0-521-27735-3	For example , when books should n't be copied by hand any longer , authors took the advantage and start writing original books and evaluation -- i.e. literary criticism -- unlike in the previous times ( #AUTHOR_TAG ) .	"['Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', 'Terms as �chapter�, �page� or �foot- note� simply become meaningless in the new texts, or they highly change their meaning.', 'When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', ""For example , when books should n't be copied by hand any longer , authors took the advantage and start writing original books and evaluation -- i.e. literary criticism -- unlike in the previous times ( #AUTHOR_TAG ) ."", 'Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', 'For example, a �web page� is more similar to an infinite canvas than a written page (McCloud, 2001).', 'Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', 'From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an �opera aperta� (open work), as Eco would define it (1962).', 'From a more pessimistic one, an author may feel to have lost power in this openness.', 'Henceforth the collaborative traits of blogs and wikis (McNeill, 2005) emphasize annotation, comment, and strong editing.', 'They give more power to readers, eventually filling the gap - the so-called active readers become authors as well.', 'This situation could make new problems rise up: Who owns the text? Which role is suitable for authors? We have to analyse them before presenting the architecture of Novelle.']"	0	"['Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', 'Terms as chapter, page or foot- note simply become meaningless in the new texts, or they highly change their meaning.', 'When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', ""For example , when books should n't be copied by hand any longer , authors took the advantage and start writing original books and evaluation -- i.e. literary criticism -- unlike in the previous times ( #AUTHOR_TAG ) ."", 'Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', 'From a more pessimistic one, an author may feel to have lost power in this openness.', 'This situation could make new problems rise up: Who owns the text? Which role is suitable for authors? We have to analyse them before presenting the architecture of Novelle.']"
CC994	P00-1007	Incorporating Compositional Evidence in Memory-Based Partial Parsing	a linear observed time statistical parser based on maximum entropy models	['A Ratnaparkhi']		This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.	The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , Collins ( 1997 ) , and #AUTHOR_TAG , and became a common testbed .	['The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , Collins ( 1997 ) , and #AUTHOR_TAG , and became a common testbed .']	1	['The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , Collins ( 1997 ) , and #AUTHOR_TAG , and became a common testbed .']
CC486	J05-3003	Gaussian coordinates and the large scale universe	automatic extraction of subcategorization frames for czech	['Anoop Sarkar', 'Daniel Zeman']	related work	We present some novel machine learning techniques for the identification of subcategorization information for verbs in Czech. We compare three different statistical techniques applied to this problem. We show how the learning algorithm can be used to discover previously unknown subcategorization frames from the Czech Prague Dependency Treebank. The algorithm can then be used to label dependents of a verb in the Czech treebank as either arguments or adjuncts. Using our techniques, we are able to achieve 88 % precision on unseen parsed text.	#AUTHOR_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic	"['Approaches using treebank-based data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data.', 'Kinyon and Prolo (2002) describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank.', 'This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'Each of these sequences was categorized as a modifier or argument.', 'Arguments were then mapped to traditional syntactic functions.', 'For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic function is subject.', 'In general, argumenthood was preferred over adjuncthoood.', 'As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is.', '#AUTHOR_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic', 'Czech is a language with a freer word order than English and so configurational information cannot be relied upon.', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'Finding subcategorization frames involves filtering adjuncts from the observed frame.', 'This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score.', 'The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more).', 'Marinov and Hemming (2004) present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank (Simov, Popova, and Osenova 2002).', ""In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming's system collects both arguments and adjuncts."", 'It then uses the binomial log-likelihood ratio to filter incorrect frames.', 'The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees.', 'The work done for Bulgarian is small-scale, however, as Marinov and Hemming are working with a preliminary version of the treebank with 580 sentences.']"	0	['Kinyon and Prolo (2002) describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank.', '#AUTHOR_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more).']
CC859	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	automatic speech recognition the development of the sphinx system appendix i	['K F Lee']			A formula for the test set perplexity ( #AUTHOR_TAG ) is :13	['A formula for the test set perplexity ( #AUTHOR_TAG ) is :13']	0	['A formula for the test set perplexity ( #AUTHOR_TAG ) is :13']
CC382	J02-3002	Periods, Capitalized Words, etc.	a maximum entropy approach to identifying sentence boundaries”	['Jeffrey C Reynar', 'Adwait Ratnaparkhi']		We present a trainable model for identifying sentence boundaries in raw text. Given a corpus annotated with sentence boundaries, our model learns to classify each occurrence of ., ?, and ! as either a valid or invalid sentence boundary. The training procedure requires no hand-crafted rules, lexica, part-of-speech tags, or domain-specific information. The model can therefore be trained easily on any genre of English, and should be trainable on any other Roman-alphabet language. Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains.Comment: 4 pages, uses aclap.sty and covingtn.st	Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .	['Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .', 'Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.', 'Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.', 'This implies an investment in the data annotation phase.']	0	['Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .', 'Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.', 'Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.', 'This implies an investment in the data annotation phase.']
CC679	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	arabic tokenization partofspeech tagging and morphological disambiguation in one fell swoop	['Nizar Habash', 'Owen Rambow']	related work	We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including part-of-speech tagging) Arabic words in one process. We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from the output of the analyzer. We obtain accuracy rates on all tasks in the high nineties.	Put differently, we are interested in the tradeoff between relevance and accu- racy.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit ( #AUTHOR_TAG ; Habash, Rambow, and Roth 2012)15 (see Table 2, columns 5�7).	['So far we discussed optimal (gold) conditions.', 'But in prac- tice, POS tags are annotated by automatic taggers, so parsers get predicted POS tags as input, as opposed to gold (human-annotated) tags.14', 'The more informative the tag set, the less accurate the tag prediction might be, so the effect on overall parsing quality is unclear.', 'Put differently, we are interested in the tradeoff between relevance and accu- racy.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit ( #AUTHOR_TAG ; Habash, Rambow, and Roth 2012)15 (see Table 2, columns 5�7).', 'It turned out that BW, the best gold performer but with lowest POS pre- diction accuracy (81.8%), suffered the biggest drop (11.4%) and was the worst performer with predicted tags.', 'The simplest tag set, CATIB6, and its extension, CATIBEX, benefited from the highest POS prediction accuracy (97.7%), and their performance suffered the least.', 'CATIBEX was the best performer with predicted POS tags.', 'Performance drop and POS prediction accuracy are given in columns 8 and 9.']	5	['So far we discussed optimal (gold) conditions.', 'But in prac- tice, POS tags are annotated by automatic taggers, so parsers get predicted POS tags as input, as opposed to gold (human-annotated) tags.14', 'The more informative the tag set, the less accurate the tag prediction might be, so the effect on overall parsing quality is unclear.', 'Put differently, we are interested in the tradeoff between relevance and accu- racy.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit ( #AUTHOR_TAG ; Habash, Rambow, and Roth 2012)15 (see Table 2, columns 5�7).', 'It turned out that BW, the best gold performer but with lowest POS pre- diction accuracy (81.8%), suffered the biggest drop (11.4%) and was the worst performer with predicted tags.', 'The simplest tag set, CATIB6, and its extension, CATIBEX, benefited from the highest POS prediction accuracy (97.7%), and their performance suffered the least.', 'CATIBEX was the best performer with predicted POS tags.']
CC1113	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	tree edit distance for textual entailment	['Milen Kouleykov', 'Bernardo Magnini']	introduction		All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and Moldovan , 2005 ) , or adopting transformation-based techniques ( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .	['All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and Moldovan , 2005 ) , or adopting transformation-based techniques ( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .', 'Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores.', 'WordNet, the most widely used resource in TE, provides all the three types of information.', 'Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable.', 'Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text.', 'Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis.']	0	['All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and Moldovan , 2005 ) , or adopting transformation-based techniques ( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .', 'Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores.', 'WordNet, the most widely used resource in TE, provides all the three types of information.']
CC375	J02-3002	Periods, Capitalized Words, etc.	tagging sentence boundaries”	['Andrei Mikheev']	conclusion	In this paper we tackle sentence boundary disambiguation through a part-of-speech (POS) tagging framework. We describe necessary changes in text tokenization and the implementation of a POS tagger and provide results of an evaluation of this system on two corpora. We also describe an extension of the traditional POS tagging by combining it with the document-centered approach to proper name identification and abbreviation handling. This made the resulting system robust to domain and topic shifts.	This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in #AUTHOR_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .	"['With all its strong points, there are a number of restrictions to the proposed approach.', 'First, in its present form it is suitable only for processing of reasonably ""wellbehaved"" texts that consistently use capitalization (mixed case) and do not contain much noisy data.', 'Thus, for instance, we do not expect our system to perform well on single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on optical character reader-generated texts.', 'We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in #AUTHOR_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .']"	1	['This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in #AUTHOR_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .']
CC1083	P10-2059	Classification of Feedback Expressions in Multimodal Data	distinguishing the communicative functions of gestures	['Kristiina Jokinen', 'Costanza Navarretta', 'Patrizia Paggio']	introduction	This paper deals with the results of a machine learning experiment conducted on annotated gesture data from two case studies (Danish and Estonian). The data concern mainly facial displays, that are annotated with attributes relating to shape and dynamics, as well as communicative function. The results of the experiments show that the granularity of the attributes used seems appropriate for the task of distinguishing the desired communicative functions. This is a promising result in view of a future automation of the annotation task.	For example , Jokinen and Ragni ( 2007 ) and #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .	['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example , Jokinen and Ragni ( 2007 ) and #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .', 'Related are also the studies by Rieks op den Akker and Schulz (2008) and Murray and Renals (2008): both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multi- modal corpus.']	0	['Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example , Jokinen and Ragni ( 2007 ) and #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .']
CC339	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	outstanding issues in anaphora resolution	['Ruslan Mitkov']			The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( #AUTHOR_TAGa ) .	['The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( #AUTHOR_TAGa ) .', 'A fundamental question that needs further investigation is how far the performance of anaphora resolution algorithms can go and what the limitations of knowledge-poor methods are.', 'In particular, more research should be carried out on the factors influencing the performance of these algorithms.', 'One of the impediments to the evaluation or fuller utilization of machine learning techniques is the lack of widely available corpora annotated for anaphoric or coreferential links.', 'More work toward the proposal of consistent and comprehensive evaluation is necessary; so too is work in multilingual contexts.', 'Some of these challenges have been addressed in the papers published in this issue, but ongoing research will continue to address them in the near future.']	3	['The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( #AUTHOR_TAGa ) .', 'Some of these challenges have been addressed in the papers published in this issue, but ongoing research will continue to address them in the near future.']
CC849	J91-2003	On compositional semantics	the flow of thought and the flow of languagequot	['W L Chafe']	introduction		#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .	['Although there are other discussions of the paragraph as a central element of discourse (e.g.', '#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']	1	['Although there are other discussions of the paragraph as a central element of discourse (e.g.', '#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']
CC72	D09-1056	The role of named entities in web people search	entitybased crossdocument coreferencing using the vector space model	['Amit Bagga', 'Breck Baldwin']	related work		The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) ( Agirre and Edmonds , 2006 ) and Cross-document Coreference ( CDC ) ( #AUTHOR_TAG ) .	['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) ( Agirre and Edmonds , 2006 ) and Cross-document Coreference ( CDC ) ( #AUTHOR_TAG ) .', 'Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task -Web People Search -on its own (Artiles et al., 2005;Artiles et al., 2007).']	0	['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) ( Agirre and Edmonds , 2006 ) and Cross-document Coreference ( CDC ) ( #AUTHOR_TAG ) .', 'Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task -Web People Search -on its own (Artiles et al., 2005;Artiles et al., 2007).']
CC618	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	queryrelevant summarization using faqs	['A Berger', 'V Mittal']		This paper introduces a statistical model for query-relevant summarization: succinctly characterizing the relevance of a document to a query. Learning parameter values for the proposed model requires a large collection of summarized documents, which we do not have, but as a proxy, we use a collection of FAQ (frequently-asked question) documents. Taking a learning approach enables a principled, quantitative evaluation of the proposed system, and the results of some initial experiments---on a collection of Usenet FAQs and on a FAQ-like set of customer-submitted questions to several large retail companies---suggest the plausibility of learning for summarization.	In FAQs , #AUTHOR_TAG employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document .	['In FAQs , #AUTHOR_TAG employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document .', 'They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.', 'compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models).', 'Jijkoun and de Rijke ( 2005) compared different variants of retrieval techniques.', 'Soricut and Brill (2006) compared a predictive approach (statistical translation), a retrieval approach based on a language-model, and a hybrid approach which combines statistical chunking and traditional retrieval.', 'Two significant differences between help-desk and FAQs are the following.']	0	['In FAQs , #AUTHOR_TAG employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document .', 'They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.', 'compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models).', 'Jijkoun and de Rijke ( 2005) compared different variants of retrieval techniques.', 'Soricut and Brill (2006) compared a predictive approach (statistical translation), a retrieval approach based on a language-model, and a hybrid approach which combines statistical chunking and traditional retrieval.', 'Two significant differences between help-desk and FAQs are the following.']
CC559	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	the interaction of domain knowledge and linguistic structure in natural language processing interpreting hypernymic propositions in biomedical text	['Thomas C Rindflesch', 'Marcelo Fiszman']	introduction	Interpretation of semantic propositions in free-text documents such as MEDLINE citations would provide valuable support for biomedical applications, and several approaches to semantic interpretation are being pursued in the biomedical informatics community. In this paper, we describe a methodology for interpreting linguistic structures that encode hypernymic propositions, in which a more specific concept is in a taxonomic relationship with a more general concept. In order to effectively process these constructions, we exploit underspecified syntactic analysis and structured domain knowledge from the Unified Medical Language System (UMLS). After introducing the syntactic processing on which our system depends, we focus on the UMLS knowledge that supports interpretation of hypernymic propositions. We first use semantic groups from the Semantic Network to ensure that the two concepts involved are compatible; hierarchical information in the Metathesaurus then determines which concept is more general and which more specific. A preliminary evaluation of a sample based on the semantic group Chemicals and Drugs provides 83% precision. An error analysis was conducted and potential solutions to the problems encountered are presented. The research discussed here serves as a paradigm for investigating the interaction between domain knowledge and linguistic structure in natural language processing, and could also make a contribution to research on automatic processing of discourse structure. Additional implications of the system we present include its integration in advanced semantic interpretation processors for biomedical text and its use for information extraction in specific domains. The approach has the potential to support a range of applications, including information retrieval and ontology engineering.	Second , software for utilizing this ontology already exists : MetaMap ( Aronson 2001 ) identifies concepts in free text , and SemRep ( #AUTHOR_TAG ) extracts relations between the concepts .	['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'This domain is well-suited for exploring the posed research questions for several reasons.', 'First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'Second , software for utilizing this ontology already exists : MetaMap ( Aronson 2001 ) identifies concepts in free text , and SemRep ( #AUTHOR_TAG ) extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', 'The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.', 'Third, the paradigm of evidence-based medicine (Sackett et al. 2000) provides a task-based model of the clinical information-seeking process.', 'The PICO framework (Richardson et al. 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system.', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']	0	['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'This domain is well-suited for exploring the posed research questions for several reasons.', 'First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'Second , software for utilizing this ontology already exists : MetaMap ( Aronson 2001 ) identifies concepts in free text , and SemRep ( #AUTHOR_TAG ) extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', 'The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.', 'The PICO framework (Richardson et al. 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system.']
CC1119	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	syntacticsemantic structures for textual entailment recognition	['Yashar Mehdad', 'Alessandro Moschitti', 'Fabio Massimo Zanzotto']	introduction	In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning.	Using the basic solution proposed by ( #AUTHOR_TAG ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions :	['This paper investigates the idea, still unexplored, of a tighter integration of MT and TE algorithms and techniques.', 'Our aim is to embed cross-lingual processing techniques inside the TE recognition process in order to avoid any dependency on external MT components, and eventually gain full control of the system�s behaviour.', 'Along this direction, we start from the acquisition and use of lexical knowl- edge, which represents the basic building block of any TE system.', 'Using the basic solution proposed by ( #AUTHOR_TAG ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions :']	1	['Using the basic solution proposed by ( #AUTHOR_TAG ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions :']
CC219	D13-1115	Integrating Theory and Practice: A Daunting Task	indexing by latent semantic analysis	['Scott Deerwester', 'Susan T Dumais', 'George W Furnas', 'Thomas K Landauer', 'Richard Harshman']	related work	"A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (""semantic structure"") in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising."	Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .	['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']	0	['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']
CC1537	W06-1705	Annotated web as corpus	finding syntactic structure in unparsed corpora the gsearch corpus query system computers and the humanities	['S Corley', 'M Corley', 'F Keller', 'M Crocker', 'S Trewin']	related work		The Gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .	"['""Real-time"" linguistic analysis of web data at the syntactic level has been piloted by the Linguist\'s Search Engine (LSE).', 'Using this tool, linguists can either perform syntactic searches via parse trees on a pre-analysed web collection of around three million sentences from the Internet Archive (www.archive.org)', 'or build their own collections from AltaVista search engine results.', 'The second method pushes the new collection onto a queue for the LSE annotator to analyse.', 'A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server.', 'The Gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .', 'Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up.', 'In contrast, the Sketch Engine system to assist lexicographers to construct dictionary entries requires large pre-annotated corpora.', ""A word sketch is an automatic one-page corpus-derived summary of a word's grammatical and collocational behaviour."", 'Word Sketches were first used to prepare the Macmillan English Dictionary for Advanced Learners (2002, edited by Michael Rundell).', 'They have also served as the starting point for high-accuracy Word Sense Disambiguation.', 'More recently, the Sketch Engine was used to develop the new edition of the Oxford Thesaurus of English (2004, edited by Maurice Waite).']"	0	"['or build their own collections from AltaVista search engine results.', 'The second method pushes the new collection onto a queue for the LSE annotator to analyse.', 'A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server.', 'The Gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .', ""A word sketch is an automatic one-page corpus-derived summary of a word's grammatical and collocational behaviour."", 'Word Sketches were first used to prepare the Macmillan English Dictionary for Advanced Learners (2002, edited by Michael Rundell).', 'They have also served as the starting point for high-accuracy Word Sense Disambiguation.', 'More recently, the Sketch Engine was used to develop the new edition of the Oxford Thesaurus of English (2004, edited by Maurice Waite).']"
CC483	J05-3003	Gaussian coordinates and the large scale universe	extracting tree adjoining grammars from bracketed corpora	['Fei Xia']		ExtractingTreeAdjoiningGrammarsfromBracketedCorp oraFeiXiaDepartmentofComputerandInformationScienceUniversityofPennsylvania3401WalnutStreet,Suite400APhiladelphiaPA19104,USAfxia@linc.cis.upenn.eduAbstractInthispap er,werep ortourorkonextractinglexi-calizedtreeadjoininggrammars(LTAGs)frompar-tiallybracketedcorp ora.Thealgorithm rstfullybracketsthecorp ora,thenextractselementarytrees(etrees),and nally ltersoutinvalidusinglinguisticknowledge.Weshowthatthesetofex-tractedetreesmaynotb ecompleteenoughtocoverthewholelanguage,butthiswillnothaveabigim-pactonparsing.1Intro ductionLexicalizedTreeAdjoiningGrammar(LAG)isatree-rewritingformalism.Itismoreexpressivethanacontext-freegrammar(CFG),1andthereforeb et-terformalismforrepresentingvariousphenomenainnaturallanguages.Inthelastdecade,ithasb eenappliedtovariousNLPtaskssuchasparsing(Srini-vas,1997),machinetranslation(Palmeretal.,1998),informationretrieval(ChandrasekarandSrinivas,1997),generation(StoneandDoran,1997;McCoyetal.,1992),andsummarizationapplications(Bald-winetal.,1997).Awide-coverageLTGforapar-ticularnaturallanguageoftencontainsthousandsoftreesandtakesyearstobuild.Therehasb eenworkonextractingCFGs(Shi-raietal.,1995;Charniak,1996;Krotovandoth-ers,1998)andlexicalizedtreegrammars(Neumann,1998;Srinivas,1997)frombracketedcorp ora.Inthispap er,weprop oseanewmetho dforlearningLTAGsfromsuchcorp ora.22PennTreebankandLAG2.1PennTreebankInthispap er,weusetheEnglishPennTreebankasourbracketedcorpus,whichincludesab out1millionTheauthorwishestothankChung-hyeHan,AravindJoshi,MarthaPalmer,CarlosProlo,Ano opSarkar,andthreeanonymousreviewersformanyhelpfulcomments.1LTAGismoreexpressivethanCFformalismb othinweakandstronggenerativecapacity,e.g.itcanhandlecrossdep endencyelegantly.2Arelatedworkis(Srinivas,1997),butitsgoalnottolearnanewLTAGbuttoextracttheusefulinformation,suchasdep endencyandfrequencyoftrees,foranexistingLTAG.	Following Hockenmaier , Bierner , and Baldridge ( 2002 ) , #AUTHOR_TAG , and Miyao , Ninomiya , and Tsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ .	['In addition to evaluating the quality of our extracted semantic forms, we also examined the rate at which they are induced.', 'This can be expressed as a measure of the coverage of the induced lexicon on new data.', 'Following Hockenmaier , Bierner , and Baldridge ( 2002 ) , #AUTHOR_TAG , and Miyao , Ninomiya , and Tsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ .', 'We then compare this to a test lexicon from Section 23.', 'Table 27 shows the results of the evaluation of the coverage of an induced lexicon for verbs only.', 'There is a corresponding semantic form in the reference lexicon for 89.89% of the verbs in Section 23. 10.11% of the entries in the test lexicon did not appear in the reference lexicon.', 'Within this group, we can distinguish between known words, which have an entry in the reference lexicon, and unknown words, which do not exist at all in the reference lexicon.', 'In the same way we make the distinction  between known frames and unknown frames.', 'There are, therefore, four different cases in which an entry may not appear in the reference lexicon.', 'Table 27 shows that the most common case is that of known verbs occurring with a different, although known, subcategorization frame (7.85%).']	5	['In addition to evaluating the quality of our extracted semantic forms, we also examined the rate at which they are induced.', 'This can be expressed as a measure of the coverage of the induced lexicon on new data.', 'Following Hockenmaier , Bierner , and Baldridge ( 2002 ) , #AUTHOR_TAG , and Miyao , Ninomiya , and Tsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ .', 'We then compare this to a test lexicon from Section 23.', 'Table 27 shows the results of the evaluation of the coverage of an induced lexicon for verbs only.', 'There is a corresponding semantic form in the reference lexicon for 89.89% of the verbs in Section 23. 10.11% of the entries in the test lexicon did not appear in the reference lexicon.', 'Within this group, we can distinguish between known words, which have an entry in the reference lexicon, and unknown words, which do not exist at all in the reference lexicon.', 'In the same way we make the distinction  between known frames and unknown frames.', 'There are, therefore, four different cases in which an entry may not appear in the reference lexicon.', 'Table 27 shows that the most common case is that of known verbs occurring with a different, although known, subcategorization frame (7.85%).']
CC185	D13-1115	Integrating Theory and Practice: A Daunting Task	improving video activity recognition using object recognition and text mining	['Tanvi S Motwani', 'Raymond J Mooney']	related work	Abstract. Recognizing activities in real-world videos is a chal-lenging AI problem. We present a novel combination of standard activity classification, object recognition, and text mining to learn effective activity recognizers without ever explicitly labeling train-ing videos. We cluster verbs used to describe videos to automatically discover classes of activities and produce a labeled training set. This labeled data is then used to train an activity classifier based on spatio-temporal features. Next, text mining is employed to learn the correla-tions between these verbs and related objects. This knowledge is then used together with the outputs of an off-the-shelf object recognizer and the trained activity classifier to produce an improved activity rec-ognizer. Experiments on a corpus of YouTube videos demonstrate the effectiveness of the overall approach.	To name a few examples , Rohrbach et al. ( 2010 ) and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos .	['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , Rohrbach et al. ( 2010 ) and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos .']	0	['To name a few examples , Rohrbach et al. ( 2010 ) and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos .']
CC730	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	the sdc speech understanding system in lea	['J Barnett', 'M Berstein', 'R Gillman', 'I Kameny']			A number of speech understanding systems have been developed during the past fifteen years ( #AUTHOR_TAG , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .	['A number of speech understanding systems have been developed during the past fifteen years ( #AUTHOR_TAG , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']	1	['A number of speech understanding systems have been developed during the past fifteen years ( #AUTHOR_TAG , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.']
CC113	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	a tale of two parsers investigating and combining graphbased and transitionbased dependency parsing	['Y Zhang', 'S Clark']	experiments	Graph-based and transition-based approaches to dependency parsing adopt very different views of the problem, each view having its own strengths and limitations. We study both approaches under the framework of beam-search. By developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods. More importantly, we propose a beam-search-based parser that combines both graph-based and transition-based parsing into a single system for training and decoding, showing that it outperforms both the pure graph-based and the pure transition-based parsers. Testing on the English and Chinese Penn Treebank data, the combined system gave state-of-the-art accuracies of 92.1% and 86.2%, respectively.	• Transition-based: An implementation of the transition-based dependency parsing framework ( Nivre , 2008 ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists.	['• Transition-based: An implementation of the transition-based dependency parsing framework ( Nivre , 2008 ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists.', 'The features used by all models are: the part-ofspeech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of the top word on the stack (if available); dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer (if available).', 'All feature conjunctions are included.']	5	['• Transition-based: An implementation of the transition-based dependency parsing framework ( Nivre , 2008 ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists.']
CC122	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	characterizing the errors of datadriven dependency parsing models	['R McDonald', 'J Nivre']	experiments	We present a comparative error analysis of the two dominant approaches in datadriven dependency parsing: global, exhaustive, graph-based models, and local, greedy, transition-based models. We show that, in spite of similar performance overall, the two models produce different types of errors, in a way that can be explained by theoretical properties of the two models. This analysis leads to new directions for parser development.	Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #AUTHOR_TAG ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction (Yates and Etzioni, 2009) and textual entailment (Berant et al., 2010).	['The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well.', 'Labeled attachment score is used as our base model loss function.', 'In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_�,_)(i__) For each word of the sentence we compute the dis- tance between the word�s position i and the posi- tion of the words head _i.', 'The arc-length score is the summed length of all those with correct head assignments (_(_�i,_i) is 1 if the predicted head and the correct head match, 0 otherwise).', 'The score is normalized by the summed arc lengths for the sentence.', 'The labeled version of this score requires that the labels of the arc are also correct.', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #AUTHOR_TAG ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction (Yates and Etzioni, 2009) and textual entailment (Berant et al., 2010).']	4	['The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well.', 'Labeled attachment score is used as our base model loss function.', 'The arc-length score is the summed length of all those with correct head assignments (_(_i,_i) is 1 if the predicted head and the correct head match, 0 otherwise).', 'The score is normalized by the summed arc lengths for the sentence.', 'The labeled version of this score requires that the labels of the arc are also correct.', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #AUTHOR_TAG ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction (Yates and Etzioni, 2009) and textual entailment (Berant et al., 2010).']
CC1160	P97-1063	A word-to-word model of translational equivalence	a geometric approach to mapping bitext correspondencequot	['I D Melamed']	method		Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( #AUTHOR_TAGc ) .	"['2We could just as easily use other symmetric ""association"" measures, such as ¢2  or the Dice coefficient (Smadja, 1992).', 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.', 'Then vk and uk+l will also co-occur more often than expected by chance.', 'The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk.', 'Models of translational equivalence that are ignorant of indirect associations have ""a tendency ... to be confused by collocates"" (Dagan et al., 1993).', 'Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( #AUTHOR_TAGc ) .', ""The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest."", 'The competitive linking algorithm implements this heuristic:']"	0	['Now, suppose that uk and Uk+z often co-occur within their language.', 'Then vk and uk+l will also co-occur more often than expected by chance.', 'Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( #AUTHOR_TAGc ) .']
CC1542	W06-1705	Annotated web as corpus	the corpusbased study of language change in progress the extra value of tagged corpora	['C Mair']	introduction		In addition , the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .	['In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']	0	['This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .']
CC1510	W06-1104	Automatically creating datasets for measures of semantic relatedness	using information content to evaluate semantic similarity	['Michael Lesk']		Evaluating Semantic similarity has a widely application areas range from Psychology, Linguistics, Cognitive Science to Artificial Intelligence. This paper proposes the merely use of HowNet to evaluate Information Content (IC) as the semantic similarity of two terms or word senses. While the conventional ways of measuring the IC of word senses must depend on both an ontology like WordNet and a large corpus, experiments of this paper prove that the semantic similarity measured in this method is easy to calculate and more approach to human judgments.	dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .	['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	0	['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']
CC693	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	an efficient algorithm for easyfirst nondirectional dependency parsing	['Yoav Goldberg', 'Michael Elhadad']	introduction		The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser ( #AUTHOR_TAG ) .	['results, assignment features, specifically CASE, are very helpful in MSA, though only under gold conditions: Because CASE is rarely explicit in the typically undiacritized written MSA, it has a dismal accuracy rate, which makes it useless when used in a machine-predicted (real, non-gold) condition.', 'In contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions.', 'This is likely a result of MSA having a rich agreement system, covering both verb-subject and noun-adjective relations.', 'The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser ( #AUTHOR_TAG ) .']	5	['The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser ( #AUTHOR_TAG ) .']
CC1230	W00-1017	WIT	a robust system for natural spoken dialogue	['James F Allen', 'Bradford W Miller', 'Eric K Ringger', 'Teresa Sikorsld']	introduction	This paper describes a system that leads us to believe in the feasibility of constructing natural spoken dialogue systems in task-oriented domains. It specifically addresses the issue of robust interpretation of speech in the presence of recognition errors. Robustness is achieved by a combination of statistical error post-correction, syntactically- and semantically-driven robust parsing, and extensive use of the dialogue context. We present an evaluation of the system using time-to-completion and the quality of the final solution that suggests that most native speakers of English can use the system successfully with virtually no training.Comment: uuencoded, gzipped PostScript. Includes extra Appendi	The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .	['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']	0	['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .']
CC1403	W04-0910	Paraphrastic grammars	discovery of inference rules for question answering natural language engineering	['Dekang Lin', 'Patrick Pantel']	introduction		For instance , ( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .	['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance , ( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .', 'Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']	0	['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance , ( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .', 'Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']
CC691	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	a statistical parser for czech	['Michael Collins', 'Jan Hajic', 'Lance Ramshaw', 'Christoph Tillmann']	related work	This paper considers statistical parsing of Czech, which differs radically from English in at least two respects: (1) it is a highly inflected language, and (2) it has relatively free word order. These differences are likely to pose new problems for techniques that have been developed on English. We describe our experience in building on the parsing model of (Collins 97). Our final results- 80% dependency accuracy - represent good progress towards the 91% accuracy of the parser on English (Wall Street Journal) text.	#AUTHOR_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) .	"['Much work has been done on the use of morphological features for parsing of morphologically rich languages.', '#AUTHOR_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) .', 'This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size ≈3000+).', 'They also report that the use of gender, number, and person features did not yield any improvements.', 'The results for Czech are the opposite of our results for Arabic, as we will see.', 'This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajič and Vidová-Hladká 1998) compared with Arabic (≈14.0%,', 'see Table 3).', 'Similarly, Cowan and Collins (2005) report that the use of a subset of Spanish morphological features (number for adjectives, determiners, nouns, pronouns, and verbs; and mode for verbs) outperforms other combinations.', 'Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.', 'We also find that the number feature helps for Arabic.', ""Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima'an (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing.""]"	0	['#AUTHOR_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) .', 'They also report that the use of gender, number, and person features did not yield any improvements.']
CC134	D12-1037	Discriminative Training for Log-Linear Based SMT	minimum error rate training in statistical machine translation	['Franz Josef Och']	introduction	Minimum Error Rate Training (MERT) is an effective means to estimate the feature func-tion weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature func-tion its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, candidates in MERT are represented as N-best lists which contain the N most probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm that allows for efficiently constructing and repre-senting the exact error surface of all trans-lations that are encoded in a phrase lattice. Compared to N-best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experi-ments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N-best MERT.	Motivated by (#AUTHOR_TAG, 2003; Smith and Eisner, 2006), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW\u2212WbII2+ A \ufffd j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier. with exp[\u03b1W \u00b7 h(fj, e)] P\u03b1(e|fj; W) = (7) Ee'Ec; exp[\u03b1W \u00b7 h(fj, e')], where \u03b1 > 0 is a real number valued smoother.	"['Due to the existence of L2 norm in objective function (5), the optimization algorithm MERT can not be applied for this question since the exact line search routine does not hold here.', ""Motivated by (#AUTHOR_TAG, 2003; Smith and Eisner, 2006), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW−WbII2+ A � j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier. with exp[αW · h(fj, e)] Pα(e|fj; W) = (7) Ee'Ec; exp[αW · h(fj, e')], where α > 0 is a real number valued smoother."", 'One can see that, in the extreme case, for α —* oc, (6) converges to (5).']"	4	"[""Motivated by (#AUTHOR_TAG, 2003; Smith and Eisner, 2006), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW\\u2212WbII2+ A \\ufffd j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier. with exp[\\u03b1W \\u00b7 h(fj, e)] P\\u03b1(e|fj; W) = (7) Ee'Ec; exp[\\u03b1W \\u00b7 h(fj, e')], where \\u03b1 > 0 is a real number valued smoother."", 'One can see that, in the extreme case, for a --* oc, (6) converges to (5).']"
CC1599	W06-3309	Generative content models for structural analysis of medical abstracts	information needs in office practice are they being met	['David G Covell', 'Gwen C Uman', 'Phil R Manning']	introduction	We studied the self-reported information needs of 47 physicians during a half day of typical office practice. The physicians raised 269 questions about patient management. Questions related to all medical specialties and were highly specific to the individual patient's problem. Subspecialists most frequently asked questions related to other subspecialties. Only 30% of physicians' information needs were met during the patient visit, usually by another physician or other health professional. Reasons print sources were not used included the age of textbooks in the office, poor organization of journal articles, inadequate indexing of books and drug information sources, lack of knowledge of an appropriate source, and the time required to find the desired information. Better methods are needed to provide answers to questions that arise in office practice.	The need for information systems to support physicians at the point of care has been well studied ( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 ) .	"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied ( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 ) .', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']"	0	['For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied ( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 ) .']
CC999	P00-1012	The order of prenominal adjectives in natural language generation	distributional clustering of english words	['Fernando Pereira', 'Naftali Tishby', 'Lilian Lee']	conclusion	"We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical ""soft"" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data."	More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; #AUTHOR_TAG ) could be applied to extract semantic classes from the corpus itself .	['While the combined MBL method achieves reasonably good results even given the limitations of the evaluation method, there is still clearly room for improvement.', 'Future work will pursue at least two directions for improving the results.', 'First, while semantic information is not available for all adjectives, it is clearly available for some.', 'Furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available.', 'More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; #AUTHOR_TAG ) could be applied to extract semantic classes from the corpus itself .', 'Since the constraints on adjective ordering in English depend largely on semantic classes, the addition of semantic information to the model ought to improve the results.']	3	['More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; #AUTHOR_TAG ) could be applied to extract semantic classes from the corpus itself .']
CC945	K15-1002	A Joint Framework for Coreference Resolution and Mention Head Detection	firstorder probabilistic models for coreference resolution	['A Culotta', 'M Wick', 'A McCallum']	experiments	Traditional noun phrase coreference resolution systems represent features only of pairs of noun phrases. In this paper, we propose a machine learning method that enables features over sets of noun phrases, resulting in a first-order probabilistic model for coreference. We outline a set of approximations that make this approach practical, and apply our method to the ACE coreference dataset, achieving a 45% error reduction over a comparable method that only considers features of pairs of noun phrases. This result demonstrates an example of how a firstorder logic representation can be incorporated into a probabilistic model and scaled efficiently.	We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #AUTHOR_TAG ; Bengtson and Roth , 2008 ) .	['The ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #AUTHOR_TAG ; Bengtson and Roth , 2008 ) .', 'The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents.', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'We report results on the test documents for both datasets.', ') indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.8, λ 1 = 0.2 and λ 2 = 0.3.']	5	['The ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #AUTHOR_TAG ; Bengtson and Roth , 2008 ) .', 'The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents.', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'We report results on the test documents for both datasets.', ') indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.']
CC273	E12-1068	Modeling Inflection and Word-Formation in SMT	how to avoid burning ducks combining linguistic analysis and corpus statistics for german compound processing	['Fabienne Fritzinger', 'Alexander Fraser']	experiments	Compound splitting is an important problem in many NLP applications which must be solved in order to address issues of data sparsity. Previous work has shown that linguistic approaches for German compound splitting produce a correct splitting more often, but corpus-driven approaches work best for phrase-based statistical machine translation from German to English, a worrisome contradiction. We address this situation by combining linguistic analysis with corpus-driven statistics and obtaining better results in terms of both producing splittings according to a gold standard and statistical machine translation performance.	We prepare the training data by splitting compounds in two steps , following the technique of #AUTHOR_TAG .	['We prepare the training data by splitting compounds in two steps , following the technique of #AUTHOR_TAG .', 'First, possible split points are extracted using SMOR, and second, the best split points are selected using the geometric mean of word part frequencies.', 'Training data is then stemmed as described in Section 2.3.', 'The formerly modifying words of the compound (in our example the words to the left of the rightmost word) do not have a stem markup assigned, except for two cases: i) they are nouns themselves or ii) they are particles separated from a verb.', 'In these cases, former modifiers are represented identically to their individual occurring counterparts, which helps generalization.']	5	['We prepare the training data by splitting compounds in two steps , following the technique of #AUTHOR_TAG .', 'First, possible split points are extracted using SMOR, and second, the best split points are selected using the geometric mean of word part frequencies.', 'The formerly modifying words of the compound (in our example the words to the left of the rightmost word) do not have a stem markup assigned, except for two cases: i) they are nouns themselves or ii) they are particles separated from a verb.']
CC553	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	automatically evaluating answers to definition questions	['Jimmy Lin', 'Dina Demner-Fushman']		Following recent developments in the automatic evaluation of machine translation and document summarization, we present a similar approach, implemented in a measure called Pourpre, for automatically evaluating answers to definition questions. Until now, the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system's response. The lack of automatic methods for scoring system output is an impediment to progress in the field, which we address with this work. Experiments with the TREC 2003 and TREC 2004 QA tracks indicate that rankings produced by our metric correlate highly with official rankings, and that Pourpre outperforms direct application of existing metrics.	We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization ( #AUTHOR_TAGb ) , but these features are also beyond the capabilities of current summarization systems .	"['The most important characteristic of answers, as recommended by Ely et al. (2005) in their study of real-world physicians, is that they focus on bottom-line clinical advice-information that physicians can directly act on.', 'Ideally, answers should integrate information from multiple clinical studies, pointing out both similarities and differences.', 'The system should collate concurrences, that is, if multiple abstracts arrive at the same conclusion-it need not be repeated unless the physician wishes to ""drill down""; the system should reconcile contradictions, for example, if two abstracts disagree on a particular treatment because they studied different patient populations.', 'We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization ( #AUTHOR_TAGb ) , but these features are also beyond the capabilities of current summarization systems .']"	1	['We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization ( #AUTHOR_TAGb ) , but these features are also beyond the capabilities of current summarization systems .']
CC1246	W00-1312	Cross-lingual information retrieval using hidden Markov models	a comparative study of query and document translation for crosslanguage information retrievalquot	['D W Oard']	related work	Cross-language retrieval systems use queries in one natural language to guide retrieval of documents that might be written in another. Acquisition and representation of translation knowledge plays a central role in this process. This paper explores the utility of two sources of translation knowledge for cross-language retrieval. We have implemented six query translation techniques that use bilingual term lists and one based on direct use of the translation output from an existing machine translation system; these are compared with a document translation technique that uses output from the same machine translation system. Average precision measures on a TREC collection suggest that arbitrarily selecting a single dictionary translation is typically no less effective than using every translation in the dictionary, that query translation using a machine translation system can achieve somewhat better effectiveness than simpler techniques, and that document translation may result in further improvements in retrieval effectiveness under some conditions.	One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ) .	['Many approaches to cross-lingual IR have been published.', 'One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ) .', 'For most languages, there are no MT systems at all.', 'Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived.']	1	['One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ) .']
CC1019	P02-1001	Parameter estimation for probabilistic finite-state transducers	hidden markov models with finite state supervision in	['E Ristad']		In this chapter we provide a supervised training paradigm for hidden Markov models (HMMs). Unlike popular ad-hoc approaches, our paradigm is completely general, need not make any simplifying assumptions about independence, and can take better advantage of the information contained in the training corpus.	For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX #AUTHOR_TAG , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .11	['As training data we are given a set of observed (input, output) pairs, (xi, yi).', 'These are assumed to be independent random samples from a joint dis- tribution of the form f_�(x, y); the goal is to recover the true _�.', 'Samples need not be fully observed (partly supervised training): thus xi _ __, yi _ �_ may be given as regular sets in which input and output were observed to fall.', 'For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX #AUTHOR_TAG , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .11']	0	['As training data we are given a set of observed (input, output) pairs, (xi, yi).', 'These are assumed to be independent random samples from a joint dis- tribution of the form f_(x, y); the goal is to recover the true _.', 'For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX #AUTHOR_TAG , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .11']
CC796	J90-3003	Prosodic phrasing for speech synthesis of written telecommunications by the deaf	performance structures a psycholinguistic and linguistic appraisal	['J P Gee', 'F Grosjean']	introduction		Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .	['The psycholinguistic studies of Martin (1970), Allen (1975), Hitlinger et al. (1976, Grosjean et al. (1979), Dommergues and Grosjean (1983), and Gee and Grosjean (1983), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing: mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly.', 'Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .']	1	['The psycholinguistic studies of Martin (1970), Allen (1975), Hitlinger et al. (1976, Grosjean et al. (1979), Dommergues and Grosjean (1983), and Gee and Grosjean (1983), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing: mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly.', 'Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .']
CC497	J05-4005	Chinese Word Segmentation and Named Entity Recognition: A Pragmatic Approach	a stochastic finitestate wordsegmentation algorithm for chinese	['Richard Sproat', 'Chilin Shih', 'William Gale', 'Nancy Chang']	related work		A previous work along this line is #AUTHOR_TAG , which is based on weighted finite-state transducers ( FSTs ) .	"['We believe that the identification of OOV words should not be treated as a problem separate from word segmentation.', 'We propose a unified approach that solves both problems simultaneously.', 'A previous work along this line is #AUTHOR_TAG , which is based on weighted finite-state transducers ( FSTs ) .', 'Our approach is similarly motivated but is based on a different mechanism: linear mixture models.', 'As we shall see, the models provide a more flexible framework for incorporating various kinds of lexical and statistical information.', ""Many types of OOV words that are not covered in Sproat's system can be dealt with in our system."", 'The linear models we used are originally derived from linear discriminant functions widely used for pattern classification (Duda, Hart, and Stork 2001) and have been recently introduced into NLP tasks by Collins and Duffy (2001).', 'Other frameworks of Chinese word segmentation, which are similar to the linear models, include maximum entropy models (Xue 2003) and conditional random fields (Peng, Feng, and McCallum 2004).', 'They also use a unified approach to word breaking and OOV identification.']"	1	['We believe that the identification of OOV words should not be treated as a problem separate from word segmentation.', 'We propose a unified approach that solves both problems simultaneously.', 'A previous work along this line is #AUTHOR_TAG , which is based on weighted finite-state transducers ( FSTs ) .', 'Our approach is similarly motivated but is based on a different mechanism: linear mixture models.', 'As we shall see, the models provide a more flexible framework for incorporating various kinds of lexical and statistical information.', 'The linear models we used are originally derived from linear discriminant functions widely used for pattern classification (Duda, Hart, and Stork 2001) and have been recently introduced into NLP tasks by Collins and Duffy (2001).', 'They also use a unified approach to word breaking and OOV identification.']
CC1619	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	semantic role labelling using different syntactic views	['Sameer Pradhan', 'Wayne Ward', 'Kadri Hacioglu', 'James H Martin', 'Daniel Jurafsky']	related work	Semantic role labeling is the process of annotating the predicate-argument structure in text with semantic labels. In this paper we present a state-of-the-art baseline semantic role labeling system based on Support Vector Machine classifiers. We show improvements on this system by: i) adding new features including features extracted from dependency parses, ii) performing feature selection and calibration and iii) combining parses obtained from semantic parsers trained using different syntactic views. Error analysis of the baseline system showed that approximately half of the argument identification errors resulted from parse errors in which there was no syntactic constituent that aligned with the correct argument. In order to address this problem, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses. All of the reported techniques resulted in performance improvements.	Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 ) .	['In current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 ) .']	0	['Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 ) .']
CC256	E03-1005	An efficient implementation of a new DOP model	efficient algorithms for parsing the dop model	['J Goodman']		Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model to a small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree. Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data. We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers.	Most DOP models , such as in Bod ( 1993 ) , #AUTHOR_TAG , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e.	"[""Most DOP models , such as in Bod ( 1993 ) , #AUTHOR_TAG , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."", 'most probable) tree as a candidate for the best tree of a sentence.', 'We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by ""Likelihood-DOP"" the PCFG-reduction of Bod (2001) given in Section 2.2.']"	0	"[""Most DOP models , such as in Bod ( 1993 ) , #AUTHOR_TAG , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."", 'most probable) tree as a candidate for the best tree of a sentence.']"
CC1310	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	lilfes — towards a practical hpsg parsers	['Takaki Makino', 'Minoru Yoshida', 'Kentaro Torisawa', 'Jun’ichi Tsujii']	experiments		The RenTAL system is implemented in LiLFeS ( #AUTHOR_TAG ) 2 .	['The RenTAL system is implemented in LiLFeS ( #AUTHOR_TAG ) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;.', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus (Marcus et al., 1994) 6 (the average length is 6.32 words).', 'This result empirically attested the strong equivalence of our algorithm.']	5	['The RenTAL system is implemented in LiLFeS ( #AUTHOR_TAG ) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;.', 'This result empirically attested the strong equivalence of our algorithm.']
CC159	D13-1038	Embodied Collaborative Referring Expression Generation in Situated Human-Robot Interaction	towards mediating shared perceptual basis in situated dialogue	['Changsong Liu', 'Rui Fang', 'Joyce Y Chai']		To enable effective referential grounding in situated human robot dialogue, we have conducted an empirical study to investigate how conversation partners collaborate and mediate shared basis when they have mismatched visual perceptual capabilities. In particular, we have developed a graph-based representation to capture linguistic discourse and visual discourse, and applied inexact graph matching to ground references. Our empirical results have shown that, even when computer vision algorithms produce many errors (e.g. 84.7% of the objects in the environment are mis-recognized), our approach can still achieve 66% accuracy in referential grounding. These results demonstrate that, due to its error-tolerance nature, inexact graph matching provides a potential solution to mediate shared perceptual basis for referential grounding in situated interaction.	Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG ) .	['Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (Harnad, 1990).', 'Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG ) .', 'For the referring expression generation task here, we also need a lexicon with grounded semantics.']	2	['Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG ) .']
CC591	J09-1005	Unsupervised Type and Token Identification of Idiomatic Expressions	automatic identification of noncompositional phrases	['Dekang Lin']			Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #AUTHOR_TAG .	['Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #AUTHOR_TAG .', 'We use the automatically built thesaurus of Lin (1998) to find words similar to each constituent, in order to automatically generate variants.', 'Variants are generated by replacing either the noun or the verb constituent of a pair with a semantically (and syntactically) similar word. 3', 'xamples of automatically generated variants for the pair spill, bean are pour, bean , stream, bean , spill, corn , and spill, rice .']	4	['Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #AUTHOR_TAG .', 'We use the automatically built thesaurus of Lin (1998) to find words similar to each constituent, in order to automatically generate variants.', 'Variants are generated by replacing either the noun or the verb constituent of a pair with a semantically (and syntactically) similar word. 3']
CC1255	W01-0706	Exploring evidence for shallow parsing	introduction to the conll2000 shared task chunking	['E F Tjong Kim Sang', 'S Buchholz']	experiments		The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) .	['The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) .', 'In this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'The goal in this case is therefore to accurately predict a collection of ¢ £ ¢ different types of phrases.', 'The chunk types are based on the syntactic category part of the bracket label in the Treebank.', 'Roughly, a chunk contains everything to the left of and including the syntactic head of the constituent of the same name.', 'The phrases are: adjective phrase (ADJP), adverb phrase (ADVP), conjunction phrase (CONJP), interjection phrase (INTJ), list marker (LST), noun phrase (NP), preposition phrase (PP), particle (PRT), subordinated clause (SBAR), unlike coordinated phrase (UCP), verb phrase (VP).', '(See details in (Tjong Kim Sang and Buchholz, 2000).)']	5	['The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) .', 'The chunk types are based on the syntactic category part of the bracket label in the Treebank.', 'Roughly, a chunk contains everything to the left of and including the syntactic head of the constituent of the same name.', '(See details in (Tjong Kim Sang and Buchholz, 2000).)']
CC383	J02-3002	Periods, Capitalized Words, etc.	one term or two” in	['Kenneth Church']	introduction	Objective SYNTAX score II (SSII) is a long-term mortality prediction model to guide the decision making of the heart-team between coronary artery bypass grafting or percutaneous coronary intervention (PCI) in patients with left main or three-vessel coronary artery disease. This study aims to investigate the long-term predictive value of SSII for all-cause mortality in patients with one- or two-vessel disease undergoing PCI. Methods A total of 628 patients (76% men, mean age: 61+-10 years) undergoing PCI due to stable angina pectoris (43%) or acute coronary syndrome (57%), included between January 2008 and June 2013, were eligible for the current study. SSII was calculated using the original SYNTAX score website (www.syntaxscore.com). Cox regression analysis was used to assess the association between continuous SSII and long-term all-cause mortality. The area under the receiver-operating characteristic curve was used to assess the performance of SSII. Results SSII ranged from 6.6 to 58.2 (median: 20.4, interquartile range: 16.1-26.8). In multivariable analysis, SSII proved to be an independent significant predictor for 4.5-year mortality (hazard ratio per point increase: 1.10; 95% confidence interval: 1.07-1.13; p<0.001). In terms of discrimination, SSII had a concordance index of 0.77. Conclusion In addition to its established value in patients with left main and three-vessel disease, SSII may also predict long-term mortality in PCI-treated patients with one- or two-vessel disease	#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''	"['Disambiguation of capitalized words in mixed-case texts has received little attention in the natural language processing and information retrieval communities, but in fact it plays an important role in many tasks.', 'In mixed-case texts capitalized words usually denote proper names (names of organizations, locations, people, artifacts, etc.), but there are special positions in the text where capitalization is expected.', 'Such mandatory positions include the first word in a sentence, words in titles with all significant words capitalized or table entries, a capitalized word after a colon or open quote, and the first word in a list entry, among others.', 'Capitalized words in these and some other positions present a case of ambiguity: they can stand for proper names, as in White later said . . .', ', or they can be just capitalized common words, as in White elephants are . . . .', 'The disambiguation of capitalized words in ambiguous positions leads to the identification of proper names (or their derivatives), and in this article we will use these two terms and the term case normalization interchangeably.', ""#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''"", 'Obviously these differences arise because some capitalized words stand for proper names (such as Continental, the name of an airline) and some do not.']"	0	"[""#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''"", 'Obviously these differences arise because some capitalized words stand for proper names (such as Continental, the name of an airline) and some do not.']"
CC1249	W00-1312	Cross-lingual information retrieval using hidden Markov models	corpusbased stemming using cooccurrence of word variantsquot	['J Xu', 'W B Croft']	experiments		A cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem Spanish words .	['For Spanish, we downloaded a bilingual English-Spanish lexicon from the Internet (http://www.activa.arrakis.es)', 'containing around 22,000 English words (16,000 English stems) and processed it similarly.', 'Each English word has around 1.5 translations on average.', 'A cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem Spanish words .', 'One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon.', 'This is useful for translating proper nouns, which often have identical spellings in English and Spanish but are routinely excluded from a lexicon.']	5	['Each English word has around 1.5 translations on average.', 'A cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem Spanish words .']
CC335	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	multilingual anaphora resolution	['Ruslan Mitkov']		This paper presents amultilingual robust, knowledge-poor approach to resolvingpronouns in technical manuals. This approach is a modification of the practicalapproach (Mitkov 1998a) and operates on texts pre-processed by apart-of-speech tagger. Input is checked against agreementand a number of antecedent indicators. Candidates are assigned scores by eachindicator and the candidate with the highest aggregate score isreturned as the antecedent. We propose this approach as aplatform for multilingual pronoun resolution. The robust approach was initiallydeveloped and tested for English, but we have also adaptedand tested it for Polish and Arabic. For bothlanguages, we found that adaptation required minimummodification and that further, even if used unmodified, the approachdelivers acceptable success rates. Preliminary evaluation reports high successrates in the range of over 90%.	Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; #AUTHOR_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .	['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; #AUTHOR_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']	0	['The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; #AUTHOR_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.']
CC701	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	better arabic parsing baselines evaluations and analysis	['Spence Green', 'Christopher D Manning']	related work	In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design. First, we identify sources of syntactic ambiguity understudied in the existing parsing literature. Second, we show that although the Penn Arabic Treebank is similar to other tree-banks in gross statistical terms, annotation consistency remains problematic. Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG. Fourth, we show how to build better models for three different parsers. Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2--5% F1.	As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .	['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']	0	['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.']
CC1300	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	a lexicalized tree adjoining grammar for english	['The XTAG Research Group']	introduction	This paper presents a sizable grammar for English written in the Tree Adjoining grammar (TAG) formalism. The grammar uses a TAG that is both lexicalized (Schabes, Abeille, Joshi 1988) and feature-based (VijayShankar, Joshi 1988). In this paper, we describe a wide range of phenomena that it covers. A Lexicalized TAG (LTAG) is organized around a lexicon, which associates sets of elementary trees (instead of just simple categories) with the lexical items. A Lexicalized TAG consists of a finite set of trees associated with lexical items, and operations (adjunction and substitution) for composing the trees. A lexical item is called the anchor of its corresponding tree and directly determines both the tree's structure and its syntactic features. In particular, the trees define the domain of locality over which constraints are specified and these constraints are local with respect to their anchor. In this paper, the basic tree structures of the English LTAG are described, along with some relevant features. The interaction between the morphological and the syntactic components of the lexicon is also explained. Next, the properties of the different tree structures are discussed. The use of S complements exclusively allows us to take full advantage of the treatment of unbounded dependencies originally presented in Joshi (1985) and Kroch and Joshi (1985). Structures for auxiliaries and raising-verbs which use adjunction trees are also discussed. We present a representation of prepositional complements that is based on extended elementary trees. This representation avoids the need for preposition incorporation in order to account for double whquestions (preposition stranding and pied-piping) and the pseudo-passive. A treatment of light verb constructions is also given, similar to what Abeille (1988c) has presented. Again, neither noun nor adjective incorporation is needed to handle double passives and to account for CNPC violations in these constructions. TAG'S extended domain of locality allows us to handle, within a single level of syntactic description, phenomena that in other frameworks require either dual analyses or reanalysis. In addition, following Abeille and Schabes (1989), we describe how to deal with semantic non compositionality in verb-particle combinations, light verb constructions and idioms, without losing the internal syntactic composition of these structures. The last sections discuss current work on PRO, case, anaphora and negation, and outline future work on copula constructions and small clauses, optional arguments, adverb movement and the nature of syntactic rules in a lexicalized framework. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-90-24. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/527 A Lexicalized Tree Adjoining Grammar For English MS-CIS-90-24 LINC LAB 170 Anne Abeillh Kathleen Bishop Sharon Cote Yves Schabes Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 19104-6389	We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .	"['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']"	5	"['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .']"
CC652	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	a systematic comparison of various statistical alignment models	['Franz Josef Och', 'Hermann Ney']		We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.	We used a standard implementation of IBM Model 4 ( #AUTHOR_TAG ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves .	['The intent of this experimental section is to evaluate the gains from using constraints during learning, hence the main comparison is between HMM trained with normal EM vs. trained with PR plus constraints.', 'We also report results for IBM Model 4, because it is often used as the default word alignment model, and can be used as a reference.', 'However, we would like to note that IBM Model 4 is a more complex model, able to capture more structure, albeit at the cost of intractable inference.', 'Because our approach is orthogonal to the base model used, the constraints described here could be applied in principle to IBM Model 4 if exact inference was efficient, hopefully yielding similar improvements.', 'We used a standard implementation of IBM Model 4 ( #AUTHOR_TAG ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves .', 'We trained IBM Model 4 using the default configuration of the']	5	['We also report results for IBM Model 4, because it is often used as the default word alignment model, and can be used as a reference.', 'However, we would like to note that IBM Model 4 is a more complex model, able to capture more structure, albeit at the cost of intractable inference.', 'We used a standard implementation of IBM Model 4 ( #AUTHOR_TAG ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves .']
CC1502	W06-1104	Automatically creating datasets for measures of semantic relatedness	automatic text processing the transformation analysis and retrieval of information by computer	['Gerard Salton']	experiments		The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #AUTHOR_TAG ) .	"['The three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger (Schmid, 1995).', ""The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #AUTHOR_TAG ) .""]"	5	"['The three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger (Schmid, 1995).', ""The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #AUTHOR_TAG ) .""]"
CC513	J06-2002	Generating Referring Expressions that Involve Gradable Properties	fitting words vague language in context linguistics and philosophy	['Alice Kyburg', 'Michael Morreau']	introduction		The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #AUTHOR_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .	['Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999).', 'The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #AUTHOR_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', 'Our own proposal will abstract away from the effects of linguistic context.', 'We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words.', 'This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form of']	0	['Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999).', 'The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #AUTHOR_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', 'Our own proposal will abstract away from the effects of linguistic context.']
CC594	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	bridging the lexical chasm statistical approaches to answerfinding	['A Berger', 'R Caruana', 'D Cohn', 'D Freitag', 'V Mittal']	method		â¢ Only an automatic evaluation was performed , which relied on having model responses ( Berger and Mittal 2000 ; #AUTHOR_TAG ) .	['â\x80¢ Only an automatic evaluation was performed , which relied on having model responses ( Berger and Mittal 2000 ; #AUTHOR_TAG ) .']	1	['â\x80¢ Only an automatic evaluation was performed , which relied on having model responses ( Berger and Mittal 2000 ; #AUTHOR_TAG ) .']
CC519	J06-2002	Generating Referring Expressions that Involve Gradable Properties	data structures and algorithms	['Alfred V Aho', 'John E Hopcroft', 'Jeffrey D Ullman']	introduction	Multi-adaptive Galerkin methods are extensions of the standard continuous and discontinuous Galerkin methods for the numerical solution of initial value problems for ordinary or partial differential equations. In particular, the multi-adaptive methods allow individual and adaptive time steps to be used for different components or in different regions of space. We present algorithms for efficient multi-adaptive time-stepping, including the recursive construction of time slabs and adaptive time step selection. We also present data structures for efficient storage and interpolation of the multi-adaptive solution. The efficiency of the proposed algorithms and data structures is demonstrated for a series of benchmark problems.Comment: ACM Transactions on Mathematical Software 35(3), 24 pages (2008	Worst case , calculating the set corresponding with such a property , of the form size ( x ) = maxm , for example , involves sorting the distractors as to their size , which may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [ #AUTHOR_TAG ] Chapter 8 ) .	['If computing the intersection of two sets takes constant time then this makes the complexity of interpreting non-vague descriptions linear: O(nd), where nd is the number of properties used.', 'In a vague description, the property last added to the description is context dependent.', 'Worst case , calculating the set corresponding with such a property , of the form size ( x ) = maxm , for example , involves sorting the distractors as to their size , which may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [ #AUTHOR_TAG ] Chapter 8 ) .', 'Once again, the most time-consuming part of the calculation can be performed off-line, since it is the same for all referring expressions.']	0	['Worst case , calculating the set corresponding with such a property , of the form size ( x ) = maxm , for example , involves sorting the distractors as to their size , which may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [ #AUTHOR_TAG ] Chapter 8 ) .', 'Once again, the most time-consuming part of the calculation can be performed off-line, since it is the same for all referring expressions.']
CC480	J05-3003	Gaussian coordinates and the large scale universe	lexical functional grammar volume 34 of syntax and semantics	['Mary Dalrymple']	introduction		In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; #AUTHOR_TAG ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .	['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; #AUTHOR_TAG ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']	0	['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; #AUTHOR_TAG ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']
CC350	J02-3002	Periods, Capitalized Words, etc.	robust partofspeech tagging using a hidden markov model computer speech and language	['Julian Kupiec']			We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #AUTHOR_TAG ] , Brill 's [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .	"['In our markup convention (Section 2), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations.', 'Consequently a POS tagger can naturally treat them similarly to any other ambiguous words.', 'There is, however, one difference in the implementation of such a tagger.', 'Normally, a POS tagger operates on text spans that form a sentence.', 'This requires resolving sentence boundaries before tagging.', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #AUTHOR_TAG ] , Brill 's [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", 'The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.', 'This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.', 'At the same time since this token is unambiguous, it is not affected by the history.', 'A trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other.']"	1	"['In our markup convention (Section 2), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations.', 'Consequently a POS tagger can naturally treat them similarly to any other ambiguous words.', 'There is, however, one difference in the implementation of such a tagger.', 'Normally, a POS tagger operates on text spans that form a sentence.', 'This requires resolving sentence boundaries before tagging.', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #AUTHOR_TAG ] , Brill 's [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", 'The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.', 'This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.', 'At the same time since this token is unambiguous, it is not affected by the history.', 'A trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other.']"
CC610	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	assessing agreement on classification tasks the kappa statistic	['J Carletta']	method	Currently, computational linguists and cognitive scientists working in the area of discourse and dialogue argue that their subjective judgments are reliable using several different statistics, none of which are easily interpretable or comparable to each other. Meanwhile, researchers in content analysis have already experienced the same difficulties and come up with a solution in the kappa statistic. We discuss what is wrong with reliability measures as they are currently used for discourse and dialogue work in computational linguistics and cognitive science, and argue that we would be better off as a field adopting techniques from content analysis.	Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures ( #AUTHOR_TAG ) .	"['Each evaluation set contained 80 cases, randomly selected from the corpus in proportion to the size of each data set, where a case contained a request e-mail, the model response, and the responses generated by the two methods being compared.', 'Each judge was given of these cases, and was asked to assess the generated responses on the four criteria listed previously. 14', 'e maximized the coverage of this study by allocating different cases to each judge, thus avoiding a situation where a particularly good or bad set of cases is evaluated by all judges.', ""In addition, we tried to ensure that the sets of cases shown to the judges were of similar quality, so that the judges' assessments would be comparable."", 'Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures ( #AUTHOR_TAG ) .', 'However, it is still necessary to We asked the judges to leave a question unanswered if they felt they did not have the technical knowledge to make a judgment, but this did not occur.']"	5	['Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures ( #AUTHOR_TAG ) .']
CC804	J91-2003	On compositional semantics	cohesion in english	['M A K Halliday', 'R Hasan']	introduction	Despite all the dire predictions, Germany continues to exhibit a high level of social cohesion. Even the country's growing cultural and religious diversity is not at odds with its degree of togetherness. Yet, there are clear indications of potential threats: for example, the generally perceived lack of social justice and the gaping cleavage in togetherness between east and west as well as between structurally weak regions and such that are flourishing. To that end, cohesion is much weaker in Germany's eastern federal states than its western ones. The federal states with the highest levels of cohesion are Saarland, Baden-Wurttemberg, and Bavaria. These are the core findings from the Social Cohesion Radar (SCR), for which Bertelsmann Stiftung collected fresh data in 2017 by surveying more than 5,000 people throughout the country. The goal was to examine cohesion in terms of its strengths, weaknesses, causes, and effects. For this empirical study, social cohesion is defined as the quality of communal life and is viewed as a multidimensional phenomenon. High levels of cohesion result from strong social relations, a positive feeling of connectedness to the community, and a strong focus on the common good	Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , #AUTHOR_TAG , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .	['Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , #AUTHOR_TAG , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']	1	['Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , #AUTHOR_TAG , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']
CC365	J02-3002	Periods, Capitalized Words, etc.	russian morphology an engineering approach	['Andrei Mikheev', 'Liubov Liubushkina']	experiments	Morphological analysis, which is at the heart of the processing of natural language requires computationally effective morphological processors. In this paper an approach to the organization of an inflectional morphological model and its application for the Russian language are described. The main objective of our morphological processor is not the classification of word constituents, but rather an efficient computational recognition of morpho-syntactic features of words and the generation of words according to requested morpho-syntactic features. Another major concern that the processor aims to address is the ease of extending the lexicon. The templated word-paradigm model used in the system has an engineering flavour: paradigm formation rules are of a bottom-up (word specific) nature rather than general observations about the language, and word formation units are segments of words rather than proper morphemes. This approach allows us to handle uniformly both general cases and exceptions, and requires extremely simple data structures and control mechanisms which can be easily implemented as a finite-state automata. The morphological processor described in this paper is fully implemented for a substantial subset of Russian (more then 1,500,000 word-tokens - 95,000 word paradigms) and provides an extensive list of morpho-syntactic features together with stress positions for words utilized in its lexicon. Special dictionary management tools were built for browsing, debugging and extension of the lexicon. The actual implementation was done in C and C++, and the system is available for the MS-DOS, MS-Windows and UNIX platforms.	Before using the DCA method , we applied a Russian morphological processor ( #AUTHOR_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. .	['Since, unlike English, Russian is a highly inflected language, we had to deal with the case normalization issue.', 'Before using the DCA method , we applied a Russian morphological processor ( #AUTHOR_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. .', 'For words that could be normalized to several main forms (polysemy), when secondary forms of different words coincided, we retained all the main forms.', 'Since the documents in the BBC news corpus were rather short, we applied the cache module, as described in Section 11.1.', 'This allowed us to reuse information across the documents.']	5	['Before using the DCA method , we applied a Russian morphological processor ( #AUTHOR_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. .']
