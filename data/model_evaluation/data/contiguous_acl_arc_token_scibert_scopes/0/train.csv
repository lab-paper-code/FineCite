token_context,word_context,seg_context,sent_cotext,label
"['##ing to an n - gram target language model ), in effect forms a multiengine mt system as described by  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['multiple translation systems, with a postvalidation process via the web ( amounting to an n - gram target language model ), in effect forms a multiengine mt system as described by  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['##ing to an n - gram target language model ), in effect forms a multiengine mt system as described by  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['translations gave rise to a number of automatically constructed linguistic resources : ( 1 ) the original ( source, target ) phrasal translation pairs, ( 2 ) the marker lexicon, ( 3 ) the gen11 thanks are due to one of the anonymous reviewers for pointing out that our webmt system, seeded with input from multiple translation systems, with a postvalidation process via the web ( amounting to an n - gram target language model ), in effect forms a multiengine mt system as described by  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']",1
"['', ' #TAUTHOR_TAG attempt to']","['distribution.', ' #TAUTHOR_TAG attempt to']","['a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', ' #TAUTHOR_TAG attempt to']","['ebmt systems, from the initial proposal by  #AUTHOR_TAG to the recent collection of  #AUTHOR_TAG, are premised on the availability of subsentential alignments derived from the input bitext.', 'there is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', 'kay and roscheisen ( 1993 ) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', ' #TAUTHOR_TAG attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', ' #AUTHOR_TAG replicates the work of fung and mckeown with different language pairs using the simpler metric of levenshtein distance.', ' #AUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent english - greek clauses on the basis of word occurrence and co - occurrence probabilities.', 'the respective lengths of the putative alignments in terms of characters is also an important factor.', ' #AUTHOR_TAG observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'accordingly, they generate lexical correspondences by means of co - occurrence measures and string similarity metrics.', 'more specifically, the notion of the phrasal lexicon ( used first by becker 1975 ) has been used successfully in a number of areas']",0
"['##¢ learnability  #TAUTHOR_TAG a¢ text generation ( hovy 1988 ; milosavljevic, tulloch,']","['##¢ learnability  #TAUTHOR_TAG a¢ text generation ( hovy 1988 ; milosavljevic, tulloch,']","['##¢ learnability  #TAUTHOR_TAG a¢ text generation ( hovy 1988 ; milosavljevic, tulloch,']","['##¢ learnability  #TAUTHOR_TAG a¢ text generation ( hovy 1988 ; milosavljevic, tulloch, and dale 1996 ) a¢ speech generation ( rayner and carter 1997 ) a¢ localization ( sch [UNK] aler 1996']",0
"['cicekli and g [UNK] uvenir ( 1996 ),  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, inter alia']","['cicekli and g [UNK] uvenir ( 1996 ),  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, inter alia']","['cicekli and g [UNK] uvenir ( 1996 ),  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, inter alia']","['distinguishes chunks from "" patterns, "" as we do : his chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', 'once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm : "" for each pair of chunk pairs using the algorithm described above, the patterns in ( 26 ) are derived from the chunks in ( 25 ) : of course, many other researchers also try to extract generalized templates.', ' #AUTHOR_TAG identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', ' #AUTHOR_TAG combines lexical and dependency mappings to form his generalizations.', 'other similar approaches include those of cicekli and g [UNK] uvenir ( 1996 ),  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, inter alia']",0
"['##r 1987 ) a¢ text generation  #TAUTHOR_TAG ; milosavljevic, tulloch, and dale 1996 ) a¢ speech generation ( rayner']","['##¢ learnability ( zernik and dyer 1987 ) a¢ text generation  #TAUTHOR_TAG ; milosavljevic, tulloch, and dale 1996 ) a¢ speech generation ( rayner']","['##r 1987 ) a¢ text generation  #TAUTHOR_TAG ; milosavljevic, tulloch, and dale 1996 ) a¢ speech generation ( rayner']","['##¢ learnability ( zernik and dyer 1987 ) a¢ text generation  #TAUTHOR_TAG ; milosavljevic, tulloch, and dale 1996 ) a¢ speech generation ( rayner and carter 1997 ) a¢ localization ( sch [UNK] aler 1996']",0
['ebmt systems ( cfxxx  #TAUTHOR_TAG ; veale and way 1997 ; carl 1999 ).'],['ebmt systems ( cfxxx  #TAUTHOR_TAG ; veale and way 1997 ; carl 1999 ).'],"['longer chunks, as is usual in most ebmt systems ( cfxxx  #TAUTHOR_TAG ; veale and way 1997 ; carl 1999 ).']","['translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'in order to calculate a ranking for each tl sentence produced, we multiply the weights of each chunk used in its construction.', 'note that this ensures that greater importance is attributed to longer chunks, as is usual in most ebmt systems ( cfxxx  #TAUTHOR_TAG ; veale and way 1997 ; carl 1999 ).']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['to be little more than a search - and - replace engine, albeit a rather sophisticated one  #TAUTHOR_TAG']","[""quite a short space of time, translation memory ( tm ) systems have become a very useful tool in the translator's armory."", 'tm systems store a set of source, target translation pairs in their databases.', 'if a new input string cannot be found exactly in the translation database, a search is conducted for close ( or "" fuzzy "" ) matches of the input string, and these are retrieved together with their translations for the translator to manipulate into the final, output translation.', 'from this description, it should be clear that tm systems do not translate : indeed, some researchers consider them to be little more than a search - and - replace engine, albeit a rather sophisticated one  #TAUTHOR_TAG']",0
"['simpler metric of levenshtein distance.', ' #TAUTHOR_TAG use a tagged parallel corpus to']","['simpler metric of levenshtein distance.', ' #TAUTHOR_TAG use a tagged parallel corpus to']","['fung and mckeown with different language pairs using the simpler metric of levenshtein distance.', ' #TAUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent english - greek clauses on']","['ebmt systems, from the initial proposal by  #AUTHOR_TAG to the recent collection of  #AUTHOR_TAG, are premised on the availability of subsentential alignments derived from the input bitext.', 'there is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', 'kay and roscheisen ( 1993 ) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'fung and mc  #AUTHOR_TAG attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', ' #AUTHOR_TAG replicates the work of fung and mckeown with different language pairs using the simpler metric of levenshtein distance.', ' #TAUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent english - greek clauses on the basis of word occurrence and co - occurrence probabilities.', 'the respective lengths of the putative alignments in terms of characters is also an important factor.', ' #AUTHOR_TAG observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'accordingly, they generate lexical correspondences by means of co - occurrence measures and string similarity metrics.', 'more specifically, the notion of the phrasal lexicon ( used first by becker 1975 ) has been used successfully in a number of areas']",0
"['; morgan, meier, and newport 1989 ) a¢ monolingual grammar induction ( juola 1998 ) a¢ grammar optimization ( juola 1994 ) a¢ insights into universal grammar ( juola 1998 ) a¢ machine translation ( juola 1994, 1997 ;  #TAUTHOR_TAG ; gough, way, and hearne 2002']","['; morgan, meier, and newport 1989 ) a¢ monolingual grammar induction ( juola 1998 ) a¢ grammar optimization ( juola 1994 ) a¢ insights into universal grammar ( juola 1998 ) a¢ machine translation ( juola 1994, 1997 ;  #TAUTHOR_TAG ; gough, way, and hearne 2002']","['##r 1983 ; morgan, meier, and newport 1989 ) a¢ monolingual grammar induction ( juola 1998 ) a¢ grammar optimization ( juola 1994 ) a¢ insights into universal grammar ( juola 1998 ) a¢ machine translation ( juola 1994, 1997 ;  #TAUTHOR_TAG ; gough, way, and hearne 2002']","['##¢ language learning ( green 1979 ; mori and moeser 1983 ; morgan, meier, and newport 1989 ) a¢ monolingual grammar induction ( juola 1998 ) a¢ grammar optimization ( juola 1994 ) a¢ insights into universal grammar ( juola 1998 ) a¢ machine translation ( juola 1994, 1997 ;  #TAUTHOR_TAG ; gough, way, and hearne 2002']",0
['##¢ language learning  #TAUTHOR_TAG ; mori and mo'],['##¢ language learning  #TAUTHOR_TAG ; mori and moeser'],['##¢ language learning  #TAUTHOR_TAG ; mori and mo'],"['##¢ language learning  #TAUTHOR_TAG ; mori and moeser 1983 ; morgan, meier, and newport 1989 ) a¢ monolingual grammar induction ( juola 1998 ) a¢ grammar optimization ( juola 1994 ) a¢ insights into universal grammar ( juola 1998 ) a¢ machine translation ( juola 1994, 1997 ; veale and way 1997 ; gough, way, and hearne 2002']",0
"['multiple translation systems, with a postvalidation process via the web ( amounting to an n - gram target language model ), in effect forms a multiengine mt system as described by  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG']","['multiple translation systems, with a postvalidation process via the web ( amounting to an n - gram target language model ), in effect forms a multiengine mt system as described by  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG']","['multiple translation systems, with a postvalidation process via the web ( amounting to an n - gram target language model ), in effect forms a multiengine mt system as described by  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG']","['are due to one of the anonymous reviewers for pointing out that our webmt system, seeded with input from multiple translation systems, with a postvalidation process via the web ( amounting to an n - gram target language model ), in effect forms a multiengine mt system as described by  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG']",1
"['recently,  #TAUTHOR_TAG have proposed the exploitation of tms at']","['recently,  #TAUTHOR_TAG have proposed the exploitation of tms at']","['recently,  #TAUTHOR_TAG have proposed the exploitation of tms at']","['recently,  #TAUTHOR_TAG have proposed the exploitation of tms at a subsentential level, while carl, way, and sch [UNK] aler ( 2002 ) and sch [UNK] aler, way, and  #AUTHOR_TAG, pages 108 - - 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment.', 'this, they suggest, may result in a paradigm shift from tm to ebmt via the phrasal lexicon : translators are on the whole wary of mt technology, but once subsentential alignment is enabled, translators will become aware of the benefits to be gained from _ source, target _ phrasal segments, and from there they suggest that it is a reasonably short step to enabling an automated solution via the recombination element of ebmt systems such as those described in [ carl and way 2003 ]']",0
"['is a pair of nonparallel corpora.', ' #TAUTHOR_TAG replicates']","['is a pair of nonparallel corpora.', ' #TAUTHOR_TAG replicates']","['a pair of nonparallel corpora.', ' #TAUTHOR_TAG replicates']","['ebmt systems, from the initial proposal by  #AUTHOR_TAG to the recent collection of  #AUTHOR_TAG, are premised on the availability of subsentential alignments derived from the input bitext.', 'there is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', 'kay and roscheisen ( 1993 ) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'fung and mc  #AUTHOR_TAG attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', ' #TAUTHOR_TAG replicates the work of fung and mckeown with different language pairs using the simpler metric of levenshtein distance.', ' #AUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent english - greek clauses on the basis of word occurrence and co - occurrence probabilities.', 'the respective lengths of the putative alignments in terms of characters is also an important factor.', ' #AUTHOR_TAG observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'accordingly, they generate lexical correspondences by means of co - occurrence measures and string similarity metrics.', 'more specifically, the notion of the phrasal lexicon ( used first by becker 1975 ) has been used successfully in a number of areas']",0
"[', 1997 ) assumes that']","[', 1997 ) assumes that']","[', 1997 ) assumes that words ending in - ed are verbs.', 'however, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category.', 'instead, we take advantage of the fact that the initial phrasal chunks correspond']","[', 1997 ) assumes that words ending in - ed are verbs.', 'however, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category.', 'instead, we take advantage of the fact that the initial phrasal chunks correspond to rule right - hand sides.', 'that is, for a rule in the penn treebank vp −→ vbg, np, pp, we are certain ( if the annotators have done their job correctly ) that the first word in each of the strings corresponding to this right - hand side is a vbg, that is, a present participle.', 'given this information, in such cases we tag such words with the < lex > tag.', 'taking expanding the board to 14 members −→ augmente le conseila 14 membres as an example, we extract the chunks in ( 24 we ignore here the trivially true lexical chunk "" < quant > 14 : 14.']",1
['so in the near future using the weighted majority algorithm  #TAUTHOR_TAG'],['so in the near future using the weighted majority algorithm  #TAUTHOR_TAG'],"['wordlevel lexicons.', 'we have yet to import such a constraint into our model, but we plan to do so in the near future using the weighted majority algorithm  #TAUTHOR_TAG']","['', 'we have yet to import such a constraint into our model, but we plan to do so in the near future using the weighted majority algorithm  #TAUTHOR_TAG']",3
[';  #TAUTHOR_TAG ; carl 1999 ).'],[';  #TAUTHOR_TAG ; carl 1999 ).'],"['longer chunks, as is usual in most ebmt systems ( cfxxx sato and nagao 1990 ;  #TAUTHOR_TAG ; carl 1999 ).']","['translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'in order to calculate a ranking for each tl sentence produced, we multiply the weights of each chunk used in its construction.', 'note that this ensures that greater importance is attributed to longer chunks, as is usual in most ebmt systems ( cfxxx sato and nagao 1990 ;  #TAUTHOR_TAG ; carl 1999 ).']",0
"['hoc validation and ( if required ) correction process based on  #TAUTHOR_TAG.', 'grefenstette shows that the web']","['hoc validation and ( if required ) correction process based on  #TAUTHOR_TAG.', 'grefenstette shows that the web']","['into a chunk that was generalized from a masculine plural np.', 'however, rather than output this wrong translation directly, we use a post hoc validation and ( if required ) correction process based on  #TAUTHOR_TAG.', 'grefenstette shows that the web can be used as a filter on translation quality simply by searching']","['problem of boundary friction is clearly visible here : we have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural np.', 'however, rather than output this wrong translation directly, we use a post hoc validation and ( if required ) correction process based on  #TAUTHOR_TAG.', 'grefenstette shows that the web can be used as a filter on translation quality simply by searching for competing translation candidates and selecting the one that is found most often.', 'rather than search for competing candidates, we select the "" best "" translation and have its morphological variants searched for on - line.', ""in the example above, namely, the personal computers, we search for les ordinateurs personnels versus the wrong alternatives le / la / l'ordinateurs personnels."", 'interestingly, using lycos, and setting the search language to french, the correct form les ordinateurs personnels is uniquely preferred over the other alternatives, as it is found 2, 454 times, whereas the others are not found at all.', '']",5
"['##ial alignment of source, target chunks.', ' #TAUTHOR_TAG, 1997 ) conducts some small experiments using his metla system to show the viability of this approach for english a > french and english a > urdu.', 'for']","['alignment of source, target chunks.', ' #TAUTHOR_TAG, 1997 ) conducts some small experiments using his metla system to show the viability of this approach for english a > french and english a > urdu.', 'for']","['##ial alignment of source, target chunks.', ' #TAUTHOR_TAG, 1997 ) conducts some small experiments using his metla system to show the viability of this approach for english a > french and english a > urdu.', 'for']","['that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks.', ' #TAUTHOR_TAG, 1997 ) conducts some small experiments using his metla system to show the viability of this approach for english a > french and english a > urdu.', 'for the english −→ french language pair, juola gives results of 61 % correct translation when the system is tested on the training corpus, and 36 % accuracy when it is evaluated with test data.', 'for english −→  #AUTHOR_TAG, page 213 ) notes that "" the system learned the original training corpus...', 'perfectly and could reproduce it without errors "" ; that is, it scored 100 % accuracy when tested against the training corpus.', 'on novel test sentences, he gives results of 72 % correct translation.', 'in their gaijin system,  #AUTHOR_TAG give a result of 63 % accurate translations obtained for english −→ german on a test set of 791 sentences from coreldraw manuals']",0
"['; morgan, meier, and newport 1989 ) a¢ monolingual grammar induction ( juola 1998 ) a¢ grammar optimization  #TAUTHOR_TAG a¢ insights into universal grammar ( juola 1998 ) a¢ machine translation ( juola 1994, 1997 ; veale']","['; morgan, meier, and newport 1989 ) a¢ monolingual grammar induction ( juola 1998 ) a¢ grammar optimization  #TAUTHOR_TAG a¢ insights into universal grammar ( juola 1998 ) a¢ machine translation ( juola 1994, 1997 ; veale']","['##r 1983 ; morgan, meier, and newport 1989 ) a¢ monolingual grammar induction ( juola 1998 ) a¢ grammar optimization  #TAUTHOR_TAG a¢ insights into universal grammar ( juola 1998 ) a¢ machine translation ( juola 1994, 1997 ; veale']","['##¢ language learning ( green 1979 ; mori and moeser 1983 ; morgan, meier, and newport 1989 ) a¢ monolingual grammar induction ( juola 1998 ) a¢ grammar optimization  #TAUTHOR_TAG a¢ insights into universal grammar ( juola 1998 ) a¢ machine translation ( juola 1994, 1997 ; veale and way 1997 ; gough, way, and hearne 2002']",0
"['found in  #TAUTHOR_TAG.', ""in block's""]","['found in  #TAUTHOR_TAG.', ""in block's approach, word alignments""]","['a final processing stage, we generalize over the marker lexicon following a process found in  #TAUTHOR_TAG.', ""in block's approach, word alignments are assigned probabilities by means of a statistical word alignment tool."", 'in a subsequent stage, chunk pairs are']","['a final processing stage, we generalize over the marker lexicon following a process found in  #TAUTHOR_TAG.', ""in block's approach, word alignments are assigned probabilities by means of a statistical word alignment tool."", 'in a subsequent stage, chunk pairs are extracted, which are then generalized to produce a set of translation templates for each source, target segment']",5
['generalized further using a methodology based on  #TAUTHOR_TAG'],['generalized further using a methodology based on  #TAUTHOR_TAG'],"['onto a japanese noun - case marker segment, once one has identified the sets of marker tags in the languages to be translated.', 'following construction of the marker lexicon, the ( source, target ) chunks are generalized further using a methodology based on  #TAUTHOR_TAG']","['language learning ( green 1979 ; mori and moeser 1983 ; morgan, meier, and newport 1989 ) • monolingual grammar induction ( juola 1998 ) • grammar optimization ( juola 1994 ) • insights into universal grammar ( juola 1998 ) • machine translation ( juola 1994  #AUTHOR_TAG veale and way 1997 ; gough, way, and hearne 2002 ) with respect to translation, a potential problem in using the marker hypothesis is that some languages do not have marker words such as articles, for instance.', ' #AUTHOR_TAG work showed that artificial languages, both with and without specific marker words, may be learned more accurately and quickly if such psycholinguistic cues exist.', 'the research of  #AUTHOR_TAG showed a similar effect due to case marking on pseudowords in such artificial languages, and  #AUTHOR_TAG demonstrated that languages that do not permit pronouns as substitutes for phrases also provide evidence in favor of the marker hypothesis.', ' #AUTHOR_TAG juola\'s (, 1998 work on grammar optimization and induction shows that context - free grammars can be converted to "" marker - normal form. ""', 'however, marker - normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto - one mapping between a terminal symbol and a word.', ' #AUTHOR_TAG, page 23 ) observes that "" a slightly more general mapping, where two adjacent terminal symbols can be merged into a single lexical item ( for example, a word and its case - marking ), can capture this sort of result quite handily. ""', 'work using the marker hypothesis for mt adapts this monolingual mapping for pairs of languages : it is reasonably straightforward to map an english determiner - noun sequence onto a japanese noun - case marker segment, once one has identified the sets of marker tags in the languages to be translated.', 'following construction of the marker lexicon, the ( source, target ) chunks are generalized further using a methodology based on  #TAUTHOR_TAG to permit a limited form of insertion in the translation process.', 'as a byproduct of the chosen methodology, we also derive a standard "" word - level "" translation lexicon.', 'these various resources render the set of original translation pairs far more useful in deriving translations of previously unseen input']",5
"['.', 'in their gaijin system,  #TAUTHOR_TAG give a result of 63']","['when tested against the training corpus.', 'on novel test sentences, he gives results of 72 % correct translation.', 'in their gaijin system,  #TAUTHOR_TAG give a result of 63']","['when tested against the training corpus.', 'on novel test sentences, he gives results of 72 % correct translation.', 'in their gaijin system,  #TAUTHOR_TAG give a result of 63 % accurate translations obtained']","['that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks.', ' #AUTHOR_TAG juola (, 1997 conducts some small experiments using his metla system to show the viability of this approach for english −→ french and english −→ urdu.', 'for the english −→ french language pair, juola gives results of 61 % correct translation when the system is tested on the training corpus, and 36 % accuracy when it is evaluated with test data.', 'for english −→  #AUTHOR_TAG, page 213 ) notes that "" the system learned the original training corpus...', 'perfectly and could reproduce it without errors "" ; that is, it scored 100 % accuracy when tested against the training corpus.', 'on novel test sentences, he gives results of 72 % correct translation.', 'in their gaijin system,  #TAUTHOR_TAG give a result of 63 % accurate translations obtained for english a > german on a test set of 791 sentences from coreldraw manuals']",1
"[';  #TAUTHOR_TAG ; morgan, meier, and newport 1989 ) a¢ monolingual grammar induction ( juola 1998 ) a¢ grammar optimization ( juola 1994 ) a¢ insights into universal grammar ( juola 1998 ) a¢ machine translation ( juola 1994, 1997 ; veale']","[';  #TAUTHOR_TAG ; morgan, meier, and newport 1989 ) a¢ monolingual grammar induction ( juola 1998 ) a¢ grammar optimization ( juola 1994 ) a¢ insights into universal grammar ( juola 1998 ) a¢ machine translation ( juola 1994, 1997 ; veale']","[';  #TAUTHOR_TAG ; morgan, meier, and newport 1989 ) a¢ monolingual grammar induction ( juola 1998 ) a¢ grammar optimization ( juola 1994 ) a¢ insights into universal grammar ( juola 1998 ) a¢ machine translation ( juola 1994, 1997 ; veale']","['##¢ language learning ( green 1979 ;  #TAUTHOR_TAG ; morgan, meier, and newport 1989 ) a¢ monolingual grammar induction ( juola 1998 ) a¢ grammar optimization ( juola 1994 ) a¢ insights into universal grammar ( juola 1998 ) a¢ machine translation ( juola 1994, 1997 ; veale and way 1997 ; gough, way, and hearne 2002']",0
"['; morgan, meier, and newport 1989 ) a¢ monolingual grammar induction  #TAUTHOR_TAG a¢ grammar optimization ( juola 1994 ) a¢ insights into universal grammar ( juola 1998 ) a¢ machine translation ( juola 1994, 1997 ; veale']","['; morgan, meier, and newport 1989 ) a¢ monolingual grammar induction  #TAUTHOR_TAG a¢ grammar optimization ( juola 1994 ) a¢ insights into universal grammar ( juola 1998 ) a¢ machine translation ( juola 1994, 1997 ; veale']","['##r 1983 ; morgan, meier, and newport 1989 ) a¢ monolingual grammar induction  #TAUTHOR_TAG a¢ grammar optimization ( juola 1994 ) a¢ insights into universal grammar ( juola 1998 ) a¢ machine translation ( juola 1994, 1997 ; veale']","['##¢ language learning ( green 1979 ; mori and moeser 1983 ; morgan, meier, and newport 1989 ) a¢ monolingual grammar induction  #TAUTHOR_TAG a¢ grammar optimization ( juola 1994 ) a¢ insights into universal grammar ( juola 1998 ) a¢ machine translation ( juola 1994, 1997 ; veale and way 1997 ; gough, way, and hearne 2002']",0
"['with test data.', 'for english a > urdu,  #TAUTHOR_TAG, page 213 ) notes that "" the system learned the original training corpus']","['with test data.', 'for english a > urdu,  #TAUTHOR_TAG, page 213 ) notes that "" the system learned the original training corpus']","['with test data.', 'for english a > urdu,  #TAUTHOR_TAG, page 213 ) notes that "" the system learned the original training corpus']","['that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of _ source, target _ chunks.', ' #AUTHOR_TAG, 1997 ) conducts some small experiments using his metla system to show the viability of this approach for english _ _ french and english _ _ urdu.', 'for the english _ _ french language pair, juola gives results of 61 % correct translation when the system is tested on the training corpus, and 36 % accuracy when it is evaluated with test data.', 'for english a > urdu,  #TAUTHOR_TAG, page 213 ) notes that "" the system learned the original training corpus... perfectly and could reproduce it without errors ; that is, it scored 100 % accuracy when tested against the training corpus.', 'on novel test sentences, he gives results of 72 % correct translation.', 'in their gaijin system,  #AUTHOR_TAG give a result of 63 % accurate translations obtained for english _ _ german on a test set of 791 sentences from coreldraw manuals']",0
"['cicekli and g [UNK] uvenir ( 1996 ), mc  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG, inter alia']","['cicekli and g [UNK] uvenir ( 1996 ), mc  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG, inter alia']","['cicekli and g [UNK] uvenir ( 1996 ), mc  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG, inter alia']","['distinguishes chunks from "" patterns, "" as we do : his chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', 'once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm : "" for each pair of chunk pairs using the algorithm described above, the patterns in ( 26 ) are derived from the chunks in ( 25 ) : of course, many other researchers also try to extract generalized templates.', ' #AUTHOR_TAG identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', ' #AUTHOR_TAG combines lexical and dependency mappings to form his generalizations.', 'other similar approaches include those of cicekli and g [UNK] uvenir ( 1996 ), mc  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG, inter alia']",0
"[', following a methodology based on  #TAUTHOR_TAG, to']","[', following a methodology based on  #TAUTHOR_TAG, to']","[', following a methodology based on  #TAUTHOR_TAG, to']","['this section, we describe how the memory of our ebmt system is seeded with a set of translations obtained from web - based mt systems.', 'from this initial resource, we subsequently derive a number of different databases that together allow many new input sentences to be translated that it would not be possible to translate in other systems.', 'first, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon.', ""this is then generalized, following a methodology based on  #TAUTHOR_TAG, to generate the ` ` generalized marker lexicon.''"", 'finally, as a result of the we refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http : / / www. up. univ - mrs. fr / [UNK] / biblios / ptp. htm.', 'methodology chosen, we automatically derive a fourth resource, namely, a "" word - level lexicon.']",5
"['; milosavljevic, tulloch, and dale 1996 ) a¢ speech generation  #TAUTHOR_TAG a¢ localization ( sch [UNK] aler 1996']","['; milosavljevic, tulloch, and dale 1996 ) a¢ speech generation  #TAUTHOR_TAG a¢ localization ( sch [UNK] aler 1996']","['##r 1987 ) a¢ text generation ( hovy 1988 ; milosavljevic, tulloch, and dale 1996 ) a¢ speech generation  #TAUTHOR_TAG a¢ localization ( sch [UNK] aler 1996']","['##¢ learnability ( zernik and dyer 1987 ) a¢ text generation ( hovy 1988 ; milosavljevic, tulloch, and dale 1996 ) a¢ speech generation  #TAUTHOR_TAG a¢ localization ( sch [UNK] aler 1996']",0
"[""'  #TAUTHOR_TAG is used to segment""]","[""marker hypothesis''  #TAUTHOR_TAG is used to segment""]","[""'  #TAUTHOR_TAG is used to segment the phrasal lexicon into a ` ` marker lexicon.''"", 'the marker hypothesis is a universal psycholinguistic constraint which states that natural languages are']","[""set of translations is stored separately, and for each set the ` ` marker hypothesis''  #TAUTHOR_TAG is used to segment the phrasal lexicon into a ` ` marker lexicon.''"", 'the marker hypothesis is a universal psycholinguistic constraint which states that natural languages are "" marked "" for complex syntactic structure at surface form by a closed set of specific lexemes and morphemes.', 'that is, a basic phrase - level segmentation of an input sentence can be achieved by exploiting a closed list of known marker words to signal the start and end of each segment']",5
"['is, where  #TAUTHOR_TAG substitutes variables for various words in his']","['is, where  #TAUTHOR_TAG substitutes variables for various words in his templates, we replace certain lexical items with']","['is, where  #TAUTHOR_TAG substitutes variables for various words in his']","['is, where  #TAUTHOR_TAG substitutes variables for various words in his templates, we replace certain lexical items with their marker tag.', 'given that examples such as < det > a : un are likely to exist in the word - level lexicon, they may be inserted at the point indicated by the marker tag to form the correct translation un bon homme.', 'we thus cluster on marker words to improve the coverage of our system ( see section 5 for results that show exactly how clustering on marker words helps ) ; others ( notably brown [ 2000, 2003 ] ) use clustering techniques to determine equivalence classes of individual words that can occur in the same context, and in so doing derive translation templates from individual translation examples']",1
"['.', 'nevertheless,  #TAUTHOR_TAG, page 23 ) observes that']","['terminal symbol and a word.', 'nevertheless,  #TAUTHOR_TAG, page 23 ) observes that ` ` a slightly more general mapping, where two adjacent terminal symbols']","['do not have a oneto - one mapping between a terminal symbol and a word.', 'nevertheless,  #TAUTHOR_TAG, page 23 ) observes that']","['language learning ( green 1979 ; mori and moeser 1983 ; morgan, meier, and newport 1989 ) • monolingual grammar induction ( juola 1998 ) • grammar optimization ( juola 1994 ) • insights into universal grammar ( juola 1998 ) • machine translation ( juola 1994  #AUTHOR_TAG veale and way 1997 ; gough, way, and hearne 2002 ) with respect to translation, a potential problem in using the marker hypothesis is that some languages do not have marker words such as articles, for instance.', ' #AUTHOR_TAG work showed that artificial languages, both with and without specific marker words, may be learned more accurately and quickly if such psycholinguistic cues exist.', 'the research of  #AUTHOR_TAG showed a similar effect due to case marking on pseudowords in such artificial languages, and  #AUTHOR_TAG demonstrated that languages that do not permit pronouns as substitutes for phrases also provide evidence in favor of the marker hypothesis.', ' #AUTHOR_TAG juola\'s (, 1998 work on grammar optimization and induction shows that context - free grammars can be converted to "" marker - normal form. ""', 'however, marker - normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto - one mapping between a terminal symbol and a word.', ""nevertheless,  #TAUTHOR_TAG, page 23 ) observes that ` ` a slightly more general mapping, where two adjacent terminal symbols can be merged into a single lexical item ( for example, a word and its case - marking ), can capture this sort of result quite handily.''"", 'work using the marker hypothesis for mt adapts this monolingual mapping for pairs of languages : it is reasonably straightforward to map an english determiner - noun sequence onto a japanese noun - case marker segment, once one has identified the sets of marker tags in the languages to be translated.', 'following construction of the marker lexicon, the source, target chunks are generalized further using a methodology based on  #AUTHOR_TAG to permit a limited form of insertion in the translation process.', 'as a byproduct of the chosen methodology, we also derive a standard "" word - level "" translation lexicon.', 'these various resources render the set of original translation pairs far more useful in deriving translations of previously unseen input']",0
"['cicekli and g [UNK] uvenir ( 1996 ), mc  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG, inter alia']","['cicekli and g [UNK] uvenir ( 1996 ), mc  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG, inter alia']","['cicekli and g [UNK] uvenir ( 1996 ), mc  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG, inter alia']","['distinguishes chunks from "" patterns, "" as we do : his chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', 'once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm : "" for each pair of chunk pairs using the algorithm described above, the patterns in ( 26 ) are derived from the chunks in ( 25 ) : of course, many other researchers also try to extract generalized templates.', ' #AUTHOR_TAG identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', ' #AUTHOR_TAG combines lexical and dependency mappings to form his generalizations.', 'other similar approaches include those of cicekli and g [UNK] uvenir ( 1996 ), mc  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG, inter alia']",0
"['generate lexical correspondences by means of co - occurrence measures and string similarity metrics.', 'more specifically, the notion of the phrasal lexicon ( used first by  #TAUTHOR_TAG has been used successfully in a number of areas']","['generate lexical correspondences by means of co - occurrence measures and string similarity metrics.', 'more specifically, the notion of the phrasal lexicon ( used first by  #TAUTHOR_TAG has been used successfully in a number of areas']","['of word occurrence and co - occurrence probabilities.', 'the respective lengths of the putative alignments in terms of characters is also an important factor.', ' #AUTHOR_TAG observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'accordingly, they generate lexical correspondences by means of co - occurrence measures and string similarity metrics.', 'more specifically, the notion of the phrasal lexicon ( used first by  #TAUTHOR_TAG has been used successfully in a number of areas']","['ebmt systems, from the initial proposal by  #AUTHOR_TAG to the recent collection of  #AUTHOR_TAG, are premised on the availability of subsentential alignments derived from the input bitext.', 'there is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', 'kay and roscheisen ( 1993 ) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'fung and mc  #AUTHOR_TAG attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', ' #AUTHOR_TAG replicates the work of fung and mckeown with different language pairs using the simpler metric of levenshtein distance.', ' #AUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent english - greek clauses on the basis of word occurrence and co - occurrence probabilities.', 'the respective lengths of the putative alignments in terms of characters is also an important factor.', ' #AUTHOR_TAG observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'accordingly, they generate lexical correspondences by means of co - occurrence measures and string similarity metrics.', 'more specifically, the notion of the phrasal lexicon ( used first by  #TAUTHOR_TAG has been used successfully in a number of areas']",0
['initial proposal by  #TAUTHOR_TAG to'],['initial proposal by  #TAUTHOR_TAG to'],"['ebmt systems, from the initial proposal by  #TAUTHOR_TAG to the recent collection of  #AUTHOR_TAG,']","['ebmt systems, from the initial proposal by  #TAUTHOR_TAG to the recent collection of  #AUTHOR_TAG, are premised on the availability of subsentential alignments derived from the input bitext.', 'there is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', 'kay and roscheisen ( 1993 ) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'fung and mc  #AUTHOR_TAG attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', ' #AUTHOR_TAG replicates the work of fung and mckeown with different language pairs using the simpler metric of levenshtein distance.', ' #AUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent english - greek clauses on the basis of word occurrence and co - occurrence probabilities.', 'the respective lengths of the putative alignments in terms of characters is also an important factor.', ' #AUTHOR_TAG observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'accordingly, they generate lexical correspondences by means of co - occurrence measures and string similarity metrics.', 'more specifically, the notion of the phrasal lexicon ( used first by becker 1975 ) has been used successfully in a number of areas']",0
"['', ' #TAUTHOR_TAG combines lexical and dependency mappings to']","['set of translation patterns.', ' #TAUTHOR_TAG combines lexical and dependency mappings to']","['a set of translation patterns.', ' #TAUTHOR_TAG combines lexical and dependency mappings to']","['distinguishes chunks from "" patterns, "" as we do : his chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', 'once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm : "" for each pair of chunk pairs using the algorithm described above, the patterns in ( 26 ) are derived from the chunks in ( 25 ) : of course, many other researchers also try to extract generalized templates.', ' #AUTHOR_TAG identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', ' #TAUTHOR_TAG combines lexical and dependency mappings to form his generalizations.', 'other similar approaches include those of cicekli and guvenir ( 1996 ), mc  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, inter alia']",0
"['.', 'further details about the properties of entropy can be found in textbooks on information theory ( e. g.,  #TAUTHOR_TAG.', 'determining the parse']","['v is a random variable that can take any possible outcome in set v, and p ( v ) = pr ( v = v ) is the density function.', 'further details about the properties of entropy can be found in textbooks on information theory ( e. g.,  #TAUTHOR_TAG.', 'determining the parse']","['v is a random variable that can take any possible outcome in set v, and p ( v ) = pr ( v = v ) is the density function.', 'further details about the properties of entropy can be found in textbooks on information theory ( e. g.,  #TAUTHOR_TAG.', 'determining the parse tree for a sentence from a set of possible parses can be viewed as assigning a value to a random variable.', 'thus, a direct application of the entropy definition to the probability distribution of the parses for sentence w in g computes']","['v is a random variable that can take any possible outcome in set v, and p ( v ) = pr ( v = v ) is the density function.', 'further details about the properties of entropy can be found in textbooks on information theory ( e. g.,  #TAUTHOR_TAG.', 'determining the parse tree for a sentence from a set of possible parses can be viewed as assigning a value to a random variable.', 'thus, a direct application of the entropy definition to the probability distribution of the parses for sentence w in g computes its tree entropy, te ( w, g ), the expected number of bits needed to encode the distribution of possible parses for w.', 'however, we may not wish to compare two sentences with different numbers of parses by their entropy directly.', 'if the parse probability distributions for both sentences are uniform, the sentence with more parses will have a higher entropy.', 'because longer sentences typically have more parses, using entropy directly would result in a bias toward selecting long sentences.', 'to normalize for the number of parses, the uncertainty - based evaluation function, f unc, is defined as a measurement of similarity between the actual probability distribution of the parses and a hypothetical uniform distribution for that set of parses.', 'in particular, we divide the tree entropy by the log of the number of parses :']",0
['of  #TAUTHOR_TAG and'],['of  #TAUTHOR_TAG and'],"['mitchell 1998 ), in which two sufficiently different learners help each other learn by labeling training data for one another.', 'the work of  #TAUTHOR_TAG and']","['from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing.', 'for example,  #AUTHOR_TAG consider the case in which acquiring additional human - annotated training data is not possible.', 'they show that parser performance can be improved by using boosting and bagging techniques with multiple parsers.', 'this approach assumes that there are enough existing labeled data to train the individual parsers.', 'another technique for making better use of unlabeled data is cotraining ( blum and mitchell 1998 ), in which two sufficiently different learners help each other learn by labeling training data for one another.', 'the work of  #TAUTHOR_TAG and steedman, osborne, et al. ( 2003 ) suggests that co - training can be helpful for statistical parsing.', ' #AUTHOR_TAG have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large - scale training.', 'similar approaches are being explored for parsing hwa et al. 2003 )']",0
"['to pp - attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'some examples include text categorization  #TAUTHOR_TAG, base noun phrase chunk']","['to pp - attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'some examples include text categorization  #TAUTHOR_TAG, base noun phrase chunking ( ngai and']","['to pp - attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'some examples include text categorization  #TAUTHOR_TAG, base noun phrase chunking ( ngai and ya']","['selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'in addition to pp - attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'some examples include text categorization  #TAUTHOR_TAG, base noun phrase chunking ( ngai and yarowsky 2000 ), part - of - speech tagging ( engelson dagan 1996 ), spelling confusion set disambiguation ( banko and brill 2001 ), and word sense disambiguation ( fujii et al. 1998 )']",0
"['##raining  #TAUTHOR_TAG, in which two sufficiently different learners help each other learn by labeling training data for one another.', 'the work of  #AUTHOR_TAG and suggests that co - training can be helpful']","['is cotraining  #TAUTHOR_TAG, in which two sufficiently different learners help each other learn by labeling training data for one another.', 'the work of  #AUTHOR_TAG and suggests that co - training can be helpful']","['making better use of unlabeled data is cotraining  #TAUTHOR_TAG, in which two sufficiently different learners help each other learn by labeling training data for one another.', 'the work of  #AUTHOR_TAG and suggests that co - training can be helpful']","['from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing.', 'for example,  #AUTHOR_TAG consider the case in which acquiring additional human - annotated training data is not possible.', 'they show that parser performance can be improved by using boosting and bagging techniques with multiple parsers.', 'this approach assumes that there are enough existing labeled data to train the individual parsers.', 'another technique for making better use of unlabeled data is cotraining  #TAUTHOR_TAG, in which two sufficiently different learners help each other learn by labeling training data for one another.', 'the work of  #AUTHOR_TAG and suggests that co - training can be helpful for statistical parsing.', ' #AUTHOR_TAG have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large - scale training.', 'similar approaches are being explored for parsing hwa et al. 2003 )']",0
"[') formalism  #TAUTHOR_TAG ; hwa 1998 ), and']","['pltig ) formalism  #TAUTHOR_TAG ; hwa 1998 ), and']","[') formalism  #TAUTHOR_TAG ; hwa 1998 ), and']","[""applying sample selection to training a pp - attachment model, we have observed that all effective evaluation functions make use of the model's current hypothesis in estimating the training utility of the candidates."", 'although knowledge about the problem space seems to help sharpening the learning curve initially, overall, it is not a good predictor.', 'in this section, we investigate whether these observations hold true for training statistical parsing models as well.', ""moreover, in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain, we have performed the study on two parsing models : one based on a context - free variant of tree - adjoining grammars ( joshi, levy, and takahashi 1975 ), the probabilistic lexicalized tree insertion grammar ( pltig ) formalism  #TAUTHOR_TAG ; hwa 1998 ), and collins's model 2 parser ( 1997 )."", 'although both models are lexicalized, statistical parsers, their learning algorithms are different.', 'the collins parser is a fully supervised, history - based learner that models the parameters of the parser by taking statistics directly from the training data.', ""in contrast, pltig's expectation - maximization - based induction algorithm is partially supervised ; the model's parameters are estimated indirectly from the training data""]",5
"['- off models  #TAUTHOR_TAG, and a maximumentropy model ( ratnaparkhi 1998 ).', 'following the tradition of using learning ppattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the']","['backed - off models  #TAUTHOR_TAG, and a maximumentropy model ( ratnaparkhi 1998 ).', 'following the tradition of using learning ppattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the']","['- off models  #TAUTHOR_TAG, and a maximumentropy model ( ratnaparkhi 1998 ).', 'following the tradition of using learning ppattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a pp - attachment model.', 'we use the collins - brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier']","['common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it.', 'researchers have proposed many computational models for resolving ppattachment ambiguities.', 'some well - known approaches include rule - based models ( brill and resnik 1994 ), backed - off models  #TAUTHOR_TAG, and a maximumentropy model ( ratnaparkhi 1998 ).', 'following the tradition of using learning ppattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a pp - attachment model.', 'we use the collins - brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier']",0
"['##ing confusion set disambiguation  #TAUTHOR_TAG, and word sense disambiguation ( fujii et al. 1998 )']","['to pp - attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'some examples include text categorization ( lewis and catlett 1994 ), base noun phrase chunking ( ngai and yarowsky 2000 ), part - of - speech tagging ( engelson dagan 1996 ), spelling confusion set disambiguation  #TAUTHOR_TAG, and word sense disambiguation ( fujii et al. 1998 )']","['##ing confusion set disambiguation  #TAUTHOR_TAG, and word sense disambiguation ( fujii et al. 1998 )']","['selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'in addition to pp - attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'some examples include text categorization ( lewis and catlett 1994 ), base noun phrase chunking ( ngai and yarowsky 2000 ), part - of - speech tagging ( engelson dagan 1996 ), spelling confusion set disambiguation  #TAUTHOR_TAG, and word sense disambiguation ( fujii et al. 1998 )']",0
"['inside probabilities  #TAUTHOR_TAG, we can efficiently']","['inside probabilities  #TAUTHOR_TAG, we can efficiently']","['the bottom - up, dynamic programming technique ( see the appendix for details ) of computing inside probabilities  #TAUTHOR_TAG, we can efficiently']","['the bottom - up, dynamic programming technique ( see the appendix for details ) of computing inside probabilities  #TAUTHOR_TAG, we can efficiently compute the probability of the sentence, p ( w | g ).', 'similarly, the algorithm can be modified to compute the']",5
['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],"['', 'would be labeled as "" ( ( several fund managers ) ( expect ( ( a rough market ) ( this morning ) ) ( before ( prices stabilize ) ) ). ) ""', 'our algorithm is similar to the approach taken by  #TAUTHOR_TAG for inducing pcfg parsers']",1
"[' #TAUTHOR_TAG.', 'current']","[' #TAUTHOR_TAG.', 'current state - of - the - art statistical parsers ( collins 1999 ; charniak 2000 )']","['is provided as a part of the training data  #TAUTHOR_TAG.', 'current state - of - the - art statistical parsers']","['learning tasks for natural language processing require supervised training ; that is, the system successfully learns a concept only if it has been given annotated training data.', 'for example, while it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data  #TAUTHOR_TAG.', 'current state - of - the - art statistical parsers ( collins 1999 ; charniak 2000 ) are all trained on large annotated corpora such as the penn treebank ( marcus, santorini, and marcinkiewicz 1993 ).', 'however, supervised training data are difficult to obtain ; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest.', 'for example, one might need lexical - semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora.', 'because supervised training demands significant human involvement ( e. g., annotating the syntactic structure of each sentence by hand ), creating a new corpus is a labor - intensive and time - consuming endeavor.', ""the goal of this work is to minimize a system's reliance on annotated training data""]",0
"['to pp - attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'some examples include text categorization ( lewis and catlett 1994 ), base noun phrase chunking  #TAUTHOR_TAG, part -']","['to pp - attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'some examples include text categorization ( lewis and catlett 1994 ), base noun phrase chunking  #TAUTHOR_TAG, part - of - speech tagging ( engelson dagan 1996 ), spelling confusion set']","['to pp - attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'some examples include text categorization ( lewis and catlett 1994 ), base noun phrase chunking  #TAUTHOR_TAG, part -']","['selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'in addition to pp - attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'some examples include text categorization ( lewis and catlett 1994 ), base noun phrase chunking  #TAUTHOR_TAG, part - of - speech tagging ( engelson dagan 1996 ), spelling confusion set disambiguation ( banko and brill 2001 ), and word sense disambiguation ( fujii et al. 1998 )']",0
"[';  #TAUTHOR_TAG, and']","[';  #TAUTHOR_TAG, and']","['waters 1993 ;  #TAUTHOR_TAG, and']","[""applying sample selection to training a pp - attachment model, we have observed that all effective evaluation functions make use of the model's current hypothesis in estimating the training utility of the candidates."", 'although knowledge about the problem space seems to help sharpening the learning curve initially, overall, it is not a good predictor.', 'in this section, we investigate whether these observations hold true for training statistical parsing models as well.', ""moreover, in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain, we have performed the study on two parsing models : one based on a context - free variant of tree - adjoining grammars ( joshi, levy, and takahashi 1975 ), the probabilistic lexicalized tree insertion grammar ( pltig ) formalism ( schabes and waters 1993 ;  #TAUTHOR_TAG, and collins's model 2 parser ( 1997 )."", 'although both models are lexicalized, statistical parsers, their learning algorithms are different.', 'the collins parser is a fully supervised, history - based learner that models the parameters of the parser by taking statistics directly from the training data.', ""in contrast, pltig's expectation - maximization - based induction algorithm is partially supervised ; the model's parameters are estimated indirectly from the training data""]",5
"['', ' #TAUTHOR_TAG have shown, in the context of base noun identification, that combining sample selection and']","['statistical parsing.', ' #TAUTHOR_TAG have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large - scale training.', 'similar approaches are being explored for parsing hwa et al. 2003 )']","['statistical parsing.', ' #TAUTHOR_TAG have shown, in the context of base noun identification, that combining sample selection and']","['from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing.', 'for example,  #AUTHOR_TAG consider the case in which acquiring additional human - annotated training data is not possible.', 'they show that parser performance can be improved by using boosting and bagging techniques with multiple parsers.', 'this approach assumes that there are enough existing labeled data to train the individual parsers.', 'another technique for making better use of unlabeled data is cotraining ( blum and mitchell 1998 ), in which two sufficiently different learners help each other learn by labeling training data for one another.', 'the work of  #AUTHOR_TAG and suggests that co - training can be helpful for statistical parsing.', ' #TAUTHOR_TAG have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large - scale training.', 'similar approaches are being explored for parsing hwa et al. 2003 )']",0
['statistical parsers  #TAUTHOR_TAG ; charniak'],"['schabes 1992 ).', 'current state - of - the - art statistical parsers  #TAUTHOR_TAG ; charniak']","['schabes 1992 ).', 'current state - of - the - art statistical parsers  #TAUTHOR_TAG ; charniak 2000 )']","['learning tasks for natural language processing require supervised training ; that is, the system successfully learns a concept only if it has been given annotated training data.', 'for example, while it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data ( pereira and schabes 1992 ).', 'current state - of - the - art statistical parsers  #TAUTHOR_TAG ; charniak 2000 ) are all trained on large annotated corpora such as the penn treebank ( marcus, santorini, and marcinkiewicz 1993 ).', 'however, supervised training data are difficult to obtain ; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest.', 'for example, one might need lexical - semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora.', 'because supervised training demands significant human involvement ( e. g., annotating the syntactic structure of each sentence by hand ), creating a new corpus is a labor - intensive and time - consuming endeavor.', ""the goal of this work is to minimize a system's reliance on annotated training data""]",0
"['models  #TAUTHOR_TAG, backed - off models']","['models  #TAUTHOR_TAG, backed - off models']","['based models  #TAUTHOR_TAG, backed - off models']","['common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it.', 'researchers have proposed many computational models for resolving ppattachment ambiguities.', 'some well - known approaches include rule - based models  #TAUTHOR_TAG, backed - off models ( collins and brooks 1995 ), and a maximumentropy model ( ratnaparkhi 1998 ).', 'following the tradition of using learning ppattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a pp - attachment model.', 'we use the collins - brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier']",0
"['the first experiment, we use an induction algorithm  #TAUTHOR_TAG a ) based on the']","['the first experiment, we use an induction algorithm  #TAUTHOR_TAG a ) based on the expectation - maximization ( em ) principle that induces parsers for pltigs.', '']","['the first experiment, we use an induction algorithm  #TAUTHOR_TAG a ) based on the expectation - maximization ( em ) principle that induces parsers for pltigs.', 'the algorithm performs heuristic search through an iterative reestimation procedure']","['the first experiment, we use an induction algorithm  #TAUTHOR_TAG a ) based on the expectation - maximization ( em ) principle that induces parsers for pltigs.', ""the algorithm performs heuristic search through an iterative reestimation procedure to find local optima : sets of values for the grammar parameters that maximizes the grammar's likelihood of generating the training data."", 'in principle, the algorithm supports unsupervised learning ; however, because the search space has too many local optima, the algorithm tends to converge on a model that is unsuitable for parsing.', 'here, we consider a partially supervised variant in which we assume that the learner is given the phrasal boundaries of the training sentences but not the label of the constituent units.', '']",5
"['current hypothesis is unable to label a candidate or is uncertain about it, then the candidate might be a good training example  #TAUTHOR_TAG.', 'the underlying assumption is that an uncertain']","['current hypothesis is unable to label a candidate or is uncertain about it, then the candidate might be a good training example  #TAUTHOR_TAG.', 'the underlying assumption is that an uncertain']","['of the hypothesis : testing the candidates on the current working hypothesis shows the type of input data on which the hypothesis may perform weakly.', 'that is, if the current hypothesis is unable to label a candidate or is uncertain about it, then the candidate might be a good training example  #TAUTHOR_TAG.', 'the underlying assumption is that an uncertain output is likely to be wrong']","['of the hypothesis : testing the candidates on the current working hypothesis shows the type of input data on which the hypothesis may perform weakly.', 'that is, if the current hypothesis is unable to label a candidate or is uncertain about it, then the candidate might be a good training example  #TAUTHOR_TAG.', 'the underlying assumption is that an uncertain output is likely to be wrong']",0
"[',  #TAUTHOR_TAG ; hwa et']","[',  #TAUTHOR_TAG ; hwa et al. 2003 )']","[',  #TAUTHOR_TAG ; hwa et']","['from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing.', 'for example,  #AUTHOR_TAG consider the case in which acquiring additional human - annotated training data is not possible.', 'they show that parser performance can be improved by using boosting and bagging techniques with multiple parsers.', 'this approach assumes that there are enough existing labeled data to train the individual parsers.', 'another technique for making better use of unlabeled data is cotraining ( blum and mitchell 1998 ), in which two sufficiently different learners help each other learn by labeling training data for one another.', 'the work of  #AUTHOR_TAG and suggests that co - training can be helpful for statistical parsing.', ' #AUTHOR_TAG have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large - scale training.', 'similar approaches are being explored for parsing ( steedman,  #TAUTHOR_TAG ; hwa et al. 2003 )']",0
"['described by  #TAUTHOR_TAG.', 'for this learning problem, the supervision is the one - bit information of']","['described by  #TAUTHOR_TAG.', 'for this learning problem, the supervision is the one - bit information of']","['described by  #TAUTHOR_TAG.', 'for this learning problem, the supervision is the one - bit information of']","['collins - brooks pp - attachment classification algorithm.', 'preposition, and the prepositional noun phrase, respectively, and a specifies the attachment classification.', 'for example, ( wrote a book in three days, attach - verb ) would be annotated as ( wrote, book, in, days, verb ).', 'the head words can be automatically extracted using a heuristic table lookup in the manner described by  #TAUTHOR_TAG.', 'for this learning problem, the supervision is the one - bit information of whether p should attach to v or to n.', 'in order to learn the attachment preferences of prepositional phrases, the system builds attachment statistics for each the characteristic tuple of all training examples.', 'a characteristic tuple is some subset of the four head words in the example, with the condition that one of the elements must be the preposition.', 'each training example forms eight characteristic tuples']",5
"['model  #TAUTHOR_TAG.', 'following the tradition of using learning ppattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the']","['), and a maximumentropy model  #TAUTHOR_TAG.', 'following the tradition of using learning ppattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the']","['and brooks 1995 ), and a maximumentropy model  #TAUTHOR_TAG.', 'following the tradition of using learning ppattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a pp - attachment model.', 'we use the collins - brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier']","['common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it.', 'researchers have proposed many computational models for resolving ppattachment ambiguities.', 'some well - known approaches include rule - based models ( brill and resnik 1994 ), backed - off models ( collins and brooks 1995 ), and a maximumentropy model  #TAUTHOR_TAG.', 'following the tradition of using learning ppattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a pp - attachment model.', 'we use the collins - brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier']",0
"['inside probabilities ).', 'we follow the notation convention of  #TAUTHOR_TAG']","['computing the probability of the entire sentence from the probabilities of substrings ( called inside probabilities ).', 'we follow the notation convention of  #TAUTHOR_TAG']","['inside probabilities ).', 'we follow the notation convention of  #TAUTHOR_TAG']","['illustrative purposes, we describe the computation process using a pcfg expressed in chomsky normal form. 14', 'the basic idea is to compose the tree entropy of the entire sentence from the tree entropy of the subtrees.', 'the process is similar to that for computing the probability of the entire sentence from the probabilities of substrings ( called inside probabilities ).', 'we follow the notation convention of  #TAUTHOR_TAG']",5
"['##ner 1994 ;  #TAUTHOR_TAG.', 'for computationally intensive problems, such as parsing, keeping multiple learners may be impractical']","['tuv ( cohn, atlas, and ladner 1994 ;  #TAUTHOR_TAG.', 'for computationally intensive problems, such as parsing, keeping multiple learners may be impractical']","['##ner 1994 ;  #TAUTHOR_TAG.', 'for computationally intensive problems, such as parsing, keeping multiple learners may be impractical']","['traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set.', 'there are two types of selection algorithms : committee - based and single learner.', 'a committee - based selection algorithm works with multiple learners, each maintaining a different hypothesis ( perhaps pertaining to different aspects of the problem ).', 'the candidate examples that lead to the most disagreements among the different learners are considered to have the highest tuv ( cohn, atlas, and ladner 1994 ;  #TAUTHOR_TAG.', 'for computationally intensive problems, such as parsing, keeping multiple learners may be impractical']",0
"['duden  #TAUTHOR_TAG.', 'we will refer to this']","['duden  #TAUTHOR_TAG.', 'we will refer to this']","['and 2, 000 against the duden  #TAUTHOR_TAG.', 'we will refer to this work and the methods and results presented by']","['achieve an f - score of 77 % against the oald when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each.', 'their system recognizes 15 frames, and these do not contain details of subcategorizedfor prepositions.', 'still, to date this is the largest number of verbs used in any of the evaluations of the systems for english described in section 3.  #AUTHOR_TAG evaluate 914 czech verbs against a custom - made gold standard and record a token recall of 88 %.', 'however, their evaluation does not examine the extracted subcategorization frames but rather the argument - adjunct distinctions posited by their system.', 'the largest lexical evaluation we know of is that of schulte im  #AUTHOR_TAG b ) for german.', 'she evaluates 3, 000 german verbs with a token frequency between 10 and 2, 000 against the duden  #TAUTHOR_TAG.', 'we will refer to this work and the methods and results presented by schulte im walde again in sections 6. 2 and 6. 3']",0
"['.', ' #TAUTHOR_TAG report on manually analyzing an open - class']","['recall.', ' #TAUTHOR_TAG report on manually analyzing an open - class']","['', ' #TAUTHOR_TAG report on manually analyzing an open - class vocabulary of']","['', 'it is also possible that because of human error, comlex contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized - for directional prepositional phrases.', 'this is because the aim of the comlex project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs.', 'lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples.', 'as a generalization,  #AUTHOR_TAG notes that lexicons such as comlex tend to demonstrate high precision but low recall.', ' #TAUTHOR_TAG report on manually analyzing an open - class vocabulary of 35, 000 head words for predicate subcategorization information and comparing the results against the subcategorization details in comlex.', 'precision was quite high ( 95 % ), but recall was low ( 84 % ).', 'this has an effect on both the precision and recall scores of our system against comlex.', 'in order to ascertain the effect of using comlex as a gold standard for our induced lexicon, we carried out some more - detailed error analysis, the results of which are summarized in table 26.', 'we randomly selected 80 false negatives ( fn ) and 80 false positives ( fp ) across a range of active frame types containing prepositional and particle detail taken from penn - iii and manually examined them in order to classify them as "" correct "" or "" incorrect. ""', 'of the 80 fps, 33 were manually judged to be legitimate subcategorization frames.', 'for example, as table 26 shows, there are a number of correct transitive verbs ( [ subj, obj ] ) in our automatically induced lexicon which are not included in comlex.', 'this examination was also useful in highlighting to us the frame types on which']",1
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],0
"[' #TAUTHOR_TAG ],']","[' #TAUTHOR_TAG ],']",[' #TAUTHOR_TAG'],"['modern syntactic theories ( e. g., lexical - functional grammar [ lfg ] [ kaplan and bresnan 1982 ; bresnan 2001 ; dalrymple 2001 ], head - driven phrase structure grammar [ hpsg ] [ pollard and sag 1994 ], tree - adjoining grammar [ tag ] [  #TAUTHOR_TAG ], and combinatory categorial grammar [ ccg ] [ ades and steedman 1982 ] ), the lexicon is the central repository for much morphological, syntactic, and semantic information']",0
"['categories ),  #TAUTHOR_TAG questions the viability and universality of such an']","['of phrase structure ( cfg categories ),  #TAUTHOR_TAG questions the viability and universality of such an']","['of phrase structure ( cfg categories ),  #TAUTHOR_TAG questions the viability and universality of such an approach']","['many linguistic theories state subcategorization requirements in terms of phrase structure ( cfg categories ),  #TAUTHOR_TAG questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language - specific constituent structure level.', 'lfg argues that subcategorization requirements are best stated at the f - structure level, in functional rather than phrasal terms.', 'this is because of the assumption that abstract grammatical functions are primitive concepts as opposed to derivatives of phrase structural position.', 'in lfg, the subcategorization requirements of a particular predicate are expressed by its semantic form : focus ( ↑ subj ) ( ↑ obl on ) in figure 1']",0
"['the extracted frames.', ' #TAUTHOR_TAG attempts to improve on']","['the extracted frames.', ' #TAUTHOR_TAG attempts to improve on']","['the extracted frames.', ' #TAUTHOR_TAG attempts to improve on']","['will divide more - general approaches to subcategorization frame acquisition into two groups : those which extract information from raw text and those which use preparsed and hand - corrected treebank data as their input.', 'typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', ' #AUTHOR_TAG relies on morphosyntactic cues in the untagged brown corpus as indicators of six predefined subcategorization frames.', 'the frames do not include details of specific prepositions.', 'brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', ' #AUTHOR_TAG run a finite - state np parser on a pos - tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'the experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', "" #AUTHOR_TAG employ an additional statistical method based on log - linear models and bayes'theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", ' #TAUTHOR_TAG attempts to improve on the approach of  #AUTHOR_TAG by passing raw text through a stochastic tagger and a finite - state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co - occur.', 'he assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'the extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( bht ), following  #AUTHOR_TAG.', 'applying his technique to approximately four million words of new york times newswire, manning acquired 4, 900 verb - subcategorization frame pairs for 3, 104 verbs, an average of 1. 6 frames per verb.', ' #AUTHOR_TAG predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the comlex ( macleod, grishman, and meyers 1994 ) and anlt ( boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection.', 'the frames incorporate control information and details of specific prepositions.', ' #AUTHOR_TAG refine the bht with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'recent work by  #AUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on levin [ 1993 ] ) for obtaining more accurate back - off estimates for hypothesis selection']",0
"['from penn - iii.', ' #TAUTHOR_TAG, by comparison, employ 163 distinct predefined frames']","['from penn - iii.', ' #TAUTHOR_TAG, by comparison, employ 163 distinct predefined frames']","['from penn - iii.', ' #TAUTHOR_TAG, by comparison, employ 163 distinct predefined frames']","['an absolute threshold of five occurrences, we still generate 162 frame types from penn - ii and 221 from penn - iii.', ' #TAUTHOR_TAG, by comparison, employ 163 distinct predefined frames']",0
"['to interpret ldds.', 'unlike our approach, those of  #TAUTHOR_TAG and hockenmaier, bierner, and  #AUTHOR_TAG include a substantial initial']","['to interpret ldds.', 'unlike our approach, those of  #TAUTHOR_TAG and hockenmaier, bierner, and  #AUTHOR_TAG include a substantial initial']","['to interpret ldds.', 'unlike our approach, those of  #TAUTHOR_TAG and hockenmaier, bierner, and  #AUTHOR_TAG include a substantial initial correction']","['', 'each node is subsequently assigned the relevant category based on its constituent type and surface configuration.', 'the algorithm handles "" like "" coordination and exploits the traces used in the treebank in order to interpret ldds.', 'unlike our approach, those of  #TAUTHOR_TAG and hockenmaier, bierner, and  #AUTHOR_TAG include a substantial initial correction and clean - up of the penn - ii trees.', ' #AUTHOR_TAG and  #AUTHOR_TAG describe a methodology for acquiring an english hpsg from the penn - ii treebank.', 'manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified hpsg derivation trees : head / argument / modifier distinctions are made for each node in the tree']",1
['hpsg  #TAUTHOR_TAG ; xia 1999 ; hoc'],"['hpsg  #TAUTHOR_TAG ; xia 1999 ; hockenmaier, bierner,']",['hpsg  #TAUTHOR_TAG ; xia 1999 ; hoc'],"['from the extraction of theory - neutral subcategorization lexicons, there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as ltag, ccg, and hpsg  #TAUTHOR_TAG ; xia 1999 ; hockenmaier, bierner, and baldridge 2004 ; nakanishi, miyao, and tsujii 2004 ).', 'in this article we present an approach to automating the process of lexical acquisition for lfg ( i. e., grammatical - function - based systems ).', '']",0
"['per verb.', ' #TAUTHOR_TAG predefine 163 verbal subc']","['per verb.', ' #TAUTHOR_TAG predefine 163 verbal']","['. 6 frames per verb.', ' #TAUTHOR_TAG predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the comlex ( macleod, grishman,']","['will divide more - general approaches to subcategorization frame acquisition into two groups : those which extract information from raw text and those which use preparsed and hand - corrected treebank data as their input.', 'typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', ' #AUTHOR_TAG relies on morphosyntactic cues in the untagged brown corpus as indicators of six predefined subcategorization frames.', 'the frames do not include details of specific prepositions.', 'brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', ' #AUTHOR_TAG run a finite - state np parser on a pos - tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'the experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', "" #AUTHOR_TAG employ an additional statistical method based on log - linear models and bayes'theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", ' #AUTHOR_TAG attempts to improve on the approach of  #AUTHOR_TAG by passing raw text through a stochastic tagger and a finite - state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co - occur.', 'he assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'the extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( bht ), following  #AUTHOR_TAG.', 'applying his technique to approximately four million words of new york times newswire, manning acquired 4, 900 verb - subcategorization frame pairs for 3, 104 verbs, an average of 1. 6 frames per verb.', ' #TAUTHOR_TAG predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the comlex ( macleod, grishman, and meyers 1994 ) and anlt ( boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection.', 'the frames incorporate control information and details of specific prepositions.', ' #AUTHOR_TAG refine the bht with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'recent work by  #AUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on levin [ 1993 ] ) for obtaining more accurate back - off estimates for hypothesis selection']",0
"['tiger treebank  #TAUTHOR_TAG and penn chinese treebank ( xue,']","['tiger treebank  #TAUTHOR_TAG and penn chinese treebank ( xue, chiou, and palmer 2002 ), extracting wide - coverage, probabilistic lfg grammar approximations and lexical resources for german ( cahill']","['have also applied our more general unification grammar acquisition methodology to the tiger treebank  #TAUTHOR_TAG and penn chinese treebank ( xue, chiou, and palmer 2002 ), extracting wide - coverage, probabilistic lfg grammar approximations and lexical resources for german ( cahill']","['have also applied our more general unification grammar acquisition methodology to the tiger treebank  #TAUTHOR_TAG and penn chinese treebank ( xue, chiou, and palmer 2002 ), extracting wide - coverage, probabilistic lfg grammar approximations and lexical resources for german ( cahill et al. 2003 ) and chinese ( burke, lam, et al. 2004 ).', 'the lexical resources, however, have not yet been evaluated. this, and much else, has to await further research']",5
['through completeness and coherence well - formedness conditions on f - structure  #TAUTHOR_TAG : an'],['through completeness and coherence well - formedness conditions on f - structure  #TAUTHOR_TAG : an f - structure is locally complete'],['subcategorization requirements expressed by semantic forms are enforced at f - structure level through completeness and coherence well - formedness conditions on f - structure  #TAUTHOR_TAG : an f - structure is locally complete iff it contains all the governable grammatical functions'],['subcategorization requirements expressed by semantic forms are enforced at f - structure level through completeness and coherence well - formedness conditions on f - structure  #TAUTHOR_TAG : an f - structure is locally complete iff it contains all the governable grammatical functions that its predicate governs'],0
[';  #TAUTHOR_TAG is a member of the'],[';  #TAUTHOR_TAG is a member of the'],"['functional grammar ( kaplan and bresnan 1982 ; bresnan 2001 ;  #TAUTHOR_TAG is a member of the family of constraint - based grammars.', 'it posits minimally two levels of syntactic representation : 2']","['functional grammar ( kaplan and bresnan 1982 ; bresnan 2001 ;  #TAUTHOR_TAG is a member of the family of constraint - based grammars.', 'it posits minimally two levels of syntactic representation : 2 c ( onstituent ) - structure encodes details of surface syntactic constituency, whereas f ( unctional ) - structure expresses abstract syntactic information about predicate - argument - modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', 'c - structure takes the form of phrase structure trees and is defined in terms of cfg rules and lexical entries.', 'f - structure is produced from functional annotations on the nodes of the c - structure and implemented in terms of recursive feature structures ( attribute - value matrices ).', 'this is exemplified by the analysis of the string the inquiry soon focused on the judge ( wsj 0267 72 ) using the grammar in figure 1, which results in the annotated c - structure and f - structure in figure 2']",0
[';  #TAUTHOR_TAG ; dalrymple'],[';  #TAUTHOR_TAG ; dalrymple'],"['functional grammar ( kaplan and bresnan 1982 ;  #TAUTHOR_TAG ; dalrymple 2001 ) is a member of the family of constraint - based grammars.', 'it posits minimally two levels of syntactic representation : 2']","['functional grammar ( kaplan and bresnan 1982 ;  #TAUTHOR_TAG ; dalrymple 2001 ) is a member of the family of constraint - based grammars.', 'it posits minimally two levels of syntactic representation : 2 c ( onstituent ) - structure encodes details of surface syntactic constituency, whereas f ( unctional ) - structure expresses abstract syntactic information about predicate - argument - modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', 'c - structure takes the form of phrase structure trees and is defined in terms of cfg rules and lexical entries.', 'f - structure is produced from functional annotations on the nodes of the c - structure and implemented in terms of recursive feature structures ( attribute - value matrices ).', 'this is exemplified by the analysis of the string the inquiry soon focused on the judge ( wsj 0267 72 ) using the grammar in figure 1, which results in the annotated c - structure and f - structure in figure 2']",0
"['pred value for judge in figure 1.', 'according to  #TAUTHOR_TAG, lfg assumes the following universally available inventory of grammatical']","['pred value for judge in figure 1.', 'according to  #TAUTHOR_TAG, lfg assumes the following universally available inventory of grammatical']","[') ( ↑ obl on ).', 'the argument list can be empty, as in the pred value for judge in figure 1.', 'according to  #TAUTHOR_TAG, lfg assumes the following universally available inventory of grammatical functions :']","['value of the pred attribute in an f - structure is a semantic form π gf 1, gf 2,...', ', gf n, where π is a lemma and gf a grammatical function.', 'the semantic form provides an argument list gf 1, gf 2,...', ', gf n specifying the governable grammatical functions ( or arguments ) required by the predicate to form a grammatical construction.', 'in figure 1 the verb focus requires a subject and an oblique object introduced by the preposition on : focus ( ↑ subj ) ( ↑ obl on ).', 'the argument list can be empty, as in the pred value for judge in figure 1.', 'according to  #TAUTHOR_TAG, lfg assumes the following universally available inventory of grammatical functions : subj ( ect ), obj ( ect ), obje, comp, xcomp, obl ( ique ) e, adj ( unct ), xadj.', 'obj θ and obl θ represent families of grammatical functions indexed by their semantic role, represented by the theta subscript.', 'this list of grammatical functions is divided into governable ( subcategorizable ) grammatical functions ( arguments ) and nongovernable ( nonsubcategorizable ) grammatical functions ( modifiers / adjuncts ), as summarized in table 1']",0
"['of the systems for english described in section 3.', ' #TAUTHOR_TAG evaluate']","['of the systems for english described in section 3.', ' #TAUTHOR_TAG evaluate 914 czech verbs against a custom - made']","['prepositions.', 'still, to date this is the largest number of verbs used in any of the evaluations of the systems for english described in section 3.', ' #TAUTHOR_TAG evaluate 914 czech verbs against a custom - made gold standard and record']","['achieve an f - score of 77 % against the oald when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each.', 'their system recognizes 15 frames, and these do not contain details of subcategorized - for prepositions.', 'still, to date this is the largest number of verbs used in any of the evaluations of the systems for english described in section 3.', ' #TAUTHOR_TAG evaluate 914 czech verbs against a custom - made gold standard and record a token recall of 88 %.', 'however, their evaluation does not examine the extracted subcatego - rization frames but rather the argumentadjunct distinctions posited by their sys - tem.', 'the largest lexical evaluation we know of is that of schulte im  #AUTHOR_TAG b ) for german.', 'she evaluates 3, 000 german verbs with a token frequency between 10 and 2, 000 against the duden ( dudenredaktion 2001 ).', 'we will refer to this work and the methods and results presented by schulte im walde again in sections 6. 2 and 6. 3']",1
"['to + infinitive clause  #TAUTHOR_TAG.', 'with only a slight modification, our system, along with the details provided by the automatically generated f - structures, allows us to']","['to + infinitive clause  #TAUTHOR_TAG.', 'with only a slight modification, our system, along with the details provided by the automatically generated f - structures, allows us to']","['to + infinitive clause  #TAUTHOR_TAG.', 'with only a slight modification, our system, along with the details provided by the automatically generated f - structures, allows us to']","['syntactic functions comp and xcomp refer to clausal complements with different predicate control patterns as described in section 2. however, as it stands, neither of these functions betrays anything about the syntactic nature of the constructs in question.', 'many lexicons, both automatically acquired and manually created, are more fine grained in their approaches to subcategorized clausal arguments, differentiating, for example, between a that - clause and a to + infinitive clause  #TAUTHOR_TAG.', 'with only a slight modification, our system, along with the details provided by the automatically generated f - structures, allows us to extract frames with an equivalent level of detail.', 'for example, to identify a that - clause, we']",0
[' #TAUTHOR_TAG in combination with a variation of collins'],"["" #TAUTHOR_TAG in combination with a variation of collins's ( 1997 )""]",[' #TAUTHOR_TAG in combination with a variation of collins'],"['has been carried out on the extraction of formalism - specific lexical resources from the penn - ii treebank, in particular tag, ccg, and hpsg.', 'as these formalisms are fully lexicalized with an invariant ( ltag and ccg ) or limited ( hpsg ) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'chen and vijay -  #AUTHOR_TAG explore a number of related approaches to the extraction of a lexicalized tag from the penn - ii treebank with the aim of constructing a statistical model for parsing.', ""the extraction procedure utilizes a head percolation table as introduced by  #TAUTHOR_TAG in combination with a variation of collins's ( 1997 ) approach to the differentiation between complement and adjunct."", '']",0
"['), following  #TAUTHOR_TAG.', 'applying his technique to']","['using the binomial hypothesis theory ( bht ), following  #TAUTHOR_TAG.', 'applying his technique to']","['), following  #TAUTHOR_TAG.', 'applying his technique to approximately four million words of new york times newswire, manning acquired 4,']","['will divide more - general approaches to subcategorization frame acquisition into two groups : those which extract information from raw text and those which use preparsed and hand - corrected treebank data as their input.', 'typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', ' #AUTHOR_TAG relies on morphosyntactic cues in the untagged brown corpus as indicators of six predefined subcategorization frames.', 'the frames do not include details of specific prepositions.', 'brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', ' #AUTHOR_TAG run a finite - state np parser on a pos - tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'the experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', "" #AUTHOR_TAG employ an additional statistical method based on log - linear models and bayes'theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", ' #AUTHOR_TAG attempts to improve on the approach of  #AUTHOR_TAG by passing raw text through a stochastic tagger and a finite - state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co - occur.', 'he assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'the extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( bht ), following  #TAUTHOR_TAG.', 'applying his technique to approximately four million words of new york times newswire, manning acquired 4, 900 verb - subcategorization frame pairs for 3, 104 verbs, an average of 1. 6 frames per verb.', ' #AUTHOR_TAG predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the comlex ( macleod, grishman, and meyers 1994 ) and anlt ( boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection.', 'the frames incorporate control information and details of specific prepositions.', ' #AUTHOR_TAG refine the bht with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'recent work by  #AUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on levin [ 1993 ] ) for obtaining more accurate back - off estimates for hypothesis selection']",0
['applied lexical - redundancy rules  #TAUTHOR_TAG to automatically convert'],['applied lexical - redundancy rules  #TAUTHOR_TAG to automatically convert'],['applied lexical - redundancy rules  #TAUTHOR_TAG to automatically convert the active comlex frames'],"['applied lexical - redundancy rules  #TAUTHOR_TAG to automatically convert the active comlex frames to their passive counterparts : for example, subjects are demoted to optional by oblique agents, and direct objects become subjects.', 'the resulting precision was very high ( from 72. 3 % to 80. 2 % ), and there was the expected drop in recall when prepositional details were included ( from 54. 7 % to 29. 3 % )']",5
"['- structures are presented in table 2.', 'both use the evaluation software and triple encoding presented in  #TAUTHOR_TAG.', 'the first of these is against the dcu 105, a']","['of the f - structure annotations.', 'the results of two different evaluations of the automatically generated f - structures are presented in table 2.', 'both use the evaluation software and triple encoding presented in  #TAUTHOR_TAG.', 'the first of these is against the dcu 105, a gold - standard set of 105 hand - coded f -']","['- structures are presented in table 2.', 'both use the evaluation software and triple encoding presented in  #TAUTHOR_TAG.', 'the first of these is against the dcu 105, a gold - standard set of 105 hand - coded f -']","['order to ensure the quality of the semantic forms extracted by our method, we must first ensure the quality of the f - structure annotations.', 'the results of two different evaluations of the automatically generated f - structures are presented in table 2.', 'both use the evaluation software and triple encoding presented in  #TAUTHOR_TAG.', 'the first of these is against the dcu 105, a gold - standard set of 105 hand - coded f -']",5
"['700 dependency bank  #TAUTHOR_TAG, a']","['700 dependency bank  #TAUTHOR_TAG, a']","['dependency bank  #TAUTHOR_TAG, a set of 700 randomly selected sentences from section 23 which have been parsed']","['', 'there is, however, a risk of overfitting when evaluation is limited to a gold standard of this size.', 'more recently, burke, cahill, et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available parc 700 dependency bank  #TAUTHOR_TAG, a set of 700 randomly selected sentences from section 23 which have been parsed, converted to dependency format, and manually corrected and extended by human validators.', '']",0
['on  #TAUTHOR_TAG and  #AUTHOR_TAG'],['on  #TAUTHOR_TAG and  #AUTHOR_TAG'],['on  #TAUTHOR_TAG and  #AUTHOR_TAG'],"['and  #AUTHOR_TAG describe a methodology for acquiring an english hpsg from the penn - ii treebank.', 'manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified hpsg derivation trees : head / argument / modifier distinctions are made for each node in the tree based on  #TAUTHOR_TAG and  #AUTHOR_TAG ; the whole tree is then converted to a binary tree ; heuristics are applied to deal with phenomena such as ldds and coordination and to correct some errors in the tree - bank, and finally an hpsg category is assigned to each node in the tree in accordance with its cfg category.', 'in the next phase of the process ( externalization ), hpsg lexical entries are automatically extracted from the annotated trees through the application of inverse schemata']",5
"['] [  #TAUTHOR_TAG ] ), the lexicon is the central repository']","['ccg ] [  #TAUTHOR_TAG ] ), the lexicon is the central repository']","['combinatory categorial grammar [ ccg ] [  #TAUTHOR_TAG ] ), the lexicon is the central repository']","['modern syntactic theories ( e. g., lexical - functional grammar [ lfg ] [ kaplan and bresnan 1982 ; bresnan 2001 ; dalrymple 2001 ], head - driven phrase structure grammar [ hpsg ] [ pollard and sag 1994 ], tree - adjoining grammar [ tag ] [ joshi 1988 ], and combinatory categorial grammar [ ccg ] [  #TAUTHOR_TAG ] ), the lexicon is the central repository for much morphological, syntactic, and semantic information']",0
"['genabith, way, and sadler 1999 ;  #TAUTHOR_TAG ; sadler, van genabith, and way 2000 ) was']","['genabith, way, and sadler 1999 ;  #TAUTHOR_TAG ; sadler, van genabith, and way 2000 ) was']","['. g., van genabith, way, and sadler 1999 ;  #TAUTHOR_TAG ; sadler, van genabith, and way 2000 ) was applied only to small data sets ( fewer than 200 sentences )']","['first step in the application of our methodology is the production of a treebank annotated with lfg f - structure information.', 'f - structures are attribute - value structures which represent abstract syntactic information, approximating to basic predicate - argument - modifier structures.', 'most of the early work on automatic f - structure annotation ( e. g., van genabith, way, and sadler 1999 ;  #TAUTHOR_TAG ; sadler, van genabith, and way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept.', 'however, more recent work ( cahill et al. 2002 ; cahill, mccarthy, et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the penn - ii treebank ( marcus et al. 1994 ), containing more than 1, 000, 000 words and 49, 000 sentences']",0
['by  #TAUTHOR_TAG on the filtering phase of this'],['by  #TAUTHOR_TAG on the filtering phase of this'],"['and use it to filter the induced frames.', 'recent work by  #TAUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on levin [ 1993 ] )']","['will divide more - general approaches to subcategorization frame acquisition into two groups : those which extract information from raw text and those which use preparsed and hand - corrected treebank data as their input.', 'typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', ' #AUTHOR_TAG relies on morphosyntactic cues in the untagged brown corpus as indicators of six predefined subcategorization frames.', 'the frames do not include details of specific prepositions.', 'brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', ' #AUTHOR_TAG run a finite - state np parser on a pos - tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'the experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', "" #AUTHOR_TAG employ an additional statistical method based on log - linear models and bayes'theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", ' #AUTHOR_TAG attempts to improve on the approach of  #AUTHOR_TAG by passing raw text through a stochastic tagger and a finite - state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co - occur.', 'he assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'the extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( bht ), following  #AUTHOR_TAG.', 'applying his technique to approximately four million words of new york times newswire, manning acquired 4, 900 verb - subcategorization frame pairs for 3, 104 verbs, an average of 1. 6 frames per verb.', ' #AUTHOR_TAG predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the comlex ( macleod, grishman, and meyers 1994 ) and anlt ( boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection.', 'the frames incorporate control information and details of specific prepositions.', ' #AUTHOR_TAG refine the bht with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'recent work by  #TAUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on levin [ 1993 ] ) for obtaining more accurate back - off estimates for hypothesis selection']",0
"['rate of accession may also be represented graphically.', ' #AUTHOR_TAG and  #TAUTHOR_TAG, it was observed that tree']","['rate of accession may also be represented graphically.', ' #AUTHOR_TAG and  #TAUTHOR_TAG, it was observed that treebank grammars ( cfgs']","['rate of accession may also be represented graphically.', ' #AUTHOR_TAG and  #TAUTHOR_TAG, it was observed that treebank grammars ( cf']","['rate of accession may also be represented graphically.', ' #AUTHOR_TAG and  #TAUTHOR_TAG, it was observed that treebank grammars ( cfgs extracted from treebanks ) are very large and grow with the size of the treebank.', 'we were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity.', 'figure 8 graphs the rate of induction of semantic form and cfg rule types from penn - iii ( the wsj and parse - annotated brown corpus combined ).', 'because of the variation in the size of sections between the brown and the wsj, we plotted accession against word count.', 'the first part of the graph ( up to 1, 004, 414 words']",0
['on  #AUTHOR_TAG and  #TAUTHOR_TAG'],['on  #AUTHOR_TAG and  #TAUTHOR_TAG'],['on  #AUTHOR_TAG and  #TAUTHOR_TAG'],"['and  #AUTHOR_TAG describe a methodology for acquiring an english hpsg from the penn - ii treebank.', 'manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified hpsg derivation trees : head / argument / modifier distinctions are made for each node in the tree based on  #AUTHOR_TAG and  #TAUTHOR_TAG ; the whole tree is then converted to a binary tree ; heuristics are applied to deal with phenomena such as ldds and coordination and to correct some errors in the tree - bank, and finally an hpsg category is assigned to each node in the tree in accordance with its cfg category.', 'in the next phase of the process ( externalization ), hpsg lexical entries are automatically extracted from the annotated trees through the application of inverse schemata']",5
"['from the data.', ' #TAUTHOR_TAG describe a simple']","['from the data.', ' #TAUTHOR_TAG describe a simple']","['from the data.', ' #TAUTHOR_TAG describe a simple tool which uses fine - grained rules']","['using treebank - based data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data.', ' #TAUTHOR_TAG describe a simple tool which uses fine - grained rules to identify the arguments of verb occurrences in the penn - ii treebank.', 'this is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'each of these sequences was categorized as a modifier or argument.', 'arguments were then mapped to traditional syntactic functions.', 'for example, the tag sequence np - sbj denotes a mandatory argument, and its syntactic function is subject.', 'in general, argumenthood was preferred over adjuncthoood.', ' #AUTHOR_TAG does not include an evaluation, currently it is impossible to say how effective their technique is.', ' #AUTHOR_TAG present an approach to learn previously unknown frames for czech from the prague dependency bank ( hajic 1998 ).', 'czech is a language with a freer word order than english and so configurational information cannot be relied upon.', 'in a dependency tree, the set of all dependents of the verb make up a so - called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'finding subcategorization frames involves filtering adjuncts from the observed frame.', 'this is achieved using three different hypothesis tests : bht, log - likelihood ratio, and t - score.', 'the system learns 137 subcategorization frames from 19, 126 sentences for 914 verbs ( those which occurred five times or more ).', ' #AUTHOR_TAG present preliminary work on the automatic extraction of subcategorization frames for bulgarian from the bultreebank ( simov, popova, and osenova 2002 ).', ""in a similar way to that of  #AUTHOR_TAG, marinov and hemming's system collects both arguments and adjuncts."", 'it then uses the binomial log - likelihood ratio to filter incorrect frames.', 'the bultreebank trees are annotated with hpsg - typed feature structure information and thus contain more detail than the dependency trees.', 'the work done for bulgarian is small - scale, however, as marinov and hemming are working with a preliminary version of the treebank with 580 sentences']",0
"['##4 verbs ( those which occurred five times or more ).', ' #TAUTHOR_TAG present preliminary']","['914 verbs ( those which occurred five times or more ).', ' #TAUTHOR_TAG present preliminary']","['##4 verbs ( those which occurred five times or more ).', ' #TAUTHOR_TAG present preliminary work on the automatic extraction of subc']","['', 'arguments were then mapped to traditional syntactic functions.', 'for example, the tag sequence np - sbj denotes a mandatory argument, and its syntactic function is subject.', 'in general, argumenthood was preferred over adjuncthoood.', ' #AUTHOR_TAG does not include an evaluation, currently it is impossible to say how effective their technique is.', ' #AUTHOR_TAG present an approach to learn previously unknown frames for czech from the prague dependency bank ( hajic 1998 ).', 'czech is a language with a freer word order than english and so configurational information cannot be relied upon.', 'in a dependency tree, the set of all dependents of the verb make up a so - called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'finding subcategorization frames involves filtering adjuncts from the observed frame.', 'this is achieved using three different hypothesis tests : bht, log - likelihood ratio, and t - score.', 'the system learns 137 subcategorization frames from 19, 126 sentences for 914 verbs ( those which occurred five times or more ).', ' #TAUTHOR_TAG present preliminary work on the automatic extraction of subcategorization frames for bulgarian from the bultreebank ( simov, popova, and osenova 2002 ).', ""in a similar way to that of  #AUTHOR_TAG, marinov and hemming's system collects both arguments and adjuncts."", '']",0
"[', 996.', ' #TAUTHOR_TAG also presents a similar method']","['to 8, 996.', ' #TAUTHOR_TAG also presents a similar method']","[', 996.', ' #TAUTHOR_TAG also presents a similar method']","['', 'the number of frame types extracted ( i. e., an elementary tree without a specific lexical anchor ) ranged from 2, 366 to 8, 996.', ' #TAUTHOR_TAG also presents a similar method for the extraction of a tag from the penn treebank.', '']",0
['] [  #TAUTHOR_TAG ; bresnan'],['lfg ] [  #TAUTHOR_TAG ; bresnan'],"['- functional grammar [ lfg ] [  #TAUTHOR_TAG ; bresnan 2001 ; dalrymple 2001 ], head - driven phrase structure grammar [ hpsg ] [ pollard and sag 1994 ],']","['modern syntactic theories ( e. g., lexical - functional grammar [ lfg ] [  #TAUTHOR_TAG ; bresnan 2001 ; dalrymple 2001 ], head - driven phrase structure grammar [ hpsg ] [ pollard and sag 1994 ], tree - adjoining grammar [ tag ] [ joshi 1988 ], and combinatory categorial grammar [ ccg ] [ ades and steedman 1982 ] ), the lexicon is the central repository for much morphological, syntactic, and semantic information']",0
"[', and humor.', 'it has been shown  #TAUTHOR_TAG that the subc']","['of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor.', 'it has been shown  #TAUTHOR_TAG that the']","[', mystery and detective fiction, and humor.', 'it has been shown  #TAUTHOR_TAG that the subcategorization tendencies']","['we have applied our methodology to the penn - iii treebank, a more balanced corpus resource with a number of text genres.', 'penn - iii consists of the wsj section from penn - ii as well as a parse - annotated subset of the brown corpus.', 'the brown corpus comprises 24, 242 trees compiled from a variety of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor.', 'it has been shown  #TAUTHOR_TAG that the subcategorization tendencies of verbs vary across linguistic domains.', 'our aim, therefore, is to increase the scope of the induced lexicon not only in terms of the verb lemmas for which there are entries, but also in terms of the frames with which they co - occur.', 'the f - structure annotation algorithm was extended with only minor amendments to cover the parsed brown corpus.', 'the most important of these was the way in which we distinguish between oblique and adjunct.', 'we noted in section 4 that our method of assigning an oblique annotation in penn - ii was precise, albeit conservative.', 'because of a change of annotation policy in penn - iii, the - clr tag ( indicating a close relationship between a pp and the local syntactic head ), information which we had previously exploited, is no longer used.', 'for penn - iii the algorithm annotates all pps which do not carry a penn adverbial functional tag ( such as - tmp or - loc ) and occur as the sisters of the verbal head of a vp as obliques']",4
['- shanker 2000 ;  #TAUTHOR_TAG ; hoc'],"['vijay - shanker 2000 ;  #TAUTHOR_TAG ; hockenmaier, bierner,']",['- shanker 2000 ;  #TAUTHOR_TAG ; hoc'],"['from the extraction of theory - neutral subcategorization lexicons, there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as ltag, ccg, and hpsg ( chen and vijay - shanker 2000 ;  #TAUTHOR_TAG ; hockenmaier, bierner, and baldridge 2004 ; nakanishi, miyao, and tsujii 2004 ).', 'in this article we present an approach to automating the process of lexical acquisition for lfg ( i. e., grammatical - function - based systems ).', '']",0
"['comlex ( macleod, grishman, and meyers 1994 ) and anlt  #TAUTHOR_TAG dictionaries and adding around 30 frames found by manual inspection.', 'the frames incorporate control information and details of specific prepositions.', ' #AUTHOR_TAG refine the bh']","['comlex ( macleod, grishman, and meyers 1994 ) and anlt  #TAUTHOR_TAG dictionaries and adding around 30 frames found by manual inspection.', 'the frames incorporate control information and details of specific prepositions.', ' #AUTHOR_TAG refine the bht with a priori information about the probabilities of']","['6 frames per verb.', ' #AUTHOR_TAG predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the comlex ( macleod, grishman, and meyers 1994 ) and anlt  #TAUTHOR_TAG dictionaries and adding around 30 frames found by manual inspection.', 'the frames incorporate control information and details of specific prepositions.', ' #AUTHOR_TAG refine the bht with a priori information about the probabilities of subc']","['will divide more - general approaches to subcategorization frame acquisition into two groups : those which extract information from raw text and those which use preparsed and hand - corrected treebank data as their input.', 'typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', ' #AUTHOR_TAG relies on morphosyntactic cues in the untagged brown corpus as indicators of six predefined subcategorization frames.', 'the frames do not include details of specific prepositions.', 'brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', ' #AUTHOR_TAG run a finite - state np parser on a pos - tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'the experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', "" #AUTHOR_TAG employ an additional statistical method based on log - linear models and bayes'theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", ' #AUTHOR_TAG attempts to improve on the approach of  #AUTHOR_TAG by passing raw text through a stochastic tagger and a finite - state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co - occur.', 'he assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'the extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( bht ), following  #AUTHOR_TAG.', 'applying his technique to approximately four million words of new york times newswire, manning acquired 4, 900 verb - subcategorization frame pairs for 3, 104 verbs, an average of 1. 6 frames per verb.', ' #AUTHOR_TAG predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the comlex ( macleod, grishman, and meyers 1994 ) and anlt  #TAUTHOR_TAG dictionaries and adding around 30 frames found by manual inspection.', 'the frames incorporate control information and details of specific prepositions.', ' #AUTHOR_TAG refine the bht with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'recent work by  #AUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on levin [ 1993 ] ) for obtaining more accurate back - off estimates for hypothesis selection.', ' #AUTHOR_TAG use a handwritten head -']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['utilize the automatic annotation algorithm of  #AUTHOR_TAG and cahill, mc  #AUTHOR_TAG to derive a version of penn - ii in which each node in each tree is annotated with lfg functional annotations in the form of attribute - value structure equations.', 'the algorithm uses categorial, configurational, local head, and penn - ii functional and trace information.', 'the annotation procedure is dependent on locating the head daughter, for which an amended version of  #TAUTHOR_TAG is used.', 'the head is annotated with the lfg equation ↑ = ↓.', 'linguistic generalizations are provided over the left ( the prefix ) and the right ( suffix ) context of the head for each syntactic category occurring as the mother nodes of such heads.', ""to give a simple example, the rightmost np to the left of a vp head under an s is likely to be the subject of the sentence ( ↑ subj = ↓ ), while the leftmost np to the right of the v head of a vp is most probably the verb's object ( ↑ obj = ↓ )."", 'cahill, mc  #AUTHOR_TAG provide four classes of annotation principles : one for noncoordinate configurations, one for coordinate configurations, one for traces ( long - distance dependencies ), and a final "" catch all and clean up "" phase']",5
"[';  #TAUTHOR_TAG ], head - driven phrase structure']","[';  #TAUTHOR_TAG ], head - driven phrase structure']","['and bresnan 1982 ; bresnan 2001 ;  #TAUTHOR_TAG ], head - driven phrase structure grammar [ hpsg ] [ pollard and sag 1994 ],']","['modern syntactic theories ( e. g., lexical - functional grammar [ lfg ] [ kaplan and bresnan 1982 ; bresnan 2001 ;  #TAUTHOR_TAG ], head - driven phrase structure grammar [ hpsg ] [ pollard and sag 1994 ], tree - adjoining grammar [ tag ] [ joshi 1988 ], and combinatory categorial grammar [ ccg ] [ ades and steedman 1982 ] ), the lexicon is the central repository for much morphological, syntactic, and semantic information']",0
"['penn - ii treebank  #TAUTHOR_TAG, containing more than 1, 000, 000 words and 49, 000 sentences']","['penn - ii treebank  #TAUTHOR_TAG, containing more than 1, 000, 000 words and 49, 000 sentences']","['. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the penn - ii treebank  #TAUTHOR_TAG, containing more than 1, 000, 000 words and 49, 000 sentences']","['first step in the application of our methodology is the production of a treebank annotated with lfg f - structure information.', 'f - structures are attribute - value structures which represent abstract syntactic information, approximating to basic predicate - argument - modifier structures.', 'most of the early work on automatic f - structure annotation ( e. g., van genabith, way, and sadler 1999 ; frank 2000 ; sadler, van genabith, and way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept.', 'however, more recent work ( cahill et al. 2002 ; cahill, mccarthy, et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the penn - ii treebank  #TAUTHOR_TAG, containing more than 1, 000, 000 words and 49, 000 sentences']",0
"[' #TAUTHOR_TAG ],']","[' #TAUTHOR_TAG ], tree - adjoining']","['and bresnan 1982 ; bresnan 2001 ; dalrymple 2001 ], head - driven phrase structure grammar [ hpsg ] [  #TAUTHOR_TAG ],']","['modern syntactic theories ( e. g., lexical - functional grammar [ lfg ] [ kaplan and bresnan 1982 ; bresnan 2001 ; dalrymple 2001 ], head - driven phrase structure grammar [ hpsg ] [  #TAUTHOR_TAG ], tree - adjoining grammar [ tag ] [ joshi 1988 ], and combinatory categorial grammar [ ccg ] [ ades and steedman 1982 ] ), the lexicon is the central repository for much morphological, syntactic, and semantic information']",0
"['', 'following hockenmaier, bierner, and  #AUTHOR_TAG,  #TAUTHOR_TAG, and miyao, ninomiya, and  #AUTHOR_TAG, we']","['induced lexicon on new data.', 'following hockenmaier, bierner, and  #AUTHOR_TAG,  #TAUTHOR_TAG, and miyao, ninomiya, and  #AUTHOR_TAG, we']","['', 'this can be expressed as a measure of the coverage of the induced lexicon on new data.', 'following hockenmaier, bierner, and  #AUTHOR_TAG,  #TAUTHOR_TAG, and miyao, ninomiya, and  #AUTHOR_TAG, we extract a reference lexicon from']","['', 'this can be expressed as a measure of the coverage of the induced lexicon on new data.', 'following hockenmaier, bierner, and  #AUTHOR_TAG,  #TAUTHOR_TAG, and miyao, ninomiya, and  #AUTHOR_TAG, we extract a reference lexicon from sections 02 - - 21 of the wsj.', 'we then compare this to a test lexicon from section 23.', 'table 27 shows the results of the evaluation of the coverage of an induced lexicon for verbs only.', 'there is a corresponding semantic form in the reference lexicon for 89. 89 % of the verbs in section 23. 10. 11 % of the entries in the test lexicon did not appear in the reference lexicon.', 'within this group, we can distinguish between known words, which have an entry in the reference lexicon, and unknown words, which do not exist at all in the reference lexicon.', 'in the same way we make the distinction between known frames and unknown frames.', 'there are, therefore, four different cases in which an entry may not appear in the reference lexicon.', 'table 27 shows that the most common case is that of known verbs occurring with a different, although known, subcategorization frame ( 7. 85 % )']",5
"['lexicon.', ' #TAUTHOR_TAG relies on morphosyntactic cues in the untagged brown corpus as indicators of six predefined subc']","['lexicon.', ' #TAUTHOR_TAG relies on morphosyntactic cues in the untagged brown corpus as indicators of six predefined']","['the final lexicon.', ' #TAUTHOR_TAG relies on morphosyntactic cues in the untagged brown corpus as indicators of']","['will divide more - general approaches to subcategorization frame acquisition into two groups : those which extract information from raw text and those which use preparsed and hand - corrected treebank data as their input.', 'typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', ' #TAUTHOR_TAG relies on morphosyntactic cues in the untagged brown corpus as indicators of six predefined subcategorization frames.', 'the frames do not include details of specific prepositions.', 'brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', ' #AUTHOR_TAG run a finite - state np parser on a pos - tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'the experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', "" #AUTHOR_TAG employ an additional statistical method based on log - linear models and bayes'theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", ' #AUTHOR_TAG attempts to improve on the approach of  #AUTHOR_TAG by passing raw text through a stochastic tagger and a finite - state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co - occur.', 'he assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'the extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( bht ), following  #AUTHOR_TAG.', 'applying his technique to approximately four million words of new york times newswire, manning acquired 4, 900 verb - subcategorization frame pairs for 3, 104 verbs, an average of 1. 6 frames per verb.', ' #AUTHOR_TAG predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the comlex ( macleod, grishman, and meyers 1994 ) and anlt ( boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection.', 'the frames incorporate control information and details of specific prepositions.', ' #AUTHOR_TAG refine the bht with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'recent work by  #AUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on levin [ 1993 ] ) for obtaining more accurate back - off estimates for hypothesis selection']",0
"['functional grammar  #TAUTHOR_TAG ; bresnan 2001 ; dalrymple 2001 ) is a member of the family of constraint - based grammars.', '']","['functional grammar  #TAUTHOR_TAG ; bresnan 2001 ; dalrymple 2001 ) is a member of the family of constraint - based grammars.', '']","['functional grammar  #TAUTHOR_TAG ; bresnan 2001 ; dalrymple 2001 ) is a member of the family of constraint - based grammars.', 'it posits minimally two levels of syntactic representation : 2']","['functional grammar  #TAUTHOR_TAG ; bresnan 2001 ; dalrymple 2001 ) is a member of the family of constraint - based grammars.', 'it posits minimally two levels of syntactic representation : 2 c ( onstituent ) - structure encodes details of surface syntactic constituency, whereas f ( unctional ) - structure expresses abstract syntactic information about predicate - argument - modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', 'c - structure takes the form of phrase structure trees and is defined in terms of cfg rules and lexical entries.', 'f - structure is produced from functional annotations on the nodes of the c - structure and implemented in terms of recursive feature structures ( attribute - value matrices ).', 'this is exemplified by the analysis of the string the inquiry soon focused on the judge ( wsj 0267 72 ) using the grammar in figure 1, which results in the annotated c - structure and f - structure in figure 2']",0
"['effective their technique is.', ' #TAUTHOR_TAG present an']","['effective their technique is.', ' #TAUTHOR_TAG present an']","['effective their technique is.', ' #TAUTHOR_TAG present an approach to learn previously unknown frames for czech from the prague dependency bank ( hajic', 'czech is a language with a freer word order than english and so configurational information cannot be relied upon.', 'in a dependency']","['', 'arguments were then mapped to traditional syntactic functions.', 'for example, the tag sequence np - sbj denotes a mandatory argument, and its syntactic function is subject.', 'in general, argumenthood was preferred over adjuncthoood.', ' #AUTHOR_TAG does not include an evaluation, currently it is impossible to say how effective their technique is.', ' #TAUTHOR_TAG present an approach to learn previously unknown frames for czech from the prague dependency bank ( hajic', 'czech is a language with a freer word order than english and so configurational information cannot be relied upon.', 'in a dependency tree, the set of all dependents of the verb make up a so - called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'finding subcategorization frames involves filtering adjuncts from the observed frame.', 'this is achieved using three different hypothesis tests : bht, log - likelihood ratio, and t - score.', 'the system learns 137 subcategorization frames from 19, 126 sentences for 914 verbs ( those which occurred five times or more ).', ' #AUTHOR_TAG present preliminary work on the automatic extraction of subcategorization frames for bulgarian from the bultreebank ( simov, popova, and osenova 2002 ).', ""in a similar way to that of  #AUTHOR_TAG, marinov and hemming's system collects both arguments and adjuncts."", '']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[' #TAUTHOR_TAG.', '']",[' #TAUTHOR_TAG'],0
"['##ization details.', ' #TAUTHOR_TAG argues that there are cases, albeit exceptional']","['abstract functional subcategorization details.', ' #TAUTHOR_TAG argues that there are cases, albeit exceptional']","['abstract functional subcategorization details.', ' #TAUTHOR_TAG argues that there are cases, albeit exceptional ones, in which constraints on syntactic category are an issue in subc']","['order to capture cfg - based categorial information, we add a cat feature to the f - structures automatically generated from the penn - ii and penn - iii treebanks.', 'its value is the syntactic category of the lexical item whose lemma gives rise to the pred value at that particular level of embedding.', 'this makes it possible to classify words and their semantic forms based on their syntactic category and reduces the risk of inaccurate assignment of subcategorization frame frequencies due to pos ambiguity, distinguishing, for example, between the nominal and verbal occurrences of the lemma fight.', 'with this, the output for the verb impose in figure 4 is impose ( v, [ subj, obj, obl : on ] ).', 'for some of our experiments, we conflate the different verbal ( and other ) tags used in the penn treebanks to a single verbal marker ( table 4 ).', 'as a further extension, the extraction procedure reads off the syntactic category of the head of each of the subcategorized syntactic functions : impose ( v, [ subj ( n ), obj ( n ), obl : on ] ). 3 in this way, our methodology is able to produce surface syntactic as well as abstract functional subcategorization details.', ' #TAUTHOR_TAG argues that there are cases, albeit exceptional ones, in which constraints on syntactic category are an issue in subcategorization.', 'in contrast to much of the work reviewed in section 3, which limits itself to the extraction of surface syntactic subcategorization details, our system can provide this information as well as details of grammatical function']",4
['extracted lexicon against propbank  #TAUTHOR_TAG'],['extracted lexicon against propbank  #TAUTHOR_TAG'],['extracted lexicon against propbank  #TAUTHOR_TAG'],"['', 'unlike many other approaches to subcategorization frame extraction, our system properly reflects the effects of long - distance dependencies.', 'also unlike many approaches, our method distinguishes between active and passive frames.', 'finally, our system associates conditional probabilities with the frames we extract.', 'making the distinction between the behavior of verbs in active and passive contexts is particularly important for the accurate assignment of probabilities to semantic forms.', 'we carried out an extensive evaluation of the complete induced lexicon against the full comlex resource.', 'to our knowledge, this is the most extensive qualitative evaluation of subcategorization extraction in english.', 'the only evaluation of a similar scale is that carried out by schulte im  #AUTHOR_TAG b ) for german.', 'the results reported here for penn - ii compare favorably against the baseline and, in fact, are an improvement on those reported in  #AUTHOR_TAG.', 'the results for the larger, more domain - diverse penn - iii lexicon are very encouraging, in some cases almost 15 % above the baseline.', 'we believe our semantic forms are fine - grained, and by choosing to evaluate against comlex, we set our sights high : comlex is considerably more detailed than the oald or ldoce used for other earlier evaluations.', 'our error analysis also revealed some interesting issues associated with using an external standard such as comlex.', 'in the future, we hope to evaluate the automatic annotations and extracted lexicon against propbank  #TAUTHOR_TAG']",3
"['.', 'as noted above, it is well documented  #TAUTHOR_TAG that subc']","['is that the resources are not necessarily constructed from the same source data.', 'as noted above, it is well documented  #TAUTHOR_TAG that']","['an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'as noted above, it is well documented  #TAUTHOR_TAG that subc']","['drawback to using an existing external gold standard such as comlex to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'as noted above, it is well documented  #TAUTHOR_TAG that subcategorization frames ( and their frequencies ) vary across domains.', 'we have extracted frames from two sources ( the wsj and the brown corpus ), whereas comlex was built using examples from the san jose mercury news, the brown corpus, several literary works from the library of america, scientific abstracts from the u. s. department of energy, and the wsj.', '']",4
"['a grammar.', ' #TAUTHOR_TAG explore a']","['a grammar.', ' #TAUTHOR_TAG explore a']","['a grammar.', ' #TAUTHOR_TAG explore a number of related approaches to the extraction of a lexicalized tag from']","['has been carried out on the extraction of formalism - specific lexical resources from the penn - ii treebank, in particular tag, ccg, and hpsg.', 'as these formalisms are fully lexicalized with an invariant ( ltag and ccg ) or limited ( hpsg ) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', ' #TAUTHOR_TAG explore a number of related approaches to the extraction of a lexicalized tag from the penn - ii treebank with the aim of constructing a statistical model for parsing.', 'the extraction procedure utilizes a head percolation table as introduced by  #AUTHOR_TAG in combination with a variation of  #AUTHOR_TAG approach to the differentiation between complement and adjunct.', '']",0
"['rate of accession may also be represented graphically.', 'in  #TAUTHOR_TAG and  #AUTHOR_TAG, it was observed that tree']","['rate of accession may also be represented graphically.', 'in  #TAUTHOR_TAG and  #AUTHOR_TAG, it was observed that treebank grammars ( cfgs extracted from treebanks ) are very large']","['rate of accession may also be represented graphically.', 'in  #TAUTHOR_TAG and  #AUTHOR_TAG, it was observed that treebank grammars ( cf']","['rate of accession may also be represented graphically.', 'in  #TAUTHOR_TAG and  #AUTHOR_TAG, it was observed that treebank grammars ( cfgs extracted from treebanks ) are very large and grow with the size of the treebank.', 'we were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity.', 'figure 8 graphs the rate of induction of semantic form and cfg rule types from penn - iii ( the wsj and parse - annotated brown corpus combined ).', 'because of the variation in the size of sections between the brown and the wsj, we plotted accession against word count.', 'the first part of the graph ( up to 1, 004, 414 words']",0
"['and rooth 1998 ).', ' #TAUTHOR_TAG argues']","['vary across linguistic domain or genre ( carroll and rooth 1998 ).', ' #TAUTHOR_TAG argues']","['and rooth 1998 ).', ' #TAUTHOR_TAG argues']","['important type of lexical information is the subcategorization requirements of an entry ( i. e., the arguments a predicate must take in order to form a grammatical construction ).', 'lexicons, including subcategorization details, were traditionally produced by hand.', 'however, as the manual construction of lexical resources is time consuming, error prone, expensive, and rarely ever complete, it is often the case that the limitations of nlp systems based on lexicalized approaches are due to bottlenecks in the lexicon component.', 'in addition, subcategorization requirements may vary across linguistic domain or genre ( carroll and rooth 1998 ).', ' #TAUTHOR_TAG argues that, aside from missing domain - specific complementation trends, dictionaries produced by hand will tend to lag behind real language use because of their static nature.', 'given these facts, research on automating acquisition of dictionaries for lexically based nlp systems is a particularly important issue']",4
[';  #TAUTHOR_TAG ; dalrymple'],[';  #TAUTHOR_TAG ; dalrymple'],"['and bresnan 1982 ;  #TAUTHOR_TAG ; dalrymple 2001 ], head - driven phrase structure grammar [ hpsg ] [ pollard and sag 1994 ]']","['modern syntactic theories ( e. g., lexical - functional grammar [ lfg ] [ kaplan and bresnan 1982 ;  #TAUTHOR_TAG ; dalrymple 2001 ], head - driven phrase structure grammar [ hpsg ] [ pollard and sag 1994 ], tree - adjoining grammar [ tag ] [ joshi 1988 ], and combinatory categorial grammar [ ccg ] [ ades and steedman 1982 ] ), the lexicon is the central repository for much morphological, syntactic, and semantic information']",0
"['', ' #TAUTHOR_TAG run a finite - state np parser on a pos - tagged corpus']","['the induced frames.', ' #TAUTHOR_TAG run a finite - state np parser on a pos - tagged corpus']","['the induced frames.', ' #TAUTHOR_TAG run a finite - state np parser on a pos - tagged corpus']","['will divide more - general approaches to subcategorization frame acquisition into two groups : those which extract information from raw text and those which use preparsed and hand - corrected treebank data as their input.', 'typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', ' #AUTHOR_TAG relies on morphosyntactic cues in the untagged brown corpus as indicators of six predefined subcategorization frames.', 'the frames do not include details of specific prepositions.', 'brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', ' #TAUTHOR_TAG run a finite - state np parser on a pos - tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'the experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', "" #AUTHOR_TAG employ an additional statistical method based on log - linear models and bayes'theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", ' #AUTHOR_TAG attempts to improve on the approach of  #AUTHOR_TAG by passing raw text through a stochastic tagger and a finite - state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co - occur.', 'he assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'the extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( bht ), following  #AUTHOR_TAG.', 'applying his technique to approximately four million words of new york times newswire, manning acquired 4, 900 verb - subcategorization frame pairs for 3, 104 verbs, an average of 1. 6 frames per verb.', ' #AUTHOR_TAG predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the comlex ( macleod, grishman, and meyers 1994 ) and anlt ( boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection.', 'the frames incorporate control information and details of specific prepositions.', ' #AUTHOR_TAG refine the bht with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'recent work by  #AUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on levin [ 1993 ] ) for obtaining more accurate back - off estimates for hypothesis selection']",0
"['on existing examples.', 'as a generalization,  #TAUTHOR_TAG notes that lexicon']","['on existing examples.', 'as a generalization,  #TAUTHOR_TAG notes that lexicons']","['based entirely on existing examples.', 'as a generalization,  #TAUTHOR_TAG notes that lexicon']",[' #TAUTHOR_TAG'],0
"['is  #TAUTHOR_TAG,']","['is  #TAUTHOR_TAG,']","['word segmentation.', 'we propose a unified approach that solves both problems simultaneously.', 'a previous work along this line is  #TAUTHOR_TAG,']","['believe that the identification of oov words should not be treated as a problem separate from word segmentation.', 'we propose a unified approach that solves both problems simultaneously.', 'a previous work along this line is  #TAUTHOR_TAG, which is based on weighted finite - state transducers ( fsts ).', 'our approach is similarly motivated but is based on a different mechanism : linear mixture models.', 'as we shall see, the models provide a more flexible framework for incorporating various kinds of lexical and statistical information.', ""many types of oov words that are not covered in sproat's system can be dealt with in our system."", 'the linear models we used are originally derived from linear discriminant functions widely used for pattern classification ( duda, hart, and stork 2001 ) and have been recently introduced into nlp tasks by  #AUTHOR_TAG.', 'other frameworks of chinese word segmentation, which are similar to the linear models, include maximum entropy models ( xue 2003 ) and conditional random fields ( peng, feng, and mccallum 2004 ).', 'they also use a unified approach to word breaking and oov identification']",1
"['frequent is fairly well documented.', ' #TAUTHOR_TAG, for']","['frequent is fairly well documented.', ' #TAUTHOR_TAG, for']","['frequent is fairly well documented.', ' #TAUTHOR_TAG, for']","['sense ( as well as the gricean maxims ; grice 1975 ) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication.', 'we are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented.', ' #TAUTHOR_TAG, for example, discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by candy sidner ).', 'they found that, while instructional texts tended to use numerical descriptions like the 3 1 4 "" bolt, human assemblers "" unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt "" ( dale and reiter 1995 ). 6 our own experiments ( van deemter 2004 ) point in the same direction.', 'in one experiment, for example, 34 students at the university of brighton were shown six pieces of paper, each of which showed two isosceles and approximately equilateral triangles.', 'triangles of three sizes were shown, with bases of 5, 8, and 16 mm respectively.', 'on each sheet, one of the two triangles had been circled with a pencil.', 'we asked subjects to imagine themselves on the phone to someone who held a copy of the same sheet, but not necessarily with the same orientation ( e. g., possibly upside down ), and to complete the answers in the following : q : which triangle on this sheet was circled?', 'a : the............ triangle']",0
[' #TAUTHOR_TAG : far from meaning'],"['in this way, gradable adjectives are an extreme example of the "" efficiency of language ""  #TAUTHOR_TAG : far from meaning']","['in this way, gradable adjectives are an extreme example of the "" efficiency of language ""  #TAUTHOR_TAG : far from meaning something concrete like']","['in this way, gradable adjectives are an extreme example of the "" efficiency of language ""  #TAUTHOR_TAG : far from meaning something concrete like "" larger than 8 cm "" - - a concept that would have very limited applicability - - or even something more general like "" larger than the average n, "" a word like large is applicable across a wide range of different situations']",1
"[', what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat : the speaker may decide that 8 cm is enough, or the speaker may set the standards higher ( cfxxx,  #TAUTHOR_TAG.', 'the numeral']","[', what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat : the speaker may decide that 8 cm is enough, or the speaker may set the standards higher ( cfxxx,  #TAUTHOR_TAG.', 'the numeral']","[', what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat : the speaker may decide that 8 cm is enough, or the speaker may set the standards higher ( cfxxx,  #TAUTHOR_TAG.', 'the numeral']","[', what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat : the speaker may decide that 8 cm is enough, or the speaker may set the standards higher ( cfxxx,  #TAUTHOR_TAG.', 'the numeral ( whether it is implicit, as in ( 3 ), or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( kyburg and morreau 2000 ; devault and stone 2004 ) : ( 3 ), for example, implies a standard that counts 10 cm as large and 8 cm as not large.', 'our own proposal will abstract away from the effects of linguistic context.', 'we shall ask how noun phrases like the ones in ( 3 ) and ( 4 ) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words.', 'this will allow us to make the following simplification : in a definite description that expresses only properties that are needed for singling out a referent, we take the base form']",0
"['( e. g.,  #TAUTHOR_TAG was used as the sole criterion : formally, an']","['( e. g.,  #TAUTHOR_TAG was used as the sole criterion : formally, an']","['like this would be covered if the decision - theoretic property of pareto optimality ( e. g.,  #TAUTHOR_TAG was used as the sole criterion : formally, an object r e c has a pareto - optimal combination of values v iff there is no other x e c such']","['like this would be covered if the decision - theoretic property of pareto optimality ( e. g.,  #TAUTHOR_TAG was used as the sole criterion : formally, an object r e c has a pareto - optimal combination of values v iff there is no other x e c such']",0
"['adjectives within an np ( e. g.,  #TAUTHOR_TAG ; malouf 2000 ).', 'work in this']","['left - to - right arrangement of premodifying adjectives within an np ( e. g.,  #TAUTHOR_TAG ; malouf 2000 ).', 'work in this']","['##ing adjectives within an np ( e. g.,  #TAUTHOR_TAG ; malouf 2000 ).', 'work in this area is often based on assigning adjectives to a small number of categories (']","['area of current interest concerns the left - to - right arrangement of premodifying adjectives within an np ( e. g.,  #TAUTHOR_TAG ; malouf 2000 ).', ""work in this area is often based on assigning adjectives to a small number of categories ( e. g., precentral, central, postcentral, and prehead ), which predict adjectives'relative position."", 'interestingly, vague properties tend to be realized before others.', ' #AUTHOR_TAG, for example, report that "" adjectives denoting size, length, and height normally precede other nonderived adjectives "" ( e. g., the small round table is usually preferred to the round small table ).', 'semantically, this does not come as a surprise.', 'in a noun phrase of the form the three small ( - est ) [ n ], for example, the words preceding n select the three smallest elements of [ n ].', 'it follows that, to denote the three smallest elements of the set of round tables, the only option is to say the three small round tables, rather than the three round small tables.', 'the latter would mean something else, namely, the three round ones among the n small ( est ) tables ( where n is not specified ).', ""it actually seems quite possible to say this, but only when some set of small tables is contextually salient ( e. g., i don't mean those small tables, i mean the three round ones )."", 'given that n is unspecified, the noun phrase would tend to be very unclear in any other context']",0
"['the degree of precision of the measurement  #TAUTHOR_TAG, section 1. 5 ) determines which objects can be described by the gre algorithm, since it determines which objects count as having the same size']","['the degree of precision of the measurement  #TAUTHOR_TAG, section 1. 5 ) determines which objects can be described by the gre algorithm, since it determines which objects count as having the same size']","['the degree of precision of the measurement  #TAUTHOR_TAG, section 1. 5 ) determines which objects can be described by the gre algorithm, since it determines which objects count as having the same size']","['the degree of precision of the measurement  #TAUTHOR_TAG, section 1. 5 ) determines which objects can be described by the gre algorithm, since it determines which objects count as having the same size']",0
"['linguistic realization, interleaving the two processes instead ( stone and webber 1998 ;  #TAUTHOR_TAG.', 'we have separated the two phases']","['linguistic realization, interleaving the two processes instead ( stone and webber 1998 ;  #TAUTHOR_TAG.', 'we have separated the two phases because,']","['linguistic realization, interleaving the two processes instead ( stone and webber 1998 ;  #TAUTHOR_TAG.', 'we have separated the two phases']","['recent gre algorithms have done away with the separation between content determination and linguistic realization, interleaving the two processes instead ( stone and webber 1998 ;  #TAUTHOR_TAG.', 'we have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', 'consider, for instance, the list of properties l = size > 3 cm, size < 9 cm.', 'if interleaving forced us to realize the two properties in l one by one, then it would no longer be possible to combine them into, for example, the largest mouse but one ( if the facts in the kb support it ), or even into the mice between 3 and 9 cm ( since size > 3 cm is realized before size < 9 cm ).', 'clearly, sophisticated use of gradable adjectives requires a separation between cd and linguistic realization, unless one is willing to complicate linguistic realization considerably']",1
"['strongest version of the sorites paradox ( e. g.,  #TAUTHOR_TAG']","['strongest version of the sorites paradox ( e. g.,  #TAUTHOR_TAG']","['.', 'this is the strongest version of the sorites paradox ( e. g.,  #TAUTHOR_TAG']","['##g has to do more than select a distinguishing description ( i. e., one that unambiguously denotes its referent ; dale 1989 ) : the selected expression should also be felicitous.', 'consider the question, discussed in the philosophical logic literature, of whether it is legitimate, for a gradable adjective, to distinguish between "" observationally indifferent "" entities : suppose two objects x and y, are so similar that it is impossible to distinguish their sizes ; can it ever be reasonable to say that x is large and y is not?', 'a positive answer would not be psychologically plausible, since x and y are indistinguishable ; but a negative answer would prohibit any binary distinction between objects that are large and objects that are not, given that one can always construct objects x and y, one of which falls just below the divide while the other falls just above it.', 'this is the strongest version of the sorites paradox ( e. g.,  #TAUTHOR_TAG']",0
"['involve a comparison between objects ( beun and cremers 1998,  #TAUTHOR_TAG.', 'before we']","['involve a comparison between objects ( beun and cremers 1998,  #TAUTHOR_TAG.', 'before we']","['they involve a comparison between objects ( beun and cremers 1998,  #TAUTHOR_TAG.', 'before we do this, consider the tractability of the original ia.', '']","['will examine the worst - case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( beun and cremers 1998,  #TAUTHOR_TAG.', 'before we do this, consider the tractability of the original ia.', 'if the running time of findbestvalue ( r, a i ) is a constant times the number of values of the attribute a i, then the worst - case running time of ia ( and ia plur ) is o ( n v n a ), where n a equals the number of attributes in the language and n v the average number of values of all attributes.', 'this is because, in the worst case, all values of all attributes need to be attempted ( van deemter 2002 ).', 'as for the new algorithm, we focus on the crucial phases 2, 4, and 5']",0
"['be interpreted in different ways by different people  #TAUTHOR_TAG, sometimes in stark contrast with']","['be interpreted in different ways by different people  #TAUTHOR_TAG, sometimes in stark contrast with']","['be interpreted in different ways by different people  #TAUTHOR_TAG, sometimes in stark contrast with the intention']","['we are forced to be vague because the information we have ( e. g., based on perception or verbal reports ) is itself inexact.', 'such cases can be modeled by letting nlg systems take vague information ( e. g., rain [ wednesday ] = heavy ) as their input.', 'we shall focus on the more challenging case where the output of the generator is less precise than the input, as is the case in fog and dyd.', 'this can be a hazardous affair, since vague expressions tend to be interpreted in different ways by different people  #TAUTHOR_TAG, sometimes in stark contrast with the intention of the speaker / writer ( berry, knapp, and raynor 2002 ).', 'we shall therefore focus - unlike earlier computational accounts - on vague descriptions, that is, vague expressions in definite descriptions.', 'here, the context tends to obliterate the vagueness associated with the adjective.', ""suppose you enter a vet's surgery in the company of two dogs : a big one on a leash, and a tiny one in your arms."", 'the vet asks "" who\'s the patient? "", and you answer "" the big dog. ""', 'this answer will allow the vet to pick out the patient just as reliably as if you had said "" the one on the leash "" ; the fact that big is a vague term is irrelevant.', 'you omit the exact size of the dog, just like some of its other properties ( e. g., the leash ), because they do not improve the description.', 'this shows how vague properties can contribute to the precise task of identifying a referent']",0
"['come first?', ' #TAUTHOR_TAG ; also reported in']","['come first?', ' #TAUTHOR_TAG ; also reported in levelt 1989 ) show that greater differences are']","['of these should come first?', ' #TAUTHOR_TAG ; also reported in']","['if comparative properties are at the bottom of the preference order, while stronger inequalities precede weaker ones, the order is not fixed completely.', 'suppose, for example, that the kb contains information about height as well as width, then we have inequalities of the forms ( a ) height > x, ( b ) height < x, ( c ) width > x, and ( d ) width < x.', 'which of these should come first?', ' #TAUTHOR_TAG ; also reported in levelt 1989 ) show that greater differences are most likely to be chosen, presumably because they are more striking.', '']",0
"[';  #TAUTHOR_TAG, the hypothesis that incrementality is']","[';  #TAUTHOR_TAG, the hypothesis that incrementality is']","['##nn 1989 ;  #TAUTHOR_TAG, the hypothesis that incrementality is a good model of human gre seems unfalsifiable']","['ia is generally thought to be consistent with findings on human language production ( hermann and deutsch 1976 ; levelt 1989 ; pechmann 1989 ;  #TAUTHOR_TAG, the hypothesis that incrementality is a good model of human gre seems unfalsifiable until a preference order is specified for the properties on which it operates.', ""( wildly redundant descriptions can result if the'wrong'preference order are chosen. )"", 'we shall see that vague descriptions pose particular challenges to incrementality.', 'one question emerges when the ia is combined with findings on word order and incremental interpretation.', '']",0
"['level,  #TAUTHOR_TAG.', 'ia plur can refer to individuals as well as sets, since reference to a target individual r can be modeled as reference to the singleton set']","['level,  #TAUTHOR_TAG.', 'ia plur can refer to individuals as well as sets, since reference to a target individual r can be modeled as reference to the singleton set']","['fundamental level,  #TAUTHOR_TAG.', 'ia plur can refer to individuals as well as sets, since reference to a target individual r can be modeled as reference to the singleton set']","[""##bestvalue selects the'best value'from among the values of a given attribute, assuming that these are linearly ordered in terms of specificity."", 'the function selects the value that removes most distractors, but in case of a tie, the least specific contestant is chosen, as long as it is not less specific than the basic - level value ( i. e., the most commonly occurring and psychologically most fundamental level,  #TAUTHOR_TAG.', 'ia plur can refer to individuals as well as sets, since reference to a target individual r can be modeled as reference to the singleton set { r }']",0
"['salience.', ' #TAUTHOR_TAG have argued']","['salience.', ' #TAUTHOR_TAG have argued']","['salience.', ' #TAUTHOR_TAG have argued']","['', "" #TAUTHOR_TAG have argued that dale and reiter's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : no object can be too unsalient to be referred to, as long as the right properties are available."", 'in effect, this proposal ( which measured salience numerically ) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse.', 'now suppose we let gre treat salience just like other gradable attributes.', 'suppose there are ten mice, five of which are black, whose degrees of salience are 1, 1, 3, 4, and 5 ( the last one being most salient ), while the other objects in the domain ( cats, white mice ) all have a higher salience.', 'then our algorithm might generate this list of properties : l = mouse, black, salience > 4.', 'this is a distinguishing description for the black mouse whose salience is 5 : the most salient black mouse.', 'the simpler description the black mouse can be derived by stipulating that the property of being most salient can be left implicit in english.', 'the salience attribute has to be taken into account by cd, however, and this can be ensured in various ways.', 'for example, instead of testing whether c']",0
"['##th month  #TAUTHOR_TAG.', 'these intrica']","['24th month  #TAUTHOR_TAG.', 'these intricacies include']","['##th month  #TAUTHOR_TAG.', 'these intricacies include']","['##ability is especially widespread in adjectives.', 'a search of the british national corpus ( bnc ), for example, shows at least seven of the ten most frequent adjectives ( last, other, new, good, old, great, high, small, different, large ) to be gradable.', 'children use vague adjectives among their first dozens of words ( peccei 1994 ) and understand some of their intricacies as early as their 24th month  #TAUTHOR_TAG.', 'these intricacies include what ebeling and gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set ( e. g., is this hat big or is it little?, when two hats of different sizes are visible )']",0
['the standards employed  #TAUTHOR_TAG ; devault and stone 2004 ) : ( 3'],['the standards employed  #TAUTHOR_TAG ; devault and stone 2004 ) : ( 3'],['the standards employed  #TAUTHOR_TAG ; devault and stone 2004 ) : ( 3'],"[', what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat : the speaker may decide that 8 cm is enough, or the speaker may set the standards higher ( cf., kennedy 1999 ).', 'the numeral ( whether it is implicit, as in ( 3 ), or explicit ) can be construed as allowing the reader to draw inferences about the standards employed  #TAUTHOR_TAG ; devault and stone 2004 ) : ( 3 ), for example, implies a standard that counts 10 cm as large and 8 cm as not large.', 'our own proposal will abstract away from the effects of linguistic context.', 'we shall ask how noun phrases like the ones in ( 3 ) and ( 4 ) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words.', 'this will allow us to make the following simplification : in a definite description that expresses only properties that are needed for singling out a referent, we take the base form']",0
"['involve a comparison between objects  #TAUTHOR_TAG, krahmer and theune 2002 ).', 'before we']","['involve a comparison between objects  #TAUTHOR_TAG, krahmer and theune 2002 ).', 'before we']","['they involve a comparison between objects  #TAUTHOR_TAG, krahmer and theune 2002 ).', 'before we do this, consider the tractability of the original ia.', '']","['will examine the worst - case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects  #TAUTHOR_TAG, krahmer and theune 2002 ).', 'before we do this, consider the tractability of the original ia.', 'if the running time of findbestvalue ( r, a i ) is a constant times the number of values of the attribute a i, then the worst - case running time of ia ( and ia plur ) is o ( n v n a ), where n a equals the number of attributes in the language and n v the average number of values of all attributes.', 'this is because, in the worst case, all values of all attributes need to be attempted ( van deemter 2002 ).', 'as for the new algorithm, we focus on the crucial phases 2, 4, and 5']",0
"['first?', ' #AUTHOR_TAG ; also reported in  #TAUTHOR_TAG show that greater differences']","['first?', ' #AUTHOR_TAG ; also reported in  #TAUTHOR_TAG show that greater differences']","['first?', ' #AUTHOR_TAG ; also reported in  #TAUTHOR_TAG show that greater differences']","['', ' #AUTHOR_TAG ; also reported in  #TAUTHOR_TAG show that greater differences are most likely to be chosen, presumably because they are more striking.', '']",0
"[';  #TAUTHOR_TAG.', 'but ia may be replaced by']","[';  #TAUTHOR_TAG.', 'but ia may be replaced by']","[';  #TAUTHOR_TAG.', 'but ia may be replaced by any other reasonable gre algorithm,']","['account sketched in section 4 was superimposed on an incremental gre algorithm, partly because incrementality is well established in this area ( appelt 1985 ;  #TAUTHOR_TAG.', 'but ia may be replaced by any other reasonable gre algorithm, for example, one that always exactly minimizes the number of properties expressed, or one that always "" greedily "" selects the property that removes the maximum number of distractors.', 'let g be any such gre algorithm, then we can proceed as follows']",0
"[') program.', 'following  #TAUTHOR_TAG, such expressions']","['degree adjectives. 1 more specifically, we shall explore how referring expressions containing gradable adjectives can be produced by a natural language generation ( nlg ) program.', 'following  #TAUTHOR_TAG, such expressions']","[') program.', 'following  #TAUTHOR_TAG, such expressions']","['or gradable expressions pose problems to models of language, caused by their context dependence, and by the fact that they are applicable to different degrees.', 'this article focuses on gradable adjectives, also called degree adjectives. 1 more specifically, we shall explore how referring expressions containing gradable adjectives can be produced by a natural language generation ( nlg ) program.', 'following  #TAUTHOR_TAG, such expressions will be called vague descriptions even though, as we shall see, the vagueness of the adjective does not extend to the description as a whole.', 'it will be useful to generalize over different forms of the adjective, covering the superlative form ( e. g., largest ) and the comparative form ( larger ), as well as the positive or base form ( large ) of the adjective.', 'vague descriptions are worth studying because they use vagueness in a comparatively transparent way, often combining clarity of reference with indeterminacy of meaning ; as a result, they allow us to make inroads into the difficult area of research on vagueness.', 'generation offers an interesting perspective because it forces one to ask when it is a good idea to use these descriptions, in addition to asking what they mean']",5
['is discussed in the psycholinguistics of interpretation  #TAUTHOR_TAG : interpretation is'],['is discussed in the psycholinguistics of interpretation  #TAUTHOR_TAG : interpretation is'],"[').', 'a similar problem is discussed in the psycholinguistics of interpretation  #TAUTHOR_TAG : interpretation is widely']","['', 'this question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first ( section 6 ).', 'this means that the linguistic realization cannot start until cd is concluded, contradicting eye - tracking experiments suggesting that speakers start speaking while still scanning distractors ( pechmann 1989 ).', 'a similar problem is discussed in the psycholinguistics of interpretation  #TAUTHOR_TAG : interpretation is widely assumed to proceed incrementally, but vague descriptions resist strict incrementality, since an adjective in a vague description can only be fully interpreted when its comparison set is known.', 'sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.', 'summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said']",1
"['( nd log nd ) calculations ( depending on the sorting algorithm : cfxxx [  #TAUTHOR_TAG ] chapter 8 ).', 'once again, the most time - consuming part of the calculation can be performed off - line, since it is the same for all referring expressions']","['( nd log nd ) calculations ( depending on the sorting algorithm : cfxxx [  #TAUTHOR_TAG ] chapter 8 ).', 'once again, the most time - consuming part of the calculation can be performed off - line, since it is the same for all referring expressions']","['( nd log nd ) calculations ( depending on the sorting algorithm : cfxxx [  #TAUTHOR_TAG ] chapter 8 ).', 'once again, the most time - consuming part of the calculation can be performed off - line, since it is the same for all referring expressions']","['computing the intersection of two sets takes constant time then this makes the complexity of interpreting non - vague descriptions linear : o ( nd ), where nd is the number of properties used.', 'in a vague description, the property last added to the description is context dependent.', 'worst case, calculating the set corresponding with such a property, of the form size ( x ) = maxm, for example, involves sorting the distractors as to their size, which may amount to o ( n2d ) or o ( nd log nd ) calculations ( depending on the sorting algorithm : cfxxx [  #TAUTHOR_TAG ] chapter 8 ).', 'once again, the most time - consuming part of the calculation can be performed off - line, since it is the same for all referring expressions']",0
['well as the gricean maxims ;  #TAUTHOR_TAG suggests that vague descriptions'],['well as the gricean maxims ;  #TAUTHOR_TAG suggests that vague descriptions'],['well as the gricean maxims ;  #TAUTHOR_TAG suggests that vague descriptions are preferred by speakers over quantitative ones'],"['sense ( as well as the gricean maxims ;  #TAUTHOR_TAG suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication.', 'we are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented.', ' #AUTHOR_TAG, for example, discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by candy sidner ).', 'they found that, while instructional texts tended to use numerical descriptions like the 3 1 4 "" bolt, human assemblers "" unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt "" ( dale and reiter 1995 ). 6 our own experiments ( van deemter 2004 ) point in the same direction.', 'in one experiment, for example, 34 students at the university of brighton were shown six pieces of paper, each of which showed two isosceles and approximately equilateral triangles.', 'triangles of three sizes were shown, with bases of 5, 8, and 16 mm respectively.', 'on each sheet, one of the two triangles had been circled with a pencil.', 'we asked subjects to imagine themselves on the phone to someone who held a copy of the same sheet, but not necessarily with the same orientation ( e. g., possibly upside down ), and to complete the answers in the following : q : which triangle on this sheet was circled?', 'a : the............ triangle']",0
['multifaceted properties like intelligence  #TAUTHOR_TAG'],['multifaceted properties like intelligence  #TAUTHOR_TAG'],['multifaceted properties like intelligence  #TAUTHOR_TAG'],"['. multidimensionality can also slip in through the backdoor.', 'consider big, for example, when applied to 3d shapes.', 'if there exists a formula for mapping three dimensions into one ( e. g., length × width × height ) then the result is one dimension ( overall - size ), and the algorithm of section 4 can be applied verbatim.', 'but if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situation.', 'similar things hold for multifaceted properties like intelligence  #TAUTHOR_TAG']",0
"['adjectives, including the ones that  #TAUTHOR_TAG called evalu']","['adjectives, including the ones that  #TAUTHOR_TAG called evaluative ( as']","[', not immediately available ) context.', 'for some adjectives, including the ones that  #TAUTHOR_TAG called evalu']","['we said above has also disregarded elements of the "" global "" ( i. e., not immediately available ) context.', 'for some adjectives, including the ones that  #TAUTHOR_TAG called evaluative ( as opposed to dimensional ), this is clearly inadequate.', 'he argued that evaluative adjectives ( such as beautiful and its antonym ugly ; smart and its antonym stupid, etc. ) can be recognized by the way in which they compare with antonyms.', 'for example ( after bierwisch 1989 )']",0
['##an principles  #TAUTHOR_TAG'],['could be argued to follow from gricean principles  #TAUTHOR_TAG'],['##an principles  #TAUTHOR_TAG'],"['##ity.', 'unless small gaps and dichotomy forbid it, we expected that preference should be given to the base form.', 'in english, where the base form is morphologically simpler than the other two, this rule could be argued to follow from gricean principles  #TAUTHOR_TAG']",0
"['not available  #TAUTHOR_TAG.', 'the only practical alternative is']","['not available  #TAUTHOR_TAG.', 'the only practical alternative is']","['available  #TAUTHOR_TAG.', 'the only practical alternative is']","['the usefulness of nlg resides in its ability to present data in human - accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail.', 'in principle, this might be done by providing the generator with vague input - - in which case no special algorithms are needed - - but suitably contextualized vague input is often not available  #TAUTHOR_TAG.', 'the only practical alternative is to provide the generator with "" crisp "" ( i. e., quantitative ) input, allowing the generator to be hooked on to a general - purpose database.', 'it is this avenue that we have explored in this article, in combination with various ( incremental and other ) approaches to gre.', 'far from being a peculiarity of a few adjectives, vagueness is widespread.', 'we believe that our approach can be applied to a variety of situations in which vagueness affects referring expressions including, for example, r color terms ( section 9. 3 ) ; r nouns that allow different degrees of strictness ( section 9. 5 ) ; r degrees of salience ( section 9. 4 ) ; and r imprecise pointing ( section 9. 5 )']",4
"['integrated with a standard gre algorithm  #TAUTHOR_TAG,']","['integrated with a standard gre algorithm  #TAUTHOR_TAG,']","['generalizations of our method are fairly straightforward. for example, consider a relational description ( cf., dale and haddock 1991 ) involving a gradable adjective, as in the dog in the large shed.', 'cd for this type of descriptions along the lines of section 4 is not difficult once relational descriptions are integrated with a standard gre algorithm  #TAUTHOR_TAG,']","['generalizations of our method are fairly straightforward. for example, consider a relational description ( cf., dale and haddock 1991 ) involving a gradable adjective, as in the dog in the large shed.', 'cd for this type of descriptions along the lines of section 4 is not difficult once relational descriptions are integrated with a standard gre algorithm  #TAUTHOR_TAG, section 8. 6. 2 ) : suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size ( say, size 5 ) ; if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed.', 'note that this is felicitous even if the shed is not the largest one in the domain, as is true for d2 in the following situation ( contains - a = b means that a is contained by b )']",0
[';  #TAUTHOR_TAG ; son'],"[';  #TAUTHOR_TAG ; sonnenschein 1982 ), the hypothesis that incrementality is']",['##t 1989 ;  #TAUTHOR_TAG ; son'],"['ia is generally thought to be consistent with findings on human language production ( hermann and deutsch 1976 ; levelt 1989 ;  #TAUTHOR_TAG ; sonnenschein 1982 ), the hypothesis that incrementality is a good model of human gre seems unfalsifiable until a preference order is specified for the properties on which it operates.', ""( wildly redundant descriptions can result if the'wrong'preference order are chosen. )"", 'we shall see that vague descriptions pose particular challenges to incrementality.', 'one question emerges when the ia is combined with findings on word order and incremental interpretation.', '']",0
"['.,  #TAUTHOR_TAG.', '']","[',  #TAUTHOR_TAG.', '']","['.,  #TAUTHOR_TAG.', '']","['to complex boolean descriptions involving negation and disjunction ( van deemter 2004 ) appear to be largely straightforward, except for issues to do with opposites and markedness.', 'for example, the generator will have to decide whether to say the patients that are old or the patients that are not young.', '9. 3 multidimensionality 9. 3. 1 combinations of adjectives.', 'when objects are compared in terms of several dimensions, these dimensions can be weighed in different ways ( e. g.,  #TAUTHOR_TAG.', 'let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective.', 'the np the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension ( a different one for a than for c, as it happens ) while not being exceeded by any distractors in any dimension : cases like this would be covered if the decision - theoretic property of pareto optimality ( e. g., feldman 1980 ) was used as the sole criterion : formally, an object r ∈ c has a pareto - optimal combination of values v iff there is no other x ∈ c such that 1. ∃v i ∈ v : v i ( x ) > v i ( r ) and 2. ¬∃v j ∈ v : v j ( x ) < v j ( r']",0
['on human language production ( hermann and deutsch 1976 ;  #TAUTHOR_TAG ; pec'],['on human language production ( hermann and deutsch 1976 ;  #TAUTHOR_TAG ; pechmann'],['on human language production ( hermann and deutsch 1976 ;  #TAUTHOR_TAG ; pec'],"['ia is generally thought to be consistent with findings on human language production ( hermann and deutsch 1976 ;  #TAUTHOR_TAG ; pechmann 1989 ; sonnenschein 1982 ), the hypothesis that incrementality is a good model of human gre seems unfalsifiable until a preference order is specified for the properties on which it operates.', ""( wildly redundant descriptions can result if the'wrong'preference order are chosen. )"", 'we shall see that vague descriptions pose particular challenges to incrementality.', 'one question emerges when the ia is combined with findings on word order and incremental interpretation.', '']",0
"['( but see  #TAUTHOR_TAG 1999, discussed in section']","['( but see  #TAUTHOR_TAG 1999, discussed in section 7. 2 ).', 'be this as it may, we shall']","['##tz as the short man, even if hans is the only other man in the local context ( but see  #TAUTHOR_TAG 1999, discussed in section 7. 2 ).', 'be this as it may, we shall henceforth focus on local context, assuming that additional requirements on the global context can be made if necessary']","['could require that the referent of an evaluative description fall into the correct segment of the relevant dimension.', '( for fritz to be the stupid man, it is not enough for him to be the least intelligent male in the local context ; he also has to be a fairly stupid specimen in his own right. ) if this is done, it is not evident that dimensional adjectives should be treated differently : if hanss and fritzs heights are 210 and 205 cm, respectively, then it seems questionable to describe fritz as the short man, even if hans is the only other man in the local context ( but see  #TAUTHOR_TAG 1999, discussed in section 7. 2 ).', 'be this as it may, we shall henceforth focus on local context, assuming that additional requirements on the global context can be made if necessary']",0
"['referent ;  #TAUTHOR_TAG : the selected expression should also be felicitous.', 'consider the']","['referent ;  #TAUTHOR_TAG : the selected expression should also be felicitous.', 'consider the question, discussed in']","['referent ;  #TAUTHOR_TAG : the selected expression should also be felicitous.', 'consider the question, discussed in the philosophical logic literature, of']","['##g has to do more than select a distinguishing description ( i. e., one that unambiguously denotes its referent ;  #TAUTHOR_TAG : the selected expression should also be felicitous.', '']",0
"['##oglou 1999 ;  #TAUTHOR_TAG.', 'work in this']","['arrangement of premodifying adjectives within an np ( e. g., shaw and hatzivassiloglou 1999 ;  #TAUTHOR_TAG.', 'work in this']","['##oglou 1999 ;  #TAUTHOR_TAG.', 'work in this area is often based on assigning adjectives to a small number of categories (']","['area of current interest concerns the left - to - right arrangement of premodifying adjectives within an np ( e. g., shaw and hatzivassiloglou 1999 ;  #TAUTHOR_TAG.', ""work in this area is often based on assigning adjectives to a small number of categories ( e. g., precentral, central, postcentral, and prehead ), which predict adjectives'relative position."", 'interestingly, vague properties tend to be realized before others.', ' #AUTHOR_TAG, for example, report that "" adjectives denoting size, length, and height normally precede other nonderived adjectives "" ( e. g., the small round table is usually preferred to the round small table ).', 'semantically, this does not come as a surprise.', 'in a noun phrase of the form the three small ( - est ) [ n ], for example, the words preceding n select the three smallest elements of [ n ].', 'it follows that, to denote the three smallest elements of the set of round tables, the only option is to say the three small round tables, rather than the three round small tables.', 'the latter would mean something else, namely, the three round ones among the n small ( est ) tables ( where n is not specified ).', ""it actually seems quite possible to say this, but only when some set of small tables is contextually salient ( e. g., i don't mean those small tables, i mean the three round ones )."", 'given that n is unspecified, the noun phrase would tend to be very unclear in any other context']",0
"['algorithm, which adds a type - related property if none is present yet ( cfxxx,  #TAUTHOR_TAG.', 'vague uses both of these devices']","['algorithm, which adds a type - related property if none is present yet ( cfxxx,  #TAUTHOR_TAG.', 'vague uses both of these devices']","['the end of the algorithm, which adds a type - related property if none is present yet ( cfxxx,  #TAUTHOR_TAG.', 'vague uses both of these devices']","['to turn this likelihood into a certainty, one can add a test at the end of the algorithm, which adds a type - related property if none is present yet ( cfxxx,  #TAUTHOR_TAG.', 'vague uses both of these devices']",0
"[' #TAUTHOR_TAG ; thorisson 1994, for other plans )']","['the referent r exceeds that of all distractors ( nash 1950 ; cfxxx  #TAUTHOR_TAG ; thorisson 1994, for other plans )']","['the referent r exceeds that of all distractors ( nash 1950 ; cfxxx  #TAUTHOR_TAG ; thorisson 1994, for other plans )']","['our example, b is the only object that has a pareto - optimal combination of values, predicting correctly that b can be called the tall fat giraffe.', 'it seems likely, however, that people use doubly graded descriptions more liberally.', 'for example, if the example is modified by letting width ( a ) = 3. 1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall fat giraffe.', 'many alternative strategies are possible.', 'the nash arbitration plan, for example, would allow a doubly graded description whenever the product of the values for the referent r exceeds that of all distractors ( nash 1950 ; cfxxx  #TAUTHOR_TAG ; thorisson 1994, for other plans )']",0
"['the standards employed ( kyburg and morreau 2000 ;  #TAUTHOR_TAG : ( 3 ), for']","['the standards employed ( kyburg and morreau 2000 ;  #TAUTHOR_TAG : ( 3 ), for']","['the standards employed ( kyburg and morreau 2000 ;  #TAUTHOR_TAG : ( 3 ), for']","[', what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat : the speaker may decide that 8 cm is enough, or the speaker may set the standards higher ( cf., kennedy 1999 ).', 'the numeral ( whether it is implicit, as in ( 3 ), or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( kyburg and morreau 2000 ;  #TAUTHOR_TAG : ( 3 ), for example, implies a standard that counts 10 cm as large and 8 cm as not large.', 'our own proposal will abstract away from the effects of linguistic context.', 'we shall ask how noun phrases like the ones in ( 3 ) and ( 4 ) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words.', 'this will allow us to make the following simplification : in a definite description that expresses only properties that are needed for singling out a referent, we take the base form']",0
"['of these values has equal status, so the notion of a basic - level value can not play a role ( cfxxx,  #TAUTHOR_TAG.', 'if we abstract away from the']","['of these values has equal status, so the notion of a basic - level value can not play a role ( cfxxx,  #TAUTHOR_TAG.', 'if we abstract away from the']","['of these values has equal status, so the notion of a basic - level value can not play a role ( cfxxx,  #TAUTHOR_TAG.', ""if we abstract away from the role of basic - level values, then dale and reiter's findbestvalue chooses the""]","['representation of inequalities is not entirely trivial.', 'for one thing, it is convenient to view properties of the form size ( x ) < α as belonging to a different attribute than those of the form size ( x ) > α, because this causes the values of an attribute to be linearly ordered : being larger than 12 cm implies being larger than 10 cm, and so on.', 'more importantly, it will now become normal for an object to have many values for the same attribute ; c 4, for example, has the values > 6 cm, > 10 cm, and > 12 cm.', 'each of these values has equal status, so the notion of a basic - level value can not play a role ( cfxxx,  #TAUTHOR_TAG.', ""if we abstract away from the role of basic - level values, then dale and reiter's findbestvalue chooses the most general value that removes the maximal number of distractors, as we have seen."", 'thus, size ( x ) > m is preferred over size ( x ) > n iff m > n ; conversely, size ( x ) < m is preferred over size ( x ) < n iff m < n. )', 'this is reflected by the order in which the properties are listed above : once a sizerelated property is selected, later size - related properties do not remove any distractors and will therefore not be included in the description']",0
"['relational description ( cfxxx,  #TAUTHOR_TAG involving a gradable adjective, as in the dog in']","['relational description ( cfxxx,  #TAUTHOR_TAG involving a gradable adjective, as in the dog in']","['generalizations of our method are fairly straightforward.', 'for example, consider a relational description ( cfxxx,  #TAUTHOR_TAG involving a gradable adjective, as in the dog in the large shed.', 'cd for this type of descriptions along the lines of section 4 is not difficult once relational descriptions are integrated with a standard gre algorithm ( kra']","['generalizations of our method are fairly straightforward.', 'for example, consider a relational description ( cfxxx,  #TAUTHOR_TAG involving a gradable adjective, as in the dog in the large shed.', 'cd for this type of descriptions along the lines of section 4 is not difficult once relational descriptions are integrated with a standard gre algorithm ( krahmer and theune 2002, section 8. 6. 2 ) :', 'suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size ( say, size 5 ) ; if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed.', ""note that this is felicitous even if the shed is not the largest one in the domain, as is true for d 2 in the following situation ( contains - a = b means that a is contained by b ) : in other words, the dog in the large shed denotes'the dog such that there is no other shed that is equally large or larger and that contains a dog '."", 'note that it would be odd, in the above - sketched situation, to say the dog in the largest shed']",0
"['all distractors  #TAUTHOR_TAG ; cfxxx gorniak and roy 2003 ; thorisson 1994, for other plans )']","['all distractors  #TAUTHOR_TAG ; cfxxx gorniak and roy 2003 ; thorisson 1994, for other plans )']","['all distractors  #TAUTHOR_TAG ; cfxxx gorniak and roy 2003 ; thorisson 1994, for other plans )']","['our example, b is the only object that has a pareto - optimal combination of values, predicting correctly that b can be called the tall fat giraffe.', 'it seems likely, however, that people use doubly graded descriptions more liberally.', 'for example, if the example is modified by letting width ( a ) = 3. 1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall fat giraffe.', 'many alternative strategies are possible.', 'the nash arbitration plan, for example, would allow a doubly graded description whenever the product of the values for the referent r exceeds that of all distractors  #TAUTHOR_TAG ; cfxxx gorniak and roy 2003 ; thorisson 1994, for other plans )']",0
"['another do not sit comfortably within the received nlg pipeline model ( e. g.,  #TAUTHOR_TAG.', 'an example of such an inference']","['another do not sit comfortably within the received nlg pipeline model ( e. g.,  #TAUTHOR_TAG.', 'an example of such an inference']","['another do not sit comfortably within the received nlg pipeline model ( e. g.,  #TAUTHOR_TAG.', 'an example of such an inference rule is']","['inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received nlg pipeline model ( e. g.,  #TAUTHOR_TAG.', 'an example of such an inference rule is the one that transforms a list of the form mouse, > 10 cm into one of the form mouse, size ( x ) = max 2 if only two mice are larger than 10 cm.', 'the same issues also make it difficult to interleave cd and linguistic realization as proposed by various authors, because properties may need to be combined before they are expressed']",0
"['by  #TAUTHOR_TAG, where users can specify boundary values for attributes like rainfall, specifying,']","['by  #TAUTHOR_TAG, where users can specify boundary values for attributes like rainfall, specifying,']","['compute the meaning of a vague term based on the context, but uses fixed boundary values instead.', 'a more flexible approach is used by  #TAUTHOR_TAG, where users can specify boundary values for attributes like rainfall, specifying,']","['nlg systems produce gradable adjectives. the fog weather - forecast system, for example, uses numerical input ( rain [ tuesday ] = 45 mm ) to generate vague output ( heavy rain fell on tuesday, goldberg, driedger, and kitteridge 1994 ).', 'fog does not appear to have generic rules governing the use of gradable notions : it does not compute the meaning of a vague term based on the context, but uses fixed boundary values instead.', 'a more flexible approach is used by  #TAUTHOR_TAG, where users can specify boundary values for attributes like rainfall, specifying, for example, rain counts as moderate above 7 mm / h, as heavy above 20 mm / h, and so on.', 'a third approach was implemented in dial your disc ( dyd ), where the extension of a gradable adjective like famous was computed rather than specified by hand ( van deemter and odijk 1997 ).', 'to determine, for example, whether one of mozarts piano sonatas could be called a famous sonata, the system looked up the number x of compact disc recordings of this sonata ( as listed in an encyclopedia ) and compared it to the average number y of cd recordings of each of mozarts sonatas.', 'the sonata was called a famous sonata if x > > y. like dyd, the work reported in this article will abandon the use of fixed boundary values for gradable adjectives, letting these values depend on the context in which the adjective is used']",0
"['linguistic realization, interleaving the two processes instead  #TAUTHOR_TAG ; kra']","['linguistic realization, interleaving the two processes instead  #TAUTHOR_TAG ; krahmer and theune 2002 ).', 'we have separated the two phases because,']","['linguistic realization, interleaving the two processes instead  #TAUTHOR_TAG ; kra']","['recent gre algorithms have done away with the separation between content determination and linguistic realization, interleaving the two processes instead  #TAUTHOR_TAG ; krahmer and theune 2002 ).', 'we have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', 'consider, for instance, the list of properties l = size > 3 cm, size < 9 cm.', 'if interleaving forced us to realize the two properties in l one by one, then it would no longer be possible to combine them into, for example, the largest mouse but one ( if the facts in the kb support it ), or even into the mice between 3 and 9 cm ( since size > 3 cm is realized before size < 9 cm ).', 'clearly, sophisticated use of gradable adjectives requires a separation between cd and linguistic realization, unless one is willing to complicate linguistic realization considerably']",1
['language production  #TAUTHOR_TAG ;'],['language production  #TAUTHOR_TAG ; levelt'],['on human language production  #TAUTHOR_TAG ;'],"['ia is generally thought to be consistent with findings on human language production  #TAUTHOR_TAG ; levelt 1989 ; pechmann 1989 ; sonnenschein 1982 ), the hypothesis that incrementality is a good model of human gre seems unfalsifiable until a preference order is specified for the properties on which it operates.', ""( wildly redundant descriptions can result if the'wrong'preference order are chosen. )"", 'we shall see that vague descriptions pose particular challenges to incrementality.', 'one question emerges when the ia is combined with findings on word order and incremental interpretation.', '']",0
"[' #TAUTHOR_TAG, pages']","[' #TAUTHOR_TAG, pages']","['of two objects is greener even on one dimension  #TAUTHOR_TAG, pages']","['terms are a case apart.', 'if color is modeled in terms of saturation, hue, and luminosity, for instance, then an object a may be classified as greener than b on one dimension ( e. g., saturation ), but less green than b on another ( e. g., hue ).', 'this would considerably complicate the application of our algorithm to color terms, which is otherwise mostly straighforward ( section 9. 3 ).', '( the green chair, said in the presence of two greenish chairs, would refer to the one that is closest to prototypical green. )', 'a further complication is that different speakers can regard very different values as prototypical, making it difficult to assess which of two objects is greener even on one dimension  #TAUTHOR_TAG, pages 10 - - 12 ).', '( ideally, gre should also take into account that the meaning of color words can differ across different types of referent.', 'red as in red hair, e. g., differs from red as in red chair. )', 'different attitudes towards multidimensionality are possible.', 'one possibility is to be cautious and to keep aiming for distinguishing descriptions in the strict sense.', 'in this case, the program should limit the use of vague descriptions to situations where there exists a referent that has a pareto - optimal combination of values.', 'alternatively, one could allow referring expressions to be ambiguous.', 'it would be consistent with this attitude, for example, to map multiple dimensions into one overall dimension, perhaps by borrowing from principles applied in perceptual grouping, where different perceptual dimensions are mapped into one ( e. g., thorisson 1994 ).', 'the empirical basis of this line of work, however, is still somewhat weak, so the risk of referential unclarity looms large.', 'also, this attitude would go against the spirit of gre, where referring expressions have always been assumed to be distinguishing']",0
"['', ' #TAUTHOR_TAG asked subjects to']","['more than comparisons between objects.', ' #TAUTHOR_TAG asked subjects to']","['', ' #TAUTHOR_TAG asked subjects to']","['is normally perhaps unlikely that people produce language on the basis of the kind of numerical representations that our algorithm has used as input.', 'although psychological plausibility is not our aim, it is worth noting that the inequalities computed as step 2 of the algorithm of section 4 might be psychologically more plausible, since they are essentially no more than comparisons between objects.', ' #TAUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene.', 'consider the tall cup.', 'the relevant scene would contain three distractors : ( 1 ) a less tall object of the same type as the target ( e. g., a cup that is less tall ), ( 2 ) a different kind of object that previous studies had shown to be intermediate in height ( e. g., a pitcher that, while being taller than both cups, was neither short nor tall for a pitcher ), and ( 3 ) a different type of object to which the adjective is inapplicable ( e. g., a door key ).', ""across the different conditions under which the experiment was done ( e. g., allowing subjects to study the domain before or after the onset of speech ), it was found not to matter much whether the adjective applied'intrinsically'to the target object ( i. e., whether the target was tall for a cup ) : hearers identifed the target without problems in both types of situations."", 'the time subjects took before looking at the target for the first time was measured, and although these latency times were somewhat greater when the referent were not intrinsically tall than when they were, the average difference was tiny at 554 versus 538 miliseconds.', 'since latency times are thought to be sensitive to most of the problems that hearers may have in processing a text, these results suggest that, for dimensional adjectives, it is forgivable to disregard global context']",0
"['they are difficult to process  #TAUTHOR_TAG.', 'we have seen in section 4']","['they are difficult to process  #TAUTHOR_TAG.', 'we have seen in section 4. 3 that generation']","['they are difficult to process  #TAUTHOR_TAG.', 'we have seen in section 4. 3 that generation']","['has been argued that, in an incremental approach, gradable properties should be given a low preference ranking because they are difficult to process  #TAUTHOR_TAG.', 'we have seen in section 4. 3 that generation and interpretation of vague descriptions does have a slightly higher computational complexity than that of nonvague descriptions.', 'yet, by giving gradable properties a low ranking, we might cause the algorithm to underuse them, for example, in situations where gradable properties are highly relevant to the purpose of the discourse ( e. g., a fist fight between people of very different sizes ).', 'luckily, there are no semantic or algorithmic reasons for giving gradables a low ranking.', 'let us see how things would work if they were ranked more highly']",1
['combination of dimensions  #TAUTHOR_TAG ; also section'],['combination of dimensions  #TAUTHOR_TAG ; also section 8. 1 of the present'],"[', the maximal horizontal or vertical distance, or some combination of dimensions  #TAUTHOR_TAG ; also section 8. 1 of the present article )']","['the reader is asked to focus on any reasonable size measurement, for example, the maximal horizontal or vertical distance, or some combination of dimensions  #TAUTHOR_TAG ; also section 8. 1 of the present article )']",0
"[""` efficiency of language'' into account  #TAUTHOR_TAG ; see our section 2 )""]","['each of the combinations { large, chihuahua } ( which denotes the empty set ) and { large, alsatian } ( the set of all alsatians ) useless.', ""in other words, existing treatments of gradables in gre fail to take the ` ` efficiency of language'' into account  #TAUTHOR_TAG ; see our section 2 )""]","[""` efficiency of language'' into account  #TAUTHOR_TAG ; see our section 2 )""]","['plur deals with vague properties in essentially the same way as fog : attributes like size are treated as if they were not context dependent : their values always apply to the same objects, regardless of what other properties occur in the description.', 'in this way, ia could never describe the same animal as the large chihuahua and the small brown dog, for example.', 'this approach does not do justice to gradable adjectives, whether they are used in the base form, the superlative, or the comparative.', 'suppose, for example, one set a fixed quantitative boundary, making the word large true of everything above it, and false of everything below it.', 'then ia would tend to have little use for this property at all since, presumably, every chihuahua would be small and every alsatian large, making each of the combinations { large, chihuahua } ( which denotes the empty set ) and { large, alsatian } ( the set of all alsatians ) useless.', ""in other words, existing treatments of gradables in gre fail to take the ` ` efficiency of language'' into account  #TAUTHOR_TAG ; see our section 2 )""]",0
['to patient care  #TAUTHOR_TAG'],['to patient care  #TAUTHOR_TAG'],['to patient care  #TAUTHOR_TAG'],"['second facet is independent of the clinical task and pertains to the structure of a well - built clinical question.', 'the following four components have been identified as the key elements of a question related to patient care  #TAUTHOR_TAG']",0
"['is retrieved  #TAUTHOR_TAG.', 'it is the most widely accepted single - value metric in information retrieval, and is seen to balance the need for both precision and recall']","['is retrieved  #TAUTHOR_TAG.', 'it is the most widely accepted single - value metric in information retrieval, and is seen to balance the need for both precision and recall']","['is retrieved  #TAUTHOR_TAG.', 'it is the most widely accepted single - value metric in information retrieval, and is seen to balance the need for both precision and recall']","['average precision ( map ) is the average of precision values after each relevant document is retrieved  #TAUTHOR_TAG.', 'it is the most widely accepted single - value metric in information retrieval, and is seen to balance the need for both precision and recall']",5
"['formulation,  #TAUTHOR_TAG pointed out that pico frames can be employed to']","['formulation,  #TAUTHOR_TAG pointed out that pico frames can be employed to']","['', 'although originally developed as a tool to assist in query formulation,  #TAUTHOR_TAG pointed out that pico frames can be employed to structure ir results']","['', 'although originally developed as a tool to assist in query formulation,  #TAUTHOR_TAG pointed out that pico frames can be employed to structure ir results for improving precision.', 'pico - based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems ( e. g., meadow et al. 1989 ).', 'the work of  #AUTHOR_TAG demonstrates that faceted queries can be converted into simple filtering constraints to boost precision']",0
"['operates on a different set of features.', 'we first identified the most informative unigrams and bigrams using the information gain measure  #TAUTHOR_TAG, and then selected only the positive outcome predictors using odds ratio ( mladenic and grobelnik 1999 ).', 'diseasespecific terms, such']","['operates on a different set of features.', 'we first identified the most informative unigrams and bigrams using the information gain measure  #TAUTHOR_TAG, and then selected only the positive outcome predictors using odds ratio ( mladenic and grobelnik 1999 ).', 'diseasespecific terms, such']","['n - gram based classifier is also a naive bayes classifier, but it operates on a different set of features.', 'we first identified the most informative unigrams and bigrams using the information gain measure  #TAUTHOR_TAG, and then selected only the positive outcome predictors using odds ratio ( mladenic and grobelnik 1999 ).', 'diseasespecific terms, such']","['n - gram based classifier is also a naive bayes classifier, but it operates on a different set of features.', 'we first identified the most informative unigrams and bigrams using the information gain measure  #TAUTHOR_TAG, and then selected only the positive outcome predictors using odds ratio ( mladenic and grobelnik 1999 ).', 'diseasespecific terms, such as rheumatoid arthritis, were then manually removed.', 'finally, the list of features was revised by the registered nurse who participated in the annotation effort.', 'this classifier also outputs the probability of a class assignment']",5
"['##lou 2003 ;  #TAUTHOR_TAG.', 'although']","['( mckeown, elhadad, and hatzivassiloglou 2003 ;  #TAUTHOR_TAG.', 'although']","['##lou 2003 ;  #TAUTHOR_TAG.', 'although']","['addition to question answering, multi - document summarization provides a complementary approach to addressing clinical information needs.', ""the persival project, the most comprehensive study of such techniques applied on medical texts to date, leverages patient records to generate personalized summaries in response to physicians'queries ( mckeown, elhadad, and hatzivassiloglou 2003 ;  #TAUTHOR_TAG."", 'although the system incorporates both a user and a task model, it does not explicitly capture the principles of evidence - based medicine.', 'patient information is no doubt important to answering clinical questions, and our work could certainly benefit from experiences gained in the persival project']",1
"['( see also  #TAUTHOR_TAG.', ' #AUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections']","['( see also  #TAUTHOR_TAG.', ' #AUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections']","['introduction, methods, results, or conclusion using structured abstracts as training data ( see also  #TAUTHOR_TAG.', ' #AUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections']","['literature also contains work on sentence - level classification of medline abstracts for non - clinical purposes.', 'for example, mc  #AUTHOR_TAG describe a machine learning approach to automatically label sentences as belonging to introduction, methods, results, or conclusion using structured abstracts as training data ( see also  #TAUTHOR_TAG.', ' #AUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance.', 'note, however, that such labels are orthogonal to pico frame elements, and hence are not directly relevant to knowledge extraction for clinical question answering.', 'in a similar vein,  #AUTHOR_TAG report on the identification of speculative statements in medline abstracts, but once again, this work is not directly applicable to clinical question answering']",0
"[' #TAUTHOR_TAG b ), but these features are also beyond the capabilities of current']","[' #TAUTHOR_TAG b ), but these features are also beyond the capabilities of current']","['quite similar to multi - document summarization  #TAUTHOR_TAG b ), but these features are also beyond the capabilities of']","['', 'we have noted that many of these desiderata make complex question answering quite similar to multi - document summarization  #TAUTHOR_TAG b ), but these features are also beyond the capabilities of current summarization systems']",1
"['2006 ).', ' #TAUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections']","['2006 ).', ' #TAUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections']","['2006 ).', ' #TAUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections']","['literature also contains work on sentence - level classification of medline abstracts for non - clinical purposes.', 'for example, mc  #AUTHOR_TAG describe a machine learning approach to automatically label sentences as belonging to introduction, methods, results, or conclusion using structured abstracts as training data ( see also lin et al. 2006 ).', ' #TAUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance.', 'note, however, that such labels are orthogonal to pico frame elements, and hence are not directly relevant to knowledge extraction for clinical question answering.', 'in a similar vein,  #AUTHOR_TAG report on the identification of speculative statements in medline abstracts, but once again, this work is not directly applicable to clinical question answering']",0
"[';  #TAUTHOR_TAG.', 'furthermore, it is clear that traditional']","[';  #TAUTHOR_TAG.', 'furthermore, it is clear that traditional']","['( gorman, ash, and wykoff 1994 ;  #TAUTHOR_TAG.', 'furthermore, it is clear that']",[' #TAUTHOR_TAG'],0
"['on content  #TAUTHOR_TAG.', 'nevertheless,']","['on content  #TAUTHOR_TAG.', 'nevertheless,']","['on content  #TAUTHOR_TAG.', 'nevertheless, the indexing process remains firmly human - centered']","['metadata are associated with each medline citation.', 'the most important of these is the controlled vocabulary terms assigned by human indexers.', ""nlm's controlled vocabulary thesaurus, medical subject headings ( mesh ), 2 contains approximately 23, 000 descriptors arranged in a hierarchical structure and more than 151, 000 supplementary concept records ( additional chemical substance names ) within a separate thesaurus."", ""indexing is performed by approximately 100 indexers with at least bachelor's degrees in life sciences and formal training in indexing provided by nlm."", 'since mid - 2002, the library has been employing software that automatically suggests mesh headings based on content  #TAUTHOR_TAG.', 'nevertheless, the indexing process remains firmly human - centered']",0
"['', 'previously, a user study  #TAUTHOR_TAG has shown that people are reluct']","['interface.', 'previously, a user study  #TAUTHOR_TAG has shown that people are reluctant to type full natural language questions, even after being told that']","['of which concerns our assumptions about the query interface.', 'previously, a user study  #TAUTHOR_TAG has shown that people are reluct']","['design and implementation of our current system leaves many open avenues for future exploration, one of which concerns our assumptions about the query interface.', 'previously, a user study  #TAUTHOR_TAG has shown that people are reluctant to type full natural language questions, even after being told that they were using a questionanswering system and that typing complete questions would result in better performance.', 'we have argued that a query interface based on structured pico frames will yield better - formulated queries, although it is unclear whether physicians would invest the upfront effort necessary to accomplish this.', 'issuing extremely short queries appears to be an ingrained habit of information seekers today, and the dominance of world wide web searches reinforce this behavior.', 'given these trends, physicians may actually prefer the rapid back - and - forth interaction style that comes with short queries.', 'we believe that if systems can produce noticeably better results with richer queries, users will make more of an effort to formulate them.', 'this, however, presents a chicken - and - egg problem : one possible solution is to develop models that can automatically fill query frames given a couple of keywords - this would serve to kick - start the query generation process']",1
"['g.,  #TAUTHOR_TAG, rinaldi et al']","[',  #TAUTHOR_TAG, rinaldi et al. 2004 ), and was also']","['g.,  #TAUTHOR_TAG, rinaldi et al']","['application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e. g.,  #TAUTHOR_TAG, rinaldi et al. 2004 ), and was also the focus of recent workshops on question answering in restricted domains at acl 2004 and aaai 2005.', 'our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine']",0
"['ontology already exists : metamap ( aronson 2001 ) identifies concepts in free text, and semrep  #TAUTHOR_TAG extracts']","['ontology already exists : metamap ( aronson 2001 ) identifies concepts in free text, and semrep  #TAUTHOR_TAG extracts']","['utilizing this ontology already exists : metamap ( aronson 2001 ) identifies concepts in free text, and semrep  #TAUTHOR_TAG extracts']","['explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'this domain is well - suited for exploring the posed research questions for several reasons.', 'first, substantial understanding of the domain has already been codified in the unified medical language system ( umls ) ( lindberg, humphreys, and mccray 1993 ).', 'second, software for utilizing this ontology already exists : metamap ( aronson 2001 ) identifies concepts in free text, and semrep  #TAUTHOR_TAG extracts relations between the concepts.', 'both systems utilize and propagate semantic information from umls knowledge sources : the metathesaurus, the semantic network, and the specialist lexicon.', 'the 2004 version of the umls metathesaurus ( used in this work ) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'the semantic network provides a consistent categorization of all concepts represented in the umls metathesaurus.', 'third, the paradigm of evidence - based medicine ( sackett et al. 2000 ) provides a task - based model of the clinical information - seeking process.', 'the pico framework ( richardson et al. 1995 ) for capturing well - formulated clinical queries ( described in section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system.', 'the confluence of these many factors makes clinical question answering a very exciting area of research']",0
"['classification  #TAUTHOR_TAG, which can be described by the following equation']","['classification  #TAUTHOR_TAG, which can be described by the following equation']","['classification  #TAUTHOR_TAG, which can be described by the following equation']","['attempted two approaches for assigning these weights. the first method relied on ad hoc weight selection based on intuition.', 'the second involved a more principled method using confidence values generated by the base classifiers and least squares linear regression adapted for classification  #TAUTHOR_TAG, which can be described by the following equation']",5
"['feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by  #TAUTHOR_TAG.', 'their study also illustrates the']","['feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by  #TAUTHOR_TAG.', 'their study also illustrates the']","['feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by  #TAUTHOR_TAG.', 'their study also illustrates the importance of semantic classes and relations.', 'however, extraction of outcome statements from secondary sources ( meta - analyses, in this case ) differs']","['feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by  #TAUTHOR_TAG.', 'their study also illustrates the importance of semantic classes and relations.', 'however, extraction of outcome statements from secondary sources ( meta - analyses, in this case ) differs from extraction of outcomes from medline citations because secondary sources represent knowledge that has already been distilled by humans ( which may limit its scope ).', 'because secondary sources are often more consistently organized, it is possible to depend on certain surface cues for reliable extraction ( which is not possible for medline abstracts in general ).', 'our study tackles outcome identification in primary medical sources and demonstrates that respectable performance is possible with a feature - combination approach']",1
"['ontology already exists : metamap  #TAUTHOR_TAG identifies concepts in free text, and semrep ( rindf']","['ontology already exists : metamap  #TAUTHOR_TAG identifies concepts in free text, and semrep ( rindflesch']","['utilizing this ontology already exists : metamap  #TAUTHOR_TAG identifies concepts in free text, and semrep ( rindf']","['explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'this domain is well - suited for exploring the posed research questions for several reasons.', 'first, substantial understanding of the domain has already been codified in the unified medical language system ( umls ) ( lindberg, humphreys, and mccray 1993 ).', 'second, software for utilizing this ontology already exists : metamap  #TAUTHOR_TAG identifies concepts in free text, and semrep ( rindflesch and fiszman 2003 ) extracts relations between the concepts.', 'both systems utilize and propagate semantic information from umls knowledge sources : the metathesaurus, the semantic network, and the specialist lexicon.', 'the 2004 version of the umls metathesaurus ( used in this work ) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'the semantic network provides a consistent categorization of all concepts represented in the umls metathesaurus.', 'third, the paradigm of evidence - based medicine ( sackett et al. 2000 ) provides a task - based model of the clinical information - seeking process.', 'the pico framework ( richardson et al. 1995 ) for capturing well - formulated clinical queries ( described in section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system.', 'the confluence of these many factors makes clinical question answering a very exciting area of research']",0
"['', 'the work of  #TAUTHOR_TAG demonstrates that faceted']","['1989 ).', 'the work of  #TAUTHOR_TAG demonstrates that faceted']","['', 'the work of  #TAUTHOR_TAG demonstrates that faceted queries can be converted into simple filtering constraints to boost precision']","['', 'although originally developed as a tool to assist in query formulation,  #AUTHOR_TAG pointed out that pico frames can be employed to structure ir results for improving precision.', 'pico - based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems ( e. g., meadow et al. 1989 ).', 'the work of  #TAUTHOR_TAG demonstrates that faceted queries can be converted into simple filtering constraints to boost precision']",0
"['medicine  #TAUTHOR_TAG.', 'as will be described herein,']","['medicine  #TAUTHOR_TAG.', 'as will be described herein,']","['medicine  #TAUTHOR_TAG.', 'as will be described herein, the population, problem,']","['extraction of each pico element relies to a different extent on an annotated corpus of medline abstracts, created through an effort led by the first author at the national library of medicine  #TAUTHOR_TAG.', 'as will be described herein, the population, problem, and the intervention extractors are based largely on recognition of semantic types and a few manually constructed rules ; the outcome extrac - tor, in contrast, is implemented as an ensemble of classifiers trained using supervised machine learning techniques ).', 'these two very different approaches can be attributed to differences in the nature of the frame elements : whereas problems and interventions can be directly mapped to umls concepts, and populations easily mapped to patterns that include umls concepts, outcome statements follow no predictable pattern.', 'the initial goal of the annotation effort was to identify outcome statements in abstract text.', 'a physician, two registered nurses, and an engineering researcher manually identified sentences that describe outcomes in 633 medline abstracts ; a post hoc analysis demonstrates good agreement ( κ = 0. 77 ).', 'the annotated abstracts were retrieved using pubmed and attempted to model different user behaviors ranging from naive to expert ( where advanced search features were employed ).', '']",5
"[';  #TAUTHOR_TAG,']","[';  #TAUTHOR_TAG,']","['##n, and manning 1985 ; gorman, ash, and wykoff 1994 ;  #TAUTHOR_TAG,']","[', the need to answer questions related to patient care at the point of service has been well studied and documented ( covell, uman, and manning 1985 ; gorman, ash, and wykoff 1994 ;  #TAUTHOR_TAG, 2005 ).', '']",0
['algorithm does not currently integrate evidence from multiple abstracts ; although see  #TAUTHOR_TAG and  #AUTHOR_TAG'],['algorithm does not currently integrate evidence from multiple abstracts ; although see  #TAUTHOR_TAG and  #AUTHOR_TAG'],"['algorithm does not currently integrate evidence from multiple abstracts ; although see  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'furthermore, the current answer generator does not handle complex issues']","[', answer generation remains an area that awaits further exploration, although we would have to first define what a good answer should be.', 'we have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts ; although see  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'furthermore, the current answer generator does not handle complex issues such as contradictory and inconsistent statements.', 'to address these very difficult challenges, finer - grained semantic analysis of medical texts is required']",0
"['much exploration,  #TAUTHOR_TAG discovered']","['much exploration,  #TAUTHOR_TAG discovered']","['much exploration,  #TAUTHOR_TAG discovered']","['much exploration,  #TAUTHOR_TAG discovered that it was not practical to annotate pico entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues.', 'consider the following segment']",0
"['operates on a different set of features.', 'we first identified the most informative unigrams and bigrams using the information gain measure ( yang and pedersen 1997 ), and then selected only the positive outcome predictors using odds ratio  #TAUTHOR_TAG.', 'diseasespecific terms, such']","['operates on a different set of features.', 'we first identified the most informative unigrams and bigrams using the information gain measure ( yang and pedersen 1997 ), and then selected only the positive outcome predictors using odds ratio  #TAUTHOR_TAG.', 'diseasespecific terms, such']","['n - gram based classifier is also a naive bayes classifier, but it operates on a different set of features.', 'we first identified the most informative unigrams and bigrams using the information gain measure ( yang and pedersen 1997 ), and then selected only the positive outcome predictors using odds ratio  #TAUTHOR_TAG.', 'diseasespecific terms, such']","['n - gram based classifier is also a naive bayes classifier, but it operates on a different set of features.', 'we first identified the most informative unigrams and bigrams using the information gain measure ( yang and pedersen 1997 ), and then selected only the positive outcome predictors using odds ratio  #TAUTHOR_TAG.', 'diseasespecific terms, such as rheumatoid arthritis, were then manually removed.', 'finally, the list of features was revised by the registered nurse who participated in the annotation effort.', 'this classifier also outputs the probability of a class assignment']",5
"[',  #TAUTHOR_TAG']","[',  #TAUTHOR_TAG']","[',  #TAUTHOR_TAG']","['is clear that the type of answers desired by physicians require a level of semantic analysis that is beyond the current state of the art, even with the aid of existing medical ontologies.', 'for example, even the seemingly straightforward task of identifying similarities and differences in outcome statements is rendered exceedingly complex by the tremendous amount of background medical knowledge that must be brought to bear in interpreting clinical results and subtle differences in study design, objectives, and results ; the closest analogous task in computational linguistics - redundancy detection for multi - document summarization - seems easy by comparison.', 'furthermore, it is unclear if textual strings make "" good answers. ""', 'perhaps a graphical rendering of the semantic predicates present in relevant abstracts might more effectively convey the desired information ; see, for example,  #AUTHOR_TAG.', 'perhaps some variation of multi - level bulleted lists, appropriately integrated with interface elements for expanding and hiding items, might provide physicians a better overview of the information landscape ; see, for example,  #TAUTHOR_TAG']",3
"[""' questions  #TAUTHOR_TAG."", 'a number of studies (']","[""other'' questions  #TAUTHOR_TAG."", 'a number of studies ( e. g., hildebrandt, katz, and lin 2004 ) have pointed out shortcomings of']","[""' questions  #TAUTHOR_TAG."", 'a number of studies (']","[', the evaluation of answers to complex questions remains an open research problem.', 'although it is clear that measures designed for open - domain factoid questions are not appropriate, the community has not agreed on a methodology that will allow meaningful comparisons of results from different systems.', 'in sections 9 and 10, we have discussed many of these issues.', 'recently, there is a growing consensus that an evaluation methodology based on the notion of "" information nuggets "" may provide an appropriate framework for assessing the quality of answers to complex questions.', ""nugget f - score has been employed as a metric in the trec question - answering track since 2003, to evaluate so - called definition and ` ` other'' questions  #TAUTHOR_TAG."", 'a number of studies ( e. g., hildebrandt, katz, and lin 2004 ) have pointed out shortcomings of the original nugget scoring model, although a number of these issues have been recently addressed demner - fushman 2005a, 2006b ).', 'however, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken']",0
['; see  #TAUTHOR_TAG a ) for a brief'],['; see  #TAUTHOR_TAG a ) for a brief'],['; see  #TAUTHOR_TAG a ) for a brief overview'],"['', 'for an overview of systems designed to answer open - domain factoid questions, the trec qa track overview papers are a good place to start ( voorhees and tice 1999 ).', 'in addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see  #TAUTHOR_TAG a ) for a brief overview']",0
"['original nugget scoring model, although a number of these issues have been recently addressed  #TAUTHOR_TAG a, 2006b ).', 'however, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken']","['original nugget scoring model, although a number of these issues have been recently addressed  #TAUTHOR_TAG a, 2006b ).', 'however, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken']","[', katz, and lin 2004 ) have pointed out shortcomings of the original nugget scoring model, although a number of these issues have been recently addressed  #TAUTHOR_TAG a, 2006b ).', 'however, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken']","[', the evaluation of answers to complex questions remains an open research problem.', 'although it is clear that measures designed for open - domain factoid questions are not appropriate, the community has not agreed on a methodology that will allow meaningful comparisons of results from different systems.', 'in sections 9 and 10, we have discussed many of these issues.', 'recently, there is a growing consensus that an evaluation methodology based on the notion of "" information nuggets "" may provide an appropriate framework for assessing the quality of answers to complex questions.', 'nugget f - score has been employed as a metric in the trec question - answering track since 2003, to evaluate so - called definition and "" other "" questions ( voorhees 2003 ).', 'a number of studies ( e. g., hildebrandt, katz, and lin 2004 ) have pointed out shortcomings of the original nugget scoring model, although a number of these issues have been recently addressed  #TAUTHOR_TAG a, 2006b ).', 'however, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken']",0
['recommendations taxonomy  #TAUTHOR_TAG'],['recommendations taxonomy  #TAUTHOR_TAG'],['recommendations taxonomy  #TAUTHOR_TAG'],"['potential highest level of the strength of evidence for a given citation can be identified using the publication type ( a metadata field ) and mesh terms pertaining to the type of the clinical study.', 'table 5 shows our mapping from publication type and mesh headings to evidence grades based on principles defined in the strength of recommendations taxonomy  #TAUTHOR_TAG']",5
[' #TAUTHOR_TAG ; de'],[' #TAUTHOR_TAG ; de'],[' #TAUTHOR_TAG ;'],[' #TAUTHOR_TAG'],0
['qa track overview papers are a good place to start  #TAUTHOR_TAG'],['qa track overview papers are a good place to start  #TAUTHOR_TAG'],"['open - domain factoid questions, the trec qa track overview papers are a good place to start  #TAUTHOR_TAG.', 'in addition, there has been much work on the application of linguistic and semantic knowledge to']","['', 'for an overview of systems designed to answer open - domain factoid questions, the trec qa track overview papers are a good place to start  #TAUTHOR_TAG.', 'in addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see lin and demner -  #AUTHOR_TAG a ) for a brief overview']",0
[' #TAUTHOR_TAG provides a task - based model of'],[' #TAUTHOR_TAG provides a task - based model of'],"[' #TAUTHOR_TAG provides a task - based model of the clinical information - seeking process.', 'the pico framework ( richardson']","['explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'this domain is well - suited for exploring the posed research questions for several reasons.', 'first, substantial understanding of the domain has already been codified in the unified medical language system ( umls ) ( lindberg, humphreys, and mccray 1993 ).', 'second, software for utilizing this ontology already exists : metamap ( aronson 2001 ) identifies concepts in free text, and semrep ( rindflesch and fiszman 2003 ) extracts relations between the concepts.', 'both systems utilize and propagate semantic information from umls knowledge sources : the metathesaurus, the semantic network, and the specialist lexicon.', 'the 2004 version of the umls metathesaurus ( used in this work ) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'the semantic network provides a consistent categorization of all concepts represented in the umls metathesaurus.', 'third, the paradigm of evidence - based medicine  #TAUTHOR_TAG provides a task - based model of the clinical information - seeking process.', 'the pico framework ( richardson et al. 1995 ) for capturing well - formulated clinical queries ( described in section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system.', 'the confluence of these many factors makes clinical question answering a very exciting area of research']",0
['and tice 1999 ;  #TAUTHOR_TAG'],['( voorhees and tice 1999 ;  #TAUTHOR_TAG'],['and tice 1999 ;  #TAUTHOR_TAG'],"[', we would like to match structured representations derived from the question with those derived from medline citations ( taking into consideration other ebmrelevant factors ).', 'however, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the medline database and directly index their results.', 'as an alternative, we rely on pubmed to retrieve an initial set of hits that we then postprocess in greater detail - - this is the standard pipeline architecture commonly employed in other question - answering systems ( voorhees and tice 1999 ;  #TAUTHOR_TAG']",1
[' #TAUTHOR_TAG ; hirschman and gaizauskas 2001 )'],[' #TAUTHOR_TAG ; hirschman and gaizauskas 2001 )'],[' #TAUTHOR_TAG ; hirschman and gaizauskas 2001 )'],"[', we would like to match structured representations derived from the question with those derived from medline citations ( taking into consideration other ebmrelevant factors ).', 'however, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the medline database and directly index their results.', 'as an alternative, we rely on pubmed to retrieve an initial set of hits that we then postprocess in greater detail - - this is the standard pipeline architecture commonly employed in other question - answering systems  #TAUTHOR_TAG ; hirschman and gaizauskas 2001 )']",1
['clinical questions described in  #TAUTHOR_TAG'],['clinical questions described in  #TAUTHOR_TAG'],"['the main topic of the article ( sometimes a disorder ).', 'for this evaluation, we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using pubmed on the five clinical questions described in  #TAUTHOR_TAG']","['', ""for some abstracts, mesh headings can be used as ground truth, because one of the human indexers'tasks in assigning terms is to identify the main topic of the article ( sometimes a disorder )."", 'for this evaluation, we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using pubmed on the five clinical questions described in  #TAUTHOR_TAG']",5
"['. g., jacquemart and zweigenbaum 2003,  #TAUTHOR_TAG, and was also']","['( e. g., jacquemart and zweigenbaum 2003,  #TAUTHOR_TAG, and was also']","['. g., jacquemart and zweigenbaum 2003,  #TAUTHOR_TAG, and was also the focus']","['application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e. g., jacquemart and zweigenbaum 2003,  #TAUTHOR_TAG, and was also the focus of recent workshops on question answering in restricted domains at acl 2004 and aaai 2005.', 'our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine']",0
"['on metamap  #TAUTHOR_TAG, a system']","['on metamap  #TAUTHOR_TAG, a system']","['rely extensively on metamap  #TAUTHOR_TAG, a system']","['knowledge extractors rely extensively on metamap  #TAUTHOR_TAG, a system for identifying segments of text that correspond to concepts in the umls metathesaurus.', 'many of our algorithms operate at the level of coarser - grained semantic types called semantic groups ( mccray, burgun, and bodenreider 2001 ), which capture higher - level generalizations about entities ( e. g., chemicals & drugs ).', 'an additional feature we take advantage of ( when present ) is explicit section markers present in some abstracts.', '']",5
"['on preliminary results reported in  #TAUTHOR_TAG, describes']","['on preliminary results reported in  #TAUTHOR_TAG, describes']","['on preliminary results reported in  #TAUTHOR_TAG, describes extraction algorithms']","['automatic extraction of pico elements from medline citations represents a key capability integral to clinical question answering.', 'this section, which elaborates on preliminary results reported in  #TAUTHOR_TAG, describes extraction algorithms for population, problems, interventions, outcomes, and the strength of evidence.', 'for an example of a completely annotated abstract, see figure 2.', 'each individual pico extractor takes as input the abstract text of a medline citation and identifies the relevant elements : outcomes are complete sentences, while population, problems, and interventions are short noun phrases']",2
['needed  #TAUTHOR_TAG'],['needed  #TAUTHOR_TAG'],['needed  #TAUTHOR_TAG'],"['function α ( t ) maps a mesh term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task.', 'note that although our current system uses mesh headings assigned by human indexers, manually assigned terms can be replaced with automatic processing if needed  #TAUTHOR_TAG']",3
"['process.', 'the pico framework  #TAUTHOR_TAG']","['clinical information - seeking process.', 'the pico framework  #TAUTHOR_TAG']","['. 2000 ) provides a task - based model of the clinical information - seeking process.', 'the pico framework  #TAUTHOR_TAG']","['explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'this domain is well - suited for exploring the posed research questions for several reasons.', 'first, substantial understanding of the domain has already been codified in the unified medical language system ( umls ) ( lindberg, humphreys, and mccray 1993 ).', 'second, software for utilizing this ontology already exists : metamap ( aronson 2001 ) identifies concepts in free text, and semrep ( rindflesch and fiszman 2003 ) extracts relations between the concepts.', 'both systems utilize and propagate semantic information from umls knowledge sources : the metathesaurus, the semantic network, and the specialist lexicon.', 'the 2004 version of the umls metathesaurus ( used in this work ) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'the semantic network provides a consistent categorization of all concepts represented in the umls metathesaurus.', 'third, the paradigm of evidence - based medicine ( sackett et al. 2000 ) provides a task - based model of the clinical information - seeking process.', 'the pico framework  #TAUTHOR_TAG for capturing well - formulated clinical queries ( described in section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system.', 'the confluence of these many factors makes clinical question answering a very exciting area of research']",0
[';  #TAUTHOR_TAG ; miyao and tsujii'],[';  #TAUTHOR_TAG ; miyao and tsujii'],"['; preiss 2003 ;  #TAUTHOR_TAG ; miyao and tsujii 2004 ).', 'dependency - based linguistic representations are approximations of abstract predicate - argument - adjunct']","['such as these have motivated research on more abstract, dependencybased parser evaluation ( e. g., lin 1995 ; carroll, briscoe, and sanfilippo 1998 ; carroll et al. 2002 ; clark and hockenmaier 2002 ; king et al. 2003 ; preiss 2003 ;  #TAUTHOR_TAG ; miyao and tsujii 2004 ).', 'dependency - based linguistic representations are approximations of abstract predicate - argument - adjunct ( or more basic head - dependent ) structures, providing a more normalized representation abstracting away from the particulars of surface realization or cfg - tree representation, which enables meaningful cross - parser evaluation']",4
[';  #TAUTHOR_TAG ; preiss'],[';  #TAUTHOR_TAG ; preiss'],['##maier 2002 ;  #TAUTHOR_TAG ; preiss 2003 ; kaplan'],"['such as these have motivated research on more abstract, dependencybased parser evaluation ( e. g., lin 1995 ; carroll, briscoe, and sanfilippo 1998 ; carroll et al. 2002 ; clark and hockenmaier 2002 ;  #TAUTHOR_TAG ; preiss 2003 ; kaplan et al. 2004 ; miyao and tsujii 2004 ).', 'dependency - based linguistic representations are approximations of abstract predicate - argument - adjunct ( or more basic head - dependent ) structures, providing a more normalized representation abstracting away from the particulars of surface realization or cfg - tree representation, which enables meaningful cross - parser evaluation']",4
[';  #TAUTHOR_TAG ; kaplan'],[';  #TAUTHOR_TAG ; kaplan'],[';  #TAUTHOR_TAG ; kaplan'],"['such as these have motivated research on more abstract, dependencybased parser evaluation ( e. g., lin 1995 ; carroll, briscoe, and sanfilippo 1998 ; carroll et al. 2002 ; clark and hockenmaier 2002 ; king et al. 2003 ;  #TAUTHOR_TAG ; kaplan et al. 2004 ; miyao and tsujii 2004 ).', 'dependency - based linguistic representations are approximations of abstract predicate - argument - adjunct ( or more basic head - dependent ) structures, providing a more normalized representation abstracting away from the particulars of surface realization or cfg - tree representation, which enables meaningful cross - parser evaluation']",4
"[' #TAUTHOR_TAG, employs a simpler model - - a local semantic role labeling algorithm']","[' #TAUTHOR_TAG, employs a simpler model - - a local semantic role labeling algorithm - - as a first pass to generate a set of n likely complete assignments of labels to']","[' #TAUTHOR_TAG, employs a simpler model - - a local semantic role labeling algorithm']","['a rich graphical model can represent many dependencies but there are two dangers - one is that the computational complexity of training the model and searching for the most likely labeling given the tree can be prohibitive, and the other is that if too many dependencies are encoded, the model will over - fit the training data and will not generalize well.', 'we propose a model which circumvents these two dangers and achieves significant performance gains over a similar local model that does not add any dependency arcs among the random variables.', 'to tackle the efficiency problem, we adopt dynamic programming and re - ranking algorithms.', 'to avoid overfitting we encode only a small set of linguistically motivated dependencies in features over sets of the random variables.', 'our re - ranking approach, like the approach to parse re - ranking of  #TAUTHOR_TAG, employs a simpler model - - a local semantic role labeling algorithm - - as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes.', 'the joint model is restricted to these n assignments and does not have to search the exponentially large space of all possible joint labelings']",1
"[' #TAUTHOR_TAG, which selects from likely assignments generated by a model which makes stronger independence assumptions.', 'we utilize the top n assignments of our local semantic role labeling model p srl to generate likely assignments.', 'as can be seen from']","[' #TAUTHOR_TAG, which selects from likely assignments generated by a model which makes stronger independence assumptions.', 'we utilize the top n assignments of our local semantic role labeling model p srl to generate likely assignments.', 'as can be seen from']","['to be able to incorporate long - range dependencies in our models, we chose to adopt a re - ranking approach  #TAUTHOR_TAG, which selects from likely assignments generated by a model which makes stronger independence assumptions.', 'we utilize the top n assignments of our local semantic role labeling model p srl to generate likely assignments.', 'as can be seen from']","['for re - ranking.', 'for argument identification, the number of possible assignments for a parse tree with n nodes is 2 n.', 'this number can run into the hundreds of billions for a normal - sized tree.', 'for argument labeling, the number of possible assignments is ≈ 20 m, if m is the number of arguments of a verb ( typically between 2 and 5 ), and 20 is the approximate number of possible labels if considering both core and modifying arguments.', 'training a model which has such a huge number of classes is infeasible if the model does not factorize due to strong independence assumptions.', 'therefore, in order to be able to incorporate long - range dependencies in our models, we chose to adopt a re - ranking approach  #TAUTHOR_TAG, which selects from likely assignments generated by a model which makes stronger independence assumptions.', 'we utilize the top n assignments of our local semantic role labeling model p srl to generate likely assignments.', 'as can be seen from figure 8 ( a ), for relatively small values of n, our re - ranking approach does not present a serious bottleneck to performance.', 'we used a value of n = 10 for training.', 'in figure 8 ( a ) we can see that if we could pick, using an oracle, the best assignment out of the top 10 assignments according to the local model, we would achieve an f - measure of 97. 3 on all arguments.', 'increasing the number of n to 30 results in a very small gain in the upper bound on performance and a large increase in memory requirements.', 'we therefore selected n = 10 as a good compromise']",5
"[' #TAUTHOR_TAG ; althaus, karaman']","[' #TAUTHOR_TAG ; althaus, karamanis, and koller 2004 ), the input to information ordering is an unordered set of informationbearing items']","[' #TAUTHOR_TAG ; althaus, karaman']","['our previous work  #TAUTHOR_TAG ; althaus, karamanis, and koller 2004 ), the input to information ordering is an unordered set of informationbearing items represented as cf lists.', 'a set of candidate orderings is produced by creating different permutations of these lists.', 'a metric of coherence uses features from centering to compute a score for each candidate ordering and select the highest scoring ordering as the output. 9', ""a wide range of metrics of coherence can be defined in centering's terms, simply on the basis of the work we reviewed in section 3. to exemplify this, let us first assume that the ordering in example ( 3 ), which is analyzed as a sequence of cf lists in table 5, is a candidate ordering."", 'table 6 summarizes the nocbs, the violations of coherence, salience, and cheapness, and the centering transitions for this ordering. 10', 'he candidate ordering contains two nocbs in sentences ( 3e ) and ( 3f ).', 'its score according to m. nocb, the metric used by  #AUTHOR_TAG and  #AUTHOR_TAG, is 2. another ordering with fewer nocbs ( should such an ordering exist ) will be preferred over this candidate as the selected output of information ordering if m. nocb is used to guide this process.', 'm. nocb relies only on continuity.', 'because satisfying this principle is a prerequisite for the computation of every other centering feature, m. nocb is the simplest possible centering - based metric and will be used as the baseline in our experiments']",2
"['inspired by  #TAUTHOR_TAG.', 'we use the automatically built thesa']","['inspired by  #TAUTHOR_TAG.', 'we use the automatically built thesaurus of  #AUTHOR_TAG to']","[', we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity, an approach inspired by  #TAUTHOR_TAG.', 'we use the automatically built thesa']","[', we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity, an approach inspired by  #TAUTHOR_TAG.', 'we use the automatically built thesaurus of  #AUTHOR_TAG to find words similar to each constituent, in order to automatically generate variants.', 'variants are generated by replacing either the noun or the verb constituent of a pair with a semantically ( and syntactically ) similar word. 3', 'xamples of automatically generated variants for the pair spill, bean are pour, bean, stream, bean, spill, corn, and spill, rice']",4
"['##en cases.', 'we posit that this would not have a significant effect on the results, in particular for mml - based classification techniques, such as decision graphs  #TAUTHOR_TAG']","['principle, we could have used a classification method to predict clusters from the values of the confidence measures for unseen cases.', 'we posit that this would not have a significant effect on the results, in particular for mml - based classification techniques, such as decision graphs  #TAUTHOR_TAG']","['##en cases.', 'we posit that this would not have a significant effect on the results, in particular for mml - based classification techniques, such as decision graphs  #TAUTHOR_TAG']","['principle, we could have used a classification method to predict clusters from the values of the confidence measures for unseen cases.', 'we posit that this would not have a significant effect on the results, in particular for mml - based classification techniques, such as decision graphs  #TAUTHOR_TAG']",0
['are difficult to create and maintain  #TAUTHOR_TAG'],['are difficult to create and maintain  #TAUTHOR_TAG'],"['are difficult to create and maintain  #TAUTHOR_TAG.', 'in contrast, the techniques examined in this article are corpus - based and data - driven']","['automation of help - desk responses has been previously tackled using mainly knowledge - intensive paradigms, such as expert systems ( barr and tessler 1995 ) and case - based reasoning ( watson 1997 ).', 'such technologies require significant human input, and are difficult to create and maintain  #TAUTHOR_TAG.', 'in contrast, the techniques examined in this article are corpus - based and data - driven.', 'the process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus']",0
"['##¢ only an automatic evaluation was performed, which relied on having model responses ( berger and mittal 2000 ;  #TAUTHOR_TAG']","['##¢ only an automatic evaluation was performed, which relied on having model responses ( berger and mittal 2000 ;  #TAUTHOR_TAG']","['##¢ only an automatic evaluation was performed, which relied on having model responses ( berger and mittal 2000 ;  #TAUTHOR_TAG']","['##¢ only an automatic evaluation was performed, which relied on having model responses ( berger and mittal 2000 ;  #TAUTHOR_TAG']",1
"['paradigm  #TAUTHOR_TAG, where a']","['paradigm  #TAUTHOR_TAG, where a']","['- ret ).', 'this method follows a traditional information retrieval paradigm  #TAUTHOR_TAG, where a query is represented by the content terms it contains, and']","['stated herein, we studied two document - based methods : document retrieval and document prediction.', '( doc - ret ).', 'this method follows a traditional information retrieval paradigm  #TAUTHOR_TAG, where a query is represented by the content terms it contains, and the system retrieves from the corpus a set of documents that best match this query.', 'in our case, the query is a new request e - mail to be addressed by the system, and we have considered three views of the documents in the corpus']",5
[';  #TAUTHOR_TAG to'],[';  #TAUTHOR_TAG to'],[';  #TAUTHOR_TAG to'],"['', 'for the cases where retrieval took place, we used f - score ( van rijsbergen 1979 ;  #TAUTHOR_TAG to determine the similarity between the response from the top - ranked document and the real response ( the formulas for f - score and its contributing factors, recall and precision, appear in section 4. 2 ).', '']",5
"['paradigm  #TAUTHOR_TAG, where']","['paradigm  #TAUTHOR_TAG, where']","['', 'this situation suggests a response - automation approach that follows the document retrieval paradigm  #TAUTHOR_TAG, where']","['example in figure 1 ( b ) illustrates a situation where specific words in the request ( docking station and install ) are also mentioned in the response.', 'this situation suggests a response - automation approach that follows the document retrieval paradigm  #TAUTHOR_TAG, where a new request is matched with existing response documents ( e - mails ).', 'however, specific words in the request do not always match a response well, and sometimes do not match a response at all, as demonstrated by the examples in figures 1 ( a ) and 1 ( c ), respectively']",0
"['customer.', 'it is therefore no surprise that early attempts at response automation were knowledge - driven  #TAUTHOR_TAG ; watson 1997 ; delic and lahaix 1998 )']","['customer.', 'it is therefore no surprise that early attempts at response automation were knowledge - driven  #TAUTHOR_TAG ; watson 1997 ; delic and lahaix 1998 )']","['a response that does not confuse, irritate, or mislead the customer.', 'it is therefore no surprise that early attempts at response automation were knowledge - driven  #TAUTHOR_TAG ; watson 1997 ; delic and lahaix 1998 )']","[', even the automation of responses to the "" easy "" problems is a difficult task.', 'although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer.', 'it is therefore no surprise that early attempts at response automation were knowledge - driven  #TAUTHOR_TAG ; watson 1997 ; delic and lahaix 1998 ).', 'these systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance ( delic and lahaix 1998 )']",0
"[';  #TAUTHOR_TAG to cluster these experiences.', '']","[';  #TAUTHOR_TAG to cluster these experiences.', '']","[';  #TAUTHOR_TAG to cluster these experiences.', '']","['train the system by clustering the "" experiences "" of the response - generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall ( equations ( 7 ) and ( 8 ), respectively ).', 'we then use the program snob ( wallace and boulton 1968 ;  #TAUTHOR_TAG to cluster these experiences.', 'figure 8 ( a ) is a projection of the centroids of the clusters produced by snob into the three most significant dimensions discovered by principal component analysis ( pca ) - these dimensions account for 95 % of the variation in the data.', 'shows the ( unprojected ) centroid values of three of the clusters ( the top part of the figure will be discussed subsequently ). 15', 'these clusters were chosen because they illustrate clearly three situations of interest']",5
[' #TAUTHOR_TAG and case - based'],[' #TAUTHOR_TAG and case - based'],"[' #TAUTHOR_TAG and case - based reasoning ( watson 1997 ).', 'such technologies require significant human input,']","['automation of help - desk responses has been previously tackled using mainly knowledge - intensive paradigms, such as expert systems  #TAUTHOR_TAG and case - based reasoning ( watson 1997 ).', 'such technologies require significant human input, and are difficult to create and maintain ( delic and lahaix 1998 ).', 'in contrast, the techniques examined in this article are corpus - based and data - driven.', 'the process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus']",1
"['as number of syntactic phrases, grammatical mood, and grammatical person ( marom and zukerman 2006 ), but the simple binary bag - of - lemmas representation yielded similar results.', '7 we employed the libsvm package  #TAUTHOR_TAG.', 'prediction stage, the svms predict zero or more scs for']","['as number of syntactic phrases, grammatical mood, and grammatical person ( marom and zukerman 2006 ), but the simple binary bag - of - lemmas representation yielded similar results.', '7 we employed the libsvm package  #TAUTHOR_TAG.', 'prediction stage, the svms predict zero or more scs for']","['as number of syntactic phrases, grammatical mood, and grammatical person ( marom and zukerman 2006 ), but the simple binary bag - of - lemmas representation yielded similar results.', '7 we employed the libsvm package  #TAUTHOR_TAG.', 'prediction stage, the svms predict zero or more scs for each request, as shown in figure 3.', 'we then']","[""use a support vector machine ( svm ) with a radial basis function kernel to predict scs from users'requests. 7"", 'a separate svm is trained for each sc, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the sc contains a sentence from the response to this request.', 'during the for sent - pred we also experimented with grammatical and sentence - based syntactic features, such as number of syntactic phrases, grammatical mood, and grammatical person ( marom and zukerman 2006 ), but the simple binary bag - of - lemmas representation yielded similar results.', '7 we employed the libsvm package  #TAUTHOR_TAG.', 'prediction stage, the svms predict zero or more scs for each request, as shown in figure 3.', 'we then apply the following steps']",5
"[';  #TAUTHOR_TAG.', 'we chose this program']","[';  #TAUTHOR_TAG.', 'we chose this program']","[';  #TAUTHOR_TAG.', 'we chose this program']","[""idea behind the doc - pred method is similar to  #AUTHOR_TAG : response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request's features, and the response that is most representative of the predicted cluster ( closest to the centroid ) is selected."", 'in our case, the clustering is performed by the program snob, which implements mixture modeling combined with model selection based on the minimum message length ( mml ) criterion ( wallace and boulton 1968 ;  #TAUTHOR_TAG.', 'we chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters ( this interpretation is used by the sent - pred method, section 3. 2. 2 ).', 'the input to snob is a set of binary vectors, one vector per response document.', 'the values of a vector correspond to the presence or absence of each ( lemmatized ) corpus word in the document in question ( after removing stop - words and words with very low frequency ).', 'the predictive model is a decision graph ( oliver 1993 ), which, like snob, is based on the mml principle.', 'the decision graph is trained on unigram and bigram lemmas in the request as input features, 5 and the identifier of the response cluster that contains the actual response for the request as the target feature.', 'the model predicts which response cluster is most suitable for a given request, and returns the probability that this prediction is correct.', 'this probability is our indicator of whether the doc - pred method can address a new request.', 'as for the doc - ret method, an applicability threshold for this parameter is currently determined empirically ( table 3 )']",5
"[';  #TAUTHOR_TAG.', 'precision measures']","[';  #TAUTHOR_TAG.', 'precision measures']","[';  #TAUTHOR_TAG.', 'precision measures']",[' #TAUTHOR_TAG'],5
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['automation of help - desk responses has been previously tackled using mainly knowledge - intensive paradigms, such as expert systems ( barr and tessler 1995 ) and case - based reasoning  #TAUTHOR_TAG.', 'such technologies require significant human input, and are difficult to create and maintain ( delic and lahaix 1998 ).', 'in contrast, the techniques examined in this article are corpus - based and data - driven.', 'the process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus']",1
"['', ' #TAUTHOR_TAG investigated three approaches to']","['returns the templates.', ' #TAUTHOR_TAG investigated three approaches to']","['returns the templates.', ' #TAUTHOR_TAG investigated three approaches to']","['', 'in addition, rather than including actual response sentences in a reply, their system matches response sentences to pre - existing templates and returns the templates.', ' #TAUTHOR_TAG']",1
"['using the n - gram statistics package nsp  #TAUTHOR_TAG, which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that']","['using the n - gram statistics package nsp  #TAUTHOR_TAG, which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that']","['significant bigrams are obtained using the n - gram statistics package nsp  #TAUTHOR_TAG, which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation )']","['significant bigrams are obtained using the n - gram statistics package nsp  #TAUTHOR_TAG, which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation )']",5
"['##haim, and soffer 2000 ;  #TAUTHOR_TAG ; bic']","['shtalhaim, and soffer 2000 ;  #TAUTHOR_TAG ; bickel and scheffer']","['##haim, and soffer 2000 ;  #TAUTHOR_TAG ; bic']","['', 'despite this, to date, there has been little work on corpus - based approaches to help - desk response automation ( notable exceptions are carmel, shtalhaim, and soffer 2000 ;  #TAUTHOR_TAG ; bickel and scheffer 2004 ; malik, subramaniam, and kaushik 2007 ).', '']",0
"[';  #TAUTHOR_TAG.', 'an important difference between these']","['questions ) ( berger and mittal 2000 ; berger et al. 2000 ; jijkoun and de rijke 2005 ;  #TAUTHOR_TAG.', 'an important difference between these']","['; jijkoun and de rijke 2005 ;  #TAUTHOR_TAG.', 'an important difference between these applications and help -']","['applications that, like help - desk, deal with questionanswer pairs are : sum - marization of e - mail threads ( dalli, xia, and wilks 2004 ; shrestha and mckeown 2004 ), and answer extraction in faqs ( frequently asked questions ) ( berger and mittal 2000 ; berger et al. 2000 ; jijkoun and de rijke 2005 ;  #TAUTHOR_TAG.', 'an important difference between these applications and help - desk is that help - desk request e - mails are not simple queries.', 'in fact, some e - mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information.', 'therefore, the generation of a help - desk response needs to consider a request e - mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses']",1
[')  #TAUTHOR_TAG ; roy and subram'],[')  #TAUTHOR_TAG ; roy and subramaniam 2006 )'],['##¢ only qualitative observations of the responses were reported ( no formal evaluation was performed )  #TAUTHOR_TAG ; roy and subram'],['##¢ only qualitative observations of the responses were reported ( no formal evaluation was performed )  #TAUTHOR_TAG ; roy and subramaniam 2006 )'],1
"['##ator agreement measures  #TAUTHOR_TAG.', 'however, it is still necessary to we asked the judges to']","['agreement measures  #TAUTHOR_TAG.', 'however, it is still necessary to we asked the judges to']","['the same cases, we could not employ standard inter - annotator agreement measures  #TAUTHOR_TAG.', 'however, it is still necessary to we asked the judges to']","['evaluation set contained 80 cases, randomly selected from the corpus in proportion to the size of each data set, where a case contained a request e - mail, the model response, and the responses generated by the two methods being compared.', 'each judge was given of these cases, and was asked to assess the generated responses on the four criteria listed previously. 14', 'e maximized the coverage of this study by allocating different cases to each judge, thus avoiding a situation where a particularly good or bad set of cases is evaluated by all judges.', ""in addition, we tried to ensure that the sets of cases shown to the judges were of similar quality, so that the judges'assessments would be comparable."", 'because the judges do not evaluate the same cases, we could not employ standard inter - annotator agreement measures  #TAUTHOR_TAG.', 'however, it is still necessary to we asked the judges to leave a question unanswered if they felt they did not have the technical knowledge to make a judgment, but this did not occur']",5
['their organization  #TAUTHOR_TAG ; barzi'],"['their organization  #TAUTHOR_TAG ; barzilay, elhadad, and mckeown 2001 ; barzilay and mckeown']",['their organization  #TAUTHOR_TAG ; barzi'],"['', 'this task can be cast as extractive multi - document summarization.', 'unlike a document reuse approach, sentence - level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable.', 'in our work, we gather sets of sentences, and assume ( but do not employ ) existing approaches for their organization  #TAUTHOR_TAG ; barzilay, elhadad, and mckeown 2001 ; barzilay and mckeown 2005 )']",0
"['model that is most confident regarding its decision  #TAUTHOR_TAG.', 'however, in our']","['model that is most confident regarding its decision  #TAUTHOR_TAG.', 'however, in our case, the individual confidence ( applicability ) measures employed by our response - generation methods are not comparable ( e. g.,']","['common way to combine different models consists of selecting the model that is most confident regarding its decision  #TAUTHOR_TAG.', 'however, in our']","['common way to combine different models consists of selecting the model that is most confident regarding its decision  #TAUTHOR_TAG.', 'however, in our case, the individual confidence ( applicability ) measures employed by our response - generation methods are not comparable ( e. g., the retrieval score in doc - ret is different in nature from the prediction probability in doc - pred ).', 'hence, prior to selecting the most confident method, we need to find a way to compare the different measures of confidence.', 'because the performances of the different methods are comparable, we do this by establishing a link between confidence and performance.', 'in other words, our meta - level process learns to predict the performance of the different methods from their confidence levels on the basis of previous experience.', 'these predictions enable our system to recommend a particular method for handling a new ( unseen ) request ( marom, zukerman, and japkowicz 2007 )']",1
"[')  #TAUTHOR_TAG ; soricut and brill 2006 ).', 'an important difference between these']","['questions )  #TAUTHOR_TAG ; soricut and brill 2006 ).', 'an important difference between these']","[')  #TAUTHOR_TAG ; soricut and brill 2006 ).', 'an important difference between these applications and']","['applications that, like help - desk, deal with question - answer pairs are : summarization of e - mail threads ( dalli, xia, and wilks 2004 ; shrestha and mckeown 2004 ), and answer extraction in faqs ( frequently asked questions )  #TAUTHOR_TAG ; soricut and brill 2006 ).', 'an important difference between these applications and help - desk is that help - desk request e - mails are not simple queries.', 'in fact, some e - mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information.', 'therefore, the generation of a help - desk response needs to consider a request e - mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses']",1
"['. 2006 ;  #TAUTHOR_TAG.', 'the representativeness of the sample size was not discussed in any']","['the corpus itself was significantly smaller than ours ( feng et al. 2006 ;  #TAUTHOR_TAG.', 'the representativeness of the sample size was not discussed in any']","['. 2006 ;  #TAUTHOR_TAG.', 'the representativeness of the sample size was not discussed in any']","['##¢ a user study was performed, but it was either very small compared to the corpus ( carmel, shtalhaim, and soffer 2000 ; jijkoun and de rijke 2005 ), or the corpus itself was significantly smaller than ours ( feng et al. 2006 ;  #TAUTHOR_TAG.', 'the representativeness of the sample size was not discussed in any of these studies']",1
"[', which relied on having model responses  #TAUTHOR_TAG ; berger et al. 2000 )']","[', which relied on having model responses  #TAUTHOR_TAG ; berger et al. 2000 )']","['to queries.', 'these systems addressed the evaluation issue as follows.', 'a¢ only an automatic evaluation was performed, which relied on having model responses  #TAUTHOR_TAG ; berger et al. 2000 )']","['a ) we identified several systems that resemble ours in that they provide answers to queries.', 'these systems addressed the evaluation issue as follows.', 'a¢ only an automatic evaluation was performed, which relied on having model responses  #TAUTHOR_TAG ; berger et al. 2000 )']",1
[' #TAUTHOR_TAG a ) we identified several systems that resemble ours in that'],[' #TAUTHOR_TAG a ) we identified several systems that resemble ours in that'],[' #TAUTHOR_TAG a ) we identified several systems that resemble ours in that'],"[' #TAUTHOR_TAG a ) we identified several systems that resemble ours in that they provide answers to queries.', 'these systems addressed the evaluation issue as follows.', 'r only an automatic evaluation was performed, which relied on having model responses']",0
"['by the program snob, which implements mixture modeling combined with model selection based on the minimum message length ( mml ) criterion  #TAUTHOR_TAG ; wallace']","['by the program snob, which implements mixture modeling combined with model selection based on the minimum message length ( mml ) criterion  #TAUTHOR_TAG ; wallace']","['to the centroid ) is selected.', 'in our case, the clustering is performed by the program snob, which implements mixture modeling combined with model selection based on the minimum message length ( mml ) criterion  #TAUTHOR_TAG ; wallace 2005 ).', 'we chose this program']","[""idea behind the doc - pred method is similar to  #AUTHOR_TAG : response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request's features, and the response that is most representative of the predicted cluster ( closest to the centroid ) is selected."", 'in our case, the clustering is performed by the program snob, which implements mixture modeling combined with model selection based on the minimum message length ( mml ) criterion  #TAUTHOR_TAG ; wallace 2005 ).', 'we chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters ( this interpretation is used by the sent - pred method, section 3. 2. 2 ).', 'the input to snob is a set of binary vectors, one vector per response document.', 'the values of a vector correspond to the presence or absence of each ( lemmatized ) corpus word in the document in question ( after removing stop - words and words with very low frequency ).', 'the predictive model is a decision graph ( oliver 1993 ), which, like snob, is based on the mml principle.', 'the decision graph is trained on unigram and bigram lemmas in the request as input features, 5 and the identifier of the response cluster that contains the actual response for the request as the target feature.', 'the model predicts which response cluster is most suitable for a given request, and returns the probability that this prediction is correct.', 'this probability is our indicator of whether the doc - pred method can address a new request.', 'as for the doc - ret method, an applicability threshold for this parameter is currently determined empirically ( table 3 )']",5
"[',  #TAUTHOR_TAG employed a sentence retrieval']","['faqs,  #TAUTHOR_TAG employed a sentence retrieval']","[',  #TAUTHOR_TAG employed a sentence retrieval approach based on a language model where the entire response to an faq is considered a sentence, and the questions and answers are embedded in an faq document.', 'they complemented this approach with machine learning techniques that automatically learn the weights']","['faqs,  #TAUTHOR_TAG employed a sentence retrieval approach based on a language model where the entire response to an faq is considered a sentence, and the questions and answers are embedded in an faq document.', 'they complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.', 'compared two retrieval approaches ( tf. idf and query expansion ) and two predictive approaches ( statistical translation and latent variable models ).', 'jijkoun and de  #AUTHOR_TAG compared different variants of retrieval techniques.', ' #AUTHOR_TAG compared a predictive approach ( statistical translation ), a retrieval approach based on a language - model, and a hybrid approach which combines statistical chunking and traditional retrieval.', 'two significant differences between help - desk and faqs are the following']",0
['the corpus itself was significantly smaller than ours  #TAUTHOR_TAG ; leuski et'],"['the corpus itself was significantly smaller than ours  #TAUTHOR_TAG ; leuski et al. 2006 ).', 'the representativeness of the sample size was not discussed in any of these studies']",['the corpus itself was significantly smaller than ours  #TAUTHOR_TAG ; leuski et'],"['##¢ a user study was performed, but it was either very small compared to the corpus ( carmel, shtalhaim, and soffer 2000 ; jijkoun and de rijke 2005 ), or the corpus itself was significantly smaller than ours  #TAUTHOR_TAG ; leuski et al. 2006 ).', 'the representativeness of the sample size was not discussed in any of these studies']",1
"[';  #TAUTHOR_TAG ; malik, sub']","[';  #TAUTHOR_TAG ; malik,']","['kosseim 2003 ;  #TAUTHOR_TAG ; malik, sub']","['', 'despite this, to date, there has been little work on corpus - based approaches to help - desk response automation ( notable exceptions are carmel, shtalhaim, and soffer 2000 ; lapalme and kosseim 2003 ;  #TAUTHOR_TAG ; malik, subramaniam, and kaushik 2007 ).', '']",0
"['doc - pred, and svms  #TAUTHOR_TAG for sent - pred. 11']","['doc - pred, and svms  #TAUTHOR_TAG for sent - pred. 11']","['doc - pred, and svms  #TAUTHOR_TAG for sent - pred. 11 additionally, we used unigrams']","['focus of our work is on the general applicability of the different response automa - tion methods, rather than on comparing the performance of particular implementa - tion techniques.', 'hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research.', 'specifically, we used decision graphs ( oliver 1993 ) for doc - pred, and svms  #TAUTHOR_TAG for sent - pred. 11 additionally, we used unigrams for clustering documents and sentences, and unigrams and bigrams for predicting document clusters and sentence clusters ( sections 3. 1. 2 and 3. 2. 2 ).', 'because this variation was uniformly implemented for both approaches, it does not affect their relative performance.', 'these methodological variations are summarized in table 2']",5
"['adaptive greedy algorithm by  #TAUTHOR_TAG to penalize redundant sentences in cohesive clusters.', 'this is done by decre']","['adaptive greedy algorithm by  #TAUTHOR_TAG to penalize redundant sentences in cohesive clusters.', 'this is done by']","['redundant sentences.', 'after calculating the raw score of each sentence, we use a modified version of the adaptive greedy algorithm by  #TAUTHOR_TAG to penalize redundant sentences in cohesive clusters.', 'this is done by decre']","['redundant sentences.', 'after calculating the raw score of each sentence, we use a modified version of the adaptive greedy algorithm by  #TAUTHOR_TAG to penalize redundant sentences in cohesive clusters.', 'this is done by decrementing the score of a sentence that belongs to an sc for which there is a higher or equal scoring sentence ( if there are several highest - scoring sentences, we retain one sentence as a reference sentence - i. e., its score is not decremented ).', 'specifically, given a sentence s k in cluster sc l which contains a sentence with a higher or equal score, the contribution of sc l to score ( s k ) ( = pr ( sc l ) × pr ( s k | sc l ) ) is subtracted from score ( s k ).', 'after applying these penalties, we retain only the sentences whose adjusted score is greater than zero ( for a highly cohesive cluster, typically only one sentence remains )']",5
"['graph  #TAUTHOR_TAG,']","['graph  #TAUTHOR_TAG,']","['- matized ) corpus word in the document in question ( after removing stop - words and words with very low frequency ). 4', 'the predictive model is a decision graph  #TAUTHOR_TAG,']","['idea behind the doc - pred method is similar to bickel and scheffers ( 2004 ) : response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the requests features, and the response that is most representative of the predicted cluster ( closest to the centroid ) is selected.', 'in our case, the clustering is performed by the program snob, which implements mixture model - ing combined with model selection based on the minimum message length ( mml ) criterion ( wallace and boulton 1968 ; wallace 2005 ).', 'we chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters ( this interpretation is used by the sent - pred method, section 3. 2. 2 ).', 'the input to snob is a set of binary vectors, one vector per response document.', 'the values of a vector correspond to the presence or absence of each ( lem - matized ) corpus word in the document in question ( after removing stop - words and words with very low frequency ). 4', 'the predictive model is a decision graph  #TAUTHOR_TAG, which, like snob, is based on the mml principle.', 'the decision graph is trained on unigram and bigram lemmas in the request as input features, 5 and the identifier of the response cluster that contains the actual response for the request as the target feature.', 'the model predicts which response cluster is most suitable for a given re - quest, and returns the probability that this prediction is correct.', 'this probability is our indicator of whether the doc - pred method can address a new request.', 'as for the doc - ret method, an applicability threshold for this parameter is currently determined empirically ( table 3 )']",5
"['1995 ; watson 1997 ;  #TAUTHOR_TAG.', 'these']","['1995 ; watson 1997 ;  #TAUTHOR_TAG.', 'these']","['##r 1995 ; watson 1997 ;  #TAUTHOR_TAG.', 'these systems were carefully']","[', even the automation of responses to the "" easy "" problems is a difficult task.', 'although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer.', 'it is therefore no surprise that early attempts at response automation were knowledge - driven ( barr and tessler 1995 ; watson 1997 ;  #TAUTHOR_TAG.', 'these systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance ( delic and lahaix 1998 )']",0
['soffer 2000 ;  #TAUTHOR_TAG ; bic'],['soffer 2000 ;  #TAUTHOR_TAG ; bickel'],['soffer 2000 ;  #TAUTHOR_TAG ; bic'],"['are very few reported attempts at corpus - based automation of help - desk responses ( carmel, shtalhaim, and soffer 2000 ;  #TAUTHOR_TAG ; bickel and scheffer 2004 ; malik, subramaniam, and kaushik 2007 ).', '']",1
"[';  #TAUTHOR_TAG ; malik, subram']","[';  #TAUTHOR_TAG ; malik,']","['kosseim 2003 ;  #TAUTHOR_TAG ; malik, subram']","['are very few reported attempts at corpus - based automation of help - desk responses ( carmel, shtalhaim, and soffer 2000 ; lapalme and kosseim 2003 ;  #TAUTHOR_TAG ; malik, subramaniam, and kaushik 2007 ).', '']",1
"['techniques.', ' #TAUTHOR_TAG compared a predictive']","['techniques.', ' #TAUTHOR_TAG compared a predictive']","['latent variable models ).', 'jijkoun and de  #AUTHOR_TAG compared different variants of retrieval techniques.', ' #TAUTHOR_TAG compared a predictive approach ( statistical translation ),']","['faqs, employed a sentence retrieval approach based on a language model where the entire response to an faq is considered a sentence, and the questions and answers are embedded in an faq document.', 'they complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.', 'compared two retrieval approaches ( tf. idf and query expansion ) and two predictive approaches ( statistical translation and latent variable models ).', 'jijkoun and de  #AUTHOR_TAG compared different variants of retrieval techniques.', ' #TAUTHOR_TAG compared a predictive approach ( statistical translation ), a retrieval approach based on a language - model, and a hybrid approach which combines statistical chunking and traditional retrieval.', 'two significant differences between help - desk and faqs are the following']",1
"['as number of syntactic phrases, grammatical mood, and grammatical person  #TAUTHOR_TAG, but the simple binary bag - of - lemmas representation yielded similar results.', '7 we employed the libsvm package ( chang and lin 2001 ).', 'prediction stage, the svms predict zero or more scs for']","['as number of syntactic phrases, grammatical mood, and grammatical person  #TAUTHOR_TAG, but the simple binary bag - of - lemmas representation yielded similar results.', '7 we employed the libsvm package ( chang and lin 2001 ).', 'prediction stage, the svms predict zero or more scs for']","['as number of syntactic phrases, grammatical mood, and grammatical person  #TAUTHOR_TAG, but the simple binary bag - of - lemmas representation yielded similar results.', '7 we employed the libsvm package ( chang and lin 2001 ).', 'prediction stage, the svms predict zero or more scs for each request, as shown in figure 3.', 'we then']","[""use a support vector machine ( svm ) with a radial basis function kernel to predict scs from users'requests. 7"", 'a separate svm is trained for each sc, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the sc contains a sentence from the response to this request.', '6 for sent - pred we also experimented with grammatical and sentence - based syntactic features, such as number of syntactic phrases, grammatical mood, and grammatical person  #TAUTHOR_TAG, but the simple binary bag - of - lemmas representation yielded similar results.', '7 we employed the libsvm package ( chang and lin 2001 ).', 'prediction stage, the svms predict zero or more scs for each request, as shown in figure 3.', 'we then apply the following steps']",5
['by  #TAUTHOR_TAG as follows'],['by  #TAUTHOR_TAG as follows'],"['by  #TAUTHOR_TAG as follows.', 'the merging category corresponds to techniques']","['addition to the different response - generation methods, we have proposed a metalevel strategy to combine them.', 'this kind of meta - learning is referred to as stacking by the data mining community ( witten and frank 2000 ).', ' #AUTHOR_TAG implemented a supervised version of this approach for a recommender system, as opposed to our unsupervised version.', 'they also proposed two major categories of meta - learning approaches for recommender systems, merging and ensemble, each subdivided into the more specific subclasses suggested by  #TAUTHOR_TAG as follows.', ""the merging category corresponds to techniques where the individual methods affect each other in different ways ( this category encompasses burke's feature combination, cascade, feature augmentation, and meta - level sub - categories )."", ""the ensemble category corresponds to techniques where the predictions of the individual methods are combined to produce a final prediction ( this category encompasses burke's weighted, switching, and mixed sub - categories )""]",0
"['system  #TAUTHOR_TAG b ).', 'however, our more comprehensive evaluation is an automatic one that treats the responses']","['system  #TAUTHOR_TAG b ).', 'however, our more comprehensive evaluation is an automatic one that treats the responses']","['system  #TAUTHOR_TAG b ).', 'however, our more comprehensive evaluation is an automatic one that treats the responses']","['experimental set - up is designed to evaluate the ability of the different responsegeneration methods to address unseen request e - mails.', 'in particular, we want to determine the applicability of our methods to different situations, namely, whether different requests are addressed only by some methods, or whether there is a significant overlap between the methods.', 'our evaluation is performed by measuring the quality of the generated responses.', 'quality is a subjective measure, which is best judged by the users of the system ( i. e., the help - desk customers or operators ).', 'in section 5, we discuss the difficulties associated with such user studies, and describe a human - based evaluation we conducted for a small subset of the responses generated by our system  #TAUTHOR_TAG b ).', 'however, our more comprehensive evaluation is an automatic one that treats the responses generated by the help - desk operators as model responses, and performs text - based comparisons between the model responses and the automatically generated ones']",5
"[';  #TAUTHOR_TAG ; delic and lahaix 1998 ).', 'these systems were carefully']","[';  #TAUTHOR_TAG ; delic and lahaix 1998 ).', 'these systems were carefully']","[';  #TAUTHOR_TAG ; delic and lahaix 1998 ).', 'these systems were carefully']","[', even the automation of responses to the "" easy "" problems is a difficult task.', 'although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer.', 'it is therefore no surprise that early attempts at response automation were knowledge - driven ( barr and tessler 1995 ;  #TAUTHOR_TAG ; delic and lahaix 1998 ).', 'these systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance ( delic and lahaix 1998 )']",0
"['different retrieval models.', ' #TAUTHOR_TAG compared two']","['different retrieval models.', ' #TAUTHOR_TAG compared two']","['different retrieval models.', ' #TAUTHOR_TAG compared']","['faqs, employed a sentence retrieval approach based on a language model where the entire response to an faq is considered a sentence, and the questions and answers are embedded in an faq document.', 'they complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.', ' #TAUTHOR_TAG compared two retrieval approaches ( tf. idf and query expansion ) and two predictive approaches ( statistical translation and latent variable models ).', 'jijkoun and de  #AUTHOR_TAG compared different variants of retrieval techniques.', ' #AUTHOR_TAG compared a predictive approach ( statistical translation ), a retrieval approach based on a language - model, and a hybrid approach which combines statistical chunking and traditional retrieval.', 'two significant differences between help - desk and faqs are the following']",0
"[' #TAUTHOR_TAG, one approach for achieving this objective consists of applying supervised']","[' #TAUTHOR_TAG, one approach for achieving this objective consists of applying supervised']","[' #TAUTHOR_TAG, one approach for achieving this objective consists of applying supervised']","[' #TAUTHOR_TAG, one approach for achieving this objective consists of applying supervised learning, where a winning method is selected for each case in the training set, all the training cases are labeled accordingly, and then the system is trained to predict a winner for unseen cases.', 'however, in our situation, there is not always one single winner ( two methods can perform similarly well for a given request ), and there are different ways to pick winners ( for example, based on f - score or precision ).', 'therefore, such an approach would require the utilization of subjective heuristics for creating labels, which would significantly influence what is being learned.', 'instead, we adopt an unsupervised approach that finds patterns in the data - confidence values coupled with performance scores ( section 6. 1 ) - and then attempts to fit unseen data to these patterns ( section 6. 2 ).', 'heuristics are still needed in order to decide which response - generation method to apply to an unseen case, but they are applied only after the learning is complete ( section 6. 3 ).', 'in other words, the subjective process of setting performance criteria ( which should be conducted by the organization running the helpdesk ) does not influence the machine learning process']",1
['##¢ only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( lapalme and kosseim 2003 ;  #TAUTHOR_TAG'],['##¢ only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( lapalme and kosseim 2003 ;  #TAUTHOR_TAG'],['##¢ only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( lapalme and kosseim 2003 ;  #TAUTHOR_TAG'],['##¢ only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( lapalme and kosseim 2003 ;  #TAUTHOR_TAG'],1
"['. 2000 ; barzilay, elhadad, and mckeown 2001 ;  #TAUTHOR_TAG']","['al. 2000 ; barzilay, elhadad, and mckeown 2001 ;  #TAUTHOR_TAG']","['. 2000 ; barzilay, elhadad, and mckeown 2001 ;  #TAUTHOR_TAG']","['', 'this task can be cast as extractive multi - document summarization.', 'unlike a document reuse approach, sentence - level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable.', 'in our work, we gather sets of sentences, and assume ( but do not employ ) existing approaches for their organization ( goldstein et al. 2000 ; barzilay, elhadad, and mckeown 2001 ;  #TAUTHOR_TAG']",0
"[';  #TAUTHOR_TAG,']","[';  #TAUTHOR_TAG,']","['- mail threads ( dalli, xia, and wilks 2004 ;  #TAUTHOR_TAG,']","['applications that, like help - desk, deal with question - - answer pairs are : summarization of e - mail threads ( dalli, xia, and wilks 2004 ;  #TAUTHOR_TAG, and answer extraction in faqs ( frequently asked questions ) ( berger and mittal 2000 ; soricut and brill 2006 ).', 'an important difference between these applications and help - desk is that help - desk request e - mails are not simple queries.', 'in fact, some e - mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information.', 'therefore, the generation of a help - desk response needs to consider a request e - mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses']",1
"['overall insights of this research.', 'specifically, we used decision graphs  #TAUTHOR_TAG']","['overall insights of this research.', 'specifically, we used decision graphs  #TAUTHOR_TAG']","['the overall insights of this research.', 'specifically, we used decision graphs  #TAUTHOR_TAG']","['focus of our work is on the general applicability of the different response automation methods, rather than on comparing the performance of particular implementation techniques.', 'hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research.', 'specifically, we used decision graphs  #TAUTHOR_TAG for doc - pred, and svms ( vapnik 1998 ) for sent - pred. 11 additionally, we used unigrams for clustering documents and sentences, and unigrams and bigrams for predicting document clusters and sentence clusters ( sections 3. 1. 2 and 3. 2. 2 ).', 'because this variation was uniformly implemented for both approaches, it does not affect their relative performance.', 'these methodological variations are summarized in table 2']",5
"['we also employed sequence - based measures using the rouge tool set  #TAUTHOR_TAG, with similar results to']","['we also employed sequence - based measures using the rouge tool set  #TAUTHOR_TAG, with similar results to']","['we also employed sequence - based measures using the rouge tool set  #TAUTHOR_TAG, with similar results to']","['we also employed sequence - based measures using the rouge tool set  #TAUTHOR_TAG, with similar results to those obtained with the word - by - word measures']",5
"['subsequent performance, reflected by precision and recall ( equations ( 7 ) and ( 8 ), respectively ).', 'we then use the program snob  #TAUTHOR_TAG ; wallace 2005 ) to cluster these experiences.', '']","['subsequent performance, reflected by precision and recall ( equations ( 7 ) and ( 8 ), respectively ).', 'we then use the program snob  #TAUTHOR_TAG ; wallace 2005 ) to cluster these experiences.', '']","['subsequent performance, reflected by precision and recall ( equations ( 7 ) and ( 8 ), respectively ).', 'we then use the program snob  #TAUTHOR_TAG ; wallace 2005 ) to cluster these experiences.', '']","['train the system by clustering the "" experiences "" of the response - generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall ( equations ( 7 ) and ( 8 ), respectively ).', 'we then use the program snob  #TAUTHOR_TAG ; wallace 2005 ) to cluster these experiences.', 'figure 8 ( a ) is a projection of the centroids of the clusters produced by snob into the three most significant dimensions discovered by principal component analysis ( pca ) - these dimensions account for 95 % of the variation in the data.', 'shows the ( unprojected ) centroid values of three of the clusters ( the top part of the figure will be discussed subsequently ). 15', 'these clusters were chosen because they illustrate clearly three situations of interest']",5
"['system  #TAUTHOR_TAG a ).', 'our judges were']","['system  #TAUTHOR_TAG a ).', 'our judges were']","['system  #TAUTHOR_TAG a ).', 'our judges were instructed to position']","['order to address these limitations in a practical way, we conducted a small user study where we asked four judges ( graduate students from the faculty of information technology at monash university ) to assess the responses generated by our system  #TAUTHOR_TAG a ).', '']",5
['developed by  #TAUTHOR_TAG belongs to the merging'],['developed by  #TAUTHOR_TAG belongs to the merging'],"['developed by  #TAUTHOR_TAG belongs to the merging category of approaches,']","['system falls into the ensemble category, because it combines the results of the various methods into a single outcome.', ""more specifically, it belongs to burke's switching sub - category, where a single method is selected on a case - by - case basis."", 'a similar approach is taken in  #AUTHOR_TAG reading comprehension system, but their system does not perform any learning.', 'instead it uses a voting mechanism to select the answer given by the majority of methods.', ""the question answering system developed by  #TAUTHOR_TAG belongs to the merging category of approaches, where the output of an individual method can be used as input to a different method ( this corresponds to burke's cascade sub - category )."", '']",1
['of paraphrases  #TAUTHOR_TAG ; and joint unsupervised pos and parser induction across languages ( snyder and barzilay'],['of paraphrases  #TAUTHOR_TAG ; and joint unsupervised pos and parser induction across languages ( snyder and barzilay'],['of paraphrases  #TAUTHOR_TAG ; and joint unsupervised pos and parser induction across languages ( snyder and barzilay 2008 )'],"['seminal work of  #AUTHOR_TAG b ) introduced a series of probabilistic models ( ibm models 1 - 5 ) for statistical machine translation and the concept of "" word - byword "" alignment, the correspondence between words in source and target languages.', 'although no longer competitive as end - to - end translation models, the ibm models, as well as the hidden markov model ( hmm ) of  #AUTHOR_TAG, are still widely used for word alignment.', 'word alignments are used primarily for extracting minimal translation units for machine translation ( mt ) ( e. g., phrases [ koehn, och, and marcu 2003 ] and rules [ galley et al. 2004 ; chiang et al. 2005 ] ) as well as for mt system combination ( matusov, ueffing, and ney 2006 ).', 'but their importance has grown far beyond machine translation : for instance, transferring annotations between languages ( yarowsky and ngai 2001 ; hwa et al. 2005 ; ganchev, gillenwater, and taskar 2009 ) ; discovery of paraphrases  #TAUTHOR_TAG ; and joint unsupervised pos and parser induction across languages ( snyder and barzilay 2008 )']",4
"['##l and wright 1999 ).', 'here, pv ( a ) represents an ascent direction chosen as follows : for inequality constraints, it is the projected gradient  #TAUTHOR_TAG ; for equality constraints with slack, we use conjugate gradient ( noc']","['( nocedal and wright 1999 ).', 'here, pv ( a ) represents an ascent direction chosen as follows : for inequality constraints, it is the projected gradient  #TAUTHOR_TAG ; for equality constraints with slack, we use conjugate gradient ( nocedal and wright 1999 ), noting that when a = 0, the objective is not differentiable.', 'in practice this only happens at the start of optimization and we use a sub - gradient for the first direction']","['##l and wright 1999 ).', 'here, pv ( a ) represents an ascent direction chosen as follows : for inequality constraints, it is the projected gradient  #TAUTHOR_TAG ; for equality constraints with slack, we use conjugate gradient ( noc']","['the projection step uses the same inference algorithm ( forward - backward for hmms ) to compute the gradient, only modifying the local factors using the current setting of λ.', 'we optimize the dual objective using the gradient based methods shown in algorithm 1.', ""here η is an optimization precision, α is a step size chosen with the strong wolfe's rule ( nocedal and wright 1999 )."", 'here, pv ( a ) represents an ascent direction chosen as follows : for inequality constraints, it is the projected gradient  #TAUTHOR_TAG ; for equality constraints with slack, we use conjugate gradient ( nocedal and wright 1999 ), noting that when a = 0, the objective is not differentiable.', 'in practice this only happens at the start of optimization and we use a sub - gradient for the first direction']",5
"['to a poor estimate of the phrase probabilities.', 'see  #TAUTHOR_TAG for further discussion']","['to a poor estimate of the phrase probabilities.', 'see  #TAUTHOR_TAG for further discussion']","['to a poor estimate of the phrase probabilities.', 'see  #TAUTHOR_TAG for further discussion']","['', 'the two constraints help to a different extent for different corpora and translation directions, in a somewhat unpredictable manner.', 'in general our impression is that the connection between alignment quality and bleu scores is complicated, and changes are difficult to explain and justify.', 'the number of iterations for mert optimization to converge varied from 2 to 28 ; and the best choice of threshold on the development set did not always correspond to the best on the test set.', 'contrary to conventional wisdom in the mt community, bigger phrase tables did not always perform better.', 'in 14 out of 18 cases, the threshold picked was 0. 4 ( medium size phrase tables ) and the other four times 0. 2 was picked ( smaller phrase tables ).', 'when we include only high confidence alignments, more phrases are extracted but many of these are erroneous.', 'potentially this leads to a poor estimate of the phrase probabilities.', 'see  #TAUTHOR_TAG for further discussion']",0
"['are difficult to implement and extend.', 'many researchers use the giza + + software package  #TAUTHOR_TAG as a black box, selecting ibm model 4 as a compromise between alignment quality and efficiency.', '']","['are difficult to implement and extend.', 'many researchers use the giza + + software package  #TAUTHOR_TAG as a black box, selecting ibm model 4 as a compromise between alignment quality and efficiency.', '']","['are difficult to implement and extend.', 'many researchers use the giza + + software package  #TAUTHOR_TAG as a black box, selecting ibm model 4 as a compromise between alignment quality and efficiency.', '']","['models 1 and 2 and the hmm are simple and tractable probabilistic models, which produce the target sentence one target word at a time by choosing a source word and generating its translation.', 'ibm models 3, 4, and 5 attempt to capture fertility ( the tendency of each source word to generate several target words ), resulting in probabilistically deficient, intractable models that require local heuristic search and are difficult to implement and extend.', 'many researchers use the giza + + software package  #TAUTHOR_TAG as a black box, selecting ibm model 4 as a compromise between alignment quality and efficiency.', 'all of the models are asymmetric ( switching target and source languages produces drastically different results ) and the simpler models ( ibm models 1, 2, and hmm ) do not enforce bijectivity ( the majority of words translating as a single word ).', 'although there are systematic translation phenomena where one cannot hope to obtain 1 - to - 1 alignments, we observe that in over 6 different european language pairs the majority of alignments are in fact 1 - to - 1 ( 86 - 98 % ).', 'this leads to the common practice of post - processing heuristics for intersecting directional alignments to produce nearly bijective and symmetric results ( koehn, och, and marcu 2003 )']",0
"[' #TAUTHOR_TAG.', 'here, β∇ ( λ ) represents an ascent direction chosen as follows : for inequality constraints, it is the projected']","[' #TAUTHOR_TAG.', 'here, β∇ ( λ ) represents an ascent direction chosen as follows : for inequality constraints, it is the projected']","['- backward for hmms ) to compute the gradient, only modifying the local factors using the current setting of λ.', 'we optimize the dual objective using the gradient based methods shown in algorithm 1.', ""here 11 is an optimization precision, oc is a step size chosen with the strong wolfe's rule  #TAUTHOR_TAG."", 'here, β∇ ( λ ) represents an ascent direction chosen as follows : for inequality constraints, it is the projected gradient ( bertsekas 1999 ) ; for equality constraints with slack, we use conjugate gradient ( nocedal and wright 1999 ), noting that when λ = 0, the objective is not differentiable.', 'in practice this only happens']","['the projection step uses the same inference algorithm ( forward - backward for hmms ) to compute the gradient, only modifying the local factors using the current setting of λ.', 'we optimize the dual objective using the gradient based methods shown in algorithm 1.', ""here 11 is an optimization precision, oc is a step size chosen with the strong wolfe's rule  #TAUTHOR_TAG."", 'here, β∇ ( λ ) represents an ascent direction chosen as follows : for inequality constraints, it is the projected gradient ( bertsekas 1999 ) ; for equality constraints with slack, we use conjugate gradient ( nocedal and wright 1999 ), noting that when λ = 0, the objective is not differentiable.', 'in practice this only happens at the start of optimization and we use a sub - gradient for the first direction']",5
"['leaf model  #TAUTHOR_TAG.', 'unfortunately, these changes']","['leaf model  #TAUTHOR_TAG.', 'unfortunately, these changes']","['the leaf model  #TAUTHOR_TAG.', 'unfortunately, these changes']","['alignment models in general and the hmm in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments.', 'one solution to this problem is to add more complexity to the model to better reflect the translation process.', 'this is the approach taken by ibm models 4 + ( brown et al. 1993b ; och and ney 2003 ), and more recently by the leaf model  #TAUTHOR_TAG.', 'unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.', ""instead, we propose to use a learning framework called posterior regularization ( graca, ganchev, and taskar 2007 ) that incorporates side information into unsupervised estimation in the form of constraints on the model's posteriors."", 'the constraints are expressed as inequalities on the expected values under the posterior distribution of user - defined constraint features ( not necessarily the same features used by the model ).', 'because in most applications what we are interested in are the latent variables ( in this case the alignments ), constraining the posteriors allows a more direct way to achieve the desired behavior.', 'on the other hand, constraining the expected value of the features instead of adding them to the model allows us to express features that would otherwise make the model intractable.', 'for example, enforcing that each hidden state of an hmm model should be used at most once per sentence would break the markov property and make the model intractable.', 'in contrast, we will show how to enforce the constraint that each hidden state is used at most once in expectation.', 'the underlying model remains unchanged, but the learning method changes.', 'during learning, our method is similar to the em algorithm with the addition of solving an optimization problem similar to a maximum entropy problem inside the e step.', 'the following subsections present the posterior regularization framework, followed by a description of how to encode two pieces of prior information aimed at solving the problems described at the end of section 2']",1
['the open source moses  #TAUTHOR_TAG toolkit from'],['the open source moses  #TAUTHOR_TAG toolkit from www. statmt. org / moses /'],['the open source moses  #TAUTHOR_TAG toolkit from'],['the open source moses  #TAUTHOR_TAG toolkit from www. statmt. org / moses /'],5
"['##les  #TAUTHOR_TAG, and are consequently shorter sentences, whereas the en a es results are based on a corpus of parliamentary proceedings ( ko']","['( en→es ).', 'results are based on a corpus of movie subtitles  #TAUTHOR_TAG, and are consequently shorter sentences, whereas the en a es results are based on a corpus of parliamentary proceedings ( koehn 2005 ).', 'we see in figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using em']","[').', 'results are based on a corpus of movie subtitles  #TAUTHOR_TAG, and are consequently shorter sentences, whereas the en a es results are based on a corpus of parliamentary proceedings ( ko']","['this section, we compare the different alignments produced with and without pr based on how well they can be used for transfer of linguistic resources across languages.', 'we used the system proposed by  #AUTHOR_TAG.', 'this system uses a word - aligned corpus and a parser for a resource - rich language ( source language ) in order to create a parser for a resource - poor language ( target language ).', 'we consider a parse tree on the source language as a set of dependency edges to be transferred.', 'for each such edge, if both end points are aligned to words in the target language, then the edge is transferred.', 'these edges are then used as weak supervision when training a generative or discriminative dependency parser.', 'in order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised parse trees on the target side.', 'by changing the threshold in mbr decoding of alignments, we can trade off accuracy of the transferred edges vs. transferring more edges.', 'we generated supervised parses using the first - order model from the mst parser ( mcdonald, crammer, and pereira 2005 ) trained on the penn treebank for english and the conll x parses for bulgarian and spanish.', ' #AUTHOR_TAG, we filter alignment links between words with incompatible pos tags. figure 10 shows our results for transferring from english to bulgarian ( en→bg ) and from english to spanish ( en→es ).', 'results are based on a corpus of movie subtitles  #TAUTHOR_TAG, and are consequently shorter sentences, whereas the en a es results are based on a corpus of parliamentary proceedings ( koehn 2005 ).', 'we see in figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using em']",5
[';  #TAUTHOR_TAG ] ) as well as'],[';  #TAUTHOR_TAG ] ) as well as'],[';  #TAUTHOR_TAG ] ) as well as'],"['seminal work of  #AUTHOR_TAG b ) introduced a series of probabilistic models ( ibm models 1 - 5 ) for statistical machine translation and the concept of "" word - byword "" alignment, the correspondence between words in source and target languages.', 'although no longer competitive as end - to - end translation models, the ibm models, as well as the hidden markov model ( hmm ) of  #AUTHOR_TAG, are still widely used for word alignment.', 'word alignments are used primarily for extracting minimal translation units for machine translation ( mt ) ( e. g., phrases [ koehn, och, and marcu 2003 ] and rules [ galley et al. 2004 ;  #TAUTHOR_TAG ] ) as well as for mt system combination ( matusov, ueffing, and ney 2006 ).', 'but their importance has grown far beyond machine translation : for instance, transferring annotations between languages ( yarowsky and ngai 2001 ; hwa et al. 2005 ; ganchev, gillenwater, and taskar 2009 ) ; discovery of paraphrases ( bannard and callison - burch 2005 ) ; and joint unsupervised pos and parser induction across languages ( snyder and barzilay 2008 )']",0
"['use minimum bayes - risk ( mbr ) decoding  #TAUTHOR_TAG ; liang,']","['use minimum bayes - risk ( mbr ) decoding  #TAUTHOR_TAG ; liang, taskar, and klein 2006 ; ganchev, and taskar']","['use minimum bayes - risk ( mbr ) decoding  #TAUTHOR_TAG ; liang,']","['possibility that often works better is to use minimum bayes - risk ( mbr ) decoding  #TAUTHOR_TAG ; liang, taskar, and klein 2006 ; ganchev, and taskar 2007 ).', 'using this decoding we include an alignment link i − j if the posterior probability that word i aligns to word j is above some threshold.', 'this allows the accumulation of probability from several low - scoring alignments that agree on one alignment link.', 'the threshold is tuned on some small amount of labeled data - in our case the development set - to minimize some loss.', ' #AUTHOR_TAG study different loss functions that incorporate linguistic knowledge, and show significant improvement over likelihood decoding.', 'note that this could potentially result in an alignment having zero probability under the model, as many - to - many alignments can be produced in this way.', 'mbr decoding has several advantages over viterbi decoding.', 'first, independently of the particular choice of the loss function, by picking a specific threshold we can trade off precision and recall of the predicted word alignments.', 'in fact, in this work when comparing different alignment sets we do not commit to any loss function but instead compare precision vs recall curves, by generating alignments for different thresholds ( 0.. 1 ).', 'second, with this method we can ignore the null word probabilities, which tend to be poorly estimated']",5
"['similar improvements.', 'we used a standard implementation of ibm model 4  #TAUTHOR_TAG and']","['similar improvements.', 'we used a standard implementation of ibm model 4  #TAUTHOR_TAG and']","['intractable inference.', 'because our approach is orthogonal to the base model used, the constraints described here could be applied in principle to ibm model 4 if exact inference was efficient, hopefully yielding similar improvements.', 'we used a standard implementation of ibm model 4  #TAUTHOR_TAG and']","['intent of this experimental section is to evaluate the gains from using constraints during learning, hence the main comparison is between hmm trained with normal em vs. trained with pr plus constraints.', 'we also report results for ibm model 4, because it is often used as the default word alignment model, and can be used as a reference.', 'however, we would like to note that ibm model 4 is a more complex model, able to capture more structure, albeit at the cost of intractable inference.', 'because our approach is orthogonal to the base model used, the constraints described here could be applied in principle to ibm model 4 if exact inference was efficient, hopefully yielding similar improvements.', 'we used a standard implementation of ibm model 4  #TAUTHOR_TAG and because changing the existing code is not trivial, we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision / recall curves.', 'we trained ibm model 4 using the default configuration of']",5
[') ; and joint unsupervised pos and parser induction across languages  #TAUTHOR_TAG'],[') ; and joint unsupervised pos and parser induction across languages  #TAUTHOR_TAG'],['of paraphrases ( bannard and callison - burch 2005 ) ; and joint unsupervised pos and parser induction across languages  #TAUTHOR_TAG'],"['seminal work of  #AUTHOR_TAG b ) introduced a series of probabilistic models ( ibm models 1 - 5 ) for statistical machine translation and the concept of "" word - byword "" alignment, the correspondence between words in source and target languages.', 'although no longer competitive as end - to - end translation models, the ibm models, as well as the hidden markov model ( hmm ) of  #AUTHOR_TAG, are still widely used for word alignment.', 'word alignments are used primarily for extracting minimal translation units for machine translation ( mt ) ( e. g., phrases [ koehn, och, and marcu 2003 ] and rules [ galley et al. 2004 ; chiang et al. 2005 ] ) as well as for mt system combination ( matusov, ueffing, and ney 2006 ).', 'but their importance has grown far beyond machine translation : for instance, transferring annotations between languages ( yarowsky and ngai 2001 ; hwa et al. 2005 ; ganchev, gillenwater, and taskar 2009 ) ; discovery of paraphrases ( bannard and callison - burch 2005 ) ; and joint unsupervised pos and parser induction across languages  #TAUTHOR_TAG']",4
"['predictions  #TAUTHOR_TAG.', 'however, we show that it is much better to train']","['predictions  #TAUTHOR_TAG.', 'however, we show that it is much better to train']","['predictions  #TAUTHOR_TAG.', 'however, we show that it is much better to train two directional models concurrently, coupling']","['directional nature of the generative models used to recover word alignments conflicts with their interpretation as translations.', 'in practice, we see that the choice of which language is source versus target matters and changes the mistakes made by the model ( the first row of panels in figure 1 ).', 'the standard approach is to train two models independently and then intersect their predictions  #TAUTHOR_TAG.', 'however, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree.', 'let the directional models be defined as : − → p ( − → z ) ( source - target ) and ← − p ( ← − z ) ( target - source ).', 'we suppress dependence on x and y for brevity.', 'define z to range over the union of all possible directional alignments − → z ∪ ← − z.', 'we define a mixture model']",1
"['( q, 0 ) using an auxiliary distribution over the latent variables q ( z | x, y )  #TAUTHOR_TAG']","['( q, 0 ) using an auxiliary distribution over the latent variables q ( z | x, y )  #TAUTHOR_TAG']","['##ter, laird, and rubin 1977 ).', 'em maximizes g ( 0 ) via block - coordinate ascent on a lower bound f ( q, 0 ) using an auxiliary distribution over the latent variables q ( z | x, y )  #TAUTHOR_TAG']","['of the latent alignment variables z, the log - likelihood function for the hmm model is not concave, and the model is fit using the expectation maximization ( em ) algorithm ( dempster, laird, and rubin 1977 ).', 'em maximizes g ( 0 ) via block - coordinate ascent on a lower bound f ( q, 0 ) using an auxiliary distribution over the latent variables q ( z | x, y )  #TAUTHOR_TAG']",5
"['##b ;  #TAUTHOR_TAG,']","['al. 1993b ;  #TAUTHOR_TAG,']","['##b ;  #TAUTHOR_TAG,']","['alignment models in general and the hmm in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments.', 'one solution to this problem is to add more complexity to the model to better reflect the translation process.', 'this is the approach taken by ibm models 4 + ( brown et al. 1993b ;  #TAUTHOR_TAG, and more recently by the leaf model ( fraser and marcu 2007 ).', 'unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.', ""instead, we propose to use a learning framework called posterior regularization ( graca, ganchev, and taskar 2007 ) that incorporates side information into unsupervised estimation in the form of constraints on the model's posteriors."", 'the constraints are expressed as inequalities on the expected values under the posterior distribution of user - defined constraint features ( not necessarily the same features used by the model ).', 'because in most applications what we are interested in are the latent variables ( in this case the alignments ), constraining the posteriors allows a more direct way to achieve the desired behavior.', 'on the other hand, constraining the expected value of the features instead of adding them to the model allows us to express features that would otherwise make the model intractable.', 'for example, enforcing that each hidden state of an hmm model should be used at most once per sentence would break the markov property and make the model intractable.', 'in contrast, we will show how to enforce the constraint that each hidden state is used at most once in expectation.', 'the underlying model remains unchanged, but the learning method changes.', 'during learning, our method is similar to the em algorithm with the addition of solving an optimization problem similar to a maximum entropy problem inside the e step.', 'the following subsections present the posterior regularization framework, followed by a description of how to encode two pieces of prior information aimed at solving the problems described at the end of section 2']",1
"['idea of introducing constraints over a model to better guide the learning process has appeared before.', 'in the context of word alignment,  #TAUTHOR_TAG use a state - duration hmm in order to model']","['idea of introducing constraints over a model to better guide the learning process has appeared before.', 'in the context of word alignment,  #TAUTHOR_TAG use a state - duration hmm in order to model word - to - phrase translations.', 'the fertility of each source word is implicitly encoded in the durations of the hmm states.', 'without any restrictions, likelihood prefers']","['idea of introducing constraints over a model to better guide the learning process has appeared before.', 'in the context of word alignment,  #TAUTHOR_TAG use a state - duration hmm in order to model']","['idea of introducing constraints over a model to better guide the learning process has appeared before.', 'in the context of word alignment,  #TAUTHOR_TAG use a state - duration hmm in order to model word - to - phrase translations.', 'the fertility of each source word is implicitly encoded in the durations of the hmm states.', 'without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant η > 1.', 'this encourages more transitions and hence shorter phrases.', 'for the task of unsupervised dependency parsing,  #AUTHOR_TAG add a constraint of the form "" the average length of dependencies should be x "" to capture the locality of syntax ( at least half of the dependencies are between adjacent words ), using a scheme they call structural annealing.', ""they modify the model's distribution over trees p θ ( y ) by a penalty term as : p θ ( y ) ∝ p θ ( y ) e ( δ e∈y length ( e ) ), where length ( e ) is the surface length of edge e."", 'the factor δ changes from a high value to a lower one so that the preference for short edges ( hence a smaller sum ) is stronger at the start of training.', 'these two approaches also have the goal of controlling unsupervised learning, and the form of the modified distributions is reminiscent of the form that the projected posteriors take.', 'however, the approaches differ substantially from pr.  #AUTHOR_TAG make a statement of the form "" scale the total length of edges "", which depending on the value of δ will prefer to have more shorter / longer edges.', 'such statements are not data dependent.', 'depending on the value of δ, for instance if δ ≤ 0, even if the data is such that the model already uses too many short edges on average, this value of δ will push for more short edges.', 'by contrast the statements we can make in pr are of the form "" there should be more short edges than long edges "".', 'such a statement is data - dependent in the sense that if the model satisfies the constraints then we do not need to change it ; if it is far from satisfying it we might need to make very dramatic changes']",0
"['), and are consequently shorter sentences, whereas the en a es results are based on a corpus of parliamentary proceedings  #TAUTHOR_TAG.', 'we see in figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using em']","['), and are consequently shorter sentences, whereas the en a es results are based on a corpus of parliamentary proceedings  #TAUTHOR_TAG.', 'we see in figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using em']","[').', 'results are based on a corpus of movie subtitles ( tiedemann 2007 ), and are consequently shorter sentences, whereas the en a es results are based on a corpus of parliamentary proceedings  #TAUTHOR_TAG.', 'we see in figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using em']","['this section, we compare the different alignments produced with and without pr based on how well they can be used for transfer of linguistic resources across languages.', 'we used the system proposed by  #AUTHOR_TAG.', 'this system uses a word - aligned corpus and a parser for a resource - rich language ( source language ) in order to create a parser for a resource - poor language ( target language ).', 'we consider a parse tree on the source language as a set of dependency edges to be transferred.', 'for each such edge, if both end points are aligned to words in the target language, then the edge is transferred.', 'these edges are then used as weak supervision when training a generative or discriminative dependency parser.', 'in order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised parse trees on the target side.', 'by changing the threshold in mbr decoding of alignments, we can trade off accuracy of the transferred edges vs. transferring more edges.', 'we generated supervised parses using the first - order model from the mst parser ( mcdonald, crammer, and pereira 2005 ) trained on the penn treebank for english and the conll x parses for bulgarian and spanish.', ' #AUTHOR_TAG, we filter alignment links between words with incompatible pos tags. figure 10 shows our results for transferring from english to bulgarian ( en→bg ) and from english to spanish ( en→es ).', 'results are based on a corpus of movie subtitles ( tiedemann 2007 ), and are consequently shorter sentences, whereas the en a es results are based on a corpus of parliamentary proceedings  #TAUTHOR_TAG.', 'we see in figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using em']",5
"['soft union  #TAUTHOR_TAG.', 'figure 8 shows the']","['soft union  #TAUTHOR_TAG.', 'figure 8 shows the']","['', 'this heuristic is called soft union  #TAUTHOR_TAG.', 'figure 8 shows']","['discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair.', 'typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment.', 'for mt the most commonly used heuristic is called grow diagonal final ( och and ney 2003 ).', 'this starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points.', 'the alignment produced has high recall relative to the intersection and only slightly lower recall than the union.', 'in syntax transfer the intersection heuristic is normally used, because one wants to have high precision links to transfer knowledge between languages.', 'one pitfall of these symmetrization heuristics is that they can obfuscate the link between the original alignment and the ones used for a specific task, making errors more difficult to analyze.', 'because they are heuristics tuned for a particular phrasebased translation system, it is not clear when they will help and when they will hinder system performance.', 'in this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model.', 'we include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold.', 'this heuristic is called soft union  #TAUTHOR_TAG.', 'figure 8 shows the precision / recall curves after symmetrization for the en - fr corpus.', 'the posterior regularization - trained models still performed better, but the differences get smaller after doing the symmetrization.', 'this should not be very surprising, because the soft union symmetrization can be viewed as an approximation of our symmetry constraint applied only at decode time.', 'applying the symmetrization to the model with symmetry constraints does not affect performance']",5
"['ngai 2001 ;  #TAUTHOR_TAG ; ganchev,']","['ngai 2001 ;  #TAUTHOR_TAG ; ganchev,']","['ngai 2001 ;  #TAUTHOR_TAG ; ganchev,']","['seminal work of  #AUTHOR_TAG b ) introduced a series of probabilistic models ( ibm models 1 - 5 ) for statistical machine translation and the concept of "" word - byword "" alignment, the correspondence between words in source and target languages.', 'although no longer competitive as end - to - end translation models, the ibm models, as well as the hidden markov model ( hmm ) of  #AUTHOR_TAG, are still widely used for word alignment.', 'word alignments are used primarily for extracting minimal translation units for machine translation ( mt ) ( e. g., phrases [ koehn, och, and marcu 2003 ] and rules [ galley et al. 2004 ; chiang et al. 2005 ] ) as well as for mt system combination ( matusov, ueffing, and ney 2006 ).', 'but their importance has grown far beyond machine translation : for instance, transferring annotations between languages ( yarowsky and ngai 2001 ;  #TAUTHOR_TAG ; ganchev, gillenwater, and taskar 2009 ) ; discovery of paraphrases ( bannard and callison - burch 2005 ) ; and joint unsupervised pos and parser induction across languages ( snyder and barzilay 2008 )']",4
"['), and expressions translated indirectly.', 'due to this inherent ambiguity, manual annotations usually distinguish between sure correspondences for unambiguous translations, and possible, for ambiguous translations  #TAUTHOR_TAG.', 'the top row of']","['english weapons of mass destruction and german massenvernichtungswaffen ), and expressions translated indirectly.', 'due to this inherent ambiguity, manual annotations usually distinguish between sure correspondences for unambiguous translations, and possible, for ambiguous translations  #TAUTHOR_TAG.', 'the top row of']","['english weapons of mass destruction and german massenvernichtungswaffen ), and expressions translated indirectly.', 'due to this inherent ambiguity, manual annotations usually distinguish between sure correspondences for unambiguous translations, and possible, for ambiguous translations  #TAUTHOR_TAG.', 'the top row of figure 1 shows two word alignments between an english - french sentence pair.', 'we use the following notation : the alignment']","['word alignment for a parallel sentence pair represents the correspondence between words in a source language and their translations in a target language ( brown et al. 1993b ).', 'there are many reasons why a simple word - to - word ( 1 - to - 1 ) correspondence is not possible for every sentence pair : for instance, auxiliary verbs used in one language but not the other ( e. g., english he walked and french il est alle ), articles required in one language but optional in the other ( e. g., english cars use gas and portuguese os carros usam gasolina ), cases where the content is expressed using multiple words in one language and a single word in the other language ( e. g., agglutination such as english weapons of mass destruction and german massenvernichtungswaffen ), and expressions translated indirectly.', 'due to this inherent ambiguity, manual annotations usually distinguish between sure correspondences for unambiguous translations, and possible, for ambiguous translations  #TAUTHOR_TAG.', 'the top row of figure 1 shows two word alignments between an english - french sentence pair.', 'we use the following notation : the alignment on the left ( right ) will be referenced as source - target ( target - source ) and contains source ( target ) words as rows and target ( source ) words as columns.', 'each entry in the matrix corresponds to a source - target word pair, and is the candidate for an alignment link.', 'sure links are represented as squares with borders, and possible']",0
"['grow diagonal final  #TAUTHOR_TAG.', '']","['grow diagonal final  #TAUTHOR_TAG.', '']","['used heuristic is called grow diagonal final  #TAUTHOR_TAG.', '']","['discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair.', 'typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment.', 'for mt the most commonly used heuristic is called grow diagonal final  #TAUTHOR_TAG.', 'this starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points.', '']",1
"[' #TAUTHOR_TAG, 2008 ), who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi - supervised learning.', 'they']","[' #TAUTHOR_TAG, 2008 ), who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi - supervised learning.', 'they']","[' #TAUTHOR_TAG, 2008 ), who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi - supervised learning.', 'they call']","['is closely related to the work of  #TAUTHOR_TAG, 2008 ), who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi - supervised learning.', 'they call their method generalized expectation ( ge ) constraints or alternatively expectation regularization.', 'in the original ge framework, the posteriors of the model on unlabeled data are regularized directly.', 'they train a discriminative model, using conditional likelihood on labeled data and an "" expectation regularization "" penalty term on the unlabeled data']",0
['transferring annotations between languages  #TAUTHOR_TAG ; hwa et'],"['transferring annotations between languages  #TAUTHOR_TAG ; hwa et al. 2005 ; ganchev, gillenwater, and taskar']",['transferring annotations between languages  #TAUTHOR_TAG ; hwa et'],"['seminal work of  #AUTHOR_TAG b ) introduced a series of probabilistic models ( ibm models 1 - 5 ) for statistical machine translation and the concept of "" word - byword "" alignment, the correspondence between words in source and target languages.', 'although no longer competitive as end - to - end translation models, the ibm models, as well as the hidden markov model ( hmm ) of  #AUTHOR_TAG, are still widely used for word alignment.', 'word alignments are used primarily for extracting minimal translation units for machine translation ( mt ) ( e. g., phrases [ koehn, och, and marcu 2003 ] and rules [ galley et al. 2004 ; chiang et al. 2005 ] ) as well as for mt system combination ( matusov, ueffing, and ney 2006 ).', 'but their importance has grown far beyond machine translation : for instance, transferring annotations between languages  #TAUTHOR_TAG ; hwa et al. 2005 ; ganchev, gillenwater, and taskar 2009 ) ; discovery of paraphrases ( bannard and callison - burch 2005 ) ; and joint unsupervised pos and parser induction across languages ( snyder and barzilay 2008 )']",4
[' #TAUTHOR_TAG ;'],[' #TAUTHOR_TAG ;'],[' #TAUTHOR_TAG ;'],"['seminal work of  #AUTHOR_TAG b ) introduced a series of probabilistic models ( ibm models 15 ) for statistical machine translation and the concept of word - by - word alignment, the correspondence between words in source and target languages.', 'although no longer competitive as end - to - end translation models, the ibm models, as well as the hidden markov model ( hmm ) of  #AUTHOR_TAG, are still widely used for word alignment.', 'word alignments are used primarily for extracting minimal translation units for machine translation ( mt ) ( e. g., phrases [ koehn, och, and marcu 2003 ] and rules [  #TAUTHOR_TAG ; chiang et al. 2005 ] ) as well as for mt system combination ( matusov, ueffing, and ney 2006 ).', 'but their importance has grown far beyond machine translation : for instance, transferring annotations between languages ( yarowsky and ngai 2001 ; hwa et al. 2005 ; ganchev, gillenwater, and taskar 2009 ) ; discovery of paraphrases ( bannard and callison - burch 2005 ) ; and joint unsupervised pos and parser induction across languages ( snyder and barzilay 2008 )']",0
"['of unsupervised dependency parsing,  #TAUTHOR_TAG add a constraint of the form']","['of unsupervised dependency parsing,  #TAUTHOR_TAG add a constraint of the form ` `']","['use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant η > 1.', 'this encourages more transitions and hence shorter phrases.', 'for the task of unsupervised dependency parsing,  #TAUTHOR_TAG add a constraint of the form']","['idea of introducing constraints over a model to better guide the learning process has appeared before.', 'in the context of word alignment,  #AUTHOR_TAG use a state - duration hmm in order to model word - to - phrase translations.', 'the fertility of each source word is implicitly encoded in the durations of the hmm states.', 'without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant η > 1.', 'this encourages more transitions and hence shorter phrases.', ""for the task of unsupervised dependency parsing,  #TAUTHOR_TAG add a constraint of the form ` ` the average length of dependencies should be x'' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ), using a scheme they call structural annealing."", ""they modify the model's distribution over trees p θ ( y ) by a penalty term as : p θ ( y ) ∝ p θ ( y ) e ( δ e∈y length ( e ) ), where length ( e ) is the surface length of edge e."", 'the factor δ changes from a high value to a lower one so that the preference for short edges ( hence a smaller sum ) is stronger at the start of training.', 'these two approaches also have the goal of controlling unsupervised learning, and the form of the modified distributions is reminiscent of the form that the projected posteriors take.', 'however, the approaches differ substantially from pr.  #AUTHOR_TAG make a statement of the form "" scale the total length of edges "", which depending on the value of δ will prefer to have more shorter / longer edges.', 'such statements are not data dependent.', 'depending on the value of δ, for instance if δ ≤ 0, even if the data is such that the model already uses too many short edges on average, this value of δ will push for more short edges.', 'by contrast the statements we can make in pr are of the form "" there should be more short edges than long edges "".', 'such a statement is data - dependent in the sense that if the model satisfies the constraints then we do not need to change it ; if it is far from satisfying it we might need to make very dramatic changes']",0
[' #TAUTHOR_TAG and'],[' #TAUTHOR_TAG and'],[' #TAUTHOR_TAG and'],"['', 'we use the agreement checker code developed by  #TAUTHOR_TAG and evaluate our baseline ( maltparser using only core12 ), best performing model ( easy - first parser using core12 + det + lmm + person + fn * ngr g + p ), and the gold reference.', 'the agreement checker verifies, for all verb - nominal subject relations and noun - adjective relations found in the tree, whether the agreement conditions are met or not.', 'the accuracy number reflects the percentage of such relations found which meet the agreement criteria.', 'note that we use the syntax given by the tree, not the gold syntax.', 'for all three trees, however, we used gold morphological features for this evaluation even when those features were not used in the parsing task.', 'this is because we want to see to what extent the predicted morphological features help find the correct syntactic relations, not whether the predicted trees are intrinsically coherent given possibly false predicted morphology.', 'the results can be found in table 18.', 'we note that the grammaticality of the gold corpus is not 100 % ; this is approximately equally due to errors in the checking script and to annotation errors in the gold standard.', 'we take the given grammaticality of the gold corpus as a topline for this analysis.', 'nominal modification has a smaller error band between baseline and gold compared with subject - verb agreement.', 'we assume this is because subject - verb agreement is more complex ( it depends on their relative order ), and because nominal modification can have multiple structural targets, only one of which is correct, although all, however, are plausible from the point of view of agreement.', 'the error reduction relative to the gold topline is 62 % and 76 % for nominal agreement and verb agreement, respectively.', 'thus, we see that our second hypothesis - that the use of morphological features will reduce grammaticality errors in the resulting parse trees with respect to agreement phenomena - is borne out']",5
"['the buckwalter morphological analyzer  #TAUTHOR_TAG used in the patb has a core pos set of 44 tags ( core44 ) before mor - phological extension. 8', 'cross - linguistically, a core set containing around 12 tags is often assumed as a universal tag set ( rambow et']","['the buckwalter morphological analyzer  #TAUTHOR_TAG used in the patb has a core pos set of 44 tags ( core44 ) before mor - phological extension. 8', 'cross - linguistically, a core set containing around 12 tags is often assumed as a universal tag set ( rambow et al. 2006 ; petrov, das, and mcdonald 2012 ).', 'we have adapted the list from  #AUTHOR_TAG for arabic, and']","['the buckwalter morphological analyzer  #TAUTHOR_TAG used in the patb has a core pos set of 44 tags ( core44 ) before mor - phological extension. 8', 'cross - linguistically, a core set containing around 12 tags is often assumed as a universal tag set ( rambow et']","['##ally, words have associated pos tags, e. g., verb or noun, which further abstract over morphologically and syntactically similar lexemes.', 'traditional arabic grammars often describe a very general three - way distinction into verbs, nominals, and particles.', 'in comparison, the tag set of the buckwalter morphological analyzer  #TAUTHOR_TAG used in the patb has a core pos set of 44 tags ( core44 ) before mor - phological extension. 8', 'cross - linguistically, a core set containing around 12 tags is often assumed as a universal tag set ( rambow et al. 2006 ; petrov, das, and mcdonald 2012 ).', 'we have adapted the list from  #AUTHOR_TAG for arabic, and call it here core12.', 'it contains the following tags : verb ( v ), noun ( n ), adjective ( aj ), adverb ( av ), proper noun ( pn ), pronoun ( pro ), relative pronoun ( rel ), preposition ( p ), conjunction ( c ), particle ( prt ), abbreviation ( ab ), and punctuation ( pnx ).', 'the catib6 tag set can be viewed as a further reduction, with the exception that catib6 contains a passive voice tag ( a mor - phological feature ) ; this tag constitutes only 0. 5 % of the tags in the training, however']",1
['by  #TAUTHOR_TAG to evaluate the parsing'],['by  #TAUTHOR_TAG to evaluate the parsing'],"['by  #TAUTHOR_TAG to evaluate the parsing quality on sentences up to 70 tokens long.', 'we report these filtered results in']","['better comparison with work of others, we adopt the suggestion made by  #TAUTHOR_TAG to evaluate the parsing quality on sentences up to 70 tokens long.', 'we report these filtered results in table 14.', 'filtered results are consistently higher ( as expected ).', '']",5
[';  #TAUTHOR_TAG and'],[';  #TAUTHOR_TAG and'],"['marsi 2006 ;  #TAUTHOR_TAG and the catib ( habash and roth 2009 ).', ' #AUTHOR_TAG analyzed the patb for annotation consistency, and introduced an enhanced split - state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', ' #AUTHOR_TAG reports experiments on arabic parsing using his maltparser ( nivre et']","['for work on arabic ( msa ), results have been reported on the patb ( kulick, gabbard, and marcus 2006 ; diab 2007 ; green and manning 2010 ), the prague dependency treebank ( padt ) ( buchholz and marsi 2006 ;  #TAUTHOR_TAG and the catib ( habash and roth 2009 ).', ' #AUTHOR_TAG analyzed the patb for annotation consistency, and introduced an enhanced split - state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', ' #AUTHOR_TAG reports experiments on arabic parsing using his maltparser ( nivre et al. 2007 ), trained on the padt.', 'his results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the maltparser']",0
"[')  #TAUTHOR_TAG.', 'specifically, we use the portion converted from']","[')  #TAUTHOR_TAG.', 'specifically, we use the portion converted from']","[')  #TAUTHOR_TAG.', 'specifically, we use the portion converted from part 3 of the patb to the catib format, which enriches the catib dependency trees with full patb morphological information.', ""catib's dependency representation is based on traditional arabic grammar and emphasizes syntactic case relations."", 'it has a reduced pos tag set consisting of six tags only (']","['use the columbia arabic treebank ( catib )  #TAUTHOR_TAG.', 'specifically, we use the portion converted from part 3 of the patb to the catib format, which enriches the catib dependency trees with full patb morphological information.', ""catib's dependency representation is based on traditional arabic grammar and emphasizes syntactic case relations."", 'it has a reduced pos tag set consisting of six tags only ( henceforth catib6 ).', 'the tags are : nom ( non - proper nominals including nouns, pronouns, adjectives, and adverbs ), prop ( proper nouns ), vrb ( active - voice verbs ), vrb - pass ( passive - voice verbs ), prt ( particles such as prepositions or conjunctions ), and pnx ( punctuation ).', 'catib uses a standard set of eight dependency relations : sbj and obj for subject and ( direct or indirect ) object, respectively ( whether they appear pre - or postverbally ) ; idf for the idafa ( possessive ) relation ; mod for most other modifications ; and other less common relations that we will not discuss here.', 'for other patb - based pos tag sets, see sections 2. 6 and 2. 7']",5
"['parsing accuracy.', 'for example, modeling case in czech improves czech parsing  #TAUTHOR_TAG : case is relevant, not redundant,']","['parsing accuracy.', 'for example, modeling case in czech improves czech parsing  #TAUTHOR_TAG : case is relevant, not redundant,']","['parsing accuracy.', 'for example, modeling case in czech improves czech parsing  #TAUTHOR_TAG : case is relevant, not redundant,']","['languages vary with respect to which features may be most helpful given various tradeoffs among these three factors.', 'in the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.', 'for example, modeling case in czech improves czech parsing  #TAUTHOR_TAG : case is relevant, not redundant, and can be predicted with sufficient accuracy.', 'it has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages ( eryigit, nivre, and oflazer 2008 ; nivre, boguslavsky, and iomdin 2008 ;']",4
"['1  #TAUTHOR_TAG, converted']","['the experiments reported in this article, we used the training portion of patb part 3 v3. 1  #TAUTHOR_TAG, converted']","['##3. 1  #TAUTHOR_TAG, converted']","['all the experiments reported in this article, we used the training portion of patb part 3 v3. 1  #TAUTHOR_TAG, converted to the catib treebank format, as mentioned in section 2. 5.', 'we used the same training / devtest split as in  #AUTHOR_TAG ; and we further split the devtest into two equal parts : a development ( dev ) set and a blind test set.', 'for all experiments, unless specified otherwise, we used the dev set. 10 we kept the test unseen ( "" blind "" ) during training and model development.', 'statistics about this split ( after conversion to the catib dependency format ) are given in table 1']",5
"['in this article, we use a newer version of the corpus by  #TAUTHOR_TAG than the one']","['in this article, we use a newer version of the corpus by  #TAUTHOR_TAG than the one']","['in this article, we use a newer version of the corpus by  #TAUTHOR_TAG than the one we used in marton, habash, and  #AUTHOR_TAG']","['in this article, we use a newer version of the corpus by  #TAUTHOR_TAG than the one we used in marton, habash, and  #AUTHOR_TAG']",5
"['nouns ), nor rationality. in this article, we use an in - house system which provides functional gender, number, and rationality features  #TAUTHOR_TAG. see section']","['nouns ), nor rationality. in this article, we use an in - house system which provides functional gender, number, and rationality features  #TAUTHOR_TAG. see section 5. 2 for more details', '']","['nouns ), nor rationality. in this article, we use an in - house system which provides functional gender, number, and rationality features  #TAUTHOR_TAG. see section']","['', '##uation for arabic ( mada ) toolkit ( habash and rambow 2005 ; habash, rambow, and roth 2012 ). the elixir - fm analyzer ( smrz 2007 ) readily provides the functional inflectional number feature, but not full functional gender ( only for adjectives and verbs but not for nouns ), nor rationality. in this article, we use an in - house system which provides functional gender, number, and rationality features  #TAUTHOR_TAG. see section 5. 2 for more details', '']",5
"[') and the catib  #TAUTHOR_TAG.', ' #AUTHOR_TAG analyzed the patb for annotation consistency, and introduced an enhanced split - state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', ' #AUTHOR_TAG reports experiments on arabic parsing using his maltparser ( nivre']","[') and the catib  #TAUTHOR_TAG.', ' #AUTHOR_TAG analyzed the patb for annotation consistency, and introduced an enhanced split - state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', ' #AUTHOR_TAG reports experiments on arabic parsing using his maltparser ( nivre']","['marsi 2006 ; nivre 2008 ) and the catib  #TAUTHOR_TAG.', ' #AUTHOR_TAG analyzed the patb for annotation consistency, and introduced an enhanced split - state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', ' #AUTHOR_TAG reports experiments on arabic parsing using his maltparser ( nivre']","['for work on arabic ( msa ), results have been reported on the patb ( kulick, gabbard, and marcus 2006 ; diab 2007 ; green and manning 2010 ), the prague dependency treebank ( padt ) ( buchholz and marsi 2006 ; nivre 2008 ) and the catib  #TAUTHOR_TAG.', ' #AUTHOR_TAG analyzed the patb for annotation consistency, and introduced an enhanced split - state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', ' #AUTHOR_TAG reports experiments on arabic parsing using his maltparser ( nivre et al. 2007 ), trained on the padt.', 'his results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the maltparser']",0
"['easy - first parser  #TAUTHOR_TAG ( section 6 ).', ' #AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features']","['easy - first parser  #TAUTHOR_TAG ( section 6 ).', ' #AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features']","['over successfully to another parser, the easy - first parser  #TAUTHOR_TAG ( section 6 ).', ' #AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features']","['results agree with previous work on arabic and hebrew in that marking the definite article is helpful for parsing.', 'we go beyond previous work, however, and explore additional lexical and inflectional features.', 'previous work with maltparser in russian, turkish, and hindi showed gains with case but not with agreement features ( eryigit, nivre, and oflazer 2008 ; nivre, boguslavsky, and iomdin 2008 ; ).', 'our work is the first to show gains using agreement in maltparser and in arabic dependency parsing, and the first to use functional features for this task.', 'furthermore, we demonstrate that our results carry over successfully to another parser, the easy - first parser  #TAUTHOR_TAG ( section 6 ).', ' #AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.', 'these features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '9 we expect this kind of feature to yield lower gains for arabic, unless : r one uses functional feature values ( such as those used here for the first time in arabic nlp ), r one uses yet another representation level to account for the otherwise non - identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as vs ( where the verb precedes its subject ), and r one adequately represents the otherwise "" inverse "" number agreement ( a phenomenon common to other semitic languages, such as hebrew, too )']",5
"['patb3 training and dev sets manually annotated with functional gender, number, and rationality  #TAUTHOR_TAG. 18 this is the first']","['patb3 training and dev sets manually annotated with functional gender, number, and rationality  #TAUTHOR_TAG. 18 this is the first']","['elixirfm lexical resource used previously provided functional number feature values but no functional gender values, nor rat ( rationality, or humanness ) values.', 'to address this issue, we use a version of the patb3 training and dev sets manually annotated with functional gender, number, and rationality  #TAUTHOR_TAG. 18 this is the first resource providing all three features (']","['elixirfm lexical resource used previously provided functional number feature values but no functional gender values, nor rat ( rationality, or humanness ) values.', 'to address this issue, we use a version of the patb3 training and dev sets manually annotated with functional gender, number, and rationality  #TAUTHOR_TAG. 18 this is the first resource providing all three features ( elixirfm only provides functional number, and to some extent functional gender ).', 'we conducted experiments with gold features to assess the potential of these features, and with predicted fea - tures, obtained from training a simple maximum likelihood estimation classifier on this resource ( alkuhlani and habash 2012 ).', '19 the first part of table 8 shows that the rat ( rationality ) feature is very relevant ( in gold ), but suffers from low accuracy ( no gains in machine - predicted input ).', 'the next two parts show the advantages of functional gender and number ( denoted with a fn * prefix ) over their surface - based counterparts.', 'the fourth part of the table shows the combination of these functional features with the other features that participated in the best combination so far ( lmm, the extended det2, and person ) ; without rat, this combination is at least as useful as its form - based counterpart, in both gold and predicted input ; adding rat to this combination yields 0. 4 % ( absolute ) gain in gold, offering further support to the relevance of the rationality feature, but a slight decrease in predicted input, presumably due to insufficient accuracy again.', 'the last part of the table revalidates the gains achieved with the best controlled feature combination, using catibexthe best performing tag set with predicted in - put.', 'note, however, that the 1 % ( absolute ) advantage of catibex ( without additional features ) over the morphology - free core12 on machine - predicted input ( table 2 ) has shrunk with these functional feature combinations to 0. 3 %.', 'we take it as further support to the relevance of our functional morphology features, and their partial redundancy with the form - based morphological information embedded in the catibex pos tags']",5
"['toolkit  #TAUTHOR_TAG ; habash, rambow, and roth 2012 ) 15 ( see']","['the experiments with pos tags predicted by the mada toolkit  #TAUTHOR_TAG ; habash, rambow, and roth 2012 ) 15 ( see']","['is unclear.', 'put differently, we are interested in the tradeoff between relevance and accu - racy. therefore, we repeated the experiments with pos tags predicted by the mada toolkit  #TAUTHOR_TAG ; habash, rambow, and roth 2012 ) 15 ( see table 2, columns 57 ).', 'it turned out that bw, the best gold performer but with lowest pos pre - diction accuracy (']","['far we discussed optimal ( gold ) conditions.', 'but in prac - tice, pos tags are annotated by automatic taggers, so parsers get predicted pos tags as input, as opposed to gold ( human - annotated ) tags. 14', 'the more informative the tag set, the less accurate the tag prediction might be, so the effect on overall parsing quality is unclear.', 'put differently, we are interested in the tradeoff between relevance and accu - racy. therefore, we repeated the experiments with pos tags predicted by the mada toolkit  #TAUTHOR_TAG ; habash, rambow, and roth 2012 ) 15 ( see table 2, columns 57 ).', 'it turned out that bw, the best gold performer but with lowest pos pre - diction accuracy ( 81. 8 % ), suffered the biggest drop ( 11. 4 % ) and was the worst performer with predicted tags.', 'the simplest tag set, catib6, and its extension, catibex, benefited from the highest pos prediction accuracy ( 97. 7 % ), and their performance suffered the least.', 'catibex was the best performer with predicted pos tags.', 'performance drop and pos prediction accuracy are given in columns 8 and 9']",5
"[""##ar's test on non - gold las, as implemented by nilsson and  #TAUTHOR_TAG."", 'we denote p <']","['parent, ls ) are also given.', ""for statistical significance, we use mcnemar's test on non - gold las, as implemented by nilsson and  #TAUTHOR_TAG."", 'we denote p <']","[""##ar's test on non - gold las, as implemented by nilsson and  #TAUTHOR_TAG."", 'we denote p <']","['', ""for statistical significance, we use mcnemar's test on non - gold las, as implemented by nilsson and  #TAUTHOR_TAG."", 'we denote p < 0. 05 and p < 0. 01 with + and + +, respectively']",5
"['##re 2003, 2008 ; [UNK], mcdonald, and  #TAUTHOR_TAG, a transition - based parser with an']","['v1. 3 ( nivre 2003, 2008 ; [UNK], mcdonald, and  #TAUTHOR_TAG, a transition - based parser with an']","['##re 2003, 2008 ; [UNK], mcdonald, and  #TAUTHOR_TAG, a transition - based parser with an input buffer and a stack, which uses svm classifiers we use the term "" dev set "" to']","['all experiments reported in this section we used the syntactic dependency parser maltparser v1. 3 ( nivre 2003, 2008 ; [UNK], mcdonald, and  #TAUTHOR_TAG, a transition - based parser with an input buffer and a stack, which uses svm classifiers we use the term "" dev set "" to denote a non - blind test set, used for model development ( feature selection and feature engineering ).', 'we do not perform further weight optimization ( which, if done, is done on a separate "" tuning set "" ).', 'to predict the next state in the parse derivation.', '']",5
"['on this resource  #TAUTHOR_TAG. 19', 'the first']","['on this resource  #TAUTHOR_TAG. 19', 'the first']","['from training a simple maximum likelihood estimation classifier on this resource  #TAUTHOR_TAG. 19', 'the first part of']","['elixirfm lexical resource used previously provided functional number feature values but no functional gender values, nor rat ( rationality, or humanness ) values.', 'to address this issue, we use a version of the patb3 training and dev sets manually annotated with functional gender, number, and rationality ( alkuhlani and habash 2011 ). 18', 'this is the first resource providing all three features ( elixirfm only provides functional number, and to some extent functional gender ).', 'we conducted experiments with gold features to assess the potential of these features, and with predicted features, obtained from training a simple maximum likelihood estimation classifier on this resource  #TAUTHOR_TAG. 19', 'the first part of table 8 shows that the rat ( rationality ) feature is very relevant ( in gold ), but suffers from low accuracy ( no gains in machine - predicted input ).', 'the next two parts show the advantages of functional gender and number ( denoted with a fn * prefix ) over their surface - based counterparts.', 'the fourth part of the table shows the combination of these functional features with the other features that participated in the best combination so far ( lmm, the extended det2, and person ) ; without rat, this combination is at least as useful as its form - based counterpart, in both gold and predicted input ; adding rat to this combination yields 0. 4 % ( absolute ) gain in gold, offering further support to the relevance of the rationality feature, but a slight decrease in predicted input, presumably due to insufficient accuracy again.', 'the last part of the table revalidates the gains achieved with the best controlled feature combination, using catibexthe best performing tag set with predicted in - put.', 'note, however, that the 1 % ( absolute ) advantage of catibex ( without additional features ) over the morphology - free core12 on machine - predicted input ( table 2 ) has shrunk with these functional feature combinations to 0. 3 %.', 'we take it as further support to the relevance of our functional morphology features, and their partial redundancy with the form - based morphological information embedded in the catibex pos tags']",5
"['penn arabic treebank ( patb )  #TAUTHOR_TAG,']","['penn arabic treebank ( patb )  #TAUTHOR_TAG,']","['available arabic nlp tools and resources model morphology using form - based ( surface ) inflectional features, and do not mark rationality ; this includes the penn arabic treebank ( patb )  #TAUTHOR_TAG,']","['available arabic nlp tools and resources model morphology using form - based ( surface ) inflectional features, and do not mark rationality ; this includes the penn arabic treebank ( patb )  #TAUTHOR_TAG, the buckwalter morphological analyzer ( buckwalter 2004 ), and tools using them such as the morphological analysis and disambiguation for arabic ( mada ) toolkit ( habash and rambow 2005 ; habash, rambow, and roth 2012 ).', 'the elixir - fm analyzer ( smr _ 2007 ) readily provides the functional inflectional number feature, but not full functional gender ( only for adjectives and verbs but not for nouns ), nor rationality.', 'in this article, we use an in - house system which provides functional gender, number, and rationality features ( alkuhlani and habash 2012 ).', 'see section 5. 2 for more details']",1
"[') toolkit  #TAUTHOR_TAG ; habash, rambow, and roth']","['toolkit  #TAUTHOR_TAG ; habash, rambow, and roth 2012 ).', 'the elixir - fm analyzer ( smr _ 2007 ) readily provides']","[') toolkit  #TAUTHOR_TAG ; habash, rambow, and roth 2012 ).', 'the elixir - fm analyzer ( smr _ 2007 ) readily provides the functional inflectional number feature, but not full functional gender ( only for adjectives']","['available arabic nlp tools and resources model morphology using form - based ( surface ) inflectional features, and do not mark rationality ; this includes the penn arabic treebank ( patb ) ( maamouri et al. 2004 ), the buckwalter morphological analyzer ( buckwalter 2004 ), and tools using them such as the morphological analysis and disambiguation for arabic ( mada ) toolkit  #TAUTHOR_TAG ; habash, rambow, and roth 2012 ).', 'the elixir - fm analyzer ( smr _ 2007 ) readily provides the functional inflectional number feature, but not full functional gender ( only for adjectives and verbs but not for nouns ), nor rationality.', 'in this article, we use an in - house system which provides functional gender, number, and rationality features ( alkuhlani and habash 2012 ).', 'see section 5. 2 for more details']",1
['core12 ; ( b ) catib treebank tag set ( catib6 )  #TAUTHOR_TAG and'],['core12 ; ( b ) catib treebank tag set ( catib6 )  #TAUTHOR_TAG and'],['core12 ; ( b ) catib treebank tag set ( catib6 )  #TAUTHOR_TAG and'],"[""following are the various tag sets we use in this article : ( a ) the core pos tag sets core44 and the newly introduced core12 ; ( b ) catib treebank tag set ( catib6 )  #TAUTHOR_TAG and its newly introduced extension of catibex created using simple regular expressions on word form, indicating particular morphemes such as the prefix ji al + or the suffix v'+ wn ; this tag set is the best - performing tag set for arabic on predicted values as reported in section 4 ; ( c ) the patb full tag set with complete morphological tag ( bw ) ( buckwalter 2004 ) ; and two extensions of the patb reduced tag set ( penn pos, a. k. a. rts, size 24 [ diab, hacioglu, and jurafsky 2004 ] ), both outperforming it : ( d ) kulick, gabbard, and  #AUTHOR_TAG's tag set ( kulick ), size 43, one of whose most important extensions is the marking of the definite article clitic, and ( e ) diab and benajiba's ( in preparation ) extended rts tag set ( erts ), which marks gender, number, and definiteness, size 134""]",5
"['buckwalter morphological analyzer  #TAUTHOR_TAG, and tools using them']","['buckwalter morphological analyzer  #TAUTHOR_TAG, and tools using them']","['available arabic nlp tools and resources model morphology using form - based ( surface ) inflectional features, and do not mark rationality ; this includes the penn arabic treebank ( patb ) ( maamouri et al. 2004 ), the buckwalter morphological analyzer  #TAUTHOR_TAG, and tools using them']","['available arabic nlp tools and resources model morphology using form - based ( surface ) inflectional features, and do not mark rationality ; this includes the penn arabic treebank ( patb ) ( maamouri et al. 2004 ), the buckwalter morphological analyzer  #TAUTHOR_TAG, and tools using them such as the morphological analysis and disambiguation for arabic ( mada ) toolkit ( habash and rambow 2005 ; habash, rambow, and roth 2012 ).', 'the elixir - fm analyzer ( smr _ 2007 ) readily provides the functional inflectional number feature, but not full functional gender ( only for adjectives and verbs but not for nouns ), nor rationality.', 'in this article, we use an in - house system which provides functional gender, number, and rationality features ( alkuhlani and habash 2012 ).', 'see section 5. 2 for more details']",1
[')  #TAUTHOR_TAG ; nivre'],['( padt )  #TAUTHOR_TAG ; nivre'],[')  #TAUTHOR_TAG ; nivre 2008 ) and the catib ( habash'],"['for work on arabic ( msa ), results have been reported on the patb ( kulick, gabbard, and marcus 2006 ; diab 2007 ; green and manning 2010 ), the prague dependency treebank ( padt )  #TAUTHOR_TAG ; nivre 2008 ) and the catib ( habash and roth 2009 ).', ' #AUTHOR_TAG analyzed the patb for annotation consistency, and introduced an enhanced split - state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', ' #AUTHOR_TAG reports experiments on arabic parsing using his maltparser ( nivre et al. 2007 ), trained on the padt.', 'his results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the maltparser']",0
[';  #TAUTHOR_TAG'],[';  #TAUTHOR_TAG'],"['with sufficient accuracy.', 'it has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages ( eryigit, nivre, and oflazer 2008 ; nivre, boguslavsky, and iomdin 2008 ;  #TAUTHOR_TAG']","['languages vary with respect to which features may be most helpful given various tradeoffs among these three factors.', 'in the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.', 'for example, modeling case in czech improves czech parsing ( collins et al. 1999 ) : case is relevant, not redundant, and can be predicted with sufficient accuracy.', 'it has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages ( eryigit, nivre, and oflazer 2008 ; nivre, boguslavsky, and iomdin 2008 ;  #TAUTHOR_TAG']",4
['the maltparser  #TAUTHOR_TAG and the easy - first parser ( goldberg and elhadad 2010 )'],['the maltparser  #TAUTHOR_TAG and the easy - first parser ( goldberg and elhadad 2010 )'],"['adjective relations.', 'the result holds for both the maltparser  #TAUTHOR_TAG and the easy - first parser ( goldberg and elhadad 2010 )']","[', assignment features, specifically case, are very helpful in msa, though only under gold conditions : because case is rarely explicit in the typically undiacritized written msa, it has a dismal accuracy rate, which makes it useless when used in a machine - predicted ( real, non - gold ) condition.', 'in contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions.', 'this is likely a result of msa having a rich agreement system, covering both verb - subject and noun - adjective relations.', 'the result holds for both the maltparser  #TAUTHOR_TAG and the easy - first parser ( goldberg and elhadad 2010 )']",5
['with complete morphological tag ( bw )  #TAUTHOR_TAG ; and two extensions of'],['with complete morphological tag ( bw )  #TAUTHOR_TAG ; and two extensions of'],"['with complete morphological tag ( bw )  #TAUTHOR_TAG ; and two extensions of the patb reduced tag set ( penn pos,']","[""following are the various tag sets we use in this article : ( a ) the core pos tag sets core44 and the newly introduced core12 ; ( b ) catib treebank tag set ( catib6 ) ( habash and roth 2009 ) and its newly introduced extension of catibex created using simple regular expressions on word form, indicating particular morphemes such as the prefix ji al + or the suffix v'+ wn ; this tag set is the best - performing tag set for arabic on predicted values as reported in section 4 ; ( c ) the patb full tag set with complete morphological tag ( bw )  #TAUTHOR_TAG ; and two extensions of the patb reduced tag set ( penn pos, a. k. a. rts, size 24 [ diab, hacioglu, and jurafsky 2004 ] ), both outperforming it : ( d ) kulick, gabbard, and  #AUTHOR_TAG's tag set ( kulick ), size 43, one of whose most important extensions is the marking of the definite article clitic, and ( e ) diab and benajiba's ( in preparation ) extended rts tag set ( erts ), which marks gender, number, and definiteness, size 134""]",5
"['parsing of morphologically rich languages.', ' #TAUTHOR_TAG report that an optimal tag set']","['parsing of morphologically rich languages.', ' #TAUTHOR_TAG report that an optimal tag set']","['parsing of morphologically rich languages.', ' #TAUTHOR_TAG report that an optimal tag set']","['work has been done on the use of morphological features for parsing of morphologically rich languages.', ' #TAUTHOR_TAG report that an optimal tag set for parsing czech consists of a basic pos tag plus a case feature ( when applicable ).', 'this tag set ( size 58 ) outperforms the basic czech pos tag set ( size 13 ) and the complete tag set ( size ≈3000 + ).', 'they also report that the use of gender, number, and person features did not yield any improvements.', 'the results for czech are the opposite of our results for arabic, as we will see.', 'this may be due to case tagging having a lower error rate in czech ( 5. 0 % ) ( hajic and vidova - hladka 1998 ) compared with arabic ( ≈14. 0 %,', 'see table 3 ).', ' #AUTHOR_TAG report that the use of a subset of spanish morphological features ( number for adjectives, determiners, nouns, pronouns, and verbs ; and mode for verbs ) outperforms other combinations.', 'our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.', 'we also find that the number feature helps for arabic.', 'looking at hebrew, a semitic language related to  #AUTHOR_TAG report that extending pos and phrase structure tags with definiteness information helps unlexicalized pcfg parsing']",0
"['it has been brought to our attention that  #TAUTHOR_TAG are in the process of rechecking their code for errors, and rerunning']","['it has been brought to our attention that  #TAUTHOR_TAG are in the process of rechecking their code for errors, and rerunning']","['it has been brought to our attention that  #TAUTHOR_TAG are in the process of rechecking their code for errors, and rerunning']","['we do not relate to specific results in their study because it has been brought to our attention that  #TAUTHOR_TAG are in the process of rechecking their code for errors, and rerunning their experiments ( personal communication )']",1
"['adjective relations.', 'the result holds for both the maltparser ( nivre 2008 ) and the easy - first parser  #TAUTHOR_TAG']","['verb - subject and noun - adjective relations.', 'the result holds for both the maltparser ( nivre 2008 ) and the easy - first parser  #TAUTHOR_TAG']","['adjective relations.', 'the result holds for both the maltparser ( nivre 2008 ) and the easy - first parser  #TAUTHOR_TAG']","[', assignment features, specifically case, are very helpful in msa, though only under gold conditions : because case is rarely explicit in the typically undiacritized written msa, it has a dismal accuracy rate, which makes it useless when used in a machine - predicted ( real, non - gold ) condition.', 'in contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions.', 'this is likely a result of msa having a rich agreement system, covering both verb - subject and noun - adjective relations.', 'the result holds for both the maltparser ( nivre 2008 ) and the easy - first parser  #TAUTHOR_TAG']",5
"['complete morphologically enriched tag set, but instead they selectively enrich the core pos tag set with only certain morphological features.', 'a more detailed discussion of the various available arabic tag sets can be found in  #TAUTHOR_TAG']","['complete morphologically enriched tag set, but instead they selectively enrich the core pos tag set with only certain morphological features.', 'a more detailed discussion of the various available arabic tag sets can be found in  #TAUTHOR_TAG']","['complete morphologically enriched tag set, but instead they selectively enrich the core pos tag set with only certain morphological features.', 'a more detailed discussion of the various available arabic tag sets can be found in  #TAUTHOR_TAG']","['notion of "" pos tag set "" in natural language processing usually does not refer to a core set.', 'instead, the penn english treebank ( ptb ) uses a set of 46 tags, including not only the core pos, but also the complete set of morphological features ( this tag set is still fairly small since english is morphologically impoverished ).', 'in patb - tokenized msa, the corresponding type of tag set ( core pos extended with a complete description of morphology ) would contain upwards of 2, 000 tags, many of which are extremely rare ( in our training corpus of about 300, 000 words, we encounter only pos tags with complete morphology ).', 'therefore, researchers have proposed tag sets for msa whose size is similar to that of the english ptb tag set, as this has proven to be a useful size computationally.', 'these tag sets are hybrids in the sense that they are neither simply the core pos, nor the complete morphologically enriched tag set, but instead they selectively enrich the core pos tag set with only certain morphological features.', 'a more detailed discussion of the various available arabic tag sets can be found in  #TAUTHOR_TAG']",0
"['1.', 'for the corpus statistics, see table 1.', 'for more information on catib, see  #TAUTHOR_TAG and habash, faraj, and  #AUTHOR_TAG']","['1.', 'for the corpus statistics, see table 1.', 'for more information on catib, see  #TAUTHOR_TAG and habash, faraj, and  #AUTHOR_TAG']","['##ing normalization, which we only briefly touch on here ( in section 4. 1 ).', 'an example catib dependency tree is shown in figure 1.', 'for the corpus statistics, see table 1.', 'for more information on catib, see  #TAUTHOR_TAG and habash, faraj, and  #AUTHOR_TAG']","['catib treebank uses the word segmentation of the patb.', ""it splits off several categories of orthographic clitics, but not the definite article + al + ('the')."", 'in all of the experiments reported in this article, we use the gold segmentation.', 'tokenization involves further decisions on the segmented token forms, such as spelling normalization, which we only briefly touch on here ( in section 4. 1 ).', 'an example catib dependency tree is shown in figure 1.', 'for the corpus statistics, see table 1.', 'for more information on catib, see  #TAUTHOR_TAG and habash, faraj, and  #AUTHOR_TAG']",0
"['researchers, however, including  #TAUTHOR_TAG, train on predicted feature values instead.', 'it makes']","['researchers, however, including  #TAUTHOR_TAG, train on predicted feature values instead.', 'it makes']","['far, we have only evaluated models trained on gold pos tag set and morphological feature values.', 'some researchers, however, including  #TAUTHOR_TAG, train on predicted feature values instead.', 'it makes']","['far, we have only evaluated models trained on gold pos tag set and morphological feature values.', 'some researchers, however, including  #TAUTHOR_TAG, train on predicted feature values instead.', 'it makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test.', 'but we argue that it also makes sense that training on a combination of gold and predicted features ( one copy of each ) might do even better, because good predictions of feature values are reinforced ( since they repeat the gold patterns ), whereas noisy predicted feature values are still represented in training ( in patterns that do not repeat the gold ). 21', 'to test our hypothesis, we start this section by comparing three variations']",1
"['the paper by  #TAUTHOR_TAG presents additional, more sophisticated models that we do not use in this']","['the paper by  #TAUTHOR_TAG presents additional, more sophisticated models that we do not use in this']","['the paper by  #TAUTHOR_TAG presents additional, more sophisticated models that we do not use in this article']","['the paper by  #TAUTHOR_TAG presents additional, more sophisticated models that we do not use in this article']",1
"['for work on arabic ( msa ), results have been reported on the patb ( kulick, gabbard, and marcus 2006 ;  #TAUTHOR_TAG ; green and manning']","['for work on arabic ( msa ), results have been reported on the patb ( kulick, gabbard, and marcus 2006 ;  #TAUTHOR_TAG ; green and manning']","['for work on arabic ( msa ), results have been reported on the patb ( kulick, gabbard, and marcus 2006 ;  #TAUTHOR_TAG ; green and manning 2010 )']","['for work on arabic ( msa ), results have been reported on the patb ( kulick, gabbard, and marcus 2006 ;  #TAUTHOR_TAG ; green and manning 2010 ), the prague dependency treebank ( padt ) ( buchholz and marsi 2006 ; nivre 2008 ) and the catib ( habash and roth 2009 ).', ' #AUTHOR_TAG analyzed the patb for annotation consistency, and introduced an enhanced split - state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', ' #AUTHOR_TAG reports experiments on arabic parsing using his maltparser ( nivre et al. 2007 ), trained on the padt.', 'his results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the maltparser']",0
[' #TAUTHOR_TAG reports that non - projective and pseudo - projective algorithms'],[' #TAUTHOR_TAG reports that non - projective and pseudo - projective algorithms'],[' #TAUTHOR_TAG reports that non - projective and pseudo - projective algorithms'],"[' #TAUTHOR_TAG reports that non - projective and pseudo - projective algorithms outperform the ` ` eager projective algorithm in maltparser, but our training data did not contain any non - projective dependencies.', 'the nivre standard algorithm is also reported there to do better on arabic, but in a preliminary experimentation, it did slightly worse than the eager one, perhaps due to the high percentage of right branching ( left headed structures ) in our arabic training setan observation already noted in  #AUTHOR_TAG']",1
"['so on. kbler, mcdonald, and  #TAUTHOR_TAG describe']","['so on. kbler, mcdonald, and  #TAUTHOR_TAG describe']","['so on. kbler, mcdonald, and  #TAUTHOR_TAG describe a typical maltparser model configuration of attributes']","['are five default attributes in the maltparser terminology for each token in the text : word id ( ordinal position in the sentence ), word - form, pos tag, head ( parent word id ), and deprel ( the dependency relation between the current word and its parent ).', 'there are default maltparser features ( in the machine learning sense ), 12 which are the values of functions over these attributes, serving as input to the maltparser internal classifiers.', 'the most commonly used feature functions are the top of the input buffer ( next word to process, denoted buf [ 0 ] ), or top of the stack ( denoted stk [ 0 ] ) ; following items on buffer or stack are also accessible ( buf [ 1 ], buf [ 2 ], stk [ 1 ], etc. ).', 'hence maltparser features are defined as pos tag at stk [ 0 ], word - form at buf [ 0 ], and so on. kbler, mcdonald, and  #TAUTHOR_TAG describe a typical maltparser model configuration of attributes and features. 13', 'starting with it, in a series of initial controlled experiments, we settled on using buf [ 0 - 1 ] + stk [ 0 - 1 ] for word - forms, and buf [ 0 - 3 ] + stk [ 0 - 2 ] for pos tags.', 'for features of new maltparser - attributes ( discussed later ), we used buf [ 0 ] + stk [ 0 ].', '']",5
"[';  #TAUTHOR_TAG,']","[';  #TAUTHOR_TAG,']","['( kulick, gabbard, and marcus 2006 ; diab 2007 ;  #TAUTHOR_TAG, the prague dependency treebank (']","['for work on arabic ( msa ), results have been reported on the patb ( kulick, gabbard, and marcus 2006 ; diab 2007 ;  #TAUTHOR_TAG, the prague dependency treebank ( padt ) ( buchholz and marsi 2006 ; nivre 2008 ) and the catib ( habash and roth 2009 ).', ' #AUTHOR_TAG analyzed the patb for annotation consistency, and introduced an enhanced split - state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', ' #AUTHOR_TAG reports experiments on arabic parsing using his maltparser ( nivre et al. 2007 ), trained on the padt.', 'his results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the maltparser']",0
"['v1. 3  #TAUTHOR_TAG, 2008 ; [UNK], mcdonald, and nivre 2009 ), a transition - based parser with an']","['v1. 3  #TAUTHOR_TAG, 2008 ; [UNK], mcdonald, and nivre 2009 ), a transition - based parser with an']","['all experiments reported in this section we used the syntactic dependency parser maltparser v1. 3  #TAUTHOR_TAG, 2008 ; [UNK], mcdonald, and nivre 2009 ), a transition - based parser with an input buffer and a stack,']","['all experiments reported in this section we used the syntactic dependency parser maltparser v1. 3  #TAUTHOR_TAG, 2008 ; [UNK], mcdonald, and nivre 2009 ), a transition - based parser with an input buffer and a stack, which uses svm classifiers we use the term "" dev set "" to denote a non - blind test set, used for model development ( feature selection and feature engineering ).', 'we do not perform further weight optimization ( which, if done, is done on a separate "" tuning set "" ).', 'to predict the next state in the parse derivation.', '']",5
"['introduced an enhanced split - state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', ' #AUTHOR_TAG reports experiments on arabic parsing using his maltparser  #TAUTHOR_TAG, trained on the pad']","['introduced an enhanced split - state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', ' #AUTHOR_TAG reports experiments on arabic parsing using his maltparser  #TAUTHOR_TAG, trained on the padt.', 'his results are not directly comparable to ours']","['introduced an enhanced split - state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', ' #AUTHOR_TAG reports experiments on arabic parsing using his maltparser  #TAUTHOR_TAG, trained on the padt.', 'his results are not directly comparable to ours']","['for work on arabic ( msa ), results have been reported on the patb ( kulick, gabbard, and marcus 2006 ; diab 2007 ; green and manning 2010 ), the prague dependency treebank ( padt ) ( buchholz and marsi 2006 ; nivre 2008 ) and the catib.', ' #AUTHOR_TAG analyzed the patb for annotation consistency, and introduced an enhanced split - state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', ' #AUTHOR_TAG reports experiments on arabic parsing using his maltparser  #TAUTHOR_TAG, trained on the padt.', 'his results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the maltparser']",0
"['3 ).', 'similarly,  #TAUTHOR_TAG report that the use of']","['3 ).', 'similarly,  #TAUTHOR_TAG report that the use of']","['3 ).', 'similarly,  #TAUTHOR_TAG report that the use of a subset']","['work has been done on the use of morphological features for parsing of morphologically rich languages.', ' #AUTHOR_TAG report that an optimal tag set for parsing czech consists of a basic pos tag plus a case feature ( when applicable ).', 'this tag set ( size 58 ) outperforms the basic czech pos tag set ( size 13 ) and the complete tag set ( size 3000 + ).', 'they also report that the use of gender, number, and person features did not yield any improvements.', 'the results for czech are the opposite of our results for arabic, as we will see.', 'this may be due to case tagging having a lower error rate in czech ( 5. 0 % ) ( hajic and vidov - hladk 1998 ) compared with arabic ( 14. 0 %, see table 3 ).', 'similarly,  #TAUTHOR_TAG report that the use of a subset of spanish morphological features ( number for adjectives, determiners, nouns, pronouns, and verbs ; and mode for verbs ) outperforms other combinations.', 'our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.', 'we also find that the number feature helps for arabic.', 'looking at hebrew, a semitic language related to arabic, tsarfaty and simaan ( 2007 ) report that extending pos and phrase structure tags with definiteness information helps unlexicalized pcfg parsing']",0
"['construction  #TAUTHOR_TAG, p. 102 )']","['we ignore the rare "" false idafa "" construction  #TAUTHOR_TAG, p. 102 )']","['we ignore the rare "" false idafa "" construction  #TAUTHOR_TAG, p. 102 )']","['we ignore the rare "" false idafa "" construction  #TAUTHOR_TAG, p. 102 )']",0
"['first parser  #TAUTHOR_TAG.', 'as in']","['easy - first parser  #TAUTHOR_TAG.', 'as in']","['and combinations thereof - - using a different parser : the easy - first parser  #TAUTHOR_TAG.', 'as in section 4, all models are evaluated on both gold and non - gold ( machine - predicted ) feature values.', 'the easy - first parser is a shift - reduce parser']","['this section, we validate the contribution of key tag sets and morphological features - - and combinations thereof - - using a different parser : the easy - first parser  #TAUTHOR_TAG.', 'as in section 4, all models are evaluated on both gold and non - gold ( machine - predicted ) feature values.', 'the easy - first parser is a shift - reduce parser ( as is maltparser ).', 'unlike maltparser, however, it does not attempt to attach arcs "" eagerly "" as early as possible ( as in previous sections ), or at the latest possible stage ( an option we abandoned early on in preliminary experiments ).', 'instead, the easy - first parser keeps a stack of partially built treelets, and attaches them to one another in order of confidence ( from high confidence, "" easy "" attachment, to low, as estimated by the classifier ).', 'labeling the relation arcs is done in a second pass, with a separate training step, after all attachments have been decided ( the code for which was added after the publication of  #AUTHOR_TAG, which only included an unlabeled attachment version )']",5
"[';  #TAUTHOR_TAG.', 'our']","[';  #TAUTHOR_TAG.', 'our']","[', and iomdin 2008 ;  #TAUTHOR_TAG.', 'our']","['results agree with previous work on arabic and hebrew in that marking the definite article is helpful for parsing.', 'we go beyond previous work, however, and explore additional lexical and inflectional features.', 'previous work with maltparser in russian, turkish, and hindi showed gains with case but not with agreement features ( eryigit, nivre, and oflazer 2008 ; nivre, boguslavsky, and iomdin 2008 ;  #TAUTHOR_TAG.', 'our work is the first to show gains using agreement in maltparser and in arabic dependency parsing, and the first to use functional features for this task.', 'furthermore, we demonstrate that our results carry over successfully to another parser, the easy - first parser ( goldberg and elhadad 2010 ) ( section 6 ).', ' #AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.', 'these features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '9 we expect this kind of feature to yield lower gains for arabic, unless : r one uses functional feature values ( such as those used here for the first time in arabic nlp ), r one uses yet another representation level to account for the otherwise non - identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as vs ( where the verb precedes its subject ), and r one adequately represents the otherwise "" inverse "" number agreement ( a phenomenon common to other semitic languages, such as hebrew, too )']",1
"[') ( section 6 ).', ' #TAUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features']","['( goldberg and elhadad 2010 ) ( section 6 ).', ' #TAUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features']","['over successfully to another parser, the easy - first parser ( goldberg and elhadad 2010 ) ( section 6 ).', ' #TAUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features']","['results agree with previous work on arabic and hebrew in that marking the definite article is helpful for parsing.', 'we go beyond previous work, however, and explore additional lexical and inflectional features.', 'previous work with maltparser in russian, turkish, and hindi showed gains with case but not with agreement features ( eryigit, nivre, and oflazer 2008 ; nivre, boguslavsky, and iomdin 2008 ; ).', 'our work is the first to show gains using agreement in maltparser and in arabic dependency parsing, and the first to use functional features for this task.', 'furthermore, we demonstrate that our results carry over successfully to another parser, the easy - first parser ( goldberg and elhadad 2010 ) ( section 6 ).', ' #TAUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.', 'these features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '9 we expect this kind of feature to yield lower gains for arabic, unless : r one uses functional feature values ( such as those used here for the first time in arabic nlp ), r one uses yet another representation level to account for the otherwise non - identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as vs ( where the verb precedes its subject ), and r one adequately represents the otherwise "" inverse "" number agreement ( a phenomenon common to other semitic languages, such as hebrew, too )']",0
"[';  #TAUTHOR_TAG.', 'in this']","[';  #TAUTHOR_TAG.', 'in this work, we present more details on the problem of solving']","[';  #TAUTHOR_TAG.', 'in this work, we present more details on the problem of solving both within - and cross - document event coreference']","['article represents an extension of our previous work on unsupervised event coreference resolution ( bejan et al. 2009 ;  #TAUTHOR_TAG.', 'in this work, we present more details on the problem of solving both within - and cross - document event coreference as well as describe a generic framework for solving this type of problem in an unsupervised way.', 'as data sets, we consider three different resources, including our own corpus ( which is the only corpus available that encodes event coreference annotations across and within documents ).', 'in the next section, we provide additional information on how we performed the annotation of this corpus.', 'another major contribution of this article is an extended description of the unsupervised models for solving event coreference.', 'in particular, we focused on providing further explanations about the implementation of the mibp framework as well as its integration into the hdp and ihmm models.', 'finally, in this work, we significantly extended the experimental results section, which also includes a novel set of experiments performed over the ontonotes english corpus ( ldc - on 2007 )']",2
"[',  #TAUTHOR_TAG, medress']","[',  #TAUTHOR_TAG, medress']","['##l 1976, lea 1980,  #TAUTHOR_TAG, medress 1980, reddy 1976, walker 1978, and wolf']","['number of speech understanding systems have been developed during the past fifteen years ( barnett et al. 1980, dixon and martin 1979, erman et al. 1980, haton and pierrel 1976, lea 1980,  #TAUTHOR_TAG, medress 1980, reddy 1976, walker 1978, and wolf and woods 1980 ).', 'most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'while some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'a detailed description of the kinds of expectation mechanisms appearing in these systems appears in  #AUTHOR_TAG']",1
['as the one for going to a restaurant as defined in  #TAUTHOR_TAG'],['as the one for going to a restaurant as defined in  #TAUTHOR_TAG'],['as the one for going to a restaurant as defined in  #TAUTHOR_TAG'],"['the implemented system is limited to matrix - oriented problems, the theoretical system is capable of learning a wide range of problem types.', 'the only requirement on the problem or situation is that it can be entered into the expectation system in the form of examples.', ""thus, for example, it can acquire a ` ` script'' such as the one for going to a restaurant as defined in  #TAUTHOR_TAG""]",0
"[', or semantic nets as in  #AUTHOR_TAG.', 'that is, the current system learns procedures rather than data structures.', 'there is some literature on procedure acquisition such as the lisp synthesis work described in  #AUTHOR_TAG and the prolog synthesis method of  #TAUTHOR_TAG']","[', or semantic nets as in  #AUTHOR_TAG.', 'that is, the current system learns procedures rather than data structures.', 'there is some literature on procedure acquisition such as the lisp synthesis work described in  #AUTHOR_TAG and the prolog synthesis method of  #TAUTHOR_TAG']","[', or semantic nets as in  #AUTHOR_TAG.', 'that is, the current system learns procedures rather than data structures.', 'there is some literature on procedure acquisition such as the lisp synthesis work described in  #AUTHOR_TAG and the prolog synthesis method of  #TAUTHOR_TAG']","['vnlce processor may be considered to be a learning system of the tradition described, for example, in  #AUTHOR_TAG.', 'the current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in  #AUTHOR_TAG, assertional statements as in  #AUTHOR_TAG, or semantic nets as in  #AUTHOR_TAG.', 'that is, the current system learns procedures rather than data structures.', 'there is some literature on procedure acquisition such as the lisp synthesis work described in  #AUTHOR_TAG and the prolog synthesis method of  #TAUTHOR_TAG']",1
"[""' facility as described by  #TAUTHOR_TAG,""]","[""conditioning'' facility as described by  #TAUTHOR_TAG, a¢ implementation of new types of paraphrasing, a¢ checking a larger environment in the expectation acquisition algorithm""]","[""' facility as described by  #TAUTHOR_TAG,""]","[""##¢ use of low level knowledge from the speech recognition phase, a¢ use of high level knowledge about the domain in particular and the dialogue task in general, a¢ a ` ` continue'' facility and an ` ` auto - loop'' facility as described by  #AUTHOR_TAG, a¢ a ` ` conditioning'' facility as described by  #TAUTHOR_TAG, a¢ implementation of new types of paraphrasing, a¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen, and a¢ examining inter - speaker dialogue patterns""]",3
"['handling ill - formed input has been studied by  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['handling ill - formed input has been studied by  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['handling ill - formed input has been studied by  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', '']","['problem of handling ill - formed input has been studied by  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', '']",1
"[',  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","[',  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","[',  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', '']","['problem of handling ill - formed input has been studied by  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', '']",1
"[',  #TAUTHOR_TAG.', '']","[',  #TAUTHOR_TAG.', '']","[') ( ballard 1979,  #TAUTHOR_TAG.', '']","['section has given an overview of the approach to history - based expectation processing.', 'the details of the method are dependent on how the functions p, predicts, mergeable, and merge are implemented.', 'the following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve.', 'the usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.', 'an off - the - shelf speech recognition device, a nippon electric corporation dp - 200, was added to an existing natural language processing system, the natural language computer ( nlc ) ( ballard 1979,  #TAUTHOR_TAG.', '']",0
"['acquire coefficient values as in  #AUTHOR_TAG, assertional statements as in  #AUTHOR_TAG, or semantic nets as in  #TAUTHOR_TAG.', 'that is,']","['acquire coefficient values as in  #AUTHOR_TAG, assertional statements as in  #AUTHOR_TAG, or semantic nets as in  #TAUTHOR_TAG.', 'that is,']","['acquire coefficient values as in  #AUTHOR_TAG, assertional statements as in  #AUTHOR_TAG, or semantic nets as in  #TAUTHOR_TAG.', 'that is, the current system learns procedures rather than data structures.', 'there is some literature on procedure acquisition such as the lisp synthesis work described in  #AUTHOR_TAG and the prolog synthesis method of  #AUTHOR_TAG']","['vnlce processor may be considered to be a learning system of the tradition described, for example, in  #AUTHOR_TAG.', 'the current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in  #AUTHOR_TAG, assertional statements as in  #AUTHOR_TAG, or semantic nets as in  #TAUTHOR_TAG.', 'that is, the current system learns procedures rather than data structures.', 'there is some literature on procedure acquisition such as the lisp synthesis work described in  #AUTHOR_TAG and the prolog synthesis method of  #AUTHOR_TAG']",1
"['', 'another dialogue acquisition system has been developed by  #TAUTHOR_TAG.', 'however, that system has different goals : to']","['previously addressed.', 'another dialogue acquisition system has been developed by  #TAUTHOR_TAG.', 'however, that system has different goals : to']","['', 'another dialogue acquisition system has been developed by  #TAUTHOR_TAG.', 'however, that system has different goals : to']","['', 'another dialogue acquisition system has been developed by  #TAUTHOR_TAG.', 'however, that system has different goals : to enable the user to consciously design a dialogue to embody a particular human - machine interaction.', 'the acquisition system described here is aimed at dealing with ill - formed input and is completely automatic and invisible to the user.', 'it self activates to bias recognition toward historically observed patterns but is not otherwise observable']",1
"[', and  #TAUTHOR_TAG']","[', and  #TAUTHOR_TAG']","[', medress 1980, reddy 1976, walker 1978, and  #TAUTHOR_TAG.', '']","['number of speech understanding systems have been developed during the past fifteen years ( barnett et al. 1980, dixon and martin 1979, erman et al. 1980, haton and pierrel 1976, lea 1980, lowerre and reddy 1980, medress 1980, reddy 1976, walker 1978, and  #TAUTHOR_TAG.', 'most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'while some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'a detailed description of the kinds of expectation mechanisms appearing in these systems appears in  #AUTHOR_TAG']",1
"['handling ill - formed input has been studied by  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['handling ill - formed input has been studied by  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['handling ill - formed input has been studied by  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', '']","['problem of handling ill - formed input has been studied by  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', '']",1
"[' #TAUTHOR_TAG, or semantic nets as in  #AUTHOR_TAG.', 'that is, the current system learns procedures rather than data structures.', 'there is some literature on procedure acquisition such as the lisp synthesis work described in  #AUTHOR_TAG and the prolog synthesis method of  #AUTHOR_TAG']","[' #TAUTHOR_TAG, or semantic nets as in  #AUTHOR_TAG.', 'that is, the current system learns procedures rather than data structures.', 'there is some literature on procedure acquisition such as the lisp synthesis work described in  #AUTHOR_TAG and the prolog synthesis method of  #AUTHOR_TAG']","[' #TAUTHOR_TAG, or semantic nets as in  #AUTHOR_TAG.', 'that is, the current system learns procedures rather than data structures.', 'there is some literature on procedure acquisition such as the lisp synthesis work described in  #AUTHOR_TAG and the prolog synthesis method of  #AUTHOR_TAG']","['vnlce processor may be considered to be a learning system of the tradition described, for example, in  #AUTHOR_TAG.', 'the current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in  #AUTHOR_TAG, assertional statements as in  #TAUTHOR_TAG, or semantic nets as in  #AUTHOR_TAG.', 'that is, the current system learns procedures rather than data structures.', 'there is some literature on procedure acquisition such as the lisp synthesis work described in  #AUTHOR_TAG and the prolog synthesis method of  #AUTHOR_TAG']",1
[' #TAUTHOR_TAG where program'],[' #TAUTHOR_TAG where program flowcharts were constructed from traces of their behaviors'],"[' #TAUTHOR_TAG where program flowcharts were constructed from traces of their behaviors.', 'however, the "" flowcharts "" in the current project are probabilistic in']","['', 'however, these methodologies have not used historical information at the dialogue level as described here.', 'in most cases, the goal of these systems is to characterize the ill - formed input into classes of errors and to correct on that basis.', 'the work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'thus, an error in this work has no pattern but occurs probabilistically.', 'a verb is just as likely to be mis - recognized or not recognized as is a noun, adjective, determiner, etc.', 'the acquisition of dialogue as implemented in vnlce is reminiscent of the program synthesis methodology developed by  #TAUTHOR_TAG where program flowcharts were constructed from traces of their behaviors.', '']",1
"['conceptual dependence graph  #TAUTHOR_TAG,']","['conceptual dependence graph  #TAUTHOR_TAG,']","['m ( si ) need not be discussed at this point ; it could be a conceptual dependence graph  #TAUTHOR_TAG, a deep parse']","['denote the meaning of each sentence si with the notation m ( si ).', 'the exact form of m ( si ) need not be discussed at this point ; it could be a conceptual dependence graph  #TAUTHOR_TAG, a deep parse of si, or some other representation.', 'a user behavior is represented by a network, or directed graph, of such meanings.', 'at the beginning of a task, the state of the interaction is represented by the start state of the graph']",0
"[',  #TAUTHOR_TAG, red']","[',  #TAUTHOR_TAG, reddy']","[',  #TAUTHOR_TAG, red']","['number of speech understanding systems have been developed during the past fifteen years ( barnett et al. 1980, dixon and martin 1979, erman et al. 1980, haton and pierrel 1976, lea 1980, lowerre and reddy 1980,  #TAUTHOR_TAG, reddy 1976, walker 1978, and wolf and woods 1980 ).', 'most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'while some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'a detailed description of the kinds of expectation mechanisms appearing in these systems appears in  #AUTHOR_TAG']",1
"[',  #TAUTHOR_TAG, haton and pierrel']","[',  #TAUTHOR_TAG, haton and pierrel']","['martin 1979,  #TAUTHOR_TAG, haton and pierrel 1976, lea 1980, lowerre and red']","['number of speech understanding systems have been developed during the past fifteen years ( barnett et al. 1980, dixon and martin 1979,  #TAUTHOR_TAG, haton and pierrel 1976, lea 1980, lowerre and reddy 1980, medress 1980, reddy 1976, walker 1978, and wolf and woods 1980 ).', 'most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'while some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'a detailed description of the kinds of expectation mechanisms appearing in these systems appears in  #AUTHOR_TAG']",1
"['handling ill - formed input has been studied by  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,']","['handling ill - formed input has been studied by  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,']","['handling ill - formed input has been studied by  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,']","['problem of handling ill - formed input has been studied by  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', '']",1
"['( vnlc,  #TAUTHOR_TAG, which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.']","['( vnlc,  #TAUTHOR_TAG, which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.']","['( vnlc,  #TAUTHOR_TAG, which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.']","['', 'the resulting speech understanding system is called the voice natural language computer with expectation ( vnlce, fink 1983 ).', '[ the current system should be distinguished from an earlier voice system ( vnlc,  #TAUTHOR_TAG, which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.']",1
"['grammar  #TAUTHOR_TAG.', '']","['grammar  #TAUTHOR_TAG.', '']","['grammar  #TAUTHOR_TAG.', '']","['expectation parser uses an atn - like representation for its grammar  #TAUTHOR_TAG.', 'its strategy is top - down.', 'the types of sentences accepted are essentially those accepted by the original nlc grammar, imperative sentences with nested noun groups and conjunctions ( ballard 1979 ).', 'an attempt has been made to build as deep a parse as possible so that sentences with the same meaning result in identical parses.', 'sentences have the same "" meaning "" if they "" result in identical tasks being performed.', 'the various sentence structures']",5
"['##e,  #TAUTHOR_TAG.', '[']","['with expectation ( vnlce,  #TAUTHOR_TAG.', '[ the current system should be distinguished from']","['##e,  #TAUTHOR_TAG.', '[']","['', 'the resulting speech understanding system is called the voice natural language computer with expectation ( vnlce,  #TAUTHOR_TAG.', '[ the current system should be distinguished from an earlier voice system ( vnlc, biermann et al. 1985 ), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.']",0
"['fifteen years  #TAUTHOR_TAG, dix']","['fifteen years  #TAUTHOR_TAG, dixon']","[' #TAUTHOR_TAG, dix']","['number of speech understanding systems have been developed during the past fifteen years  #TAUTHOR_TAG, dixon and martin 1979, erman et al. 1980, haton and pierrel 1976, lea 1980, lowerre and reddy 1980, medress 1980, reddy 1976, walker 1978, and wolf and woods 1980 ).', 'most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'while some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'a detailed description of the kinds of expectation mechanisms appearing in these systems appears in  #AUTHOR_TAG']",1
"['', 'a detailed description of the kinds of expectation mechanisms appearing in these systems appears in  #TAUTHOR_TAG']","['', 'a detailed description of the kinds of expectation mechanisms appearing in these systems appears in  #TAUTHOR_TAG']","['', 'a detailed description of the kinds of expectation mechanisms appearing in these systems appears in  #TAUTHOR_TAG']","['number of speech understanding systems have been developed during the past fifteen years ( barnett et al. 1980, dixon and martin 1979, erman et al. 1980, haton and pierrel 1976, lea 1980, lowerre and reddy 1980, medress 1980, reddy 1976, walker 1978, and wolf and woods 1980.', 'most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'while some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'a detailed description of the kinds of expectation mechanisms appearing in these systems appears in  #TAUTHOR_TAG']",0
"[""' facility as described by  #TAUTHOR_TAG,""]","[""''facility as described by  #TAUTHOR_TAG, a¢ a""]","[""' facility as described by  #TAUTHOR_TAG,""]","[""##¢ use of low level knowledge from the speech recognition phase, a¢ use of high level knowledge about the domain in particular and the dialogue task in general, a¢ a ` ` continue'' facility and an ` ` auto - loop'' facility as described by  #TAUTHOR_TAG, a¢ a ` ` conditioning'' facility as described by  #AUTHOR_TAG, a¢ implementation of new types of paraphrasing, a¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen, and a¢ examining inter - speaker dialogue patterns""]",3
"['handling ill - formed input has been studied by  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,']","['handling ill - formed input has been studied by  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,']","['handling ill - formed input has been studied by  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,']","['problem of handling ill - formed input has been studied by  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', '']",1
"[' #TAUTHOR_TAG, assertional statements as']","[' #TAUTHOR_TAG, assertional statements as']","[' #TAUTHOR_TAG, assertional statements as']","['vnlce processor may be considered to be a learning system of the tradition described, for example, in  #AUTHOR_TAG.', 'the current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in  #TAUTHOR_TAG, assertional statements as in  #AUTHOR_TAG, or semantic nets as in  #AUTHOR_TAG.', 'that is, the current system learns procedures rather than data structures.', 'there is some literature on procedure acquisition such as the lisp synthesis work described in  #AUTHOR_TAG and the prolog synthesis method of  #AUTHOR_TAG']",1
"[')  #TAUTHOR_TAG, bi']","[')  #TAUTHOR_TAG, biermann and ballard']","[')  #TAUTHOR_TAG, bi']","['section has given an overview of the approach to history - based expectation processing.', 'the details of the method are dependent on how the functions p, predicts, mergeable, and merge are implemented.', 'the following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve.', 'the usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.', 'an off - the - shelf speech recognition device, a nippon electric corporation dp - 200, was added to an existing natural language processing system, the natural language computer ( nlc )  #TAUTHOR_TAG, biermann and ballard 1980 ).', '']",0
"[',  #TAUTHOR_TAG, walker']","[',  #TAUTHOR_TAG, walker']","[', medress 1980,  #TAUTHOR_TAG, walker 1978, and wolf']","['number of speech understanding systems have been developed during the past fifteen years ( barnett et al. 1980, dixon and martin 1979, erman et al. 1980, haton and pierrel 1976, lea 1980, lowerre and reddy 1980, medress 1980,  #TAUTHOR_TAG, walker 1978, and wolf and woods 1980 ).', 'most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'while some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'a detailed description of the kinds of expectation mechanisms appearing in these systems appears in  #AUTHOR_TAG']",1
"['with nested noun groups and conjunctions  #TAUTHOR_TAG.', 'an']","['with nested noun groups and conjunctions  #TAUTHOR_TAG.', 'an']","['the original nlc grammar, imperative sentences with nested noun groups and conjunctions  #TAUTHOR_TAG.', 'an attempt has been made to build as deep a parse as possible']","['expectation parser uses an atn - like representation for its grammar ( woods 1970 ).', 'its strategy is top - down.', 'the types of sentences accepted are essentially those accepted by the original nlc grammar, imperative sentences with nested noun groups and conjunctions  #TAUTHOR_TAG.', 'an attempt has been made to build as deep a parse as possible so that sentences with the same meaning result in identical parses.', 'sentences have the same "" meaning "" if they "" result in identical tasks being performed.', 'the various sentence structures']",0
"['we will lose by using the second.', 'how it is done is beyond the scope of this paper but is explained in detail in  #TAUTHOR_TAG']","['we will lose by using the second.', 'how it is done is beyond the scope of this paper but is explained in detail in  #TAUTHOR_TAG']","['we will lose by using the second.', 'how it is done is beyond the scope of this paper but is explained in detail in  #TAUTHOR_TAG']","['', 'specifically, it will be unable to fill in a row number should that value be missing in the incoming sentence.', 'the first option also has its drawbacks.', 'in this case, should the row number be missing in the sentence, the expectation parser will error correct the sentence to the most probable value, or the first one in the set if the probabilities are equal, here the value one for row 1.', 'thus, both options are imperfect in terms of the error correction capabilities that they can provide.', 'the comparison that must be made to determine which option is better in a given situation is how often the first will error correct incorrectly as opposed to how much error correcting power we will lose by using the second.', 'how it is done is beyond the scope of this paper but is explained in detail in  #TAUTHOR_TAG']",0
"[', or semantic nets as in  #AUTHOR_TAG.', 'that is, the current system learns procedures rather than data structures.', 'there is some literature on procedure acquisition such as the lisp synthesis work described in  #TAUTHOR_TAG and the prolog synthesis method of  #AUTHOR_TAG']","[', or semantic nets as in  #AUTHOR_TAG.', 'that is, the current system learns procedures rather than data structures.', 'there is some literature on procedure acquisition such as the lisp synthesis work described in  #TAUTHOR_TAG and the prolog synthesis method of  #AUTHOR_TAG']","[', or semantic nets as in  #AUTHOR_TAG.', 'that is, the current system learns procedures rather than data structures.', 'there is some literature on procedure acquisition such as the lisp synthesis work described in  #TAUTHOR_TAG and the prolog synthesis method of  #AUTHOR_TAG']","['vnlce processor may be considered to be a learning system of the tradition described, for example, in  #AUTHOR_TAG.', 'the current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in  #AUTHOR_TAG, assertional statements as in  #AUTHOR_TAG, or semantic nets as in  #AUTHOR_TAG.', 'that is, the current system learns procedures rather than data structures.', 'there is some literature on procedure acquisition such as the lisp synthesis work described in  #TAUTHOR_TAG and the prolog synthesis method of  #AUTHOR_TAG']",1
"[',  #TAUTHOR_TAG, lea']","[',  #TAUTHOR_TAG, lea']","[',  #TAUTHOR_TAG, lea 1980, lowerre and red']","['number of speech understanding systems have been developed during the past fifteen years ( barnett et al. 1980, dixon and martin 1979, erman et al. 1980,  #TAUTHOR_TAG, lea 1980, lowerre and reddy 1980, medress 1980, reddy 1976, walker 1978, and wolf and woods 1980 ).', 'most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'while some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'a detailed description of the kinds of expectation mechanisms appearing in these systems appears in  #AUTHOR_TAG']",1
"[""core'vocabulary in defining the words throughout the dictionary."", '(  #AUTHOR_TAG contains further description and discussion of ldoce. )', 'in this paper we focus on the exploitation of the ldoce grammar coding system ;  #AUTHOR_TAG and  #TAUTHOR_TAG describe further research in cambridge utilising different types of information available in ldoce']","[""entries and the consistent use of a controlled'core'vocabulary in defining the words throughout the dictionary."", '(  #AUTHOR_TAG contains further description and discussion of ldoce. )', 'in this paper we focus on the exploitation of the ldoce grammar coding system ;  #AUTHOR_TAG and  #TAUTHOR_TAG describe further research in cambridge utilising different types of information available in ldoce']","[""core'vocabulary in defining the words throughout the dictionary."", '(  #AUTHOR_TAG contains further description and discussion of ldoce. )', 'in this paper we focus on the exploitation of the ldoce grammar coding system ;  #AUTHOR_TAG and  #TAUTHOR_TAG describe further research in cambridge utilising different types of information available in ldoce']","['chose to employ ldoce as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""most prominent among these are the rich grammatical subcategorisations of the 60, 000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled'core'vocabulary in defining the words throughout the dictionary."", '(  #AUTHOR_TAG contains further description and discussion of ldoce. )', 'in this paper we focus on the exploitation of the ldoce grammar coding system ;  #AUTHOR_TAG and  #TAUTHOR_TAG describe further research in cambridge utilising different types of information available in ldoce']",0
"['the assignment of the v code seem to be purely syntactic.', ' #TAUTHOR_TAG and  #AUTHOR_TAG provide a more detailed analysis of the information encoded by the ldoce grammar codes and discuss their efficacy as a system of linguistic description.', ' #AUTHOR_TAG comprehensively compares different']","['the assignment of the v code seem to be purely syntactic.', ' #TAUTHOR_TAG and  #AUTHOR_TAG provide a more detailed analysis of the information encoded by the ldoce grammar codes and discuss their efficacy as a system of linguistic description.', ' #AUTHOR_TAG comprehensively compares different']","['the assignment of the v code seem to be purely syntactic.', ' #TAUTHOR_TAG and  #AUTHOR_TAG provide a more detailed analysis of the information encoded by the ldoce grammar codes and discuss their efficacy as a system of linguistic description.', ' #AUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the ldoce scheme can be evaluated']","['criteria for assignment of capital letters to verbs is not made explicit, but is influenced by the syntactic and semantic relations which hold between the verb and its arguments ; for example, i5, l5 and t5 can all be assigned to verbs which take a np subject and a sentential complement, but l5 will only be assigned if there is a fairly close semantic link between the two arguments and t5 will be used in preference to i5 if the verb is felt to be semantically two place rather than one place, such as know versus appear.', 'on the other hand, both believe and promise are assigned v3 which means they take a np object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs ; so the criteria for the assignment of the v code seem to be purely syntactic.', ' #TAUTHOR_TAG and  #AUTHOR_TAG provide a more detailed analysis of the information encoded by the ldoce grammar codes and discuss their efficacy as a system of linguistic description.', ' #AUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the ldoce scheme can be evaluated']",0
"['context of three collaborative projects  #TAUTHOR_TAG to develop a general - purpose, wide coverage morphological and syntactic analyse']","['context of three collaborative projects  #TAUTHOR_TAG to develop a general - purpose, wide coverage morphological and syntactic analyser for english.', 'one']","['frequent and rapid access to the information in the dictionary via the dictionary head words.', 'the research described below is taking place in the context of three collaborative projects  #TAUTHOR_TAG to develop a general - purpose, wide coverage morphological and syntactic analyser for english.', 'one motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by']","['developments in linguistics, and especially on grammatical theory - - for example, generalised phrase structure grammar ( gpsg )  #AUTHOR_TAG, lexical functional grammar ( lfg )  #AUTHOR_TAG - - and on natural language parsing frameworks for example, functional unification grammar ( fug )  #AUTHOR_TAG a ), patr - ii  #AUTHOR_TAG - - make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'these developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'real - time parsing imposes stringent requirements on a dictionary support environment ; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'the research described below is taking place in the context of three collaborative projects  #TAUTHOR_TAG to develop a general - purpose, wide coverage morphological and syntactic analyser for english.', 'one motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser']",0
['require ( see  #TAUTHOR_TAG'],['require ( see  #TAUTHOR_TAG'],['provide client programs with easy access to the information they require ( see  #TAUTHOR_TAG'],"['lispified ldoce file retains the broad structure of the typesetting tape and divides each entry into a number of fields - - head word, pronunciation, grammar codes, definitions, examples, and so forth.', 'however, each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see  #TAUTHOR_TAG for further discussion ).', 'for this purpose the formatting codes on the typesetting tape are crucial since they provide clues to the correct structure of this information.', 'for example, word senses are largely defined in terms of the 2000 word core vocabulary, however, in some cases other words ( themselves defined elsewhere in terms of this vocabulary ) are used.', 'these words always appear in small capitals and can therefore be recognised because they will be preceded by a font change control character.', 'in figure 1 above the definition of rivet as verb includes the noun definition of "" rivet 1\'\', as signalled by the font change and the numerical superscript which indicates that it is the first ( i. e.', 'noun entry ) homograph ; additional notation exists for word senses within homographs.', 'on the typesetting tape, font control characters are indicated by hexadecimal numbers within curly brackets.', 'in addition, there is a further complication because this sense is used in the plural and the plural morpheme must be removed before rivet can be associated with a dictionary entry.', 'however, the restructuring program can achieve this because such morphology is always italicised, so the program knows that, in the context of non - core vocabulary items, the italic font control character signals the figure 3 occurrence of a morphological variant of a ldoce head entry']",0
"['grammar codes and discuss their efficacy as a system of linguistic description.', ' #TAUTHOR_TAG comprehensively compares different']","['the assignment of the v code seem to be purely syntactic.', ' #AUTHOR_TAG and  #AUTHOR_TAG provide a more detailed analysis of the information encoded by the ldoce grammar codes and discuss their efficacy as a system of linguistic description.', ' #TAUTHOR_TAG comprehensively compares different']","['the assignment of the v code seem to be purely syntactic.', ' #AUTHOR_TAG and  #AUTHOR_TAG provide a more detailed analysis of the information encoded by the ldoce grammar codes and discuss their efficacy as a system of linguistic description.', ' #TAUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the ldoce scheme can be evaluated']","['criteria for assignment of capital letters to verbs is not made explicit, but is influenced by the syntactic and semantic relations which hold between the verb and its arguments ; for example, i5, l5 and t5 can all be assigned to verbs which take a np subject and a sentential complement, but l5 will only be assigned if there is a fairly close semantic link between the two arguments and t5 will be used in preference to i5 if the verb is felt to be semantically two place rather than one place, such as know versus appear.', 'on the other hand, both believe and promise are assigned v3 which means they take a np object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs ; so the criteria for the assignment of the v code seem to be purely syntactic.', ' #AUTHOR_TAG and  #AUTHOR_TAG provide a more detailed analysis of the information encoded by the ldoce grammar codes and discuss their efficacy as a system of linguistic description.', ' #TAUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the ldoce scheme can be evaluated']",1
"[', there are other syntactic and semantic tests for this distinction, ( see eg.  #TAUTHOR_TAG : 472 ), but these are the only ones which are explicit in the ldoce coding system']","[', there are other syntactic and semantic tests for this distinction, ( see eg.  #TAUTHOR_TAG : 472 ), but these are the only ones which are explicit in the ldoce coding system']","[', there are other syntactic and semantic tests for this distinction, ( see eg.  #TAUTHOR_TAG : 472 ), but these are the only ones which are explicit in the ldoce coding system']","[', there are other syntactic and semantic tests for this distinction, ( see eg.  #TAUTHOR_TAG : 472 ), but these are the only ones which are explicit in the ldoce coding system']",0
['epistle ) project  #TAUTHOR_TAG ; the former employs a'],['epistle ) project  #TAUTHOR_TAG ; the former employs a'],"['##ly epistle ) project  #TAUTHOR_TAG ; the former employs a dictionary of approximately 10, 000 words,']","['', 'permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the cl reference and this copyright notice are included on the first page.', 'to copy otherwise, or to republish, requires a fee and / or specific permission.', '0362 - 613x / 87 / 030203 - 218503. 00 on developing dictionary servers for office automation systems  #AUTHOR_TAG b ).', 'few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars ( eg.', ' #AUTHOR_TAG consult relatively small lexicons, typically generated by hand.', 'two exceptions to this generalisation are the linguistic string project  #AUTHOR_TAG and the ibm critique ( formerly epistle ) project  #TAUTHOR_TAG ; the former employs a dictionary of approximately 10, 000 words, most of which are specialist medical terms, the latter has well over 100, 000 entries, gathered from machine readable sources.', 'in addition, there are a number of projects under way to develop substantial lexicons from machine readable sources ( see  #AUTHOR_TAG for details ).', 'however, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.', 'in this paper we provide an evaluation of the ldoce grammar code system from this perspective']",1
['( see the introduction and  #TAUTHOR_TAG'],['of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the introduction and  #TAUTHOR_TAG'],['of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the introduction and  #TAUTHOR_TAG'],"['program which transforms the ldoce grammar codes into lexical entries utilisable by a parser takes as input the decompacted codes and produces a relatively theory neutral representation of the lexical entry for a particular word, in the sense that this representation could be further transformed into a format suitable for most current parsing systems.', 'for example, if the input were the third sense of believe, as in figure 4, the program would generate the ( partial ) entry shown in figure 8 figure 8 at the time of writing, rules for producing adequate entries to drive a parsing system have only been developed for verb codes.', 'in what follows we will describe the overall transformation strategy and the particular rules we have developed for the verb codes.', 'extending the system to handle nouns, adjectives and adverbs would present no problems of principle.', 'however, the ldoce coding of verbs is more comprehensive than elsewhere, so verbs are the obvious place to start in an evaluation of the usefulness of the coding system.', 'no attempt has been made to map any closed class entries from ldoce, as a 3, 000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the introduction and  #TAUTHOR_TAG']",0
"[""core'vocabulary in defining the words throughout the dictionary."", '(  #AUTHOR_TAG contains further description and discussion of ldoce. )', 'in this paper we focus on the exploitation of the ldoce grammar coding system ;  #TAUTHOR_TAG and  #AUTHOR_TAG describe further research in cambridge utilising different types of information available in ldoce']","[""entries and the consistent use of a controlled'core'vocabulary in defining the words throughout the dictionary."", '(  #AUTHOR_TAG contains further description and discussion of ldoce. )', 'in this paper we focus on the exploitation of the ldoce grammar coding system ;  #TAUTHOR_TAG and  #AUTHOR_TAG describe further research in cambridge utilising different types of information available in ldoce']","[""core'vocabulary in defining the words throughout the dictionary."", '(  #AUTHOR_TAG contains further description and discussion of ldoce. )', 'in this paper we focus on the exploitation of the ldoce grammar coding system ;  #TAUTHOR_TAG and  #AUTHOR_TAG describe further research in cambridge utilising different types of information available in ldoce']","['chose to employ ldoce as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""most prominent among these are the rich grammatical subcategorisations of the 60, 000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled'core'vocabulary in defining the words throughout the dictionary."", '(  #AUTHOR_TAG contains further description and discussion of ldoce. )', 'in this paper we focus on the exploitation of the ldoce grammar coding system ;  #TAUTHOR_TAG and  #AUTHOR_TAG describe further research in cambridge utilising different types of information available in ldoce']",0
"['ascot project  #TAUTHOR_TAG.', 'in this project, a new lexicon is being manually derived from ldoce.', 'the coding system']","['ascot project  #TAUTHOR_TAG.', 'in this project, a new lexicon is being manually derived from ldoce.', 'the coding system']","['by the ascot project  #TAUTHOR_TAG.', 'in this project, a new lexicon is being manually derived from ldoce.', 'the coding system']","['type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted ( see  #AUTHOR_TAG for further comment ).', 'one approach to this problem is that taken by the ascot project  #TAUTHOR_TAG.', 'in this project, a new lexicon is being manually derived from ldoce.', 'the coding system for the new lexicon is a slightly modified and simplified version of the ldoce scheme, without any loss of generalisation and expressive power.', 'more importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis.', 'in the medium term this approach, though time consuming, will be of some utility for producing more reliable lexicons for natural language processing']",0
['ii system  #TAUTHOR_TAG and references'],['system  #TAUTHOR_TAG and references'],[' #TAUTHOR_TAG and references'],"['output of the transformation program can be used to derive entries which are appropriate for particular grammatical formalisms.', 'to demonstrate that this is possible we have implemented a system which constructs dictionary entries for the patr - ii system  #TAUTHOR_TAG and references therein ).', 'patr - ii was chosen because it has been reimplemented in cambridge and was therefore, available ; however, the task would be nearly identical if we were constructing entries for a system based on gpsg, fug or lfg.', 'we the latter employs a grammatical formalism based on gpsg ; the comparatively theory neutral lexical entries that we construct from ldoce should translate straightforwardly into this framework as well']",5
"['ascot project  #TAUTHOR_TAG.', 'in this project, a new lexicon is being manually derived from ldoce.', 'the coding system']","['ascot project  #TAUTHOR_TAG.', 'in this project, a new lexicon is being manually derived from ldoce.', 'the coding system']","['by the ascot project  #TAUTHOR_TAG.', 'in this project, a new lexicon is being manually derived from ldoce.', 'the coding system']","['type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted ( see  #AUTHOR_TAG for further comment ).', 'one approach to this problem is that taken by the ascot project  #TAUTHOR_TAG.', 'in this project, a new lexicon is being manually derived from ldoce.', 'the coding system for the new lexicon is a slightly modified and simplified version of the ldoce scheme, without any loss of generalisation and expressive power.', 'more importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis.', 'in the medium term this approach, though time consuming, will be of some utility for producing more reliable lexicons for natural language processing']",0
[' #TAUTHOR_TAG and'],[' #TAUTHOR_TAG and'],['the linguistic string project  #TAUTHOR_TAG and the ibm critique ('],"['', 'permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the cl reference and this copyright notice are included on the first page.', 'to copy otherwise, or to republish, requires a fee and / or specific permission.', '0362 - 613x / 87 / 030203 - 218503. 00 on developing dictionary servers for office automation systems  #AUTHOR_TAG b ).', 'few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars ( eg.', ' #AUTHOR_TAG consult relatively small lexicons, typically generated by hand.', 'two exceptions to this generalisation are the linguistic string project  #TAUTHOR_TAG and the ibm critique ( formerly epistle ) project  #AUTHOR_TAG ; the former employs a dictionary of approximately 10, 000 words, most of which are specialist medical terms, the latter has well over 100, 000 entries, gathered from machine readable sources.', 'in addition, there are a number of projects under way to develop substantial lexicons from machine readable sources ( see  #AUTHOR_TAG for details ).', 'however, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.', 'in this paper we provide an evaluation of the ldoce grammar code system from this perspective']",1
"['.,  #TAUTHOR_TAG ;']","['held in secondary storage.', 'lisp is not particularly well suited for interfacing to complex, structured objects, and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in, eg.,  #TAUTHOR_TAG ;']","['.,  #TAUTHOR_TAG ;']","['is a well recognised problem with providing computational support for machine readable dictionaries, in particular where issues of access are concerned.', ""on the one hand, dictionaries exhibit far too much structure for conventional techniques for managing'flat'text to apply to them."", 'on the other hand, the equally large amounts of free text in dictionary entries, as well as the implicitly marked relationships commonly used to encode linguistic information, makes a dictionary difficult to represent as a structured database of a standard, eg.', 'relational, type.', 'in addition, in order to link the machine readable version of ldoce to our development environment, and eventually to our natural language processing systems, we need to provide fast access from lisp to data held in secondary storage.', 'lisp is not particularly well suited for interfacing to complex, structured objects, and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in, eg.,  #TAUTHOR_TAG ; on the other hand a method of access was clearly required, which was flexible enough to support a range of applications intending to make use of the ldoce tape']",0
"['.', 'in addition,  #TAUTHOR_TAG note that our object raising']","['appear to be counterexamples to our object raising rule.', 'in addition,  #TAUTHOR_TAG note that our object raising']","['of these verbs take sentential complements and therefore they appear to be counterexamples to our object raising rule.', 'in addition,  #TAUTHOR_TAG note that our object raising rule would assign mean to']","['four verbs which are misclassified as object equi and which do not have t5 codes anywhere in their entries are elect, love, represent and require.', 'none of these verbs take sentential complements and therefore they appear to be counterexamples to our object raising rule.', 'in addition,  #TAUTHOR_TAG note that our object raising rule would assign mean to this category incorrectly.', 'mean is assigned both a v3 and a t5 category in the code field associated with sense 2 ( i. e.', '"" intend "" ), however, when it is used in this sense it must be treated as an object equi verb']",1
['( see  #TAUTHOR_TAG for details )'],['( see  #TAUTHOR_TAG for details )'],['substantial lexicons from machine readable sources ( see  #TAUTHOR_TAG for details )'],"['', 'permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the cl reference and this copyright notice are included on the first page.', 'to copy otherwise, or to republish, requires a fee and / or specific permission.', '0362 - 613x / 87 / 030203 - 218503. 00 on developing dictionary servers for office automation systems  #AUTHOR_TAG b ).', 'few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars ( eg.', ' #AUTHOR_TAG consult relatively small lexicons, typically generated by hand.', 'two exceptions to this generalisation are the linguistic string project  #AUTHOR_TAG and the ibm critique ( formerly epistle ) project  #AUTHOR_TAG ; the former employs a dictionary of approximately 10, 000 words, most of which are specialist medical terms, the latter has well over 100, 000 entries, gathered from machine readable sources.', 'in addition, there are a number of projects under way to develop substantial lexicons from machine readable sources ( see  #TAUTHOR_TAG for details ).', 'however, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.', 'in this paper we provide an evaluation of the ldoce grammar code system from this perspective']",0
['( lfg )  #TAUTHOR_TAG - - and on natural language parsing'],['( lfg )  #TAUTHOR_TAG - - and on natural language parsing'],['( lfg )  #TAUTHOR_TAG - - and on natural language parsing frameworks'],"['developments in linguistics, and especially on grammatical theory - - for example, generalised phrase structure grammar ( gpsg )  #AUTHOR_TAG, lexical functional grammar ( lfg )  #TAUTHOR_TAG - - and on natural language parsing frameworks - - for example, functional unification grammar ( fug )  #AUTHOR_TAG a ), patr - ii  #AUTHOR_TAG - - make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'these developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'real - time parsing imposes stringent requirements on a dictionary support environment ; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'the research described below is taking place in the context of three collaborative projects  #AUTHOR_TAG to develop a general - purpose, wide coverage morphological and syntactic analyser for english.', 'one motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser']",0
[' #TAUTHOR_TAG in the brandeis verb catalogue'],[' #TAUTHOR_TAG in the brandeis verb catalogue'],[' #TAUTHOR_TAG in the brandeis verb catalogue'],"['is misclassified as object raising, rather than as object equi, because the relevant code field contains a t5 code, as well as a v3 code.', 'the t5 code is marked as\'rare \', and the occurrence of prefer with a tensed sentential complement, as opposed to with an infinitive, is certainly marginal : this example also highlights a deficiency in the ldoce coding system since prefer occurs much more naturally with a sentential complement if it collocates with a modal such as "" would "".', 'this deficiency is rectified in the verb classification system employed by  #TAUTHOR_TAG in the brandeis verb catalogue']",1
"['context of three collaborative projects  #TAUTHOR_TAG to develop a general - purpose, wide coverage morphological and syntactic analyse']","['context of three collaborative projects  #TAUTHOR_TAG to develop a general - purpose, wide coverage morphological and syntactic analyser for english.', 'one']","['frequent and rapid access to the information in the dictionary via the dictionary head words.', 'the research described below is taking place in the context of three collaborative projects  #TAUTHOR_TAG to develop a general - purpose, wide coverage morphological and syntactic analyser for english.', 'one motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by']","['developments in linguistics, and especially on grammatical theory - - for example, generalised phrase structure grammar ( gpsg )  #AUTHOR_TAG, lexical functional grammar ( lfg )  #AUTHOR_TAG - - and on natural language parsing frameworks for example, functional unification grammar ( fug )  #AUTHOR_TAG a ), patr - ii  #AUTHOR_TAG - - make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'these developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'real - time parsing imposes stringent requirements on a dictionary support environment ; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'the research described below is taking place in the context of three collaborative projects  #TAUTHOR_TAG to develop a general - purpose, wide coverage morphological and syntactic analyser for english.', 'one motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser']",0
"['by the conjunction of grammar codes and word qualifiers ( see  #TAUTHOR_TAG, for further details ).', 'however, exploiting this information to the full would be a non - trivial task, because it would require accessing the relevant']","['by the conjunction of grammar codes and word qualifiers ( see  #TAUTHOR_TAG, for further details ).', 'however, exploiting this information to the full would be a non - trivial task, because it would require accessing the relevant']","['by the conjunction of grammar codes and word qualifiers ( see  #TAUTHOR_TAG, for further details ).', 'however, exploiting this information to the full would be a non - trivial task, because it would require accessing the relevant knowledge about the words contained in the qualifier fields from their ldoce entries']","['are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see  #TAUTHOR_TAG, for further details ).', 'however, exploiting this information to the full would be a non - trivial task, because it would require accessing the relevant knowledge about the words contained in the qualifier fields from their ldoce entries']",0
['of large corpora ( eg.  #TAUTHOR_TAG'],['of large corpora ( eg.  #TAUTHOR_TAG'],"[', the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora ( eg.  #TAUTHOR_TAG.', 'however, in the short term, the approach outlined in this paper will allow us to produce a relatively sophisticated and useful dictionary rapidly']","['', 'this suggests that the automatic construction of dictionaries from published sources intended for other purposes will have a limited life unless lexicography is heavily influenced by the requirements of automated natural language analysis.', 'in the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora ( eg.  #TAUTHOR_TAG.', 'however, in the short term, the approach outlined in this paper will allow us to produce a relatively sophisticated and useful dictionary rapidly']",3
"['.', ' #TAUTHOR_TAG consult relatively small lexicon']","['employ very comprehensive grammars ( eg.', ' #TAUTHOR_TAG consult relatively small lexicons, typically generated by hand']","['.', ' #TAUTHOR_TAG consult relatively small lexicons, typically generated by hand.', 'two exceptions to this generalisation']","['', 'permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the cl reference and this copyright notice are included on the first page.', 'to copy otherwise, or to republish, requires a fee and / or specific permission.', '0362 - 613x / 87 / 030203 - 218503. 00 on developing dictionary servers for office automation systems  #AUTHOR_TAG b ).', 'few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars ( eg.', ' #TAUTHOR_TAG consult relatively small lexicons, typically generated by hand.', 'two exceptions to this generalisation are the linguistic string project  #AUTHOR_TAG and the ibm critique ( formerly epistle ) project  #AUTHOR_TAG ; the former employs a dictionary of approximately 10, 000 words, most of which are specialist medical terms, the latter has well over 100, 000 entries, gathered from machine readable sources.', 'in addition, there are a number of projects under way to develop substantial lexicons from machine readable sources ( see  #AUTHOR_TAG for details ).', 'however, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.', 'in this paper we provide an evaluation of the ldoce grammar code system from this perspective']",1
"['( see  #TAUTHOR_TAG, for further discussion )']","['in other types of vp as well as in cases of np, ap and pp predication ( see  #TAUTHOR_TAG, for further discussion )']","['( 3 ) ( type 20raising ) indicates that this is a two place predicate and that, if believe ( 3 ) occurs with a syntactic direct object, as in ( 1 ) john believes the earth to be round it will function as the logical subject of the predicate complement.', 'michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of vp as well as in cases of np, ap and pp predication ( see  #TAUTHOR_TAG, for further discussion )']","['solution we have adopted is to derive a semantic classification of the particular sense of the verb under consideration on the basis of the complete set of codes assigned to that sense.', 'in any subcategorisation frame which involves a predicate complement there will be a non - transparent relationship between the superficial syntactic form and the underlying logical relations in the sentence.', 'in these situations the parser can use the semantic type of the verb to compute this relationship.', 'expanding on a suggestion of  #AUTHOR_TAG, we classify verbs as subject equi, object equi, subject raising or object raising for each sense which has a predicate complement code associated with it.', 'these terms, which derive from transformational grammar, are used as convenient labels for what we regard as a semantic distinction ; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation.', 'for example, labelling believe ( 3 ) ( type 20raising ) indicates that this is a two place predicate and that, if believe ( 3 ) occurs with a syntactic direct object, as in ( 1 ) john believes the earth to be round it will function as the logical subject of the predicate complement.', 'michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of vp as well as in cases of np, ap and pp predication ( see  #TAUTHOR_TAG, for further discussion )']",0
"[""core'vocabulary in defining the words throughout the dictionary."", ' #TAUTHOR_TAG contains further description and discussion of ldoce. )', 'in this paper we focus on the exploitation of the ldoce grammar coding system ;  #AUTHOR_TAG and  #AUTHOR_TAG describe further research in cambridge utilising different types of information available in ldoce']","[""entries and the consistent use of a controlled'core'vocabulary in defining the words throughout the dictionary."", ' #TAUTHOR_TAG contains further description and discussion of ldoce. )', 'in this paper we focus on the exploitation of the ldoce grammar coding system ;  #AUTHOR_TAG and  #AUTHOR_TAG describe further research in cambridge utilising different types of information available in ldoce']","[""core'vocabulary in defining the words throughout the dictionary."", ' #TAUTHOR_TAG contains further description and discussion of ldoce. )', 'in this paper we focus on the exploitation of the ldoce grammar coding system ;  #AUTHOR_TAG and  #AUTHOR_TAG describe further research in cambridge utilising different types of information available in ldoce']","['chose to employ ldoce as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""most prominent among these are the rich grammatical subcategorisations of the 60, 000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled'core'vocabulary in defining the words throughout the dictionary."", ' #TAUTHOR_TAG contains further description and discussion of ldoce. )', 'in this paper we focus on the exploitation of the ldoce grammar coding system ;  #AUTHOR_TAG and  #AUTHOR_TAG describe further research in cambridge utilising different types of information available in ldoce']",0
"['.', 'expanding on a suggestion of  #TAUTHOR_TAG, we classify verbs as']","['verb to compute this relationship.', 'expanding on a suggestion of  #TAUTHOR_TAG, we classify verbs as']","['', 'in these situations the parser can use the semantic type of the verb to compute this relationship.', 'expanding on a suggestion of  #TAUTHOR_TAG, we classify verbs as']","['solution we have adopted is to derive a semantic classification of the particular sense of the verb under consideration on the basis of the complete set of codes assigned to that sense.', 'in any subcategorisation frame which involves a predicate complement there will be a non - transparent relationship between the superficial syntactic form and the underlying logical relations in the sentence.', 'in these situations the parser can use the semantic type of the verb to compute this relationship.', 'expanding on a suggestion of  #TAUTHOR_TAG, we classify verbs as subject equi, object equi, subject raising or object raising for each sense which has a predicate complement code associated with it.', 'these terms, which derive from transformational grammar, are used as convenient labels for what we regard as a semantic distinction ; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation.', 'for example, labelling believe ( 3 ) ( type 20raising ) indicates that this is a two place predicate and that, if believe ( 3 ) occurs with a syntactic direct object, as in ( 1 ) john believes the earth to be round it will function as the logical subject of the predicate complement.', 'michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of vp as well as in cases of np, ap and pp predication ( see  #AUTHOR_TAG, for further discussion )']",2
"['pronunciation field is available ;  #TAUTHOR_TAG has merged information from the pronunciation and hyphenation fields, creating an enhanced phonological representation']","['pronunciation field is available ;  #TAUTHOR_TAG has merged information from the pronunciation and hyphenation fields, creating an enhanced phonological representation']","['the master ldoce file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'in addition to headwords, dictionary search through the pronunciation field is available ;  #TAUTHOR_TAG has merged information from the pronunciation and hyphenation fields, creating an enhanced phonological representation']","['the master ldoce file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'in addition to headwords, dictionary search through the pronunciation field is available ;  #TAUTHOR_TAG has merged information from the pronunciation and hyphenation fields, creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure  #AUTHOR_TAG.', 'in addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information  #AUTHOR_TAG.', 'independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample']",5
"['of  #TAUTHOR_TAG, 1985 ).', 'the codes are doubly articulated ; capital letters represent the grammatical']","['of  #TAUTHOR_TAG, 1985 ).', 'the codes are doubly articulated ; capital letters represent the grammatical']","['the grammar codes have been restructured, it still remains to be shown that the information they encode is going to be of some utility for natural language processing.', 'the grammar code system used in ldoce is based quite closely on the descriptive grammatical framework of  #TAUTHOR_TAG, 1985 ).', 'the codes are doubly articulated ; capital letters represent the grammatical relations']","['the grammar codes have been restructured, it still remains to be shown that the information they encode is going to be of some utility for natural language processing.', 'the grammar code system used in ldoce is based quite closely on the descriptive grammatical framework of  #TAUTHOR_TAG, 1985 ).', 'the codes are doubly articulated ; capital letters represent the grammatical relations which hold between a verb and its arguments and numbers represent subcategorisation frames which a verb can appear in.', 'most of the subcategorisation frames are specified by syntactic category, but some are very ill - specified ; for instance, 9 is defined as "" needs a descriptive word or phrase "".', 'in practice many adverbial and predicative complements will satisfy this code, when attached to a verb ; for example, put [ xg ] where the code marks a locative adverbial prepositional phrase vs. make under sense 14 ( hereafter written make ( 14 ) ) is coded ix9 ] where it marks a predicative noun phrase or prepositional phrase']",2
"['possible by the fact that while far from being a database in the accepted sense of the word, the ldoce typesetting tape is the only truly computerised dictionary of english  #TAUTHOR_TAG']","['possible by the fact that while far from being a database in the accepted sense of the word, the ldoce typesetting tape is the only truly computerised dictionary of english  #TAUTHOR_TAG']","['s - expression, we adopted the approach of converting the tape source into a set of list structures, one per entry.', 'our task was made possible by the fact that while far from being a database in the accepted sense of the word, the ldoce typesetting tape is the only truly computerised dictionary of english  #TAUTHOR_TAG']","['that we were targeting all envisaged access routes from ldoce to systems implemented in lisp, and since the natural data structure for lisp is the s - expression, we adopted the approach of converting the tape source into a set of list structures, one per entry.', 'our task was made possible by the fact that while far from being a database in the accepted sense of the word, the ldoce typesetting tape is the only truly computerised dictionary of english  #TAUTHOR_TAG']",0
"['the assignment of the v code seem to be purely syntactic.', ' #AUTHOR_TAG and  #TAUTHOR_TAG provide a more detailed analysis of the information encoded by the ldoce grammar codes and discuss their efficacy as a system of linguistic description.', ' #AUTHOR_TAG comprehensively compares different']","['the assignment of the v code seem to be purely syntactic.', ' #AUTHOR_TAG and  #TAUTHOR_TAG provide a more detailed analysis of the information encoded by the ldoce grammar codes and discuss their efficacy as a system of linguistic description.', ' #AUTHOR_TAG comprehensively compares different']","['the assignment of the v code seem to be purely syntactic.', ' #AUTHOR_TAG and  #TAUTHOR_TAG provide a more detailed analysis of the information encoded by the ldoce grammar codes and discuss their efficacy as a system of linguistic description.', ' #AUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the ldoce scheme can be evaluated']","['criteria for assignment of capital letters to verbs is not made explicit, but is influenced by the syntactic and semantic relations which hold between the verb and its arguments ; for example, i5, l5 and t5 can all be assigned to verbs which take a np subject and a sentential complement, but l5 will only be assigned if there is a fairly close semantic link between the two arguments and t5 will be used in preference to i5 if the verb is felt to be semantically two place rather than one place, such as know versus appear.', 'on the other hand, both believe and promise are assigned v3 which means they take a np object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs ; so the criteria for the assignment of the v code seem to be purely syntactic.', ' #AUTHOR_TAG and  #TAUTHOR_TAG provide a more detailed analysis of the information encoded by the ldoce grammar codes and discuss their efficacy as a system of linguistic description.', ' #AUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the ldoce scheme can be evaluated']",0
['- ii  #TAUTHOR_TAG - -'],[' #TAUTHOR_TAG - -'],['- ii  #TAUTHOR_TAG - -'],"['developments in linguistics, and especially on grammatical theory - - for example, generalised phrase structure grammar ( gpsg )  #AUTHOR_TAG, lexical functional grammar ( lfg )  #AUTHOR_TAG - - and on natural language parsing frameworks - - for example, functional unification grammar ( fug )  #AUTHOR_TAG a ), patr - ii  #TAUTHOR_TAG - - make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'these developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'real - time parsing imposes stringent requirements on a dictionary support environment ; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'the research described below is taking place in the context of three collaborative projects  #AUTHOR_TAG to develop a general - purpose, wide coverage morphological and syntactic analyser for english.', 'one motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser']",0
"['to entries by broad phonetic class and syllable structure  #TAUTHOR_TAG.', 'in addition, a fully flexible access system']","['to entries by broad phonetic class and syllable structure  #TAUTHOR_TAG.', 'in addition, a fully flexible access system']","['to entries by broad phonetic class and syllable structure  #TAUTHOR_TAG.', 'in addition, a fully flexible access system']","['the master ldoce file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'in addition to headwords, dictionary search through the pronunciation field is available ;  #AUTHOR_TAG has merged information from the pronunciation and hyphenation fields, creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure  #TAUTHOR_TAG.', 'in addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information  #AUTHOR_TAG.', 'independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample']",0
"[' #TAUTHOR_TAG.', 'a grammar code describes a']","[' #TAUTHOR_TAG.', 'a grammar code describes a']","[' #TAUTHOR_TAG.', 'a grammar code describes a particular pattern of behaviour of a word.', 'patterns are descriptive, and are used to convey a range of information : eg.', 'distinctions between count and mass nouns ( dog vs. desire ), predicative, postpositive and attributive adjectives ( asleep vs. elect vs. jokular ), noun complementation ( fondness, fact ) and, most importantly, verb complementation and valency']","['what follows we will discuss the format of the grammar codes in some detail as they are the focus of the current paper, however, the reader should bear in mind that they represent only one comparatively constrained field of an ldoce entry and therefore, a small proportion of the overall restructuring task.', 'figure 4 illustrates the grammar code field for the third word sense of the verb believe as it appears in the published dictionary, on the typesetting tape and after restructuring.', 'figure 4 ldoce provides considerably more syntactic information than a traditional dictionary.', 'the longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information, usually to be found only in large descriptive grammars of english ( such as  #TAUTHOR_TAG.', 'a grammar code describes a particular pattern of behaviour of a word.', 'patterns are descriptive, and are used to convey a range of information : eg.', 'distinctions between count and mass nouns ( dog vs. desire ), predicative, postpositive and attributive adjectives ( asleep vs. elect vs. jokular ), noun complementation ( fondness, fact ) and, most importantly, verb complementation and valency']",0
"['employed  #TAUTHOR_TAG : 460 ff. )', 'however, only two of these criteria are explicit in the coding system']","['employed  #TAUTHOR_TAG : 460 ff. )', 'however, only two of these criteria are explicit in the coding system']","[' #TAUTHOR_TAG : 460 ff. )', 'however, only two of these criteria are explicit in the coding system']","['', 'thirdly, it seems clear that the object raising rule is straining the limits of what can be reliably extracted from the ldoce coding system.', 'ideally, to distinguish between raising and equi verbs, a number of syntactic criteria should be employed  #TAUTHOR_TAG : 460 ff. )', 'however, only two of these criteria are explicit in the coding system']",3
"['the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information  #TAUTHOR_TAG.', 'independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample']","['the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information  #TAUTHOR_TAG.', 'independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample']","['the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information  #TAUTHOR_TAG.', 'independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample']","['the master ldoce file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'in addition to headwords, dictionary search through the pronunciation field is available ;  #AUTHOR_TAG has merged information from the pronunciation and hyphenation fields, creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure  #AUTHOR_TAG.', 'in addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information  #TAUTHOR_TAG.', 'independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample']",0
"['tested the classification of verbs into semantic types using a verb list of 139 pre - classified items drawn from the lists published in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'figure 16 gives the number of verbs classified under each category by these authors and the number successfully classified into the same categories by the system']","['tested the classification of verbs into semantic types using a verb list of 139 pre - classified items drawn from the lists published in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'figure 16 gives the number of verbs classified under each category by these authors and the number successfully classified into the same categories by the system']","['tested the classification of verbs into semantic types using a verb list of 139 pre - classified items drawn from the lists published in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'figure 16 gives the number of verbs classified under each category by these authors and the number successfully classified into the same categories by the system']","['tested the classification of verbs into semantic types using a verb list of 139 pre - classified items drawn from the lists published in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'figure 16 gives the number of verbs classified under each category by these authors and the number successfully classified into the same categories by the system']",5
"[' #TAUTHOR_TAG points out, given that no situations were envisaged']","[' #TAUTHOR_TAG points out, given that no situations were envisaged']","[' #TAUTHOR_TAG points out, given that no situations were envisaged']","['series of systems in cambridge are implemented in lisp running under unixtm.', 'they all make use of an efficient dictionary access system which services requests for s - expression entries made by client pro - grams.', 'a dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved.', 'as  #TAUTHOR_TAG points out, given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.', 'the use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation.', 'for reasons of efficiency and flexibility of customisation, namely the use of ldoce by different client programs and from different lisp and / or prolog systems, the dictionary access system is implemented in the programming language c and makes use of the inter - process communication facilities provided by the unix operating system.', 'to the lisp programmer, the creation of a dictionary process and subsequent requests for information from the dictionary appear simply as lisp function calls']",0
"['context of three collaborative projects  #TAUTHOR_TAG to develop a general - purpose, wide coverage morphological and syntactic analyse']","['context of three collaborative projects  #TAUTHOR_TAG to develop a general - purpose, wide coverage morphological and syntactic analyser for english.', 'one']","['frequent and rapid access to the information in the dictionary via the dictionary head words.', 'the research described below is taking place in the context of three collaborative projects  #TAUTHOR_TAG to develop a general - purpose, wide coverage morphological and syntactic analyser for english.', 'one motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by']","['developments in linguistics, and especially on grammatical theory - - for example, generalised phrase structure grammar ( gpsg )  #AUTHOR_TAG, lexical functional grammar ( lfg )  #AUTHOR_TAG - - and on natural language parsing frameworks for example, functional unification grammar ( fug )  #AUTHOR_TAG a ), patr - ii  #AUTHOR_TAG - - make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'these developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'real - time parsing imposes stringent requirements on a dictionary support environment ; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'the research described below is taking place in the context of three collaborative projects  #TAUTHOR_TAG to develop a general - purpose, wide coverage morphological and syntactic analyser for english.', 'one motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser']",0
['. g.  #TAUTHOR_TAG ;'],"['comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'many investigators ( e. g.', 'many investigators ( e. g.  #TAUTHOR_TAG ; elowitz']",['. g.  #TAUTHOR_TAG ;'],"['text - to - speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'many investigators ( e. g.', 'many investigators ( e. g.  #TAUTHOR_TAG ; elowitz et al. 1976 ; luce et al. 1983 ; cahn 1988 ) have suggested that the poor prosody of synthetic speech, in comparison with natural speech, is the primary factor leading to difficulties in the comprehension of fluent synthetic speech.', ""and while researchers in text - tospeech synthesis have adopted a variety of approaches to prosodic phrase generation - - from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in  #AUTHOR_TAG and o' #AUTHOR_TAG - - - the generation of appropriate prosodic phrasing in unres ~ tricted text has remained a problem."", 'as we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', '']",4
"[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']","['previous work ( bachenko et al. 1986 ), we described an experimental text - to - speech system that determined prosodic phrasing for the olive - - liberman synthesizer  #TAUTHOR_TAG.', 'the system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'while we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'this paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'like the earlier version, it has been implemented in a text - to - speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries']",0
[';  #TAUTHOR_TAG have suggested'],[';  #TAUTHOR_TAG have suggested'],['. 1983 ;  #TAUTHOR_TAG have suggested'],[' #TAUTHOR_TAG'],4
"[' #TAUTHOR_TAG.', 'two concerns']","[' #TAUTHOR_TAG.', 'two concerns']","[' #TAUTHOR_TAG.', 'two concerns']","['have built an experimental text - to - speech system that uses our analysis of prosody to generate phrase boundaries for the olive - - liberman synthesizer  #TAUTHOR_TAG.', 'two concerns motivated our implementation.', 'first, we hoped the system would provide us with a research tool for testing our ideas about syntax and phrasing against a large unrestricted collection of sentences.', 'second, we wished to investigate how well our approach would work for determining prosodic phrasing in a text - to - speech synthesizer']",5
"['status in the discourse.', ' #TAUTHOR_TAG and  #AUTHOR_TAG also examine the']","['status in the discourse.', ' #TAUTHOR_TAG and  #AUTHOR_TAG also examine the']","['status in the discourse.', ' #TAUTHOR_TAG and  #AUTHOR_TAG also examine the relation between discourse and prosodic phrasing.', 'their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from']","['', ' #AUTHOR_TAG and  #AUTHOR_TAG take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in  #AUTHOR_TAG also assume that phrasing is dependent on predicate - argument structure.', 'the problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'in 17a - f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'all of the sentences have the same x ( vy ) pattern even though y is a complement in the first case ( thefirst serious attempt ) and an adjunct in the others.', '( the complement in 17a and the adjuncts in 17b - f are italicized. )', 'the relation between discourse and prosodic phrasing has been examined in some detail by  #AUTHOR_TAG, who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'this may be the reason that word count and syllable count play a prominent role in prosodic phrasing ( see section 2. 1. 3. ).', 'to our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', ' #TAUTHOR_TAG and  #AUTHOR_TAG also examine the relation between discourse and prosodic phrasing.', 'their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing']",0
"['often refers to examples of embedded sentences.', 'sentences like 12, from  #TAUTHOR_TAG, are frequently cited.', '( square brackets mark off the np constituents that contain embed - ded sentences.']","['often refers to examples of embedded sentences.', 'sentences like 12, from  #TAUTHOR_TAG, are frequently cited.', '( square brackets mark off the np constituents that contain embed - ded sentences.']","['often refers to examples of embedded sentences.', 'sentences like 12, from  #TAUTHOR_TAG, are frequently cited.', '( square brackets mark off the np constituents that contain embed - ded sentences.']","['it comes to sentence - level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'this observation has led some researchers, e. g., cooper and paccia -  #AUTHOR_TAG, to claim a direct mapping between the syntactic phrase and the prosodic phra, ; e.', ""however, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'for example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'sentences like 12, from  #TAUTHOR_TAG, are frequently cited.', '( square brackets mark off the np constituents that contain embed - ded sentences.']",0
"[' #TAUTHOR_TAG, we described an experimental text - to - speech system that']","[' #TAUTHOR_TAG, we described an experimental text - to - speech system that']","['previous work  #TAUTHOR_TAG, we described an experimental text - to - speech system that']","['previous work  #TAUTHOR_TAG, we described an experimental text - to - speech system that determined prosodic phrasing for the olive - - liberman synthesizer ( olive and liberman 1985 ).', 'the system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'while we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'this paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'like the earlier version, it has been implemented in a text - to - speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries']",2
"['', ' #TAUTHOR_TAG proposes readjust']","['proper.', ' #TAUTHOR_TAG proposes readjustment rules similar']","['', ' #TAUTHOR_TAG proposes readjustment rules similar']","['', 'rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12 ] "" ( p.', '372 ).', 'thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', ' #TAUTHOR_TAG proposes readjustment rules similar to those of chomsky and halle, but he claims that the readjustment of structure is part of the grammar, not part of the performance model.', 'he thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2 - - that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface, ; with prosodic phonology']",0
"['psycholinguistic studies of  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG, responding to the idea of readjust']","['psycholinguistic studies of  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG, responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if']","['psycholinguistic studies of  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG, responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if read']","['psycholinguistic studies of  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG, responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'for example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i. e., the data reveal both x ( vy ) and ( xv ) y groupings.', ' #AUTHOR_TAG claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length ; for example, the main prosodic phrase break corresponds to the subject - predicate boundary in waiters who remember well ] [ serve orders correctly.', 'discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in chickens were eating i [ the remaining green vegetables, where the subject - predicate boundary finds no prosodic correspondent.']",0
"['in  #TAUTHOR_TAG,']","['in  #TAUTHOR_TAG,']","['in  #TAUTHOR_TAG,']","['work on the prosodic phrase status of clause final prepositional phrases, which we discuss below, suggests the existence of a discourse - neutral phrasing that depends on syntactic constituency mediated by string adjacency and length of a potential prosodic phrase.', '3 such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in  #TAUTHOR_TAG, which "" are often overwhelmed by the chiaroscuro of highlight and background in discourse, but retain the status of null - hypothesis patterns that emerge when there is no good reason to take some other option "" ( p. 251 ).', 'this approach to prosodic phrase boundary determination brings us closer to a framework in which phonological, syntactic, and discourse features all contribute to prosodic phrasing']",1
"['##tened structure that more accurately reflects the prosodic phrasing.', 'in  #TAUTHOR_TAG,']","['flattened structure that more accurately reflects the prosodic phrasing.', 'in  #TAUTHOR_TAG,']","['', 'the result is a flattened structure that more accurately reflects the prosodic phrasing.', 'in  #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],0
"['whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'an alternative representation based on  #TAUTHOR_TAG is presented in  #AUTHOR_TAG, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'although a grid may be more descriptively suitable for some aspects of prosody ( for example,  #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing']","['whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'an alternative representation based on  #TAUTHOR_TAG is presented in  #AUTHOR_TAG, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'although a grid may be more descriptively suitable for some aspects of prosody ( for example,  #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing']","['g & g, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'an alternative representation based on  #TAUTHOR_TAG is presented in  #AUTHOR_TAG, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'although a grid may be more descriptively suitable for some aspects of prosody ( for example,  #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing']","['g & g, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'an alternative representation based on  #TAUTHOR_TAG is presented in  #AUTHOR_TAG, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'although a grid may be more descriptively suitable for some aspects of prosody ( for example,  #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing']",1
"['to prosodic phrasing.', ' #TAUTHOR_TAG claims that prosodic phrase boundaries']","['to prosodic phrasing.', ' #TAUTHOR_TAG claims that prosodic phrase boundaries']","['possible input to prosodic phrasing.', ' #TAUTHOR_TAG claims that prosodic phrase boundaries will co - occur with grammatical functions']","['syntax / prosody misalignment may be viewed as resulting in part from semantic considerations.', 'both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', ' #TAUTHOR_TAG']",0
"['psycholinguistic studies of  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, responding to the idea of readjust']","['psycholinguistic studies of  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if']","['psycholinguistic studies of  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if read']","['psycholinguistic studies of  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'for example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i. e., the data reveal both x ( vy ) and ( xv ) y groupings.', ' #AUTHOR_TAG claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length ; for example, the main prosodic phrase break corresponds to the subject - predicate boundary in waiters who remember well ] [ serve orders correctly.', 'discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in chickens were eating i [ the remaining green vegetables, where the subject - predicate boundary finds no prosodic correspondent.']",0
[';  #TAUTHOR_TAG ; cahn'],[';  #TAUTHOR_TAG ; cahn'],[';  #TAUTHOR_TAG ; cahn 1988 ) have suggested'],"['text - to - speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'many investigators ( e. g.', 'many investigators ( e. g. allen 1976 ; elowitz et al. 1976 ;  #TAUTHOR_TAG ; cahn 1988 ) have suggested that the poor prosody of synthetic speech, in comparison with natural speech, is the primary factor leading to difficulties in the comprehension of fluent synthetic speech.', ""and while researchers in text - tospeech synthesis have adopted a variety of approaches to prosodic phrase generation - - from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in  #AUTHOR_TAG and o' #AUTHOR_TAG - - - the generation of appropriate prosodic phrasing in unres ~ tricted text has remained a problem."", 'as we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', '']",4
"['g & g, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'an alternative representation based on  #AUTHOR_TAG is presented in  #AUTHOR_TAG, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'although a grid may be more descriptively suitable for some aspects of prosody ( for example,  #TAUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing']","['g & g, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'an alternative representation based on  #AUTHOR_TAG is presented in  #AUTHOR_TAG, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'although a grid may be more descriptively suitable for some aspects of prosody ( for example,  #TAUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing']","['g & g, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'an alternative representation based on  #AUTHOR_TAG is presented in  #AUTHOR_TAG, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'although a grid may be more descriptively suitable for some aspects of prosody ( for example,  #TAUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing']","['g & g, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'an alternative representation based on  #AUTHOR_TAG is presented in  #AUTHOR_TAG, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'although a grid may be more descriptively suitable for some aspects of prosody ( for example,  #TAUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing']",1
['described in  #TAUTHOR_TAG'],['described in  #TAUTHOR_TAG'],['described in  #TAUTHOR_TAG'],"['', ' #AUTHOR_TAG and  #AUTHOR_TAG take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in  #TAUTHOR_TAG also assume that phrasing is dependent on predicate - argument structure.', 'the problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'in 17a - f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'all of the sentences have the same x ( vy ) pattern even though y is a complement in the first case ( thefirst serious attempt ) and an adjunct in the others.', '( the complement in 17a and the adjuncts in 17b - f are italicized. )', 'the relation between discourse and prosodic phrasing has been examined in some detail by  #AUTHOR_TAG, who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'this may be the reason that word count and syllable count play a prominent role in prosodic phrasing ( see section 2. 1. 3. ).', 'to our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', ' #AUTHOR_TAG and  #AUTHOR_TAG also examine the relation between discourse and prosodic phrasing.', 'their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing']",2
"[',  #TAUTHOR_TAG,']","[',  #TAUTHOR_TAG,']","[',  #TAUTHOR_TAG,']","['it comes to sentence - level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'this observation has led some researchers, e. g.,  #TAUTHOR_TAG, to claim a direct mapping between the syntactic phrase and the prosodic phrase.', ""however, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'for example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'sentences like 12, from  #AUTHOR_TAG to account for such mismatches, "" readjustment rules "" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'the result is a flattened structure that more accurately reflects the prosodic phrasing.', '']",0
['is the analysis presented in  #TAUTHOR_TAG ('],['is the analysis presented in  #TAUTHOR_TAG ( henceforth g & g )'],['predicate boundary finds no prosodic correspondent. 4 the most explicit version of this approach is the analysis presented in  #TAUTHOR_TAG ('],"['psycholinguistic studies of  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'for example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i. e., the data reveal both x ( vy ) and ( xv ) y groupings.', ' #AUTHOR_TAG claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length ; for example, the main prosodic phrase break corresponds to the subject - predicate boundary in waiters who remember well ] [ serve orders correctly.', 'discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in chickens were eating ii the remaining green vegetables, where the subject - predicate boundary finds no prosodic correspondent. 4 the most explicit version of this approach is the analysis presented in  #TAUTHOR_TAG ( henceforth g & g )']",1
"['discourse and prosodic phrasing has been examined in some detail by  #TAUTHOR_TAG, who argues']","['discourse and prosodic phrasing has been examined in some detail by  #TAUTHOR_TAG, who argues']","['discourse and prosodic phrasing has been examined in some detail by  #TAUTHOR_TAG, who argues']","['', ' #AUTHOR_TAG and  #AUTHOR_TAG take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in  #AUTHOR_TAG also assume that phrasing is dependent on predicate - argument structure.', 'the problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'in 17a - f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'all of the sentences have the same x ( vy ) pattern even though y is a complement in the first case ( thefirst serious attempt ) and an adjunct in the others.', '( the complement in 17a and the adjuncts in 17b - f are italicized. )', 'the relation between discourse and prosodic phrasing has been examined in some detail by  #TAUTHOR_TAG, who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'this may be the reason that word count and syllable count play a prominent role in prosodic phrasing ( see section 2. 1. 3. ).', 'to our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', ' #AUTHOR_TAG and  #AUTHOR_TAG also examine the relation between discourse and prosodic phrasing.', 'their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing']",0
"['formation are adopted, for the most part, from g & g,  #TAUTHOR_TAG, and the account of monosyllabic destressing in  #AUTHOR_TAG.', 'thus in our analysis, rules of phonological']","['formation are adopted, for the most part, from g & g,  #TAUTHOR_TAG, and the account of monosyllabic destressing in  #AUTHOR_TAG.', 'thus in our analysis, rules of phonological']","['rules for phonological word formation are adopted, for the most part, from g & g,  #TAUTHOR_TAG, and the account of monosyllabic destressing in  #AUTHOR_TAG.', 'thus in our analysis, rules of']","['rules for phonological word formation are adopted, for the most part, from g & g,  #TAUTHOR_TAG, and the account of monosyllabic destressing in  #AUTHOR_TAG.', '']",5
"['impossible such a construction in real time.', 'secondly, the cooperative principle of  #TAUTHOR_TAG, 1978 ), under']","['impossible such a construction in real time.', 'secondly, the cooperative principle of  #TAUTHOR_TAG, 1978 ), under']","['impossible such a construction in real time.', 'secondly, the cooperative principle of  #TAUTHOR_TAG, 1978 ), under the assumption that referential levels']","[', there are at least three arguments against iterating pt.', 'first of all, iteration would increase the complexity of building a model of a paragraph ; infinite iteration would almost certainly make impossible such a construction in real time.', 'secondly, the cooperative principle of  #TAUTHOR_TAG, 1978 ), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader.', 'finally, it has been shown by  #AUTHOR_TAG that the ratio of derived to explicit information necessary for understanding a piece of text is about 8 : 1 ; furthermore, our reading of the analysis of five paragraphs by  #AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.', 'thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh - questions']",4
"['j.  #TAUTHOR_TAG, 1982 )']","['j.  #TAUTHOR_TAG, 1982 )']","['j.  #TAUTHOR_TAG, 1982 )']","['this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others.', 'we are going to make such a comparison with the theories proposed by j.  #TAUTHOR_TAG, 1982 ) that represent a more computationally oriented approach to coherence, and those of t. a. van dijk and w.  #AUTHOR_TAG, who are more interested in addressing psychological and cognitive aspects of discourse coherence.', 'the quoted works seem to be good representatives for each of the directions ; they also point to related literature']",1
"['( cf.  #TAUTHOR_TAG, and their kin.', 'similarly, the notion of']","['of "" unique - name assumption "" ( genesereth and nilsson 1987 ), "" domain closure as - sumption "" ( ibid. ), "" domain circumscription "" ( cf.  #TAUTHOR_TAG, and their kin.', 'similarly, the notion of r + m - abduction is spiritually']","['domain closure as - sumption "" ( ibid. ), "" domain circumscription "" ( cf.  #TAUTHOR_TAG, and their kin.', 'similarly, the notion of']","['techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of "" unique - name assumption "" ( genesereth and nilsson 1987 ), "" domain closure as - sumption "" ( ibid. ), "" domain circumscription "" ( cf.  #TAUTHOR_TAG, and their kin.', 'similarly, the notion of r + m - abduction is spiritually related to the "" abduc - tive inference "" of  #AUTHOR_TAG, the "" diagnosis from first principles "" of  #AUTHOR_TAG, "" explainability "" of  #AUTHOR_TAG, and the subset principle of  #AUTHOR_TAG.', 'but, ob - viously, trying to establish precise connections for the metarules or the provability and the r + m - abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'these connections are being examined elsewhere ( zadrozny forthcoming )']",1
['.  #TAUTHOR_TAG discussed sentences of the form * this is a chair but you can sit on it'],['.  #TAUTHOR_TAG discussed sentences of the form * this is a chair but you can sit on it'],['.  #TAUTHOR_TAG discussed sentences of the form * this is a chair but you can sit on it'],['.  #TAUTHOR_TAG discussed sentences of the form * this is a chair but you can sit on it'],0
"[' #TAUTHOR_TAG, p. 672 )']","[' #TAUTHOR_TAG, p. 672 )']","['and hasan 1976 ; cf. also  #TAUTHOR_TAG, p. 672 )']","['##s are function words - - like conjunctions and some adverbs - - that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '"" and, "" "" or, "" and "" but "" are the three main coordinating connectives in english.', 'however, "" but "" does not behave quite like the other two - - semantically, "" but "" signals a contradiction, and in this role it seems to have three subfunctions :..', 'opposition ( called "" adversative "" or "" contrary - to - expectation "" by halliday and hasan 1976 ; cf. also  #TAUTHOR_TAG, p. 672 )']",0
"[',  #TAUTHOR_TAG, longacre']","[',  #TAUTHOR_TAG, longacre']","['g. chafe 1979,  #TAUTHOR_TAG, longacre 1979, hab']","['there are other discussions of the paragraph as a central element of discourse ( e. g. chafe 1979,  #TAUTHOR_TAG, longacre 1979, haberlandt et al. 1980 ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']",1
"["". g., ` ` colorless green ideas...'' ), while before the advent of chomskyan formalisms, a sentence was defined as the smallest meaningful collection of words ;  #TAUTHOR_TAG, p. 546 ) gives 10 definitions of a sentence""]","[""current formal grammars allow nonsensical but parsable collections of words ( e. g., ` ` colorless green ideas...'' ), while before the advent of chomskyan formalisms, a sentence was defined as the smallest meaningful collection of words ;  #TAUTHOR_TAG, p. 546 ) gives 10 definitions of a sentence""]","["". g., ` ` colorless green ideas...'' ), while before the advent of chomskyan formalisms, a sentence was defined as the smallest meaningful collection of words ;  #TAUTHOR_TAG, p. 546 ) gives 10 definitions of a sentence""]","[', our definition of coherence may not be restrictive enough : two collections of sentences, one referring to "" black "" ( about black pencils, black pullovers, and black poodles ), the other one about "" death "" ( war, cancer, etc. ), connected by a sentence referring to both of these, could be interpreted as one paragraph about the new, broader topic "" black + death. ""', ""this problem may be similar to the situation in which current formal grammars allow nonsensical but parsable collections of words ( e. g., ` ` colorless green ideas...'' ), while before the advent of chomskyan formalisms, a sentence was defined as the smallest meaningful collection of words ;  #TAUTHOR_TAG, p. 546 ) gives 10 definitions of a sentence""]",0
"[' #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and implicitly or explicitly']","[' #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and implicitly or explicitly']","[' #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and implicitly or explicitly']","['logical notions that we are going to consider, such as theory or model, will be finitary.', 'for example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'therefore these notions, and all other constructs we are going to define ( axioms, metarules, definitions etc. ) are computational, although usually we will not provide explicit algorithms for computing them.', 'the issues of control are not so important for us at this point ; we restrict ourselves to describing the logic.', 'this principle of finitism is also assumed by  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'mycielski 1981 )']",1
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['', ""but, obviously, there are other possibilities - - for instance, the discourse representation structures ( drs's ) of  #TAUTHOR_TAG, which have been used to translate a subset of english into logical formulas, to model text ( identified with a list of sentences ), to analyze a fragment of english, and to deal with anaphora."", 'the logical notation of  #AUTHOR_TAG is more sophisticated, and may be considered another possibility.', ' #AUTHOR_TAG formalism is richer and resembles more closely an english grammar.', ' #AUTHOR_TAG writes "" it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys. ""', 'therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'it will also be a model for our simplified logical notation ( cf.', 'section 5 ).', 'we can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences']",1
"[') or quantifier scoping  #TAUTHOR_TAG must play a role, too']","[') or quantifier scoping  #TAUTHOR_TAG must play a role, too']","['the role of focus ( grosz 1977, 1978 ; sidner 1983 ) or quantifier scoping  #TAUTHOR_TAG must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( grosz 1977, 1978 ; sidner 1983 ) or quantifier scoping  #TAUTHOR_TAG must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']",0
"['to  #TAUTHOR_TAG, p. 67 ), these']","['to  #TAUTHOR_TAG, p. 67 ), these']","['to  #TAUTHOR_TAG, p. 67 ), these two sentences are incoherent.', 'however,']","['to  #TAUTHOR_TAG, p. 67 ), these two sentences are incoherent.', 'however, the same fragment, augmented with the third sentence mary told him yesterday that the french spinach crop failed and turkey is the only country... ( ibid. )', 'suddenly ( for hobbs ) becomes coherent.', ""it seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change ; after all, the first two sentences didn't change when the third one was added."", 'on the other hand, this change is easily explained when we treat the first two sentences as a paragraph : if the third sentence is not a part of the background knowledge, the paragraph is incoherent.', 'and the paragraph obtained by adding the third sentence is coherent.', 'moreover, coherence here is clearly the result of the existence of the topic "" john likes spinach.']",1
[' #TAUTHOR_TAG p. 195 ; za'],['( cfxxx  #TAUTHOR_TAG p. 195 ; zadrozny 1987a ) : we bought the boys apples because'],['( cfxxx  #TAUTHOR_TAG p. 195 ; za'],"['adopt the three - level semantics as a formal tool for the analysis of paragraphs.', 'this semantics was constructed ( zadrozny 1987a, 1987b ) as a formal framework for default and commonsense reasoning.', 'it should not come as a surprise that we can now use this apparatus for text / discourse analysis ; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.', 'for instance, relating "" they "" to "" apples "" in the sentence ( cfxxx  #TAUTHOR_TAG p. 195 ; zadrozny 1987a ) : we bought the boys apples because they were so']",0
['of paragraph units can be found in  #AUTHOR_TAG and  #TAUTHOR_TAG'],['of paragraph units can be found in  #AUTHOR_TAG and  #TAUTHOR_TAG'],"['by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in  #AUTHOR_TAG and  #TAUTHOR_TAG']","['example of psycholinguistically oriented research work can be found in  #AUTHOR_TAG.', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that are used, by readers, to identify a paragraph : ( 1 ) the repetition of content words ( nouns, verbs, adjectives, adverbs ) ; ( 2 ) pronoun reference ; and ( 3 ) paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in  #AUTHOR_TAG and  #TAUTHOR_TAG']",0
['of five paragraphs by  #TAUTHOR_TAG strongly suggests that only'],['of five paragraphs by  #TAUTHOR_TAG strongly suggests that only'],"[': 1 ; furthermore, our reading of the analysis of five paragraphs by  #TAUTHOR_TAG strongly suggests that only']","[', there are at least three arguments against iterating pt.', 'first of all, iteration would increase the complexity of building a model of a paragraph ; infinite iteration would almost certainly make impossible such a construction in real time.', 'secondly, the cooperative principle of  #AUTHOR_TAG grice (, 1978, under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader.', 'finally, it has been shown by  #AUTHOR_TAG that the ratio of derived to explicit information necessary for understanding a piece of text is about 8 : 1 ; furthermore, our reading of the analysis of five paragraphs by  #TAUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.', 'thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh - questions']",4
"['theory and background knowledge ( cfxxx  #TAUTHOR_TAG.', 'brachman']","['theory and background knowledge ( cfxxx  #TAUTHOR_TAG.', 'brachman']","['also distinguishes between an object theory and background knowledge ( cfxxx  #TAUTHOR_TAG.', 'brachman']","['last point may be seen better if we look at some differences between our system and krypton, which also distinguishes between an object theory and background knowledge ( cfxxx  #TAUTHOR_TAG.', 'brachman et al. 1985 ).', ""krypton's a - box, encoding the object theory as a set of assertions, uses standard first order logic ; the t - box contains information expressed in a frame - based language equivalent to a fragment of fol."", ""however, the distinction between the two parts is purely functional - - that is, characterized in terms of the system's behavior."", 'from the logical point of view, the knowledge base is the union of the two boxes, i. e. a theory, and the entailment is standard.', 'in our system, we also distinguish between the "" definitional "" and factual information, but the "" definitional "" part contains collections of mutually excluding theories, not just of formulas describing a semantic network.', 'moreover, in addition to proposing this structure of r, we have described the two mechanisms for exploiting it, "" coherence "" and "" dominance, "" which are not variants of the standard first order entailment, but abduction']",1
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],1
['##n 1987 ;  #TAUTHOR_TAG to see'],['1987 ;  #TAUTHOR_TAG to see'],['##n 1987 ;  #TAUTHOR_TAG to see'],"['text concerns events happening in time.', 'naturally, we will use a logical notation in which formulas may have temporal and event compo - nents.', 'we assume that any formal interpretation of time will agree with the intuitive one.', 'so it is not necessary now to present a formal semantics here.', 'the reader may consult recent papers on this subject ( e. g. moens and steedman 1987 ;  #TAUTHOR_TAG to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ;  #AUTHOR_TAG describes how first order logic can be augmented with such an operator.', 'extending and revising  #AUTHOR_TAG formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram - matical constraint ( that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon - - - ibid. )']",0
"['""', 'according to  #TAUTHOR_TAG, paragraphs']","['', 'according to  #TAUTHOR_TAG, paragraphs']","['""', 'according to  #TAUTHOR_TAG, paragraphs']",[' #TAUTHOR_TAG'],0
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],1
[';  #TAUTHOR_TAG ; patel - schneider'],[';  #TAUTHOR_TAG ; patel - schneider'],"[';  #TAUTHOR_TAG ; patel - schneider 1985 ).', 'levesque 1984 ; frisch']","['in this paper we take modus ponens as the main rule of inference, in general one can consider deductive closures with respect to weaker, nonstandard logics, ( cfxxx levesque 1984 ;  #TAUTHOR_TAG ; patel - schneider 1985 ).', 'levesque 1984 ; frisch 1987 ; patel - schneider 1985 ).', ""but we won't pursue this topic further here""]",1
"[""a is the same as b,'' etc. are not directly represented by logical equality ; similarly, ` ` not'' is often not treated as logical negation ; cfxxx  #TAUTHOR_TAG."", ' #AUTHOR_TAG']","[""as ` ` a is b,'' ` ` a is the same as b,'' etc. are not directly represented by logical equality ; similarly, ` ` not'' is often not treated as logical negation ; cfxxx  #TAUTHOR_TAG."", ' #AUTHOR_TAG']","[""a is the same as b,'' etc. are not directly represented by logical equality ; similarly, ` ` not'' is often not treated as logical negation ; cfxxx  #TAUTHOR_TAG."", ' #AUTHOR_TAG']","['explicitly stated otherwise, we assume that formulas are expressed in a certain ( formal ) language l without equality ; the extension l ( = ) of l is going to be used only in section 5 for dealing with noun phrase references.', ""this means that natural language expressions such as ` ` a is b,'' ` ` a is the same as b,'' etc. are not directly represented by logical equality ; similarly, ` ` not'' is often not treated as logical negation ; cfxxx  #TAUTHOR_TAG."", ' #AUTHOR_TAG']",1
"['all "" in the above definitions ( cf.  #TAUTHOR_TAG b ).', 'we will have, however, no need for']","[': the notions of strong provability and strong r + m - abduction can be in - troduced by replacing "" there exists "" by "" all "" in the above definitions ( cf.  #TAUTHOR_TAG b ).', 'we will have, however, no need for "" strong "" notions in this paper.', 'also, in a practical system,']","['all "" in the above definitions ( cf.  #TAUTHOR_TAG b ).', 'we will have, however, no need for']","[': the notions of strong provability and strong r + m - abduction can be in - troduced by replacing "" there exists "" by "" all "" in the above definitions ( cf.  #TAUTHOR_TAG b ).', 'we will have, however, no need for "" strong "" notions in this paper.', 'also, in a practical system, "" satisfies "" should be probably replaced by "" violates fewest.']",1
"['selectional restrictions ( semantic feature information,  #TAUTHOR_TAG does not solve the problem,']","['selectional restrictions ( semantic feature information,  #TAUTHOR_TAG does not solve the problem,']","['selectional restrictions ( semantic feature information,  #TAUTHOR_TAG does not solve the problem,']","['selectional restrictions ( semantic feature information,  #TAUTHOR_TAG does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ' #AUTHOR_TAG hobbs (, 1982 proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using "" salience "" in choosing facts from this knowledge base']",0
[' #TAUTHOR_TAG ; webber 1987 ) to see'],[' #TAUTHOR_TAG ; webber 1987 ) to see'],['.  #TAUTHOR_TAG ; webber 1987 ) to see'],"['text concerns events happening in time.', 'naturally, we will use a logical notation in which formulas may have temporal and event components.', 'we assume that any formal interpretation of time will agree with the intuitive one.', 'so it is not necessary now to present a formal semantics here.', 'the reader may consult recent papers on this subject ( e. g.  #TAUTHOR_TAG ; webber 1987 ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ;  #AUTHOR_TAG describes how first order logic can be augmented with such an operator.', 'extending and revising  #AUTHOR_TAG formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram - matical constraint ( that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon - - - ibid. )']",0
"['will be chosen as a best interpretation of s, ( cfxxx  #TAUTHOR_TAG a, 1987b ).', 'zadrozny 1987']","['will be chosen as a best interpretation of s, ( cfxxx  #TAUTHOR_TAG a, 1987b ).', 'zadrozny 1987a  #AUTHOR_TAG b.', 'another theory, consisting of']","['r ) interpretation of the words that appear in the sentence.', 'because it is also consistent, it will be chosen as a best interpretation of s, ( cfxxx  #TAUTHOR_TAG a, 1987b ).', 'zadrozny 1987']","['it is the "" highest "" path, fint is the most plausible ( relative to r ) interpretation of the words that appear in the sentence.', 'because it is also consistent, it will be chosen as a best interpretation of s, ( cfxxx  #TAUTHOR_TAG a, 1987b ).', 'zadrozny 1987a  #AUTHOR_TAG b.', 'another theory, consisting of f ~ = { el, sh2, pl, b2 ~ dl } and s, saying that a space vehicle came into the harbor and caused a disease ~ illness is less plausible according to that ordering.', 'as it turns out, f ~ is never constructed in the process of building an interpretation of a paragraph containing the sentence s, unless assuming fint would lead to a contradiction, for instance within the higher level context of a science fiction story']",0
"[',  #TAUTHOR_TAG, hab']","[',  #TAUTHOR_TAG, haberlandt et al.']","['. g. chafe 1979, halliday and hasan 1976,  #TAUTHOR_TAG, hab']","['there are other discussions of the paragraph as a central element of discourse ( e. g. chafe 1979, halliday and hasan 1976,  #TAUTHOR_TAG, haberlandt et al. 1980 ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']",1
"['to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cfxxx  #TAUTHOR_TAG.', 'mycielski']","['to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cfxxx  #TAUTHOR_TAG.', 'mycielski 1981 )']","['in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cfxxx  #TAUTHOR_TAG.', 'mycielski 1981 )']","['logical notions that we are going to consider, such as theory or model, will be finitary.', 'for example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'therefore these notions, and all other constructs we are going to define ( axioms, metarules, definitions etc. ) are computational, although usually we will not provide explicit algorithms for computing them.', 'the issues of control are not so important for us at this point ; we restrict ourselves to describing the logic.', 'this principle of finitism is also assumed by johnson -  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cfxxx  #TAUTHOR_TAG.', 'mycielski 1981 )']",0
"['have shown elsewhere  #TAUTHOR_TAG ; zadrozny 1987a, 1987b ) that natural language programs, such as on - line grammars and dictionaries,']","['have shown elsewhere  #TAUTHOR_TAG ; zadrozny 1987a, 1987b ) that natural language programs, such as on - line grammars and dictionaries,']","['have shown elsewhere  #TAUTHOR_TAG ; zadrozny 1987a, 1987b ) that natural language programs, such as on - line grammars and dictionaries,']","['have shown elsewhere  #TAUTHOR_TAG ; zadrozny 1987a, 1987b ) that natural language programs, such as on - line grammars and dictionaries, can be used as referential levels for commonsense reasoning - - for example, to disambiguate pp attachment.', 'this means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object - level theory']",2
['arguments has been recognized before :  #TAUTHOR_TAG'],['arguments has been recognized before :  #TAUTHOR_TAG'],['arguments has been recognized before :  #TAUTHOR_TAG call'],"['necessity of this kind of merging of arguments has been recognized before :  #TAUTHOR_TAG call it abductive unification / matching,  #AUTHOR_TAG, 1979 ) refers to such operations using the terms knitting or petty conversational implicature']",0
"[' #TAUTHOR_TAG is more sophisticated, and may be']","[' #TAUTHOR_TAG is more sophisticated, and may be']","[' #TAUTHOR_TAG is more sophisticated, and may be considered another possibility.', ' #AUTHOR_TAG formalism is richer']","['', ""but, obviously, there are other possibilities - - for instance, the discourse representation structures ( drs's ) of  #AUTHOR_TAG, which have been used to translate a subset of english into logical formulas, to model text ( identified with a list of sentences ), to analyze a fragment of english, and to deal with anaphora."", 'the logical notation of  #TAUTHOR_TAG is more sophisticated, and may be considered another possibility.', ' #AUTHOR_TAG formalism is richer and resembles more closely an english grammar.', ' #AUTHOR_TAG writes "" it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys. ""', 'therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'it will also be a model for our simplified logical notation ( cf.', 'section 5 ).', 'we can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences']",1
"['johnson -  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and implicitly or explicitly']","['johnson -  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and implicitly or explicitly']","['johnson -  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and implicitly or explicitly']","['logical notions that we are going to consider, such as theory or model, will be finitary.', 'for example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'therefore these notions, and all other constructs we are going to define ( axioms, metarules, definitions etc. ) are computational, although usually we will not provide explicit algorithms for computing them.', 'the issues of control are not so important for us at this point ; we restrict ourselves to describing the logic.', 'this principle of finitism is also assumed by johnson -  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'mycielski 1981 )']",1
['consequent and presupposition  #TAUTHOR_TAG : 112 ) have collected convincing evidence of'],['consequent and presupposition  #TAUTHOR_TAG : 112 ) have collected convincing evidence of'],['consequent and presupposition  #TAUTHOR_TAG : 112 ) have collected convincing evidence of the existence'],"['textualist approach to paragraph analysis is exemplified by e. j. crothers.', 'his work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'he lists, classifies, and discusses various types of inference, by which he means, generally, ` ` the linguistic - logical notions of consequent and presupposition  #TAUTHOR_TAG : 112 ) have collected convincing evidence of the existence of language chunks - - real structures, not just orthographic conventions - - that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature ( like sentences ).', '']",0
"['in logical theories.', 'however, it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfxxx  #TAUTHOR_TAG.', 'woods 1987 )']","['in logical theories.', 'however, it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfxxx  #TAUTHOR_TAG.', 'woods 1987 )']","['in logical theories.', 'however, it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfxxx  #TAUTHOR_TAG.', 'woods 1987 )']","['the word "" up "" is given its meaning relative to our experience with gravity, it is not free to "" slip "" into its opposite.', '"" up "" means up and not down....', 'we have a model that men and women couple to produce offspring who are similar to their parents, and this model is grounded in genetics, and the semantics of kinship metaphor is grounded in this model.', 'mothers have a different role than fathers in this model, and thus there is a reason why "" death is the father of beauty "" fails poetically while "" death is the mother of beauty "" succeeds....', 'it is precisely this "" grounding "" of logical predicates in other conceptual structures that we would like to capture.', 'we investigate here only the "" grounding "" in logical theories.', 'however, it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfxxx  #TAUTHOR_TAG.', 'woods 1987 )']",0
"['english grammar.', ' #TAUTHOR_TAG, p. 14 ) writes ` ` it would be pervers']","['english grammar.', ' #TAUTHOR_TAG, p. 14 ) writes ` ` it would be perverse']","['resembles more closely an english grammar.', ' #TAUTHOR_TAG, p. 14 ) writes ` ` it would be pervers']","['assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'its details are not important for our aim of giving a semantic interpretation of paragraphs ; the main theses of our theory do not depend on a logical notation.', 'so we will use a very simple formalism, like the one above, resembling the standard first order language.', ""but, obviously, there are other possibilities - - for instance, the discourse representation structures ( drs's ) of  #AUTHOR_TAG, which have been used to translate a subset of english into logical formulas, to model text ( identified with a list of sentences ), to analyze a fragment of english, and to deal with anaphora."", 'the logical notation of  #AUTHOR_TAG is more sophisticated, and may be considered another possibility.', ' #AUTHOR_TAG formalism is richer and resembles more closely an english grammar.', "" #TAUTHOR_TAG, p. 14 ) writes ` ` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.''"", 'therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'it will also be a model for our simplified logical notation ( cf.', 'section 5 ).', 'we can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences']",1
"['research work can be found in  #TAUTHOR_TAG.', 'these authors take the position that a paragraph is a psychologically real']","['research work can be found in  #TAUTHOR_TAG.', 'these authors take the position that a paragraph is a psychologically real']","['research work can be found in  #TAUTHOR_TAG.', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that']","['example of psycholinguistically oriented research work can be found in  #TAUTHOR_TAG.', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that are used, by readers, to identify a paragraph : ( 1 ) the repetition of content words ( nouns, verbs, adjectives, adverbs ) ; ( 2 ) pronoun reference ; and ( 3 ) paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholinguistic studies that confirm the validity of paragraph units can be found in  #AUTHOR_TAG and  #AUTHOR_TAG']",0
"['have been interested in the paragraph as a unit.', ' #TAUTHOR_TAG, p']","['have been interested in the paragraph as a unit.', ' #TAUTHOR_TAG, p. 112 ), for example, bemoans the fact that his']","['have been interested in the paragraph as a unit.', ' #TAUTHOR_TAG, p']","['demonstrates that information needed to identify and resolve anaphoric references can be found, to an interesting extent, in standard dictionaries and thesauri.', '( other reference works could be treated as additional sources of world knowledge. )', 'this type of consultation uses existing natural language texts as a referential level for processing purposes.', 'it is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit.', "" #TAUTHOR_TAG, p. 112 ), for example, bemoans the fact that his ` ` theory lacks a world knowledge component, a mental ` encyclopedia,'which could be invoked to generate inferences...''."", 'with respect to that independent source of knowledge, our main contributions are two.', 'first, we identify its possible structure ( a collection of partially ordered theories ) and make formal the choice of a most plausible interpretation.', 'in other words, we recognize it as a separate logical level - - the referential level.', 'second, we suggest that natural language reference works, like dictionaries and thesauri, can quite often fill the role of the referential level']",0
"['##ture ""  #TAUTHOR_TAG, or the']","['by using only "" petty conversational implicature ""  #TAUTHOR_TAG, or the']","['by using only "" petty conversational implicature ""  #TAUTHOR_TAG, or']","['. 1. 1 was the use of a gricean maxim necessary?', 'can one deal effectivelywith the problem of reference without axiomatized gricean maxims, for instance by using only "" petty conversational implicature ""  #TAUTHOR_TAG, or the metarules of section 5. 2?', 'it seems to us that the answer is no']",0
"['of paragraphs.', 'this semantics was constructed  #TAUTHOR_TAG a, 1987b ) as a formal']","['of paragraphs.', 'this semantics was constructed  #TAUTHOR_TAG a, 1987b ) as a formal']","['the analysis of paragraphs.', 'this semantics was constructed  #TAUTHOR_TAG a, 1987b ) as a formal framework for default and commonsense reasoning.', 'it should not']","['adopt the three - level semantics as a formal tool for the analysis of paragraphs.', 'this semantics was constructed  #TAUTHOR_TAG a, 1987b ) as a formal framework for default and commonsense reasoning.', 'it should not come as a surprise that we can now use this apparatus for text / discourse analysis ; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.', 'for instance, relating "" they "" to "" apples "" in the sentence ( cf.', 'haugeland 1985 p. 195 ; zadrozny 1987a )']",5
"[' #TAUTHOR_TAG.', 'but, obviously, trying to establish precise connections']","[' #TAUTHOR_TAG.', 'but, obviously, trying to establish precise connections']","[' #TAUTHOR_TAG.', 'but, obviously, trying to establish precise connections']",[' #TAUTHOR_TAG'],1
"[',  #TAUTHOR_TAG,']","[',  #TAUTHOR_TAG,']","['. g. chafe 1979, halliday and hasan 1976, longacre 1979,  #TAUTHOR_TAG,']","['there are other discussions of the paragraph as a central element of discourse ( e. g. chafe 1979, halliday and hasan 1976, longacre 1979,  #TAUTHOR_TAG, all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']",1
"['##7, 1978 ;  #TAUTHOR_TAG or quantifier scoping ( webber']","['role of focus ( grosz 1977, 1978 ;  #TAUTHOR_TAG or quantifier scoping ( webber']","['the role of focus ( grosz 1977, 1978 ;  #TAUTHOR_TAG or quantifier scoping ( webber 1983 ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( grosz 1977, 1978 ;  #TAUTHOR_TAG or quantifier scoping ( webber 1983 ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']",0
"['referential structures we are going to use are collections of logical theories, but the concept of reference is more general.', 'some of the intuitions we associate with this notion have been very well expressed by  #TAUTHOR_TAG, pp. 7 - 8 ) :... semantics is constrained by our models of ourselves and our worlds.', 'we have models of up and down that are based by the way our bodies actually function']","['referential structures we are going to use are collections of logical theories, but the concept of reference is more general.', 'some of the intuitions we associate with this notion have been very well expressed by  #TAUTHOR_TAG, pp. 7 - 8 ) :... semantics is constrained by our models of ourselves and our worlds.', 'we have models of up and down that are based by the way our bodies actually function']","['referential structures we are going to use are collections of logical theories, but the concept of reference is more general.', 'some of the intuitions we associate with this notion have been very well expressed by  #TAUTHOR_TAG, pp. 7 - 8 ) :... semantics is constrained by our models of ourselves and our worlds.', 'we have models of up and down that are based by the way our bodies actually function']","['referential structures we are going to use are collections of logical theories, but the concept of reference is more general.', 'some of the intuitions we associate with this notion have been very well expressed by  #TAUTHOR_TAG, pp. 7 - 8 ) :... semantics is constrained by our models of ourselves and our worlds.', 'we have models of up and down that are based by the way our bodies actually function']",0
"['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing ( hirst 1987 ;  #TAUTHOR_TAG, with one difference : the activation spreads to theories that share a predicate, not through the""]","['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing ( hirst 1987 ;  #TAUTHOR_TAG, with one difference : the activation spreads to theories that share a predicate, not through the""]","['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing ( hirst 1987 ;  #TAUTHOR_TAG, with one difference : the activation spreads to theories that share a predicate, not through""]","['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing ( hirst 1987 ;  #TAUTHOR_TAG, with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text""]",1
"['to weaker, nonstandard logics, ( cfxxx  #TAUTHOR_TAG ; frisch']","['to weaker, nonstandard logics, ( cfxxx  #TAUTHOR_TAG ; frisch']","['to weaker, nonstandard logics, ( cfxxx  #TAUTHOR_TAG ; frisch 1987 ; patel - schneider 1985 ).', 'levesque 1984 ; frisch 1987 ; patel - schneider 1985 ).', ""but we won't pursue this topic further here""]","['in this paper we take modus ponens as the main rule of inference, in general one can consider deductive closures with respect to weaker, nonstandard logics, ( cfxxx  #TAUTHOR_TAG ; frisch 1987 ; patel - schneider 1985 ).', 'levesque 1984 ; frisch 1987 ; patel - schneider 1985 ).', ""but we won't pursue this topic further here""]",1
"['of focus  #TAUTHOR_TAG, 1978 ; sidner']","['of focus  #TAUTHOR_TAG, 1978 ; sidner']","['of focus  #TAUTHOR_TAG, 1978 ; sidner 1983 ) or quantifier scoping ( webber 1983 ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus  #TAUTHOR_TAG, 1978 ; sidner 1983 ) or quantifier scoping ( webber 1983 ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']",0
"['of reference in english prose  #TAUTHOR_TAG, p. 329 )']","['of reference in english prose  #TAUTHOR_TAG, p. 329 )']","['is always the more typical direction of reference in english prose  #TAUTHOR_TAG, p. 329 )']","[': in our translation from english to logic we are assuming that "" it "" is anaphoric ( with the pronoun following the element that it refers to ), not cataphoric ( the other way around ).', 'this means that the "" it "" that brought the disease in p1 will not be considered to refer to the infection "" i "" or the death "" d "" in p3.', 'this strategy is certainly the right one to start out with, since anaphora is always the more typical direction of reference in english prose  #TAUTHOR_TAG, p. 329 )']",4
"['. g.', ' #TAUTHOR_TAG ; the number of unlikely parses is severely reduced']","['is taken in computational syntactic grammars ( e. g.', ' #TAUTHOR_TAG ; the number of unlikely parses is severely reduced']","['. g.', ' #TAUTHOR_TAG ; the number of unlikely parses is severely reduced']","['can also hope for some fine - tuning of the notion of topic, which would prevent many offensive examples.', 'this approach is taken in computational syntactic grammars ( e. g.', ' #TAUTHOR_TAG ; the number of unlikely parses is severely reduced whenever possible, but no attempt is made to define only the so - called grammatical strings of a language']",0
"['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing  #TAUTHOR_TAG ; charniak 1983 ), with one difference : the activation spreads to theories that share a predicate, not through the""]","['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing  #TAUTHOR_TAG ; charniak 1983 ), with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text""]","['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing  #TAUTHOR_TAG ; charniak 1983 ), with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text""]","['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing  #TAUTHOR_TAG ; charniak 1983 ), with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text""]",1
"['.', 'we shall see this in the next example : two sentences, regarded as a fragment of paragraph, are a variation on a theme by  #TAUTHOR_TAG']","['writer did not say too much. ""', 'rather, we stress the possibility that one can axiomatize and productively use such a rule.', 'we shall see this in the next example : two sentences, regarded as a fragment of paragraph, are a variation on a theme by  #TAUTHOR_TAG']","['do not claim that gla is the best or unique way of expressing the rule "" assume that the writer did not say too much. ""', 'rather, we stress the possibility that one can axiomatize and productively use such a rule.', 'we shall see this in the next example : two sentences, regarded as a fragment of paragraph, are a variation on a theme by  #TAUTHOR_TAG']","['do not claim that gla is the best or unique way of expressing the rule "" assume that the writer did not say too much. ""', 'rather, we stress the possibility that one can axiomatize and productively use such a rule.', 'we shall see this in the next example : two sentences, regarded as a fragment of paragraph, are a variation on a theme by  #TAUTHOR_TAG']",2
['by  #TAUTHOR_TAG ; cfxxx also quirk'],['by  #TAUTHOR_TAG ; cfxxx also quirk'],['by  #TAUTHOR_TAG ; cfxxx also quirk'],"['##s are function words - - like conjunctions and some adverbs - - that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '"" and, "" "" or, "" and "" but "" are the three main coordinating connectives in english.', 'however, "" but "" does not behave quite like the other two - - semantically, "" but "" signals a contradiction, and in this role it seems to have three subfunctions :..', 'opposition ( called "" adversative "" or "" contrary - to - expectation "" by  #TAUTHOR_TAG ; cfxxx also quirk et al. 1972, p. 672 )']",0
"['.', ' #TAUTHOR_TAG, hall']","['', ' #TAUTHOR_TAG, halliday and hasan']","['.', ' #TAUTHOR_TAG, hall']","['there are other discussions of the paragraph as a central element of discourse ( e. g.', ' #TAUTHOR_TAG, halliday and hasan 1976, longacre 1979, haberlandt et al. 1980 ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']",1
"['johnson -  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and implicitly or explicitly']","['johnson -  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and implicitly or explicitly']","['johnson -  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and implicitly or explicitly']","['logical notions that we are going to consider, such as theory or model, will be finitary.', 'for example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'therefore these notions, and all other constructs we are going to define ( axioms, metarules, definitions etc. ) are computational, although usually we will not provide explicit algorithms for computing them.', 'the issues of control are not so important for us at this point ; we restrict ourselves to describing the logic.', 'this principle of finitism is also assumed by johnson -  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'mycielski 1981 )']",1
['of paragraph units can be found in  #TAUTHOR_TAG and  #AUTHOR_TAG'],['of paragraph units can be found in  #TAUTHOR_TAG and  #AUTHOR_TAG'],"['by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in  #TAUTHOR_TAG and  #AUTHOR_TAG']","['example of psycholinguistically oriented research work can be found in  #AUTHOR_TAG.', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that are used, by readers, to identify a paragraph : ( 1 ) the repetition of content words ( nouns, verbs, adjectives, adverbs ) ; ( 2 ) pronoun reference ; and ( 3 ) paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in  #TAUTHOR_TAG and  #AUTHOR_TAG']",0
"['have shown elsewhere ( jensen and binot 1988 ;  #TAUTHOR_TAG a, 1987b ) that natural language programs, such as on - line grammars and dictionaries,']","['have shown elsewhere ( jensen and binot 1988 ;  #TAUTHOR_TAG a, 1987b ) that natural language programs, such as on - line grammars and dictionaries,']","['have shown elsewhere ( jensen and binot 1988 ;  #TAUTHOR_TAG a, 1987b ) that natural language programs, such as on - line grammars and dictionaries, can be used as referential levels']","['have shown elsewhere ( jensen and binot 1988 ;  #TAUTHOR_TAG a, 1987b ) that natural language programs, such as on - line grammars and dictionaries, can be used as referential levels for commonsense reasoning - - for example, to disambiguate pp attachment.', 'this means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object - level theory']",2
"['that the iteration of the pt operation to form a closure is needed ( cfxxx  #TAUTHOR_TAG b ).', 'zadrozny 1987b )']","['that the iteration of the pt operation to form a closure is needed ( cfxxx  #TAUTHOR_TAG b ).', 'zadrozny 1987b )']","['important information about a formula.', 'this immediate information may be insufficient to decide the truth of certain predicates.', 'it would seem therefore that the iteration of the pt operation to form a closure is needed ( cfxxx  #TAUTHOR_TAG b ).', 'zadrozny 1987b )']","['an interpretation of a paragraph does not mean finding all of its possible meanings ; the implausible ones should not be computed at all.', 'this viewpoint has been reflected in the definition of a partial theory as a most plausible interpretation of a sequence of predicates.', 'now we want to restrict the notion of a partial theory by introducing the formal notions of topic and coherence.', 'we can then later ( section 5. 2 ) define p - models - - a category of models corresponding to paragraphs - - as models of coherent theories that satisfy all metalevel conditions.', 'the partial theories pick up from the referential level the most obvious or the most important information about a formula.', 'this immediate information may be insufficient to decide the truth of certain predicates.', 'it would seem therefore that the iteration of the pt operation to form a closure is needed ( cfxxx  #TAUTHOR_TAG b ).', 'zadrozny 1987b )']",1
"['.', 'later,  #TAUTHOR_TAG, 1982 ) proposed a']","['isolated features offer only part of the background knowledge necessary for reference disambiguation.', 'later,  #TAUTHOR_TAG, 1982 ) proposed a']","['isolated features offer only part of the background knowledge necessary for reference disambiguation.', 'later,  #TAUTHOR_TAG, 1982 ) proposed']","['selectional restrictions ( semantic feature information, hobbs 1977 ) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""later,  #TAUTHOR_TAG, 1982 ) proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using ` ` salience'' in choosing facts from this knowledge base""]",0
"['matching,  #TAUTHOR_TAG, 1979 ) refers to such operations using the terms knitting or petty conversational impl']","['unification / matching,  #TAUTHOR_TAG, 1979 ) refers to such operations using the terms knitting or petty conversational implicature']","[',  #TAUTHOR_TAG, 1979 ) refers to such operations using the terms knitting or petty conversational implicature']","['necessity of this kind of merging of arguments has been recognized before : charniak and mc  #AUTHOR_TAG call it abductive unification / matching,  #TAUTHOR_TAG, 1979 ) refers to such operations using the terms knitting or petty conversational implicature']",0
['quotation operator ;  #TAUTHOR_TAG describes how first order logic'],['quotation operator ;  #TAUTHOR_TAG describes how first order logic'],['a quotation operator ;  #TAUTHOR_TAG describes how first order logic'],"['', 'the text concerns events happening in time.', 'naturally, we will use a logical notation in which formulas may have temporal and event components.', 'we assume that any formal interpretation of time will agree with the intuitive one.', 'so it is not necessary now to present a formal semantics here.', 'the reader may consult recent papers on this subject ( e. g.', 'moens and steedman 1987 ; webber 1987 ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ;  #TAUTHOR_TAG describes how first order logic can be augmented with such an operator.', 'extending and revising  #AUTHOR_TAG formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint ( "" that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon "" - - - ibid. ).', 'however, as noted before, we will use a simplified version of such a logical notation ; we will have only time, event, result, and property as primitives.', 'after these remarks we can begin constructing the model of the example paragraph.', 'we assume that constants are introduced by nps.', 'we have then ( i ) constants s, m, d, i, b, 1347 satisfying : ship ( s ), messina ( m ), disease ( d ), infection ( i ), death ( b ), year ( 1347 )']",0
"[',  #TAUTHOR_TAG, p. 8 ) says that the']","[',  #TAUTHOR_TAG, p. 8 ) says that the']","[',  #TAUTHOR_TAG, p. 8 ) says that the sentence']",[' #TAUTHOR_TAG'],4
"['', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG']","['proper response generation.', 'current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG']","[', in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG']","['the past few years, there has been a gradual paradigm shift in speech recognition research both in the u. s. and in europe.', 'in addition to continued research on the transcription problem, i. e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 this shift is at least partly brought on by the realization that many of the applications involving human / machine interface using speech require an "" understanding "" of the intended message.', 'in fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG']",0
['formula for the test set perplexity  #TAUTHOR_TAG is :'],['formula for the test set perplexity  #TAUTHOR_TAG is :'],['formula for the test set perplexity  #TAUTHOR_TAG is :'],['formula for the test set perplexity  #TAUTHOR_TAG is :'],0
"['- end  #TAUTHOR_TAG.', 'the']","['and with a functioning database back - end  #TAUTHOR_TAG.', 'the']","['- end  #TAUTHOR_TAG.', '']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( lamel et al. 1986 ).', 'the second version ( rm ) concerns the resource management task ( pallett 1989 ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end  #TAUTHOR_TAG.', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']",5
['automatically by the computer  #TAUTHOR_TAG'],['automatically by the computer  #TAUTHOR_TAG'],['response generation components was done automatically by the computer  #TAUTHOR_TAG'],"['obtain training and test data for this task, we had a number of naive subjects use the system as if they were trying to obtain actual information.', 'their speech was recorded in a simulation mode in which the speech recognition component was excluded.', 'instead, an experimenter in a separate room typed in the utterances as spoken by the subject.', 'subsequent processing by the natural language and response generation components was done automatically by the computer  #TAUTHOR_TAG']",5
[''],[''],[''],[''],0
"['by  #AUTHOR_TAG and  #TAUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass']","['by  #AUTHOR_TAG and  #TAUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass']","['by  #AUTHOR_TAG and  #TAUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass']","['far, we have added all semantic filters by hand, and they are implemented in a hard - fail mode, i. e., if the semantic restrictions fail, the node dies.', 'this strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains.', 'in principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.', 'then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals.', 'this approach resembles the work by  #AUTHOR_TAG and  #TAUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', 'there is obviously a great deal more work to be done in this important area']",1
"['a number of phonological rules.', 'the search algorithm is the standard viterbi search  #TAUTHOR_TAG, except that the match involves a network - to - network alignment problem rather than sequence - to - sequence']","['a number of phonological rules.', 'the search algorithm is the standard viterbi search  #TAUTHOR_TAG, except that the match involves a network - to - network alignment problem rather than sequence - to - sequence']","['a number of phonological rules.', 'the search algorithm is the standard viterbi search  #TAUTHOR_TAG, except that the match involves a network - to - network alignment problem rather than sequence - to - sequence']","['', 'this aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'the recognizer for these systems is the summit system ( zue et al. 1989 ), which uses a segmental - based framework and includes an auditory model in the front - end processing.', 'the lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'the search algorithm is the standard viterbi search  #TAUTHOR_TAG, except that the match involves a network - to - network alignment problem rather than sequence - to - sequence']",5
[' #TAUTHOR_TAG showing up as complements'],[' #TAUTHOR_TAG showing up as complements'],"['the same case frame  #TAUTHOR_TAG showing up as complements.', 'for instance, the set']","['filters can also be used to prevent multiple versions of the same case frame  #TAUTHOR_TAG showing up as complements.', 'for instance, the set of complements [ from - place ], [ to - place ], and [ at - time ] are freely ordered following a movement verb such as "" leave. ""', '']",5
"['', 'representative systems are described in  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['sundial project.', 'representative systems are described in  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['participants of the esprit sundial project.', 'representative systems are described in  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['the past few years, there has been a gradual paradigm shift in speech recognition research both in the u. s. and in europe.', 'in addition to continued research on the transcription problem, i. e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 this shift is at least partly brought on by the realization that many of the applications involving human / machine interface using speech require an "" understanding "" of the intended message.', 'in fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']",0
"[' #TAUTHOR_TAG.', 'ultimately']","[' #TAUTHOR_TAG.', 'ultimately']","['- best outputs, giving a significant improvement in performance  #TAUTHOR_TAG.', 'ultimately']","['', ""we have not yet made use of tina's probabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort n - best outputs, giving a significant improvement in performance  #TAUTHOR_TAG."", ""ultimately we want to incorporate tina's probabilities directly into the a * search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model""]",5
"['( timit ) was developed for the 450 phonetically rich sentences of the timit database  #TAUTHOR_TAG.', '']","['( timit ) was developed for the 450 phonetically rich sentences of the timit database  #TAUTHOR_TAG.', '']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database  #TAUTHOR_TAG.', '']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database  #TAUTHOR_TAG.', 'the second version ( rm ) concerns the resource management task ( pallett 1989 ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( zue et al. 1990 ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']",5
['atis domain  #TAUTHOR_TAG represents our most promising approach to this problem'],['atis domain  #TAUTHOR_TAG represents our most promising approach to this problem'],"['', 'however, the method we are currently using in the atis domain  #TAUTHOR_TAG represents our most promising approach to this problem.', 'we have decided to limit semantic frame types to a small set of choices']","['how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'however, the method we are currently using in the atis domain  #TAUTHOR_TAG represents our most promising approach to this problem.', '']",3
"[', atis  #TAUTHOR_TAG et al']","['vicinity of mit and harvard university.', 'the second one, atis  #TAUTHOR_TAG et al. 1991 ), is']","['our case, the vicinity of mit and harvard university.', 'the second one, atis  #TAUTHOR_TAG et al']","['currently have two application domains that can carry on a spoken dialog with a user.', 'one, the voyager domain ( zue et al. 1990 ), answers questions about places of interest in an urban area, in our case, the vicinity of mit and harvard university.', 'the second one, atis  #TAUTHOR_TAG et al. 1991 ), is a system for accessing data in the official 80 stephanie seneff tina : a natural language system for spoken language applications airline guide and booking flights.', 'work continues on improving all aspects of these domains. our current research is directed at a number of different remaining issues']",5
"['by  #TAUTHOR_TAG and  #AUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass']","['by  #TAUTHOR_TAG and  #AUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass']","['on whether to accept the proposals.', 'this approach resembles the work by  #TAUTHOR_TAG and  #AUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass']","['far, we have added all semantic filters by hand, and they are implemented in a hard - fail mode, i. e., if the semantic restrictions fail, the node dies.', 'this strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains.', 'in principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.', 'then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals.', 'this approach resembles the work by  #TAUTHOR_TAG and  #AUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', 'there is obviously a great deal more work to be done in this important area']",1
"[', an n - gram back - off model  #TAUTHOR_TAG']","[', an n - gram back - off model  #TAUTHOR_TAG']","[', an n - gram back - off model  #TAUTHOR_TAG']","['appendix walks through a pedagogical example to parse spoken digit sequences up to three long, as in "" three hundred and sixteen. ""', 'included is a set of initial contextfree rules, a set of training sentences, an illustration of how to compute the path probabilities from the training sentences, and an illustration of both parsing and perplexity computation for a test sentence.', 'since there are only five training sentences, a number of the arcs of the original grammar are lost after training.', 'this is a problem to be aware of in building grammars from example sentences.', 'in the absence of a sufficient amount of training data, some arcs will inevitably be zeroed out.', 'unless it is desired to intentionally filter these out as being outside of the new domain, one can insert some arbitrarily small probability for these arcs, using, for example, an n - gram back - off model  #TAUTHOR_TAG']",0
[' #TAUTHOR_TAG by its generator'],"['[ end ] node is encountered.', 'to a first approximation, a current - focus reaches only nodes that are c - commanded  #TAUTHOR_TAG by its generator']","['', 'to a first approximation, a current - focus reaches only nodes that are c - commanded  #TAUTHOR_TAG by its generator.', '']","['', 'that is to say, the current - focus is a feature, like verb - mode, that is blocked when an [ end ] node is encountered.', 'to a first approximation, a current - focus reaches only nodes that are c - commanded  #TAUTHOR_TAG by its generator.', 'finally, certain blocker nodes block the transfer of the float - object to their children']",0
"['voyager domain  #TAUTHOR_TAG, answers questions']","['voyager domain  #TAUTHOR_TAG, answers questions']","[', the voyager domain  #TAUTHOR_TAG, answers questions']","['_ currently have two application domains that can carry on a spoken dialog with a user.', 'one, the voyager domain  #TAUTHOR_TAG, answers questions about places of interest in an urban area, in our case, the vicinity of mit and harvard university.', 'the second one, atis ( seneff et al. 1991 ), is a system for accessing data in the official airline guide and booking flights.', 'work continues on improving all aspects of these domains']",5
"['##ns  #TAUTHOR_TAG,']","['atns  #TAUTHOR_TAG,']","['##ns  #TAUTHOR_TAG,']","[""example used to illustrate the power of atns  #TAUTHOR_TAG, ` ` john was believed to have been shot,'' also parses correctly, because the [ object ] node following the verb ` ` believed'' acts as both an absorber and a ( re ) generator."", '']",1
"['', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['proper response generation.', 'current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","[', in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['the past few years, there has been a gradual paradigm shift in speech recognition research both in the u. s. and in europe.', 'in addition to continued research on the transcription problem, i. e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 this shift is at least partly brought on by the realization that many of the applications involving human / machine interface using speech require an "" understanding "" of the intended message.', 'in fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']",0
['resource management task  #TAUTHOR_TAG that has been popular within'],['resource management task  #TAUTHOR_TAG that has been popular within'],['( rm ) concerns the resource management task  #TAUTHOR_TAG that has been popular within'],"['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( lamel et al. 1986 ).', 'the second version ( rm ) concerns the resource management task  #TAUTHOR_TAG that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( zue et al. 1990 ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']",5
"['the a * algorithm  #TAUTHOR_TAG as applied to speech recognition, the actual path score is']","['the a * algorithm  #TAUTHOR_TAG as applied to speech recognition, the actual path score is']","['the a * algorithm  #TAUTHOR_TAG as applied to speech recognition, the actual path score is typically augmented with an estimated score for the unseen portion.', 'unless some kind of normalization is done, the short theories have an unfair advantage, simply']","['modification of this scheme is necessary when the input stream is not deterministic.', 'for the a * algorithm  #TAUTHOR_TAG as applied to speech recognition, the actual path score is typically augmented with an estimated score for the unseen portion.', 'unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied.', 'with a deterministic word sequence it seems reasonable to assume probability 1. 0 for what has been found']",5
"['summit system  #TAUTHOR_TAG, which uses a segmental - based']","['summit system  #TAUTHOR_TAG, which uses a segmental - based']","['will not be covered in detail.', 'the recognizer for these systems is the summit system  #TAUTHOR_TAG, which uses a segmental - based framework']","['', 'this aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'the recognizer for these systems is the summit system  #TAUTHOR_TAG, which uses a segmental - based framework and includes an auditory model in the front - end processing.', 'the lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'the search algorithm is the standard viterbi search ( viterbi 1967 ), except that the match involves a network - to - network alignment problem rather than sequence - to - sequence']",5
['##ns  #TAUTHOR_TAG and'],['atns  #TAUTHOR_TAG and'],"['##ns  #TAUTHOR_TAG and the treatment of bounded domination metavariables in lexical functional grammars ( lfgs ) ( bresnan 1982, p. 235 ff. ), but it is different from these in that the process of filling the hold register equivalent involves two steps separately initiated by two independent nodes']","['section describes how tina handles several issues that are often considered to be part of the task of a parser.', 'these include agreement constraints, semantic restrictions, subject - tagging for verbs, and long distance movement ( often referred to as gaps, or the trace, as in "" ( which article ) / do you think i should read ( ti )? "" ) ( chomsky 1977 ).', 'the gap mechanism resembles the hold register idea of atns  #TAUTHOR_TAG and the treatment of bounded domination metavariables in lexical functional grammars ( lfgs ) ( bresnan 1982, p. 235 ff. ), but it is different from these in that the process of filling the hold register equivalent involves two steps separately initiated by two independent nodes']",1
"['', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG']","['proper response generation.', 'current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG']","[', in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG']","['the past few years, there has been a gradual paradigm shift in speech recognition research both in the u. s. and in europe.', 'in addition to continued research on the transcription problem, i. e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 this shift is at least partly brought on by the realization that many of the applications involving human / machine interface using speech require an "" understanding "" of the intended message.', 'in fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['we first integrated this recognizer with tina, we used a "" wire "" connection, in that the recognizer produced a single best output, which was then passed to tina for parsing.', 'a simple word - pair grammar constrained the search space.', 'if the parse failed, then the sentence was rejected.', ""we have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse  #TAUTHOR_TAG to produce these ` ` n - best'' alternatives, we make use of a standard a * search algorithm ( hart 1968, jelinek 1976 )."", 'both the a * and the viterbi search are left - to - right search algorithms.', 'however, the a * search is contrasted with the viterbi search in that the set of active hypotheses take up unequal segments of time.', 'that is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold']",5
"['system the parser acts as a filter only on completed candidate solutions  #TAUTHOR_TAG, the tightly coupled system allows the']","['system the parser acts as a filter only on completed candidate solutions  #TAUTHOR_TAG, the tightly coupled system allows the']","['in the loosely coupled system the parser acts as a filter only on completed candidate solutions  #TAUTHOR_TAG, the tightly coupled system allows the parser']","['in the loosely coupled system the parser acts as a filter only on completed candidate solutions  #TAUTHOR_TAG, the tightly coupled system allows the parser to discard partial theories that have no way of continuing.', 'following the viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""we have not yet made use of tina'sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort n - best outputs, giving a significant improvement in performance ( goodine et al. 1991 )."", ""ultimately we want to incorporate tina'sprobabilities directly into the a * search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model""]",5
['##rs provided by  #TAUTHOR_TAG defines a formal lexical'],['dlrs provided by  #TAUTHOR_TAG defines a formal lexical'],['##rs provided by  #TAUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : a rewrite system'],"['thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'the formalization of dlrs provided by  #TAUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : a rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in figure 1.', 'this description can then be given the standard set - theoretical interpretation of  #AUTHOR_TAG, 1994 )']",0
['to  #TAUTHOR_TAG for a more detailed discussion of our use of constraint propagation'],['to  #TAUTHOR_TAG for a more detailed discussion of our use of constraint propagation'],"['more specific.', 'this technique closely resembles the off - line constraint propagation technique described by  #AUTHOR_TAG.', 'the reader is referred to  #TAUTHOR_TAG for a more detailed discussion of our use of constraint propagation']","['', 'when c is the common information, and d1,..., dk are the definitions of the interaction predicate called, we use distributivity to factor out c in ( c a d1 ) v -.. v ( c a dk ) : we compute c a ( d1 v... v dk ), where the r ) are assumed to contain no further common factors.', 'once we have computed c, we use it to make the extended lexical entry more specific.', 'this technique closely resembles the off - line constraint propagation technique described by  #AUTHOR_TAG.', 'the reader is referred to  #TAUTHOR_TAG for a more detailed discussion of our use of constraint propagation']",0
"['##lexical rules  #TAUTHOR_TAG, 31 ).', 'a similar method is included in']","['and the parser makes no distinction between lexical and nonlexical rules  #TAUTHOR_TAG, 31 ).', 'a similar method is included in']","['##lexical rules  #TAUTHOR_TAG, 31 ).', 'a similar method is included in']","['common approach to lexical rules is to encode them as unary phrase structure rules.', 'this approach is taken, for example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules  #TAUTHOR_TAG, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the cuf system ( dbrre and eisele 1991 ; d6rre and dorna 1993b ) or the tfs system ( emele and zajac 1990 ; emele 1994 ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']",1
"['19.', '27  #TAUTHOR_TAG argue that semi - productivity of lexical rules, which can be understood as a']","['19.', '27  #TAUTHOR_TAG argue that semi - productivity of lexical rules, which can be understood as a']","['way these predicates interconnect is represented in figure 19.', '27  #TAUTHOR_TAG argue that semi - productivity of lexical rules, which can be understood as a generalization of exceptions to lexical rules, can be integrated with our']","['way these predicates interconnect is represented in figure 19.', '27  #TAUTHOR_TAG argue that semi - productivity of lexical rules, which can be understood as a generalization of exceptions to lexical rules, can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry.', '28 in order to distinguish the different interaction predicates for the different classes of lexical entries, the compiler indexes the names of the interaction predicates.', 'since for expository reasons we will only discuss one kind of lexical entry in this paper, we will not show those indices in the examples given']",0
[' #TAUTHOR_TAG to'],[' #TAUTHOR_TAG to'],['sag 1994 ) or the complement extraction lexical rule  #TAUTHOR_TAG to'],"['the authors are listed alphabetically.', 'sfb 340, kleine wilhelmstr.', '113, d - 72074 tiibingen, germany.', 'email : { dm, minnen } @ sfs. nphil. uni - tuebingen. de', 'url : http : / / www. sfs. nphil. uni - tuebingen. de / sfb', '/ b4home. html 1 this is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( hinrichs and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement extraction lexical rule  #TAUTHOR_TAG to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']",0
"['lkb system  #TAUTHOR_TAG.', 'both the input and output of a lexical']","['lkb system  #TAUTHOR_TAG.', 'both the input and output of a lexical rule,']","[', adopted in the lkb system  #TAUTHOR_TAG.', 'both the input and output of a lexical rule,']","['', 'this conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as, for example, adopted in the lkb system  #TAUTHOR_TAG.', 'both the input and output of a lexical rule, i. e., the mother and the daughter of a phrase structure rule, are available during a generation or parsing process.', 'as a result, in addition to the information present in the lexical entry, syntactic information can be accessed to execute the constraints on the input of a lexical rule.', 'the computational treatment of lexical rules that we propose in this paper is essentially a domain - specific refinement of such an approach to lexical rules.', '']",0
"[' #TAUTHOR_TAG.', 'given a']","[' #TAUTHOR_TAG.', 'given a']","['of clauses with a finitely failed body  #TAUTHOR_TAG.', 'given']","['', 'we thereby eliminate the redundant nondeterminism resulting from multiply defined frame predicates.', 'the elimination of redundant nondeterminism is based on unfold / fold transformation techniques ( tamaki and sato 1984 ).', '29 the unfolding transformation is also referred to as partial execution, for example, by  #AUTHOR_TAG.', 'intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile - time.', 'as a result, the literal can be removed from the body of 29 this improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body  #TAUTHOR_TAG.', '']",1
"[';  #TAUTHOR_TAG can be used to circumvent constraint propagation.', 'encoding the disjunc']","[';  #TAUTHOR_TAG can be used to circumvent constraint propagation.', 'encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments,']","[';  #TAUTHOR_TAG can be used to circumvent constraint propagation.', 'encoding the disjunctive possibilities for lexical rule application in this']","['in certain cases an extension of the constraint language with named disjunctions or contexted constraints ( maxwell and kaplan 1989 ; eisele and dorre 1990 ;  #TAUTHOR_TAG can be used to circumvent constraint propagation.', 'encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments, makes all relevant lexical information available at lexical lookup.', 'for analyses proposing infinite lexica, though, a definite clause encoding of disjunctive possibilities is still necessary and constraint propagation is indispensable for efficient processing']",0
"['specification language introduced in  #TAUTHOR_TAG.', 'the reader interested in that']","['specification language introduced in  #TAUTHOR_TAG.', 'the reader interested in that']","['into a discussion of the full lexical rule specification language introduced in  #TAUTHOR_TAG.', 'the reader interested in that language and']","['translation of the lexical rule into a predicate is trivial.', 'the result is displayed description language.', '12 in order to focus on the computational aspects of the covariation approach, in this paper we will not go into a discussion of the full lexical rule specification language introduced in  #TAUTHOR_TAG.', 'the reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 a more detailed presentation can be found in minnen ( in preparation ).', '14 we use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15  #AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention.', 'we here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form ( g6tz 1994 ).', 'computationally, a subsumption test could equally well be used in our compiler']",0
"[' #TAUTHOR_TAG, 10 ) is a linguistic example.', 'the in - specification of this lexical']","[' #TAUTHOR_TAG, 10 ) is a linguistic example.', 'the in - specification of this lexical']","[' #TAUTHOR_TAG, 10 ) is a linguistic example.', 'the in - specification of this lexical rule']","['terminology used in the literature varies.', 'types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'to avoid confusion, we will only use the terminology introduced in the text.', '4 this interpretation of the signature is sometimes referred to as closed world ( gerdemann and king 1994 ; gerdemann 1995 ).', '5 an in - depth discussion including a comparison of both approaches is provided in calcagno, meurers, and pollard ( in preparation ).', '6 the partial - vp topicalization lexical rule proposed by  #TAUTHOR_TAG, 10 ) is a linguistic example.', 'the in - specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'in the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement']",0
"['##demann and king 1994 ;  #TAUTHOR_TAG.', '5 an in - depth discussion including a comparison of both approaches is provided in calcagn']","['to as closed world ( gerdemann and king 1994 ;  #TAUTHOR_TAG.', '5 an in - depth discussion including a comparison of both approaches is provided in calcagno,']","['##demann and king 1994 ;  #TAUTHOR_TAG.', '5 an in - depth discussion including a comparison of both approaches is provided in calcagn']","['terminology used in the literature varies.', 'types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'to avoid confusion, we will only use the terminology introduced in the text.', '4 this interpretation of the signature is sometimes referred to as closed world ( gerdemann and king 1994 ;  #TAUTHOR_TAG.', '5 an in - depth discussion including a comparison of both approaches is provided in calcagno, meurers, and pollard ( in preparation ).', '6 the partial - vp topicalization lexical rule proposed by  #AUTHOR_TAG, 10 ) is a linguistic example.', 'the in - specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'in the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement']",0
['more compact representation of lexical rules where explicit framing would require the rules to be split up  #TAUTHOR_TAG'],['more compact representation of lexical rules where explicit framing would require the rules to be split up  #TAUTHOR_TAG'],['more compact representation of lexical rules where explicit framing would require the rules to be split up  #TAUTHOR_TAG'],"['the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.', 'this is so since the lexical rule in figure 2 "" ( like all lexical rules in hpsg ) preserves all properties of the input not mentioned in the rule. ""', '( pollard and sag [ 1994, 314 ], following flickinger [ 1987 ] ).', 'this idea of preserving properties can be considered an instance of the well - known frame problem in ai ( mccarthy and hayes 1969 ), and we will therefore refer to the specifications left implicit by the linguist as the frame specification, or simply frame, of a lexical rule.', 'not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up  #TAUTHOR_TAG']",0
"['mentioned.', 'as shown in  #TAUTHOR_TAG this is a well - motivated convention since it avoids splitting up lexical rules to transfer the specifications that']","['mentioned.', 'as shown in  #TAUTHOR_TAG this is a well - motivated convention since it avoids splitting up lexical rules to transfer the specifications that']","['in hpsg that only the properties changed by a lexical rule need be mentioned.', 'as shown in  #TAUTHOR_TAG this is a well - motivated convention since it avoids splitting up lexical rules to transfer the specifications that']","['computational treatment expanding out the lexicon cannot be used for the increasing number of hpsg analyses that propose lexical rules that would result in an infinite lexicon.', 'most current hpsg analyses of dutch, german, italian, and french fall into that category.', '1 furthermore, since lexical rules in such an approach only serve in a precompilation step, the generalizations captured by the lexical rules cannot be used at run - time.', 'finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry.', 'this conflicts with the standard assumption made in hpsg that only the properties changed by a lexical rule need be mentioned.', 'as shown in  #TAUTHOR_TAG this is a well - motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries']",4
"['fold transformation techniques  #TAUTHOR_TAG.', '29']","['transformation techniques  #TAUTHOR_TAG.', '29']","['fold transformation techniques  #TAUTHOR_TAG.', '29 the unfolding transformation is also referred to']","['elimination of redundant nondeterminism is based on unfold / fold transformation techniques  #TAUTHOR_TAG.', '29 the unfolding transformation is also referred to as partial execution, for example, by  #AUTHOR_TAG.', 'intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile - time.', 'as a result, the literal can be removed from the body of the clause.', 'whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation']",5
[';  #TAUTHOR_TAG ; san'],['captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ;  #TAUTHOR_TAG ; sanfilippo'],"['##nged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ;  #TAUTHOR_TAG ; san']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ;  #TAUTHOR_TAG ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
['- ii  #TAUTHOR_TAG and can be used to encode lexical rules as binary relations in'],"['( copestake 1993, 31 ).', 'a similar method is included in patr - ii  #TAUTHOR_TAG and can be used to encode lexical rules as binary relations in']",['- ii  #TAUTHOR_TAG and can be used to encode lexical rules as binary relations in the cuf system ( dor'],"['common approach to lexical rules is to encode them as unary phrase structure rules.', 'this approach is taken, for example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii  #TAUTHOR_TAG and can be used to encode lexical rules as binary relations in the cuf system ( dorre and eisele 1991 ; done and dorna 1993b ) or the tfs system ( emele and zajac 1990 ; emele 1994 ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']",1
"[';  #TAUTHOR_TAG.', 'the covariation']","[';  #TAUTHOR_TAG.', 'the covariation']","['##na 1993b ) or the tfs system ( emele and zajac 1990 ;  #TAUTHOR_TAG.', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']","['common approach to lexical rules is to encode them as unary phrase structure rules.', 'this approach is taken, for example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the cuf system ( dorre and eisele 1991 ; done and dorna 1993b ) or the tfs system ( emele and zajac 1990 ;  #TAUTHOR_TAG.', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']",1
[';  #TAUTHOR_TAG and'],[';  #TAUTHOR_TAG and'],['; calcagno 1995 ;  #TAUTHOR_TAG and'],"['the setup of king provides a clear formal basis for basic hpsg grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'two formalizations of lexical rules as used by hpsg linguists have been proposed, the meta - level lexical rules ( mlrs ; calcagno 1995 ;  #TAUTHOR_TAG and the.', '']",0
['##demann and  #TAUTHOR_TAG ; gotz and me'],['was implemented in prolog by the authors in cooperation with dieter martini for the controll system ( gerdemann and  #TAUTHOR_TAG ; gotz and'],['##demann and  #TAUTHOR_TAG ; gotz and me'],"['computational treatment of lexical rules as covariation in lexical entries was implemented in prolog by the authors in cooperation with dieter martini for the controll system ( gerdemann and  #TAUTHOR_TAG ; gotz and meurers 1997a ).', 'we tested the covariation approach with a complex grammar implementing an hpsg analysis covering the so - called aux - flip phenomenon, and partial - vp topicalization in the three clause types of german ( hinrichs, meurers, and nakazawa 1994 ).', 'this test grammar includes eight lexical rules ; some serve syntactic purposes, like the partial - vp topicalization lexical rule, others are of morphological nature as, for example, an inflectional lexical rule that relates nonfinite verbs to their finite form.', 'our compiler distinguished seven word classes.', 'some nouns and most verbal lexical entries fed lexical rules, and a single base lexical entry resulted in up to 12 derivations']",5
[';  #TAUTHOR_TAG ; calcagno and pollard'],[';  #TAUTHOR_TAG ; calcagno and pollard'],[';  #TAUTHOR_TAG ; calcagno and pollard 1995 ) and'],"['the setup of king provides a clear formal basis for basic hpsg grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'two formalizations of lexical rules as used by hpsg linguists have been proposed, the meta - level lexical rules ( mlrs ;  #TAUTHOR_TAG ; calcagno and pollard 1995 ) and the.', '']",0
['verbal complement  #TAUTHOR_TAG'],['verbal complement  #TAUTHOR_TAG'],['the arguments of a verbal complement  #TAUTHOR_TAG'],"['the authors are listed alphabetically.', 'sfb 340, kleine wilhelmstr.', '113, d - 72074 tiibingen, germany.', 'email : { dm, minnen } @ sfs. nphil. uni - tuebingen. de', 'url : http : / / www. sfs. nphil. uni - tuebingen. de / sfb', '/ b4home. html 1 this is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement  #TAUTHOR_TAG that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement cliticization lexical rule ( miller and sag 1993 ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']",0
[' #TAUTHOR_TAG consists of computing the'],[' #TAUTHOR_TAG consists of computing the'],[' #TAUTHOR_TAG consists of computing the transitive closure'],"['common computational treatment of lexical rules adopted, for example, in the ale system  #TAUTHOR_TAG consists of computing the transitive closure of the base lexical entries under lexical rule application at compile - time.', 'while this provides a front - end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation.', 'we mentioned in section 2. 2 that eliminating lexical rules in a precompilation step makes it impossible to process lexical rules or lexical entries that impose constraints that can only be properly executed once information from syntactic processing is available.', 'a related problem is that for analyses resulting in infinite lexica, the number of lexical rule applications needs to be limited.', 'in the ale system, for example, a depth bound can be specified for this purpose.', 'finally, as shown in section 6, using an expanded out lexicon can be less time and space efficient than using a lexicon encoding that makes computational use of generalizations over lexical information, as, for example, the covariation encoding']",1
"['logic that provides the formal architecture required by  #AUTHOR_TAG was defined by  #TAUTHOR_TAG, 1994 ).', 'the formal language of king allows the']","['logic that provides the formal architecture required by  #AUTHOR_TAG was defined by  #TAUTHOR_TAG, 1994 ).', 'the formal language of king allows the']","['logic that provides the formal architecture required by  #AUTHOR_TAG was defined by  #TAUTHOR_TAG, 1994 ).', 'the formal language of king allows the expression']","['logic that provides the formal architecture required by  #AUTHOR_TAG was defined by  #TAUTHOR_TAG, 1994 ).', 'the formal language of king allows the expression of grammatical principles using type assignments to refer to the type of an object and path equalities to require the ( token ) identity of objects.', 'these atomic expressions can be combined using conjunction, disjunction, and negation.', 'the expressions are interpreted by a set - theoretical semantics']",0
"[', for example, by  #TAUTHOR_TAG.', 'intuitively understood, unfolding comprises']","[', for example, by  #TAUTHOR_TAG.', 'intuitively understood, unfolding comprises']","['fold transformation techniques ( tamaki and sato 1984 ). 29', 'the unfolding transformation is also referred to as partial execution, for example, by  #TAUTHOR_TAG.', 'intuitively understood, unfolding comprises']","['elimination of redundant nondeterminism is based on unfold / fold transformation techniques ( tamaki and sato 1984 ). 29', 'the unfolding transformation is also referred to as partial execution, for example, by  #TAUTHOR_TAG.', 'intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile - time.', 'as a result, the literal can be removed from the body of the clause.', 'whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation']",0
"['1993b ) or the tfs system  #TAUTHOR_TAG ; emele 1994 ).', 'the covariation']","['1993b ) or the tfs system  #TAUTHOR_TAG ; emele 1994 ).', 'the covariation']","['##na 1993b ) or the tfs system  #TAUTHOR_TAG ; emele 1994 ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']","['common approach to lexical rules is to encode them as unary phrase structure rules.', 'this approach is taken, for example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the cuf system ( dorre and eisele 1991 ; done and dorna 1993b ) or the tfs system  #TAUTHOR_TAG ; emele 1994 ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']",1
"[""be given the standard set - theoretical interpretation of  #TAUTHOR_TAG, 1994 ). '"", '11']","[""be given the standard set - theoretical interpretation of  #TAUTHOR_TAG, 1994 ). '"", '11']","[""be given the standard set - theoretical interpretation of  #TAUTHOR_TAG, 1994 ). '"", '11 10 note that the passivization lexical rule in figure 2 is only']","['', ""this description can then be given the standard set - theoretical interpretation of  #TAUTHOR_TAG, 1994 ). '"", '11 10 note that the passivization lexical rule in figure 2 is only intended to illustrate the mechanism.', 'we do not make the linguistic claim that passives should be analyzed using such a lexical rule.', 'for space reasons, the synsem feature is abbreviated by its first letter.', 'the traditional ( first i rest ) list notation is used, and the operator • stands for the append relation in the usual way.', '1l  #AUTHOR_TAG proposes to unify these two steps by including an update operator in the the computational treatment we discuss in the rest of the paper follows this setup in that it automatically computes, for each lexical rule specification, the frames necessary to preserve the properties not changed by it.', '12 we will show that the detection and specification of frames and the use of program transformation to advance their integration into the lexicon encoding is one of the key ingredients of the covariation approach to hpsg lexical rules']",0
[';  #TAUTHOR_TAG ; frank'],[';  #TAUTHOR_TAG ; frank'],"['##nged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ;  #TAUTHOR_TAG ; frank 1994 ; opalka 1995 ; san']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ;  #TAUTHOR_TAG ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
[' #TAUTHOR_TAG proposes to uni'],[' #TAUTHOR_TAG proposes to unify these two steps by including an update operator in the description language'],[' #TAUTHOR_TAG proposes to uni'],[' #TAUTHOR_TAG proposes to unify these two steps by including an update operator in the description language'],0
['the constraint language with named disjunctions or contexted constraints  #TAUTHOR_TAG ; eisele and dor'],"['the constraint language with named disjunctions or contexted constraints  #TAUTHOR_TAG ; eisele and dorre 1990 ; griffith 1996 ) can be used to circumvent constraint propagation.', 'encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments,']","['the constraint language with named disjunctions or contexted constraints  #TAUTHOR_TAG ; eisele and dorre 1990 ; griffith 1996 ) can be used to circumvent constraint propagation.', 'encoding the disjunctive possibilities for lexical rule application in this']","['in certain cases an extension of the constraint language with named disjunctions or contexted constraints  #TAUTHOR_TAG ; eisele and dorre 1990 ; griffith 1996 ) can be used to circumvent constraint propagation.', 'encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments, makes all relevant lexical information available at lexical lookup.', 'for analyses proposing infinite lexica, though, a definite clause encoding of disjunctive possibilities is still necessary and constraint propagation is indispensable for efficient processing']",0
['of lexical rules  #TAUTHOR_TAG has been used in many natural language'],['of lexical rules  #TAUTHOR_TAG has been used in many natural language'],"['powerful mechanism of lexical rules  #TAUTHOR_TAG has been used in many natural language processing systems.', 'in this section we briefly discuss']","['powerful mechanism of lexical rules  #TAUTHOR_TAG has been used in many natural language processing systems.', 'in this section we briefly discuss some of the more prominent approaches and compare them with the treatment proposed in this paper']",0
"['problem in at  #TAUTHOR_TAG, and we will therefore']","['problem in at  #TAUTHOR_TAG, and we will therefore']","['##er [ 1987 ] ).', 'this idea of preserving properties can be considered an instance of the well - known frame problem in at  #TAUTHOR_TAG, and we will therefore']","['the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.', 'this is so since the lexical rule in figure 2 "" ( like all lexical rules in hpsg ) preserves all properties of the input not mentioned in the rule. ""', '( pollard and sag [ 1994, 314 ], following flickinger [ 1987 ] ).', 'this idea of preserving properties can be considered an instance of the well - known frame problem in at  #TAUTHOR_TAG, and we will therefore refer to the specifications left implicit by the linguist as the frame specification, or simply frame, of a lexical rule.', 'not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( meurers 1994 )']",1
"['passing technique  #TAUTHOR_TAG, we ensure that upon']","['by a unit clause.', 'using an accumulator passing technique  #TAUTHOR_TAG, we ensure that upon']","['is encoded by a unit clause.', 'using an accumulator passing technique  #TAUTHOR_TAG, we ensure that upon execution']","['', 'using an accumulator passing technique  #TAUTHOR_TAG, we ensure that upon execution of a call to the interaction predicate q _ 1 a new lexical entry is derived as the result of successive application of a number of lexical rules.', 'because of the word class specialization step discussed in section 3. 3, the execution avoids trying out many lexical rule applications that are guaranteed to fail']",5
"['can be found in minnen ( in preparation ).', '14 we use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15  #TAUTHOR_TAG show that the']","['can be found in minnen ( in preparation ).', '14 we use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15  #TAUTHOR_TAG show that the']","['the relevant details in that paper.', '13 a more detailed presentation can be found in minnen ( in preparation ).', '14 we use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15  #TAUTHOR_TAG show that the question of']","['translation of the lexical rule into a predicate is trivial.', 'the result is displayed description language.', '12 in order to focus on the computational aspects of the covariation approach, in this paper we will not go into a discussion of the full lexical rule specification language introduced in  #AUTHOR_TAG.', 'the reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 a more detailed presentation can be found in minnen ( in preparation ).', '14 we use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15  #TAUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention.', 'we here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form ( g6tz 1994 ).', 'computationally, a subsumption test could equally well be used in our compiler']",0
[';  #TAUTHOR_TAG ; oliv'],['captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ;  #TAUTHOR_TAG ; oliva'],"['##nged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ;  #TAUTHOR_TAG ; oliv']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ;  #TAUTHOR_TAG ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
[';  #TAUTHOR_TAG ;'],[';  #TAUTHOR_TAG ; opalka'],[';  #TAUTHOR_TAG ;'],"['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ;  #TAUTHOR_TAG ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
['##demann and  #TAUTHOR_TAG ;'],['to as closed world ( gerdemann and  #TAUTHOR_TAG ;'],['##demann and  #TAUTHOR_TAG ;'],"['terminology used in the literature varies.', 'types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'to avoid confusion, we will only use the terminology introduced in the text.', '4 this interpretation of the signature is sometimes referred to as closed world ( gerdemann and  #TAUTHOR_TAG ; gerdemann 1995 ).', '5 an in - depth discussion including a comparison of both approaches is provided in calcagno, meurers, and pollard ( in preparation ).', '6 the partial - vp topicalization lexical rule proposed by  #AUTHOR_TAG, 10 ) is a linguistic example.', 'the in - specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'in the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement']",0
['( pollard and sag 1994 ) or the complement cliticization lexical rule  #TAUTHOR_TAG to'],['( pollard and sag 1994 ) or the complement cliticization lexical rule  #TAUTHOR_TAG to'],['the complement extraction lexical rule ( pollard and sag 1994 ) or the complement cliticization lexical rule  #TAUTHOR_TAG to'],"['the authors are listed alphabetically.', 'sfb 340, kleine wilhelmstr.', '113, d - 72074 tiibingen, germany.', 'email : { dm, minnen } @ sfs. nphil. uni - tuebingen. de', 'url : http : / / www. sfs. nphil. uni - tuebingen. de / sfb', 'de url : http : / / www. sfs. nphil. uni - tuebingen. de / sfb / b4home. html 1 this is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( hinrichs and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement cliticization lexical rule  #TAUTHOR_TAG to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']",0
[' #TAUTHOR_TAG would be'],[' #TAUTHOR_TAG would be'],[' #TAUTHOR_TAG would be a lexical rule deriving predicative signs from'],"['a linguistic example based on the signature given by  #TAUTHOR_TAG would be a lexical rule deriving predicative signs from nonpredicative ones, i. e., changing the prd value of substantive signs from - to +, much like the lexical rule for nps given by  #AUTHOR_TAG, fn. 20 ).', 'in such a predicative lexical rule ( which we only note as an example and not as a linguistic proposal ) the subtype of the head object undergoing the rule as well as the value of the features only appropriate for the subtypes of substantive either is lost or must be specified by a separate rule for each of the subtypes']",0
"['', 'the computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by gotz and  #TAUTHOR_TAG, 1996, 1997b )']","['time after lexical lookup.', 'the computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by gotz and  #TAUTHOR_TAG, 1996, 1997b )']","['some time after lexical lookup.', 'the computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by gotz and  #TAUTHOR_TAG, 1996, 1997b )']","['relations are a convenient way of encoding the interaction of lexical rules, as they readily support various program transformations to improve the encoding : we show that the definite relations produced by the compiler can be refined by program transformation techniques to increase efficiency.', 'the resulting encoding allows the execution of lexical rules on - the - fly, i. e., coroutined with other constraints at some time after lexical lookup.', 'the computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by gotz and  #TAUTHOR_TAG, 1996, 1997b ) for encoding the main building block of hpsg grammars - - the implicative constraints - - as a logic program']",2
['captured using lexical underspecification  #TAUTHOR_TAG ; krie'],['captured using lexical underspecification  #TAUTHOR_TAG ; krieger and nerbonne'],"['##nged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification  #TAUTHOR_TAG ; krie']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification  #TAUTHOR_TAG ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
"[';  #TAUTHOR_TAG.', 'the lexical entries are only partially specified,']","[';  #TAUTHOR_TAG.', 'the lexical entries are only partially specified,']","['; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ;  #TAUTHOR_TAG.', 'the lexical entries are only partially specified,']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ;  #TAUTHOR_TAG.', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
[' #TAUTHOR_TAG or'],[' #TAUTHOR_TAG or'],['the complement extraction lexical rule  #TAUTHOR_TAG or the complement cliticization lexical rule ( miller and sag 1993 ) to'],"['the authors are listed alphabetically.', 'sfb 340, kleine wilhelmstr.', '113, d - 72074 tiibingen, germany.', 'email : { dm, minnen } @ sfs. nphil. uni - tuebingen. de', 'url : http : / / www. sfs. nphil. uni - tuebingen. de / sfb', '/ b4home. html 1 this is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( hinrichs and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule  #TAUTHOR_TAG or the complement cliticization lexical rule ( miller and sag 1993 ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']",0
"['on the research results reported in  #TAUTHOR_TAG, 1996 ), we propose a new computational treatment of lexical rules that overcomes these short - comings']","['on the research results reported in  #TAUTHOR_TAG, 1996 ), we propose a new computational treatment of lexical rules that overcomes these short - comings']","['on the research results reported in  #TAUTHOR_TAG, 1996 ), we propose a new computational treatment of lexical rules that overcomes these short - comings']","['on the research results reported in  #TAUTHOR_TAG, 1996 ), we propose a new computational treatment of lexical rules that overcomes these short - comings and results in a more efficient processing of lexical rules as used in hpsg.', 'we developed a compiler that takes as its input a set of lexical rules, deduces the nec - essary transfer of properties not changed by the individual lexical rules, and encodes the set of lexical rules and their interaction into definite relations constraining lexical entries.', 'each lexical entry is automatically extended with a definite clause encoding of the lexical rule applications which the entry can undergo.', 'the definite clauses thereby introduce what we refer to as systematic covariationinlexicalentries']",4
"[', in lkb  #TAUTHOR_TAG where lexical rules']","[', in lkb  #TAUTHOR_TAG where lexical rules']","[', in lkb  #TAUTHOR_TAG where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and']","['common approach to lexical rules is to encode them as unary phrase structure rules.', 'this approach is taken, for example, in lkb  #TAUTHOR_TAG where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the cuf system ( dbrre and eisele 1991 ; d6rre and dorna 1993b ) or the tfs system ( emele and zajac 1990 ; emele 1994 ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']",1
"['latter case, we can also take care of transferring the value of z.', 'however, as discussed by  #TAUTHOR_TAG, creating several instances of lexical']","['latter case, we can also take care of transferring the value of z.', 'however, as discussed by  #TAUTHOR_TAG, creating several instances of lexical']","['those with t2 as their c value.', 'in the latter case, we can also take care of transferring the value of z.', 'however, as discussed by  #TAUTHOR_TAG, creating several instances of lexical rules can be avoided.', 'instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.', 'this is accomplished by having each lexical rule predicate']","['ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'in the above example, this would result in two lexical rules : one for words with tl as their c value and one for those with t2 as their c value.', 'in the latter case, we can also take care of transferring the value of z.', 'however, as discussed by  #TAUTHOR_TAG, creating several instances of lexical rules can be avoided.', 'instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.', 'this is accomplished by having each lexical rule predicate call a so - called framepredicate, which can have multiple defining clauses.', 'so for the lexical rule 1, the frame specification is taken care of by extending the predicate in figure 6 with a call to a frame predicate, as shown in figure 8.']",4
"[' #AUTHOR_TAG, 215 ) in terms of the setup of  #TAUTHOR_TAG, ch.', 'this lexical']","[' #AUTHOR_TAG, 215 ) in terms of the setup of  #TAUTHOR_TAG, ch.', 'this lexical']","[' #AUTHOR_TAG, 215 ) in terms of the setup of  #TAUTHOR_TAG, ch.', 'this lexical rule could be used in a grammar of english to']","['as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'consider, for example, the lexical rule in figure 2, which encodes a passive lexical rule like the one presented by  #AUTHOR_TAG, 215 ) in terms of the setup of  #TAUTHOR_TAG, ch.', 'this lexical rule could be used in a grammar of english to relate past participle forms of verbs to their passive form2 ° the rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'the index that the subject bore in the input is assigned to an optional prepositional complement in the output']",0
[';  #TAUTHOR_TAG ; riehemann'],[';  #TAUTHOR_TAG ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo'],"['##nged as a mechanism for expressing generalizations over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ;  #TAUTHOR_TAG ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ; san']","['rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ;  #TAUTHOR_TAG ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
"['presented by  #TAUTHOR_TAG, 215 ) in terms of the setup of  #AUTHOR_TAG, ch.', 'this lexical']","['presented by  #TAUTHOR_TAG, 215 ) in terms of the setup of  #AUTHOR_TAG, ch.', 'this lexical']","['presented by  #TAUTHOR_TAG, 215 ) in terms of the setup of  #AUTHOR_TAG, ch.', 'this lexical rule could be used in a grammar of english to']","['as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'consider, for example, the lexical rule in figure 2, which encodes a passive lexical rule like the one presented by  #TAUTHOR_TAG, 215 ) in terms of the setup of  #AUTHOR_TAG, ch.', 'this lexical rule could be used in a grammar of english to relate past participle forms of verbs to their passive form2 ° the rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'the index that the subject bore in the input is assigned to an optional prepositional complement in the output']",1
['##rs ;'],['( dlrs ;'],['##rs ;'],"['the setup of king provides a clear formal basis for basic hpsg grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'two formalizations of lexical rules as used by hpsg linguists have been proposed, the meta - level lexical rules ( mlrs ; calcagno 1995 ; calcagno and pollard 1995 ) and the description - level lexical rules ( dlrs ;']",0
"['', ' #TAUTHOR_TAG present detailed']","['', ' #TAUTHOR_TAG present detailed']","['', ' #TAUTHOR_TAG present detailed studies on']","['', ' #TAUTHOR_TAG present detailed studies on the task of named entity recognition, which discusses and compares different methods on multiple aspects including chunk representation, inference method, utility of non - local features, and integration of external knowledge.', 'ner can be regarded as a sequential labeling problem, which can be modeled by several proposed models, e. g.', 'hidden markov model  #AUTHOR_TAG or conditional random fields  #AUTHOR_TAG.', 'the typical bio representation was introduced in  #AUTHOR_TAG ; oc representations were introduced in  #AUTHOR_TAG, while  #AUTHOR_TAG further study nested named entity recognition, which employs a tree structure as a representation of identifying named entities within other named entities']",0
['of features used in  #TAUTHOR_TAG'],['of features used in  #TAUTHOR_TAG'],['of features used in  #TAUTHOR_TAG'],"[', y u, v = 1 iff mentions u, v are directly linked.', 'thus, we can construct a forest and the mentions in the same connected component ( i. e., in the same tree ) are co - referred.', 'for this mention - pair coreference model i ( u, v ), we use the same set of features used in  #TAUTHOR_TAG']",5
['solely on mention heads  #TAUTHOR_TAG'],['solely on mention heads  #TAUTHOR_TAG'],['solely on mention heads  #TAUTHOR_TAG'],"[', phrases in the brackets are mentions and the underlined simple phrases are mention heads.', 'moreover, mention boundaries can be nested ( the boundary of a mention is inside the boundary of another mention ), but mention heads never overlap.', 'this property also simplifies the problem of mention head candidate generation.', 'in the example above, the first "" they "" refers to "" multinational companies investing in china "" and the second "" they "" refers to "" domestic manufacturers, who are also suffering "".', 'in both cases, the mention heads are sufficient to support the decisions : "" they "" refers to "" companies "", and "" they "" refers to "" manufacturers "".', 'in fact, most of the features3 implemented in existing coreference resolution systems rely solely on mention heads  #TAUTHOR_TAG']",0
"['in bjorkelund and  #AUTHOR_TAG.', 'for berkeley system, we use the reported results from  #TAUTHOR_TAG']","['in bjorkelund and  #AUTHOR_TAG.', 'for berkeley system, we use the reported results from  #TAUTHOR_TAG']","['in bjorkelund and  #AUTHOR_TAG.', 'for berkeley system, we use the reported results from  #TAUTHOR_TAG']","['latest scorer is version v8. 01, but muc, b, ceaf e and conll average scores are not changed.', 'for evaluation on ace - 2004, we convert the system output and gold annotations into conll format. 12', 'we do not provide results from berkeley and hotcoref on ace - 2004 dataset as they do not directly support ace input.', 'results for hotcoref are slightly different from the results reported in bjorkelund and  #AUTHOR_TAG.', 'for berkeley system, we use the reported results from  #TAUTHOR_TAG']",1
['system  #TAUTHOR_TAG'],['system  #TAUTHOR_TAG'],['berkeley system  #TAUTHOR_TAG'],"['ace - 2004 dataset is annotated with both mention and mention heads, while the ontonotes - 5. 0', 'dataset only has mention annotations.', 'therefore, we preprocess ontonote - 5. 0 to derive mention heads using collins head rules  #AUTHOR_TAG with gold constituency parsing information and gold named entity information.', 'the parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'we set these extracted heads as gold, which enables us to train the two layer bilou - classifier described in sec.', '3. 1. 1.', 'the nonoverlapping mention head assumption in sec.', '3. 1. 1', 'can be verified empirically on both ace - 2004 and ontonotes - 5. 0', 'datasets.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system  #AUTHOR_TAG, berkeley system  #TAUTHOR_TAG and hotcoref system ( bj [UNK] orkelund and  #AUTHOR_TAG.', 'developed systems our developed system is built on the work by  #AUTHOR_TAG, using constrained latent left - linking model ( cl 3 m ) as our mention - pair coreference model in the joint framework 10.', '']",1
"[', and 106 testing documents  #TAUTHOR_TAG.', 'the ontonotes - 5']","[', and 106 testing documents  #TAUTHOR_TAG.', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task  #AUTHOR_TAG contains 3, 145 annotated']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents  #TAUTHOR_TAG.', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task  #AUTHOR_TAG contains 3']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents  #TAUTHOR_TAG.', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task  #AUTHOR_TAG contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as α = 0. 9, β = 0. 8, λ 1 = 0. 2 and λ 2 = 0. 3']",5
"['collins head rules  #TAUTHOR_TAG to identify their heads.', 'when these']","['collins head rules  #TAUTHOR_TAG to identify their heads.', 'when these']","['more noun phrases from the text and employ collins head rules  #TAUTHOR_TAG to identify their heads.', 'when these']","['of the head mentions proposed by the algorithms described in sec. 3 are positive examples.', 'we ensure a balanced training of the mention head detection model by adding sub - sampled invalid mention head candidates as negative examples.', 'specifically, after mention head candidate generation ( described in sec.', '3 ), we train on a set of candidates with precision larger than 50 %.', 'we then use illinois chunker  #AUTHOR_TAG 6 to extract more noun phrases from the text and employ collins head rules  #TAUTHOR_TAG to identify their heads.', 'when these extracted heads do not overlap with gold mention heads, we treat them as negative examples']",5
"['system  #TAUTHOR_TAG bj [UNK] orkelund and  #AUTHOR_TAG.', 'however, the lack of emphasis is not']","['system  #TAUTHOR_TAG bj [UNK] orkelund and  #AUTHOR_TAG.', 'however, the lack of emphasis is not']","['detection is rarely studied as a stand - alone research problem (  #AUTHOR_TAG is one key exception ).', 'most coreference resolution work simply mentions it in passing as a module in the pipelined system  #TAUTHOR_TAG bj [UNK] orkelund and  #AUTHOR_TAG.', 'however, the lack of emphasis is not']","['detection is rarely studied as a stand - alone research problem (  #AUTHOR_TAG is one key exception ).', 'most coreference resolution work simply mentions it in passing as a module in the pipelined system  #TAUTHOR_TAG bj [UNK] orkelund and  #AUTHOR_TAG.', 'however, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty.', 'indeed, many papers report results in terms of gold mentions versus system generated mentions, as shown in table 1.', 'current state - of - the - art systems show a very significant drop in performance when running on system generated mentions.', 'these performance gaps are worrisome, since the real goal of nlp systems is to process raw data.', '1 : performance gaps between using gold mentions and predicted mentions for three state - of - the - art coreference resolution systems.', 'performance gaps are always larger than 10 %.', ""illinois's system  #AUTHOR_TAG is evaluated on conll ( 2012conll (, 2011 ) shared task and ace - 2004 datasets."", 'it reports an average f1 score of muc, b and ceaf e metrics using conll v7. 0 scorer.', '']",0
"['it has advantages over traditional bio - representation, as shown, e. g. in  #TAUTHOR_TAG.', 'the bilourepr']","['it has advantages over traditional bio - representation, as shown, e. g. in  #TAUTHOR_TAG.', 'the bilourepresentation suggests learning classifiers that']","['it has advantages over traditional bio - representation, as shown, e. g. in  #TAUTHOR_TAG.', 'the bilourepresentation suggests learning classifiers that']","['on this assumption, the problem of identifying mention heads is a sequential phrase identification problem, and we choose to employ the bilou - representation as it has advantages over traditional bio - representation, as shown, e. g. in  #TAUTHOR_TAG.', 'the bilourepresentation suggests learning classifiers that identify the beginning, inside and last tokens of multi - token chunks as well as unit - length chunks.', 'the problem is then transformed into a simple, but constrained, 5 - class classification problem']",4
"['##ll - 2012 shared task  #TAUTHOR_TAG, contains 3, 145 annotated']","['standard split of 268 training documents, 68 development documents, and 106 testing documents  #AUTHOR_TAG.', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012 shared task  #TAUTHOR_TAG, contains 3, 145 annotated']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents  #AUTHOR_TAG.', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012 shared task  #TAUTHOR_TAG, contains 3, 145 annotated documents.', 'these documents come']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents  #AUTHOR_TAG.', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012 shared task  #TAUTHOR_TAG, contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as α = 0. 9, β = 0. 8, λ 1 = 0. 2 and λ 2 = 0. 3']",5
"['on the work by  #TAUTHOR_TAG, using constrained latent left - linking']","['on the work by  #TAUTHOR_TAG, using constrained latent left - linking']","['hotcoref system ( bjorkelund and  #AUTHOR_TAG.', 'developed systems our developed system is built on the work by  #TAUTHOR_TAG, using constrained latent left - linking model ( cl']","['', 'datasets.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system  #AUTHOR_TAG, berkeley system  #AUTHOR_TAG and hotcoref system ( bjorkelund and  #AUTHOR_TAG.', 'developed systems our developed system is built on the work by  #TAUTHOR_TAG, using constrained latent left - linking model ( cl3m ) as our mention - pair coreference model in the joint framework10.', '']",5
"['area too  #AUTHOR_TAG.', 'in this paper, we use the constrained latent left - linking model ( cl3m ) described in  #TAUTHOR_TAG in our experiments']","['the coreference area too  #AUTHOR_TAG.', 'in this paper, we use the constrained latent left - linking model ( cl3m ) described in  #TAUTHOR_TAG in our experiments']","[' #AUTHOR_TAG gained considerable popularity.', 'the early designs were easy to understand and the rules were designed manually.', 'machine learning approaches were introduced in many works  #AUTHOR_TAG.', 'the introduction of ilp methods has influenced the coreference area too  #AUTHOR_TAG.', 'in this paper, we use the constrained latent left - linking model ( cl3m ) described in  #TAUTHOR_TAG in our experiments']","['##ference resolution has been extensively studied, with several state - of - the - art approaches addressing this task  #AUTHOR_TAG bjorkelund and  #AUTHOR_TAG.', 'many of the early rule - based systems like  #AUTHOR_TAG and  #AUTHOR_TAG gained considerable popularity.', 'the early designs were easy to understand and the rules were designed manually.', 'machine learning approaches were introduced in many works  #AUTHOR_TAG.', 'the introduction of ilp methods has influenced the coreference area too  #AUTHOR_TAG.', 'in this paper, we use the constrained latent left - linking model ( cl3m ) described in  #TAUTHOR_TAG in our experiments']",5
"[', and 106 testing documents  #TAUTHOR_TAG.', 'the ontonotes - 5']","[', and 106 testing documents  #TAUTHOR_TAG.', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task  #AUTHOR_TAG contains 3, 145 annotated']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents  #TAUTHOR_TAG.', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task  #AUTHOR_TAG contains 3']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents  #TAUTHOR_TAG.', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task  #AUTHOR_TAG contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as α = 0. 9, β = 0. 8, λ 1 = 0. 2 and λ 2 = 0. 3']",5
"['details can be found in  #TAUTHOR_TAG et al. ( 2013 ).', 'the difference here is that we also consider the validity of mention heads using ( u ), ( m']","['details can be found in  #TAUTHOR_TAG et al. ( 2013 ).', 'the difference here is that we also consider the validity of mention heads using ( u ), ( m']","['details can be found in  #TAUTHOR_TAG et al. ( 2013 ).', 'the difference here is that we also consider the validity of mention heads using ( u ), ( m']","['details can be found in  #TAUTHOR_TAG et al. ( 2013 ).', 'the difference here is that we also consider the validity of mention heads using ( u ), ( m']",0
"['recent works suggest studying coreference jointly with other tasks.', ' #AUTHOR_TAG model entity coreference and event coreference jointly ;  #TAUTHOR_TAG consider joint coreference and entity - linking.', 'the']","['recent works suggest studying coreference jointly with other tasks.', ' #AUTHOR_TAG model entity coreference and event coreference jointly ;  #TAUTHOR_TAG consider joint coreference and entity - linking.', 'the']","['recent works suggest studying coreference jointly with other tasks.', ' #AUTHOR_TAG model entity coreference and event coreference jointly ;  #TAUTHOR_TAG consider joint coreference and entity - linking.', 'the work closest to ours is']","['recent works suggest studying coreference jointly with other tasks.', ' #AUTHOR_TAG model entity coreference and event coreference jointly ;  #TAUTHOR_TAG consider joint coreference and entity - linking.', 'the work closest to ours is that of  #AUTHOR_TAG, which studies a joint anaphoricity detection and coreference resolution framework.', 'while their inference objective is similar, their work assumes gold mentions are given and thus their modeling is very different']",0
"['derive mention heads using collins head rules  #TAUTHOR_TAG with gold constituency parsing information and gold named entity information.', 'the parsing']","['derive mention heads using collins head rules  #TAUTHOR_TAG with gold constituency parsing information and gold named entity information.', 'the parsing']","['derive mention heads using collins head rules  #TAUTHOR_TAG with gold constituency parsing information and gold named entity information.', 'the parsing information 9 is only needed']","['ace - 2004 dataset is annotated with both mention and mention heads, while the ontonotes - 5. 0', 'dataset only has mention annotations.', 'therefore, we preprocess ontonote - 5. 0 to derive mention heads using collins head rules  #TAUTHOR_TAG with gold constituency parsing information and gold named entity information.', 'the parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'we set these extracted heads as gold, which enables us to train the two layer bilou - classifier described in sec.', '3. 1. 1.', 'the nonoverlapping mention head assumption in sec.', '3. 1. 1', 'can be verified empirically on both ace - 2004 and ontonotes - 5. 0', 'datasets.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system  #AUTHOR_TAG, berkeley system  #AUTHOR_TAG and hotcoref system ( bjorkelund and  #AUTHOR_TAG.', 'developed systems our developed system is built on the work by  #AUTHOR_TAG, using constrained latent left - linking model ( cl 3 m ) as our mention - pair coreference model in the joint framework 10.', '']",5
['is inspired by the latent left - linking model in  #TAUTHOR_TAG and the ilp formulation from  #AUTHOR_TAG'],['is inspired by the latent left - linking model in  #TAUTHOR_TAG and the ilp formulation from  #AUTHOR_TAG'],"['is inspired by the latent left - linking model in  #TAUTHOR_TAG and the ilp formulation from  #AUTHOR_TAG.', 'the joint learning']","['', 'our work is inspired by the latent left - linking model in  #TAUTHOR_TAG and the ilp formulation from  #AUTHOR_TAG.', 'the joint learning and inference model takes as input mention head candidates ( sec.', '3 ) and jointly ( 1 ) determines if they are indeed mention heads and ( 2 ) learns a similarity metric between mentions.', 'this is done by simultaneously learning a binary mention head detection classifier and a mention - pair coreference classifier.', 'the mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones.', 'by learning and making decisions jointly, it also serves as a singleton mention head classifier, building on insights from  #AUTHOR_TAG.', 'this joint framework aims to improve performance on both mention head detection and on coreference']",5
"['0  #TAUTHOR_TAG.', ' #AUTHOR_TAG']","['( nist, 2004 ) and ontonotes - 5. 0  #TAUTHOR_TAG.', ' #AUTHOR_TAG']","['2004 ( nist, 2004 ) and ontonotes - 5. 0  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'our approach results in']","['present experiments on the two standard coreference resolution datasets, ace - 2004 ( nist, 2004 ) and ontonotes - 5. 0  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat - of - the - art results on coreference resolution ; in addition, it achieves significant performance improvement on md for both datasets']",5
"['p cat.', 'we can define pcat using a probabilistic grammar  #TAUTHOR_TAG.', 'the grammar may first generate a start or end category ( s, e ) with']","['p cat.', 'we can define pcat using a probabilistic grammar  #TAUTHOR_TAG.', 'the grammar may first generate a start or end category ( s, e ) with']","['we will call p cat.', 'we can define pcat using a probabilistic grammar  #TAUTHOR_TAG.', 'the grammar may first generate a start or end category ( s, e ) with probability p se or a special tokendeletion category (']","['the root, binary, and unary parameters, we want to choose prior means that encode our bias toward cross - linguistically - plausible categories.', 'to formalize the notion of what it means for a category to be more "" plausible "", we extend the category generator of our previous work, which we will call p cat.', 'we can define pcat using a probabilistic grammar  #TAUTHOR_TAG.', 'the grammar may first generate a start or end category ( s, e ) with probability p se or a special tokendeletion category ( d ; explained in § 5 ) with probability p del, or a standard ccg category c']",0
"[""' s ccm is an unlabeled bracketing model""]","[""' s ccm is an unlabeled bracketing model""]","[""' s ccm is an unlabeled bracketing model""]","[""' s ccm is an unlabeled bracketing model that generates the span of part - of - speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non - constituent )."", 'they found that modeling constituent context aids in parser learning because it is able to capture the observation that the same contexts tend to appear repeatedly in a corpus, even with different constituents.', 'while ccm is designed to learn which tag pairs make for likely contexts, without regard for the constituents themselves, our model attempts to learn the relationships between context categories and the types of the constituents, allowing us to take advantage of the natural a priori knowledge about which contexts fit with which constituent labels']",0
"['words to sets of potential supertags.', 'in order to estimate the parameters of our model, we develop a blocked sampler based on that of  #TAUTHOR_TAG to sample parse']","['words to sets of potential supertags.', 'in order to estimate the parameters of our model, we develop a blocked sampler based on that of  #TAUTHOR_TAG to sample parse']","['this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags.', 'in order to estimate the parameters of our model, we develop a blocked sampler based on that of  #TAUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to']","['this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags.', 'in order to estimate the parameters of our model, we develop a blocked sampler based on that of  #TAUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities.', 'however, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a metropolis - hastings step that allows us to sample efficiently from the correct posterior.', 'our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability - preferring priors on those parameters yields further gains in many scenarios']",5
[' #TAUTHOR_TAG ;'],[' #TAUTHOR_TAG ;'],[' #TAUTHOR_TAG ; the ctbccg  #AUTHOR_TAG transformation'],"['our evaluation we compared our supertagcontext approach to ( our reimplementation of ) the best - performing model of our previous work  #AUTHOR_TAG, which scm extends.', 'we evaluated on the english ccgbank  #AUTHOR_TAG, which is a transformation of the penn treebank  #TAUTHOR_TAG ; the ctbccg  #AUTHOR_TAG transformation of the penn chinese treebank  #AUTHOR_TAG ; and the ccg - tut corpus  #AUTHOR_TAG, built from the tut corpus of italian text  #AUTHOR_TAG']",5
"['the hmm supertagger of  #TAUTHOR_TAG.', 'thus, the']","['the hmm supertagger of  #TAUTHOR_TAG.', 'thus, the']","['the hmm supertagger of  #TAUTHOR_TAG.', 'thus, the right - side context prior mean θ rctx - 0 t can be biased in']","[""right - side context of a non - terminal category - - the probability of generating a category to the right of the current constituent's category - - corresponds directly to the category transitions used for the hmm supertagger of  #TAUTHOR_TAG."", ""thus, the right - side context prior mean θ rctx - 0 t can be biased in exactly the same way as the hmm supertagger's transitions : toward context supertags that connect to the constituent label""]",1
"['splits as  #TAUTHOR_TAG.', '']","['splits as  #TAUTHOR_TAG.', '']","['a test set.', 'we use the same splits as  #TAUTHOR_TAG.', '']","['corpus was divided into four distinct data sets : a set from which we extract the tag dictionaries, a set of raw ( unannotated ) sentences, a development set, and a test set.', 'we use the same splits as  #TAUTHOR_TAG.', 'since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form ( x \\ x ) / x rather than introducing special conjunction rules.', 'in order to increase the amount of raw data available to the sampler, we supplemented the english data with raw, unannotated newswire sentences from the nyt giga - word 5 corpus  #AUTHOR_TAG and supplemented italian with the out - of - domain wacky corpus  #AUTHOR_TAG.', 'for english and italian, this allowed us to use 100k raw tokens for training ( chinese uses 62k ).', 'for chinese and italian, for training efficiency, we used only raw sentences that were 50 words or fewer ( note that we did not drop tag dictionary set or test set sentences )']",5
"[' #TAUTHOR_TAG, but we do it directly in the grammar.', 'we add una']","[' #TAUTHOR_TAG, but we do it directly in the grammar.', 'we add unary']","[' #TAUTHOR_TAG, but we do it directly in the grammar.', 'we add una']","['', 'this is similar to the "" deletion "" strategy employed by  #TAUTHOR_TAG, but we do it directly in the grammar.', 'we add unary rules of the form d →u for every potential supertag u in the tree.', 'then, at each node spanning exactly two tokens ( but no higher in the tree ), we allow rules t→ d, v and t→ v, d.', 'recall that in § 3. 1, we stated that d is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary.', 'additionally, since u will still remain as the preterminal, it will be the category examined as the context by adjacent constituents']",1
"['be combinable.', 'we previously showed that incorporating this intuition into a bayesian prior can help train a ccg supertagger  #TAUTHOR_TAG']","['be combinable.', 'we previously showed that incorporating this intuition into a bayesian prior can help train a ccg supertagger  #TAUTHOR_TAG']","['be combinable.', 'we previously showed that incorporating this intuition into a bayesian prior can help train a ccg supertagger  #TAUTHOR_TAG']","['##ridge observed is that, cross - linguistically, grammars prefer simpler syntactic structures when possible, and that due to the natural correspondence of categories and syntactic structure, biasing toward simpler categories encourages simpler structures.', 'in previous work, we were able to incorporate this preference into a bayesian parsing model, biasing pcfg productions toward sim - pler categories by encoding a notion of category simplicity into a prior  #AUTHOR_TAG.', 'baldridge further notes that due to the natural associativity of ccg, adjacent categories tend to be combinable.', 'we previously showed that incorporating this intuition into a bayesian prior can help train a ccg supertagger  #TAUTHOR_TAG']",2
"['- given - category relationships from the weak supervision : the tag dictionary and raw corpus  #TAUTHOR_TAG. 4', 'this procedure attempts to automatically estimate the frequency of']","['setting the terminal production prior distributions _ term - 0 ( w ) by estimating word - given - category relationships from the weak supervision : the tag dictionary and raw corpus  #TAUTHOR_TAG. 4', 'this procedure attempts to automatically estimate the frequency of']","['setting the terminal production prior distributions _ term - 0 ( w ) by estimating word - given - category relationships from the weak supervision : the tag dictionary and raw corpus  #TAUTHOR_TAG. 4', 'this procedure attempts to automatically estimate the frequency of each word / tag combination by dividing the number of raw - corpus occurrences of each word in the dictionary evenly']","['employ the same procedure as our previous work for setting the terminal production prior distributions _ term - 0 ( w ) by estimating word - given - category relationships from the weak supervision : the tag dictionary and raw corpus  #TAUTHOR_TAG. 4', 'this procedure attempts to automatically estimate the frequency of each word / tag combination by dividing the number of raw - corpus occurrences of each word in the dictionary evenly across all of its associated tags.', 'these counts are then combined with estimates of the openness of each tag in order to assess its likelihood of appearing with new words']",5
"['not valid.', 'since we are not generating from the model, this does not introduce difficulties  #TAUTHOR_TAG']","['not valid.', 'since we are not generating from the model, this does not introduce difficulties  #TAUTHOR_TAG']","['parses with conflicting supertags are not valid.', 'since we are not generating from the model, this does not introduce difficulties  #TAUTHOR_TAG']","['ccm, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid.', 'since we are not generating from the model, this does not introduce difficulties  #TAUTHOR_TAG']",4
['x a x x of  #TAUTHOR_TAG'],['x a x x of  #TAUTHOR_TAG'],['the merge rule x a x x of  #TAUTHOR_TAG'],"['of adjacent constituents can be combined using one of a set of combination rules to form categories of higher - level constituents, as seen in figure 1.', 'the direction of the slash operator gives the behavior of the function.', 'a category ( s \\ np ) / pp might describe an intransitive verb with a prepositional phrase complement ; it combines on the right ( / ) with a constituent with category pp, and then on the left ( \\ ) with a noun phrase ( np ) that serves as its subject.', 'we follow  #AUTHOR_TAG in allowing a small set of generic, linguistically - plausible unary and binary grammar rules.', 'we further add rules for combining with punctuation to the left and right and allow for the merge rule x a x x of  #TAUTHOR_TAG']",5
"['by  #AUTHOR_TAG and used by  #TAUTHOR_TAG that samples entire parse trees.', 'for a sentence w, the strategy is to use the inside algorithm  #AUTHOR_TAG to inductively compute, for']","['by  #AUTHOR_TAG and used by  #TAUTHOR_TAG that samples entire parse trees.', 'for a sentence w, the strategy is to use the inside algorithm  #AUTHOR_TAG to inductively compute, for']","['by  #AUTHOR_TAG and used by  #TAUTHOR_TAG that samples entire parse trees.', 'for a sentence w, the strategy is to use the inside algorithm  #AUTHOR_TAG to inductively compute, for each potential non - terminal position spanning words w i through']","['sample from our proposal distribution, we use a blocked gibbs sampler based on the one proposed by  #AUTHOR_TAG and used by  #TAUTHOR_TAG that samples entire parse trees.', 'for a sentence w, the strategy is to use the inside algorithm  #AUTHOR_TAG to inductively compute, for each potential non - terminal position spanning words w i through w j−1 and category t, going "" up "" the tree, the probability of generating w i,...', ', w j−1 via any arrangement of productions that is rooted by y ij = t']",5
"['presented by  #TAUTHOR_TAG.', 'at a high level, we alternate between resampling model parameters']","['presented by  #TAUTHOR_TAG.', 'at a high level, we alternate between resampling model parameters ( _ root, _ bin, _, _, _, _, _ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences.', 'to efficiently sample new model parameters, we exploit dirichlet - multinomial conjugacy.', 'by repeating these alternating steps and accumu - lating the productions, we']","['just described and a corpus of sentences.', 'since there is no way to analytically compute these modes, we resort to gibbs sampling to find an approximate solution.', 'our strategy is based on the approach presented by  #TAUTHOR_TAG.', 'at a high level, we alternate between resampling model parameters']","['wish to infer the distribution over ccg parses, given the model we just described and a corpus of sentences.', 'since there is no way to analytically compute these modes, we resort to gibbs sampling to find an approximate solution.', 'our strategy is based on the approach presented by  #TAUTHOR_TAG.', 'at a high level, we alternate between resampling model parameters ( _ root, _ bin, _, _, _, _, _ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences.', 'to efficiently sample new model parameters, we exploit dirichlet - multinomial conjugacy.', 'by repeating these alternating steps and accumu - lating the productions, we obtain an approximation of the required posterior quantities']",5
"['##context model ( ccm ) of  #TAUTHOR_TAG,']","['important example is the constituentcontext model ( ccm ) of  #TAUTHOR_TAG,']","['important example is the constituentcontext model ( ccm ) of  #TAUTHOR_TAG,']","['important example is the constituentcontext model ( ccm ) of  #TAUTHOR_TAG, which was specifically designed to capture the linguistic observation made by  #AUTHOR_TAG that there are regularities to the contexts in which constituents appear.', 'this phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts.', 'for example, the part - of - speech ( pos ) sequence adj noun frequently occurs between the tags det and verb.', '']",0
"['subject.', 'we follow  #TAUTHOR_TAG in allowing a small set of generic, linguistically - plausible una']","['subject.', 'we follow  #TAUTHOR_TAG in allowing a small set of generic, linguistically - plausible unary and binary grammar rules.', 'we further add rules']","['subject.', 'we follow  #TAUTHOR_TAG in allowing a small set of generic, linguistically - plausible una']","['of adjacent constituents can be combined using one of a set of combination rules to form categories of higher - level constituents, as seen in figure 1.', 'the direction of the slash operator gives the behavior of the function.', 'a category ( s \\ np ) / pp might describe an intransitive verb with a prepositional phrase complement ; it combines on the right ( / ) with a constituent with category pp, and then on the left ( \\ ) with a noun phrase ( np ) that serves as its subject.', 'we follow  #TAUTHOR_TAG in allowing a small set of generic, linguistically - plausible unary and binary grammar rules.', 'we further add rules for combining with punctuation to the left and right and allow for the merge rule x → x x of  #AUTHOR_TAG']",5
"['similar to that used by  #TAUTHOR_TAG.', 'the motivation for the features was to capture declaratively decisions']","['similar to that used by  #TAUTHOR_TAG.', 'the motivation for the features was to capture declaratively decisions']","['291 features in training the spr.', 'features were discovered from the actual sentence plan trees that the spg generated through the feature derivation process described below, in a manner similar to that used by  #TAUTHOR_TAG.', 'the motivation for the features was to capture declaratively decisions']","['##boost, like other machine learning programs of the boosting family, can handle a very large number of features.', 'therefore, instead of carefully choosing a small number of features by hand which may be useful, we generated a very large number of features and let rank - boost choose the relevant ones.', 'in total, we used 3, 291 features in training the spr.', 'features were discovered from the actual sentence plan trees that the spg generated through the feature derivation process described below, in a manner similar to that used by  #TAUTHOR_TAG.', 'the motivation for the features was to capture declaratively decisions made by the randomized spg.', 'we avoided features specific to particular text plans by discarding those that occurred fewer than 10 times']",1
[') community uses hand - written rules to approximate the distribution of linguistic phenomena in a corpus ( see  #TAUTHOR_TAG'],['natural language generation ( nlg ) community uses hand - written rules to approximate the distribution of linguistic phenomena in a corpus ( see  #TAUTHOR_TAG'],[') community uses hand - written rules to approximate the distribution of linguistic phenomena in a corpus ( see  #TAUTHOR_TAG'],"['work in sentence planning in the natural language generation ( nlg ) community uses hand - written rules to approximate the distribution of linguistic phenomena in a corpus ( see  #TAUTHOR_TAG for a recent example with further references ).', 'this approach is difficult to scale due to the nonrobustness of rules and unexpected interactions  #AUTHOR_TAG, and it is difficult to develop new applications quickly.', 'presumably, this is the reason why dialog systems to date have not used this kind of sentence planning']",0
"['from training data, using techniques similar to  #TAUTHOR_TAG']","['the spr uses rules automatically learned from training data, using techniques similar to  #TAUTHOR_TAG']","['primary contribution is a method for training the', 'spr. the spr uses rules automatically learned from training data, using techniques similar to  #TAUTHOR_TAG']","['', 'number : 1 ) request ( depart - time ) in this paper, we present spot, for "" sentence planner, trainable "". we also present a new methodology for automatically training spot on the basis of feedback provided by human judges. in order to train spot, we reconceptualize its task as consisting of two distinct phases. in the first phase, the', 'sentence - plan - generator ( spg ) generates a potentially large sample of possible sentence plans for a', 'given text - plan input. in the second phase, the sentence - plan - ranker ( spr', ') ranks the sample sentence plans, and then selects the top - ranked output to input to the surface realizer. our primary contribution is a method for training the', 'spr. the spr uses rules automatically learned from training data, using techniques similar to  #TAUTHOR_TAG']",1
"['of previous aggregation components  #TAUTHOR_TAG, although the various merge operations are, to our knowledge, novel in this form']","['of previous aggregation components  #TAUTHOR_TAG, although the various merge operations are, to our knowledge, novel in this form']","['of previous aggregation components  #TAUTHOR_TAG, although the various merge operations are, to our knowledge, novel in this form']","['', 'a strength of our approach is the ability to use a very simple spg, as we explain below.', 'the basis of our spg is a set of clausecombining operations that incrementally transform a list of elementary predicate - argument representations ( the dsyntss corresponding to elementary speech acts, in our case ) into a single lexico - structural representation, by combining these representations using the following combining operations.', 'examples can be found in figure adjective.', 'this transforms a predicative use of an adjective into an adnominal construction.', 'period.', 'joins two complete clauses with a period.', 'these operations are not domain - specific and are similar to those of previous aggregation components  #TAUTHOR_TAG, although the various merge operations are, to our knowledge, novel in this form']",0
"['by  #AUTHOR_TAG,  #AUTHOR_TAG, or  #TAUTHOR_TAG are similar, but do not ( always ) explicitly represent']","['by  #AUTHOR_TAG,  #AUTHOR_TAG, or  #TAUTHOR_TAG are similar, but do not ( always ) explicitly represent']","['by  #AUTHOR_TAG,  #AUTHOR_TAG, or  #TAUTHOR_TAG are similar, but do not ( always ) explicitly represent the clause combining operations as labeled nodes.', '4 shows the result of applying the soft - merge operation when args 1 and 2 are implicit confirmations of']","['', 'figure 2 shows some of the realizations of alternative sentence plans generated by our spg for utterance sys - 3 the sp - tree is inspired by  #AUTHOR_TAG.', 'the representations used by  #AUTHOR_TAG,  #AUTHOR_TAG, or  #TAUTHOR_TAG are similar, but do not ( always ) explicitly represent the clause combining operations as labeled nodes.', '4 shows the result of applying the soft - merge operation when args 1 and 2 are implicit confirmations of the origin and destination cities. figure 8 illustrates the relationship between the sp - tree and the dsynts for alternative 8.', 'the labels and arrows show the dsyntss associated with each node in the sp - tree ( in figure 7 ), and the diagram also shows how structures are composed into larger structures by the clause combining operations.', 'the complexity of most sentence planners arises from the attempt to encode constraints on the application of, and ordering of, the operations, in order to generate a single high quality sentence plan.', 'in our approach, we do not need to encode such constraints.', 'rather, we generate a random sample of possible sentence plans for each text plan, up to a pre - specified maximum number of sentence plans, by randomly selecting among the operations according to some probability distribution']",0
"['##the algorithm was implemented by the the authors, following the description in  #TAUTHOR_TAG']","['##the algorithm was implemented by the the authors, following the description in  #TAUTHOR_TAG']","['##the algorithm was implemented by the the authors, following the description in  #TAUTHOR_TAG']","['##the algorithm was implemented by the the authors, following the description in  #TAUTHOR_TAG']",5
['ica system  #TAUTHOR_TAG'],['ica system  #TAUTHOR_TAG'],"['paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation - based learner, and the ica system  #TAUTHOR_TAG']","['paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation - based learner, and the ica system  #TAUTHOR_TAG']",1
"['data used in the experiment was selected from the penn treebank wall street journal, and is the same used by  #TAUTHOR_TAG']","['data used in the experiment was selected from the penn treebank wall street journal, and is the same used by  #TAUTHOR_TAG']","['data used in the experiment was selected from the penn treebank wall street journal, and is the same used by  #TAUTHOR_TAG']","['data used in the experiment was selected from the penn treebank wall street journal, and is the same used by  #TAUTHOR_TAG']",5
"["", which makes extensive use of indexes to speed up the rules'update ; a¢ the fasttbl algorithm ; a¢ the ica algorithm  #TAUTHOR_TAG""]","[""described in section 2 ; a¢ an improved version of tbl, which makes extensive use of indexes to speed up the rules'update ; a¢ the fasttbl algorithm ; a¢ the ica algorithm  #TAUTHOR_TAG""]","["", which makes extensive use of indexes to speed up the rules'update ; a¢ the fasttbl algorithm ; a¢ the ica algorithm  #TAUTHOR_TAG""]","[""##¢ the regular tbl, as described in section 2 ; a¢ an improved version of tbl, which makes extensive use of indexes to speed up the rules'update ; a¢ the fasttbl algorithm ; a¢ the ica algorithm  #TAUTHOR_TAG""]",1
['ica system  #TAUTHOR_TAG aims to reduce the training time by introducing independence assumptions on the training samples that dramatically'],['ica system  #TAUTHOR_TAG aims to reduce the training time by introducing independence assumptions on the training samples that dramatically'],['ica system  #TAUTHOR_TAG aims to reduce the training time by introducing independence assumptions on the training samples that dramatically'],['ica system  #TAUTHOR_TAG aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance'],0
"['the tree - cut technique described above, our previous work  #TAUTHOR_TAG extracted systematic polysemy from wordnet.', 'in this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method']","['the tree - cut technique described above, our previous work  #TAUTHOR_TAG extracted systematic polysemy from wordnet.', 'in this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method']","['the tree - cut technique described above, our previous work  #TAUTHOR_TAG extracted systematic polysemy from wordnet.', 'in this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method']","['the tree - cut technique described above, our previous work  #TAUTHOR_TAG extracted systematic polysemy from wordnet.', 'in this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method']",2
"[' #TAUTHOR_TAG, we']","[' #TAUTHOR_TAG, we']","[' #TAUTHOR_TAG, we applied this method to a small subset']","['this paper, we describes a lexicon organized around systematic polysemy.', 'the lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree - cut  #AUTHOR_TAG.', 'in our previous work  #TAUTHOR_TAG, we applied this method to a small subset of wordnet nouns and showed potential applicability.', 'in the current work, we applied the method to all nouns and verbs in wordnet, and built a lexicon in which word senses are partitioned by systematic polysemy.', 'we report results of comparing our lexicon with the wordnet cousins as well as the inter - annotator disagreement observed between two semantically annotated corpora : wordnet semcor  #AUTHOR_TAG and dso  #AUTHOR_TAG.', 'the results are quite promising : our extraction method discovered 89 % of the wordnet cousins, and the sense partitions in our lexicon yielded better values  #AUTHOR_TAG than arbitrary sense groupings on the agreement data']",2
"['our previous strategy  #TAUTHOR_TAG.', 'rather than using a termdocument matrix,']","['our previous strategy  #TAUTHOR_TAG.', 'rather than using a termdocument matrix,']","['order to obtain semantic representations of each word, we apply our previous strategy  #TAUTHOR_TAG.', 'rather than using a termdocument matrix, we had followed an approach akin to that']","['order to obtain semantic representations of each word, we apply our previous strategy  #TAUTHOR_TAG.', 'rather than using a termdocument matrix, we had followed an approach akin to that of schutze ( 1993 ), who performed svd on a nx2n term - term matrix.', 'the n here represents the n - 1 most - frequent words as well as a glob position to account for all other words not in the top n - 1.', ""the matrix is structured such that for a given word w's row, the first n columns denote words that - ncs ( [UNK], 1""]",2
"['( forbes -  #AUTHOR_TAG a ;  #TAUTHOR_TAG,']","['( forbes -  #AUTHOR_TAG a ;  #TAUTHOR_TAG,']","['prior uncertainty detection experiments ( forbes -  #AUTHOR_TAG a ;  #TAUTHOR_TAG,']","['train our dise model, we first extracted the set of speech and dialogue features shown in figure 2 from the user turns in our corpus.', 'as shown, the acoustic - prosodic features represent duration, pausing, pitch, and energy, and were normalized by the first user turn, as well as totaled and averaged over each dialogue.', 'the lexical and dialogue features consist of the current dialogue name ( i. e., one of the six physics problems ) and turn number, the current itspoke question\'s name ( e. g., t 3 in figure 1 has a unique identifier ) and depth in the discourse structure ( e. g., an itspoke remediation question after an incorrect user answer would be at one greater depth than the prior question ), a word occurrence vector for the automatically recognized text of the user turn, an automatic ( in ) correctness label, and lastly, the number of user turns since the last correct turn ( "" incorrect runs "" ).', 'we also included two user - based features, gender and pretest score.', 'note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( forbes -  #AUTHOR_TAG a ;  #TAUTHOR_TAG, we have also experimented with other features, including state - of - theart acoustic - prosodic features used in the last interspeech challenges  #AUTHOR_TAG b ) and made freely available in the opensmile toolkit  #AUTHOR_TAG.', 'to date, however, these features have only decreased the crossvalidation performance of our models.', '8 while some of our features are tutoring - specific, these have similar counterparts in other applications ( i. e., answer ( in ) correctness corresponds to a more general notion of "" response appropriateness "" in other domains, while pretest score corresponds to the general notion of domain expertise ).', 'moreover, all of our features are fully automatic and available in real - time, so that the model can be directly implemented and deployed.', 'to that end, we now describe the results of our intrinsic and extrinsic evaluations of our dise model, aimed at determining whether it is ready to be evaluated with real users']",2
['bilingual stochastic grammars  #TAUTHOR_TAG'],['bilingual stochastic grammars  #TAUTHOR_TAG'],"['automatic translation  #AUTHOR_TAG, as have been bilingual stochastic grammars  #TAUTHOR_TAG']","['state transducers, which can be learned from bilingual corpora, have been proposed for automatic translation  #AUTHOR_TAG, as have been bilingual stochastic grammars  #TAUTHOR_TAG']",0
['also shows the structural identity to bilingual grammars as used in  #TAUTHOR_TAG'],['also shows the structural identity to bilingual grammars as used in  #TAUTHOR_TAG'],['also shows the structural identity to bilingual grammars as used in  #TAUTHOR_TAG'],['also shows the structural identity to bilingual grammars as used in  #TAUTHOR_TAG'],5
['can be shown  #TAUTHOR_TAG that the use of this model with maximum likelihood parameter estimation is justified on information - theoretic grounds'],['can be shown  #TAUTHOR_TAG that the use of this model with maximum likelihood parameter estimation is justified on information - theoretic grounds'],['can be shown  #TAUTHOR_TAG that the use of this model with maximum likelihood parameter estimation is justified on information - theoretic grounds'],['can be shown  #TAUTHOR_TAG that the use of this model with maximum likelihood parameter estimation is justified on information - theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values. 3 there is no requirement that the components of f represent disjoint or statistically independent events'],4
['describe an efficient algorithm for accomplish'],['describe an efficient algorithm for accomplishing this in which approximations to pst ( tis ) are'],['describe an efficient algorithm for accomplish'],['describe an efficient algorithm for accomplishing this in which approximations to pst ( tis ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t'],0
[') modeling  #TAUTHOR_TAG'],['memd ) modeling  #TAUTHOR_TAG'],[') modeling  #TAUTHOR_TAG'],['statistical technique which has recently become popular for nlp is maximum entropy / minimum divergence ( memd ) modeling  #TAUTHOR_TAG'],5
"[' #TAUTHOR_TAG show, lexical information improves on np and vp chunking as well']","[' #TAUTHOR_TAG show, lexical information improves on np and vp chunking as well']","[' #TAUTHOR_TAG show, lexical information improves on np and vp chunking as well']","[' #TAUTHOR_TAG show, lexical information improves on np and vp chunking as well']",3
"['tested on section 23 ( table 1 ), same as used by  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, and became a common testbed']","['tested on section 23 ( table 1 ), same as used by  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, and became a common testbed']","['tested on section 23 ( table 1 ), same as used by  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, and became a common testbed']","['system was trained on the penn treebank  #AUTHOR_TAG wsj sections 221 and tested on section 23 ( table 1 ), same as used by  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, and became a common testbed']",1
"[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG )']","['learning to produce a full parse of free - text sentences ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG )']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG )']",0
"['is not aimed at handling dependencies, which require heavy use of lexical information  #TAUTHOR_TAG, for pp attachment )']","['is not aimed at handling dependencies, which require heavy use of lexical information  #TAUTHOR_TAG, for pp attachment )']","['is not aimed at handling dependencies, which require heavy use of lexical information  #TAUTHOR_TAG, for pp attachment )']","['is not aimed at handling dependencies, which require heavy use of lexical information  #TAUTHOR_TAG, for pp attachment )']",1
['approach for partial parsing was presented by  #TAUTHOR_TAG'],['approach for partial parsing was presented by  #TAUTHOR_TAG'],['approach for partial parsing was presented by  #TAUTHOR_TAG'],['approach for partial parsing was presented by  #TAUTHOR_TAG'],0
"[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG )']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG )']",0
"['of full parsers, e. g.,  #TAUTHOR_TAG as']","['of full parsers, e. g.,  #TAUTHOR_TAG as']","['of full parsers, e. g.,  #TAUTHOR_TAG as']","['results are lower than those of full parsers, e. g.,  #TAUTHOR_TAG as might be expected since much less structural data, and no lexical data are being used']",1
"['a similar vain to  #AUTHOR_TAG and  #TAUTHOR_TAG, the method extends an existing flat shallow - parsing method to handle composite structures']","['a similar vain to  #AUTHOR_TAG and  #TAUTHOR_TAG, the method extends an existing flat shallow - parsing method to handle composite structures']","['a similar vain to  #AUTHOR_TAG and  #TAUTHOR_TAG, the method extends an existing flat shallow - parsing method to handle composite structures']","['a similar vain to  #AUTHOR_TAG and  #TAUTHOR_TAG, the method extends an existing flat shallow - parsing method to handle composite structures']",3
"[',  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g.,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG )']","[',  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g.,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG )']",0
"['tested on section 23 ( table 1 ), same as used by  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG, and became a common testbed']","['tested on section 23 ( table 1 ), same as used by  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG, and became a common testbed']","['tested on section 23 ( table 1 ), same as used by  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG, and became a common testbed']","['system was trained on the penn treebank  #AUTHOR_TAG wsj sections 221 and tested on section 23 ( table 1 ), same as used by  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG, and became a common testbed']",1
"['tested on section 23 ( table 1 ), same as used by  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG, and became a common testbed']","['tested on section 23 ( table 1 ), same as used by  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG, and became a common testbed']","['tested on section 23 ( table 1 ), same as used by  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG, and became a common testbed']","['system was trained on the penn treebank  #AUTHOR_TAG wsj sections 221 and tested on section 23 ( table 1 ), same as used by  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG, and became a common testbed']",1
"['approach to partial parsing was presented by  #TAUTHOR_TAG, who extended a shallow - parsing technique to partial parsing']","['approach to partial parsing was presented by  #TAUTHOR_TAG, who extended a shallow - parsing technique to partial parsing']","['approach to partial parsing was presented by  #TAUTHOR_TAG, who extended a shallow - parsing technique to partial parsing']","['approach to partial parsing was presented by  #TAUTHOR_TAG, who extended a shallow - parsing technique to partial parsing']",0
['a similar vain to  #TAUTHOR_TAG'],['a similar vain to  #TAUTHOR_TAG'],['a similar vain to  #TAUTHOR_TAG'],"['a similar vain to  #TAUTHOR_TAG and  #AUTHOR_TAG, the method extends an existing flat shallow - parsing method to handle composite structures']",3
"['individual adjectives involved.', 'to quantify the relative strengths of these transitive inferences,  #TAUTHOR_TAG propose to']","['individual adjectives involved.', 'to quantify the relative strengths of these transitive inferences,  #TAUTHOR_TAG propose to']","['the individual adjectives involved.', 'to quantify the relative strengths of these transitive inferences,  #TAUTHOR_TAG propose to']","[', the evidence for the first order is quite a bit stronger than the evidence for the second.', 'the first ordered pairs are more frequent, as are the individual adjectives involved.', 'to quantify the relative strengths of these transitive inferences,  #TAUTHOR_TAG propose to assign a weight to each link.', 'say the order a, b occurs m times and the pair { a, b } occurs n times in total.', 'then the weight of the pair a → b is']",0
"['', 'more generally, distributional clustering techniques ( sch [UNK] utze, 1992 ;  #TAUTHOR_TAG could be applied to extract semantic classes from the corpus itself']","['limited vocabulary for which semantic information would be available.', 'more generally, distributional clustering techniques ( sch [UNK] utze, 1992 ;  #TAUTHOR_TAG could be applied to extract semantic classes from the corpus itself']","['', 'more generally, distributional clustering techniques ( sch [UNK] utze, 1992 ;  #TAUTHOR_TAG could be applied to extract semantic classes from the corpus itself.', 'since the constraints on adjective ordering in english depend largely on semantic classes, the addition of semantic information to the model ought to improve the results']","['', 'first, while semantic information is not available for all adjectives, it is clearly available for some.', 'furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available.', 'more generally, distributional clustering techniques ( sch [UNK] utze, 1992 ;  #TAUTHOR_TAG could be applied to extract semantic classes from the corpus itself.', 'since the constraints on adjective ordering in english depend largely on semantic classes, the addition of semantic information to the model ought to improve the results']",3
"[' #TAUTHOR_TAG call the direct evidence method.', 'to order the pair { a, b }, count how many times the ordered sequences']","[' #TAUTHOR_TAG call the direct evidence method.', 'to order the pair { a, b }, count how many times the ordered sequences a, b and b, a appear in the training data and output the pair in the order which occurred more often']","[' #TAUTHOR_TAG call the direct evidence method.', 'to order the pair { a, b }, count how many times the ordered sequences']","['simplest strategy for ordering adjectives is what  #TAUTHOR_TAG call the direct evidence method.', 'to order the pair { a, b }, count how many times the ordered sequences a, b and b, a appear in the training data and output the pair in the order which occurred more often']",0
"[', boosting  #TAUTHOR_TAG offers the possibility of achieving high']","[', boosting  #TAUTHOR_TAG offers the possibility of achieving high']","[', boosting  #TAUTHOR_TAG offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly']","['second area where the methods described here could be improved is in the way that multiple information sources are integrated.', 'the technique method described in section 3. 7 is a fairly crude method for combining frequency information with symbolic data.', 'it would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature  #AUTHOR_TAG.', 'in particular, boosting  #TAUTHOR_TAG offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly']",3
"['randomly assigned.', ' #TAUTHOR_TAG propose to generalize the direct evidence method so that']","['randomly assigned.', ' #TAUTHOR_TAG propose to generalize the direct evidence method so that']","['assigned.', ' #TAUTHOR_TAG propose to generalize the direct evidence method so that']","['way to think of the direct evidence method is to see that it defines a relation ≺ on the set of english adjectives.', 'given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a, then a ≺ b.', 'if the re - verse is true, and b, a is found more often than a, b, then b ≺ a.', 'if neither order appears in the training data, then neither a ≺ b nor b ≺ a and an order must be randomly assigned.', ' #TAUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation.', '']",0
"[""by the ` nitrogen'generator  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ), takes advantage of standard statistical techniques by generating a lattice of all possible strings given""]","[""by the ` nitrogen'generator  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ), takes advantage of standard statistical techniques by generating a lattice of all possible strings given""]","[""by the ` nitrogen'generator  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ), takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as""]","['problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""one approach to this more general problem, taken by the ` nitrogen'generator  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ), takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model."", 'langkilde and knight report that this strategy yields good results for problems like generating verb / object collocations and for selecting the correct morphological form of a word.', 'it also should be straightforwardly applicable to the more specific problem we are addressing here.', 'to determine the correct order for a sequence of prenominal adjectives, we can simply generate all possible orderings and choose the one with the highest probability.', 'this has the advantage of reducing the problem of adjective ordering to the problem of estimating n - gram probabilities, something which is relatively well understood']",5
"['speech recognition  #AUTHOR_TAG and machine translation  #TAUTHOR_TAG.', 'moreover, once the models']","['speech recognition  #AUTHOR_TAG and machine translation  #TAUTHOR_TAG.', 'moreover, once the models']","['speech recognition  #AUTHOR_TAG and machine translation  #TAUTHOR_TAG.', 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply']","['availability of toolkits for this weighted case  #AUTHOR_TAG van  #AUTHOR_TAG promises to unify much of statistical nlp.', ""such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding,'including classic models for speech recognition  #AUTHOR_TAG and machine translation  #TAUTHOR_TAG."", 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']",0
"['weights, but this paper uses "" weight "" to mean something else.', '[UNK] on demand  #TAUTHOR_TAG can pay off here, since only part of [UNK] may be needed subsequently.']","['weights, but this paper uses "" weight "" to mean something else.', '[UNK] on demand  #TAUTHOR_TAG can pay off here, since only part of [UNK] may be needed subsequently.']","['weights, but this paper uses "" weight "" to mean something else.', '[UNK] on demand  #TAUTHOR_TAG can pay off here, since only part of [UNK] may be needed subsequently.']","['traditionally log ( strength ) values are called weights, but this paper uses "" weight "" to mean something else.', '[UNK] on demand  #TAUTHOR_TAG can pay off here, since only part of [UNK] may be needed subsequently.']",0
"['( π∈π p ( π ), π∈π p ( π ) val ( π ) ) ∈ r ≥0 × v, from which ( 1 ) is trivial to compute.', ' #TAUTHOR_TAG give a sufficiently general finite - state']","['( π∈π p ( π ), π∈π p ( π ) val ( π ) ) ∈ r ≥0 × v, from which ( 1 ) is trivial to compute.', ' #TAUTHOR_TAG give a sufficiently general finite - state']","['( π∈π p ( π ), π∈π p ( π ) val ( π ) ) ∈ r ≥0 × v, from which ( 1 ) is trivial to compute.', ' #TAUTHOR_TAG give a sufficiently general finite - state framework to allow this : weights may fall in any set k ( instead of r ).', 'multiplication and addition are replaced by binary operations ⊗ and ⊕ on k. thus ⊗ is used to combine arc weights into a path weight and ⊕ is used to combine the weights of alternative paths.', 'to sum over infinite sets of cyclic paths we also need a closure operation *, interpreted as k * = ∞ i = 0 k i.', 'the usual finite - state algorithms work if ( k, ⊕, ⊗, * ) has the structure']","['denominator of equation ( 1 ) is the total probability of all accepting paths in x i • f • y i.', 'but while computing this, we will also compute the numerator.', 'the idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability.', 'we will enforce an invariant : the weight of any pathset π must be ( π∈π p ( π ), π∈π p ( π ) val ( π ) ) ∈ r ≥0 × v, from which ( 1 ) is trivial to compute.', ' #TAUTHOR_TAG give a sufficiently general finite - state framework to allow this : weights may fall in any set k ( instead of r ).', 'multiplication and addition are replaced by binary operations ⊗ and ⊕ on k. thus ⊗ is used to combine arc weights into a path weight and ⊕ is used to combine the weights of alternative paths.', 'to sum over infinite sets of cyclic paths we also need a closure operation *, interpreted as k * = ∞ i = 0 k i.', 'the usual finite - state algorithms work if ( k, ⊕, ⊗, * ) has the structure of a closed semiring. 15', 'rdinary probabilities fall in the semiring ( r ≥0, +, ×, * ). 16', 'our novel weights fall in a novel 14 formal derivation of ( 1 )']",5
"['algorithm  #TAUTHOR_TAG.', 'but notice that it has no backward pass.', 'in place of pushing cumulative probabilities backward to']","['algorithm  #TAUTHOR_TAG.', 'but notice that it has no backward pass.', 'in place of pushing cumulative probabilities backward to']",[' #TAUTHOR_TAG'],"['in many cases of interest, t i is an acyclic graph. 20', ""hen tarjan's method computes w 0j for each j in topologically sorted order, thereby finding t i in a linear number of ⊕ and ⊗ operations."", 'for hmms ( footnote 11 ), ti is the familiar trellis, and we would like this computation of ti to reduce to the forwardbackward algorithm  #TAUTHOR_TAG.', 'but notice that it has no backward pass.', 'in place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs ( more generally, values in v ) forward to the probabilities.', 'this is slower because our ⊕ and ⊗ are vector operations, and the vectors rapidly lose sparsity as they are added together']",0
['availability of toolkits for this weighted case  #TAUTHOR_TAG ; van  #AUTHOR_TAG promises to'],['availability of toolkits for this weighted case  #TAUTHOR_TAG ; van  #AUTHOR_TAG promises to'],['availability of toolkits for this weighted case  #TAUTHOR_TAG ; van  #AUTHOR_TAG promises to'],"['availability of toolkits for this weighted case  #TAUTHOR_TAG ; van  #AUTHOR_TAG promises to unify much of statistical nlp.', 'such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding, 1 including classic models for speech recognition  #AUTHOR_TAG and machine translation ( knight and al -  #AUTHOR_TAG.', 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']",0
"['- backward algorithm  #AUTHOR_TAG trains only hidden markov models, while  #TAUTHOR_TAG trains only stochastic edit distance']","['are limited to particular kinds of models and training regimens.', 'for example, the forward - backward algorithm  #AUTHOR_TAG trains only hidden markov models, while  #TAUTHOR_TAG trains only stochastic edit distance']","['they are limited to particular kinds of models and training regimens.', 'for example, the forward - backward algorithm  #AUTHOR_TAG trains only hidden markov models, while  #TAUTHOR_TAG trains only stochastic edit distance']","[', there is a stumbling block : where do the weights come from?', 'after all, statistical models require supervised or unsupervised training.', 'currently, finite - state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'for example, the forward - backward algorithm  #AUTHOR_TAG trains only hidden markov models, while  #TAUTHOR_TAG trains only stochastic edit distance']",0
"['nm ) time, and faster approximations  #TAUTHOR_TAG']","['nm ) time, and faster approximations  #TAUTHOR_TAG']","['nm ) time, and faster approximations  #TAUTHOR_TAG']","['therefore reintroduce a backward pass that lets us avoid ⊕ and ⊗ when computing t i ( so they are needed only to construct t i ).', 'this speedup also works for cyclic graphs and for any v.', 'write w jk as ( p jk, v jk ), and let w 1 jk = ( p 1 jk, v 1 jk ) denote the weight of the edge from j to k. 19 then it can be shown that w 0n = ( p 0n, j, k p 0j v 1 jk p kn ).', 'the forward and backward probabilities, p0j and pkn, can be computed using single - source algebraic path for the simpler semiring ( r, +, x, a ) - - or equivalently, by solving a sparse linear system of equations over r, a much - studied problem at o ( n ) space, o ( nm ) time, and faster approximations  #TAUTHOR_TAG']",0
"['the well - known kleene - sch [UNK] utzenberger construction  #TAUTHOR_TAG, taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'a full proof is straightforward, as are']","['the well - known kleene - sch [UNK] utzenberger construction  #TAUTHOR_TAG, taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'a full proof is straightforward, as are']","['the well - known kleene - sch [UNK] utzenberger construction  #TAUTHOR_TAG, taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'a full proof is straightforward, as are']","['##to prove ( 1 ) a ( 3 ), express f as an fst and apply the well - known kleene - sch [UNK] utzenberger construction  #TAUTHOR_TAG, taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'a full proof is straightforward, as are proofs of ( 3 ) _ ( 2 ), ( 2 ) _ ( 1 )']",5
"['−1 vp −1 ).', 'division is commonly used in defining f θ ( for normalization ). 19', 'multiple edges from j to k are summed into a single edge.', ' #AUTHOR_TAG.', 'efficient hardware implementation is also possible via chip - level parallelism  #TAUTHOR_TAG']","['and subtraction are also possible : − ( p, v ) = ( −p, −v ) and ( p, v ) −1 = ( p −1, −p −1 vp −1 ).', 'division is commonly used in defining f θ ( for normalization ). 19', 'multiple edges from j to k are summed into a single edge.', ' #AUTHOR_TAG.', 'efficient hardware implementation is also possible via chip - level parallelism  #TAUTHOR_TAG']","[', −v ) and ( p, v ) −1 = ( p −1, −p −1 vp −1 ).', 'division is commonly used in defining f θ ( for normalization ). 19', 'multiple edges from j to k are summed into a single edge.', ' #AUTHOR_TAG.', 'efficient hardware implementation is also possible via chip - level parallelism  #TAUTHOR_TAG']","['and subtraction are also possible : − ( p, v ) = ( −p, −v ) and ( p, v ) −1 = ( p −1, −p −1 vp −1 ).', 'division is commonly used in defining f θ ( for normalization ). 19', 'multiple edges from j to k are summed into a single edge.', ' #AUTHOR_TAG.', 'efficient hardware implementation is also possible via chip - level parallelism  #TAUTHOR_TAG']",3
"[').', 'per - state joint normalization  #TAUTHOR_TAG b, a § 8. 2 ) is similar but drops the dependence on a.', 'the difficult case is global conditional normalization.', 'it arises, for example, when training a joint model of the']","['chosen arc in dj, a ).', 'per - state joint normalization  #TAUTHOR_TAG b, a § 8. 2 ) is similar but drops the dependence on a.', 'the difficult case is global conditional normalization.', 'it arises, for example, when training a joint model of the']","[').', 'per - state joint normalization  #TAUTHOR_TAG b, a § 8. 2 ) is similar but drops the dependence on a.', 'the difficult case is global conditional normalization.', 'it arises, for example, when training a joint model of the']","['for per - state conditional normalization, let dj, a be the set of arcs from state j with input symbol a ∈ σ ; their weights are normalized to sum to 1.', 'besides computing c, the e step must count the expected number dj, a of traversals of arcs in each dj, a.', 'then the predicted vector given θ is j, a dj, a • ( expected feature counts on a randomly chosen arc in dj, a ).', 'per - state joint normalization  #TAUTHOR_TAG b, a § 8. 2 ) is similar but drops the dependence on a.', 'the difficult case is global conditional normalization.', 'it arises, for example, when training a joint model of the']",1
"['em algorithm  #TAUTHOR_TAG can maximize these functions.', 'roughly,']","['em algorithm  #TAUTHOR_TAG can maximize these functions.', 'roughly,']","['em algorithm  #TAUTHOR_TAG can maximize these functions.', 'roughly, the e step guesses hidden information : if ( x i, y i ) was generated from the current f θ,']","['em algorithm  #TAUTHOR_TAG can maximize these functions.', 'roughly, the e step guesses hidden information : if ( x i, y i ) was generated from the current f θ, which fst paths stand a chance of having been the path used?', '( guessing the path also guesses the exact input and output. )', 'the m step updates θ to make those paths more likely.', 'em alternates these steps and converges to a local optimum.', ""the m step's form depends on the parameterization and the e step serves the m step's needs""]",5
"[' #TAUTHOR_TAG, ( 4']","[' #TAUTHOR_TAG, ( 4']","[' #TAUTHOR_TAG, ( 4']","['defining conditional relations, a good regexp language is unknown to us, but they can be defined in several other ways : ( 1 ) via fsts as in fig. 1c, ( 2 ) by compilation of weighted rewrite rules  #AUTHOR_TAG, ( 3 ) by compilation of decision trees  #TAUTHOR_TAG, ( 4 ) as a relation that performs contextual left - to - right replacement of input substrings by a smaller conditional relation ( gerdemann and van  #AUTHOR_TAG, 5 ( 5 ) by conditionalization of a joint relation as discussed below']",0
"['- known algebraic path problem  #TAUTHOR_TAG ; tar an,']","['for some important remarks on efficiency : a¢ computing ti is an instance of the well - known algebraic path problem  #TAUTHOR_TAG ; tar an, 1981a ). then ti is']","['for some important remarks on efficiency : a¢ computing ti is an instance of the well - known algebraic path problem  #TAUTHOR_TAG ; tar an,']","['for some important remarks on efficiency : a¢ computing ti is an instance of the well - known algebraic path problem  #TAUTHOR_TAG ; tar an, 1981a ). then ti is the total semiring weight w0n of paths in ti from initial state 0 to final state n ( assumed wlog to be unique and un - weighted ).', 'it is wasteful to compute ti as suggested earlier, by minimizing ( _ _ xi ) _ f _ ( yi _ _ ), since then the real work is done by an _ - closure step  #AUTHOR_TAG that implements the all - pairs version of algebraic path, whereas all we need is the single - source version.', 'if n and m are the number of states and edges, 19 then both problems are o ( n3 ) in the worst case, but the single - source version can be solved in essentially o ( m ) time for acyclic graphs and other reducible flow graphs  #AUTHOR_TAG b ).', 'for a general graph  #AUTHOR_TAG b ) shows how to partition into hard subgraphs that localize the cyclicity or irreducibility, then run the o ( n3 ) algorithm on each subgraph ( thereby reducing n to as little as 1 ), and recombine the results.', 'the overhead of partitioning and recombining is essentially only o ( m )']",0
"['desired.', 'such approaches have been tried recently in restricted cases ( mc  #AUTHOR_TAG b ;  #TAUTHOR_TAG']","['desired.', 'such approaches have been tried recently in restricted cases ( mc  #AUTHOR_TAG b ;  #TAUTHOR_TAG']","['desired.', 'such approaches have been tried recently in restricted cases ( mc  #AUTHOR_TAG b ;  #TAUTHOR_TAG']","[': e −→, and a : ae −→ share a contextual "" vowel - fronting "" feature, then their weights rise and fall together with the strength of that feature.', 'the resulting machine must be normalized, either per - state or globally, to obtain a joint or a conditional distribution as desired.', 'such approaches have been tried recently in restricted cases ( mc  #AUTHOR_TAG b ;  #TAUTHOR_TAG']",0
['feature selection and avoid overfitting  #TAUTHOR_TAG'],['feature selection and avoid overfitting  #TAUTHOR_TAG'],['feature selection and avoid overfitting  #TAUTHOR_TAG'],"['- posterior estimation tries to maximize p ( θ ) • i f θ ( x i, y i ) where p ( θ ) is a prior probability.', 'in a log - linear parameterization, for example, a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting  #TAUTHOR_TAG']",5
"['_ ( x, y ) ; the goal is to recover the true _.', 'samples need not be fully observed ( partly supervised training ) : thus xi _ _ _, yi _ _ may be given as regular sets in which input and output were observed to fall.', 'for example, in ordinary hmm training, xi = e * and represents a completely hidden state sequence ( cfxxx  #TAUTHOR_TAG, who allows any regular set ), while yi is a single string representing a completely observed emission sequence.']","['training data we are given a set of observed ( input, output ) pairs, ( xi, yi ).', 'these are assumed to be independent random samples from a joint dis - tribution of the form f _ ( x, y ) ; the goal is to recover the true _.', 'samples need not be fully observed ( partly supervised training ) : thus xi _ _ _, yi _ _ may be given as regular sets in which input and output were observed to fall.', 'for example, in ordinary hmm training, xi = e * and represents a completely hidden state sequence ( cfxxx  #TAUTHOR_TAG, who allows any regular set ), while yi is a single string representing a completely observed emission sequence.']","['_ ( x, y ) ; the goal is to recover the true _.', 'samples need not be fully observed ( partly supervised training ) : thus xi _ _ _, yi _ _ may be given as regular sets in which input and output were observed to fall.', 'for example, in ordinary hmm training, xi = e * and represents a completely hidden state sequence ( cfxxx  #TAUTHOR_TAG, who allows any regular set ), while yi is a single string representing a completely observed emission sequence.']","['training data we are given a set of observed ( input, output ) pairs, ( xi, yi ).', 'these are assumed to be independent random samples from a joint dis - tribution of the form f _ ( x, y ) ; the goal is to recover the true _.', 'samples need not be fully observed ( partly supervised training ) : thus xi _ _ _, yi _ _ may be given as regular sets in which input and output were observed to fall.', 'for example, in ordinary hmm training, xi = e * and represents a completely hidden state sequence ( cfxxx  #TAUTHOR_TAG, who allows any regular set ), while yi is a single string representing a completely observed emission sequence.']",0
['speech recognition  #TAUTHOR_TAG and machine'],['speech recognition  #TAUTHOR_TAG and machine'],"['speech recognition  #TAUTHOR_TAG and machine translation ( knight and al -  #AUTHOR_TAG.', 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply']","['availability of toolkits for this weighted case  #AUTHOR_TAG van  #AUTHOR_TAG promises to unify much of statistical nlp.', ""such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding,'including classic models for speech recognition  #TAUTHOR_TAG and machine translation ( knight and al -  #AUTHOR_TAG."", 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']",0
['by an c - closure step  #TAUTHOR_TAG that implements'],['by an c - closure step  #TAUTHOR_TAG that implements'],"['ti as suggested earlier, by minimizing ( cxxi ) of o ( yixe ), since then the real work is done by an c - closure step  #TAUTHOR_TAG that implements the all - pairs version']","['', 'then t i is the total semiring weight w 0n of paths in t i from initial state 0 to final state n ( assumed wlog to be unique and unweighted ).', 'it is wasteful to compute ti as suggested earlier, by minimizing ( cxxi ) of o ( yixe ), since then the real work is done by an c - closure step  #TAUTHOR_TAG that implements the all - pairs version of algebraic path, whereas all we need is the single - source version.', 'if n and m are the number of states and edges, 19 then both problems are o ( n 3 ) in the worst case, but the single - source version can be solved in essentially o ( m ) time for acyclic graphs and other reducible flow graphs  #AUTHOR_TAG b ).', 'for a general graph t i,  #AUTHOR_TAG b ) shows how to partition into "" hard "" subgraphs that localize the cyclicity or irreducibility, then run the o ( n 3 ) algorithm on each subgraph ( thereby reducing n to as little as 1 ), and recombine the results.', 'the overhead of partitioning and recombining is essentially only o ( m )']",0
"['weighted relation.', 'undesirable consequences of this fact have been termed "" label bias ""  #TAUTHOR_TAG.', 'also, in']","['weighted relation.', 'undesirable consequences of this fact have been termed "" label bias ""  #TAUTHOR_TAG.', 'also, in']","['weighted relation.', 'undesirable consequences of this fact have been termed "" label bias ""  #TAUTHOR_TAG.', 'also, in the conditional case such per - state normalization is only correct if all states accept all input suffixes ( since']","['an easy approach is to normalize the options at each state to make the fst markovian.', 'unfortunately, the result may differ for equivalent fsts that express the same weighted relation.', 'undesirable consequences of this fact have been termed "" label bias ""  #TAUTHOR_TAG.', 'also, in the conditional case such per - state normalization is only correct if all states accept all input suffixes ( since "" dead ends "" leak probability mass ). 8', 'a better - founded approach is global normalization, which simply divides each f ( x, y )']",0
"['brief version of this work, with some additional material, first appeared as  #TAUTHOR_TAG a ).', 'a leisurely journal - length version with more details has been prepared and is available']","['brief version of this work, with some additional material, first appeared as  #TAUTHOR_TAG a ).', 'a leisurely journal - length version with more details has been prepared and is available']","['brief version of this work, with some additional material, first appeared as  #TAUTHOR_TAG a ).', 'a leisurely journal - length version with more details has been prepared and is available']","['brief version of this work, with some additional material, first appeared as  #TAUTHOR_TAG a ).', 'a leisurely journal - length version with more details has been prepared and is available']",2
"['##s  #TAUTHOR_TAG, or to extend the idea, weighted fsts']","['approximate pcfgs  #TAUTHOR_TAG, or to extend the idea, weighted fsts']","['approximate pcfgs  #TAUTHOR_TAG, or to extend the idea, weighted fsts']","[', x, y p ( v | w ) p ( w, x ) p ( y | x ) p ( z | y ), implemented by composing 4 machines. 6, 7', 'here are also procedures for defining weighted fsts that are not probabilistic  #AUTHOR_TAG.', 'arbitrary weights such as 2. 7 may be assigned to arcs or sprinkled through a regexp ( to be compiled into : / 2. 7 −→ arcs ).', 'a more subtle example is weighted fsas that approximate pcfgs  #TAUTHOR_TAG, or to extend the idea, weighted fsts that approximate joint or conditional synchronous pcfgs built for translation.', ""these are parameterized by the pcfg's parameters, but add or remove strings of the pcfg to leave an improper probability distribution""]",0
"[' #TAUTHOR_TAG, ( 3']","[' #TAUTHOR_TAG, ( 3']","[' #TAUTHOR_TAG, ( 3']","['defining conditional relations, a good regexp language is unknown to us, but they can be defined in several other ways : ( 1 ) via fsts as in fig. 1c, ( 2 ) by compilation of weighted rewrite rules  #TAUTHOR_TAG, ( 3 ) by compilation of decision trees  #AUTHOR_TAG, ( 4 ) as a relation that performs contextual left - to - right replacement of input substrings by a smaller conditional relation ( gerdemann and van  #AUTHOR_TAG, 5 ( 5 ) by conditionalization of a joint relation as discussed below']",0
"['desired.', 'such approaches have been tried recently in restricted cases  #TAUTHOR_TAG b ;  #AUTHOR_TAG']","['desired.', 'such approaches have been tried recently in restricted cases  #TAUTHOR_TAG b ;  #AUTHOR_TAG']","['desired.', 'such approaches have been tried recently in restricted cases  #TAUTHOR_TAG b ;  #AUTHOR_TAG']","[': e −→, and a : ae −→ share a contextual "" vowel - fronting "" feature, then their weights rise and fall together with the strength of that feature.', 'the resulting machine must be normalized, either per - state or globally, to obtain a joint or a conditional distribution as desired.', 'such approaches have been tried recently in restricted cases  #TAUTHOR_TAG b ;  #AUTHOR_TAG']",0
"['##s  #TAUTHOR_TAG, or to extend the idea, weighted fsts']","['approximate pcfgs  #TAUTHOR_TAG, or to extend the idea, weighted fsts']","['approximate pcfgs  #TAUTHOR_TAG, or to extend the idea, weighted fsts']","[', x, y p ( v | w ) p ( w, x ) p ( y | x ) p ( z | y ), implemented by composing 4 machines. 6, 7', 'here are also procedures for defining weighted fsts that are not probabilistic  #AUTHOR_TAG.', 'arbitrary weights such as 2. 7 may be assigned to arcs or sprinkled through a regexp ( to be compiled into : / 2. 7 −→ arcs ).', 'a more subtle example is weighted fsas that approximate pcfgs  #TAUTHOR_TAG, or to extend the idea, weighted fsts that approximate joint or conditional synchronous pcfgs built for translation.', ""these are parameterized by the pcfg's parameters, but add or remove strings of the pcfg to leave an improper probability distribution""]",0
"['- backward algorithm  #TAUTHOR_TAG trains only hidden markov models, while  #AUTHOR_TAG trains only stochastic edit distance']","['are limited to particular kinds of models and training regimens.', 'for example, the forward - backward algorithm  #TAUTHOR_TAG trains only hidden markov models, while  #AUTHOR_TAG trains only stochastic edit distance']","['they are limited to particular kinds of models and training regimens.', 'for example, the forward - backward algorithm  #TAUTHOR_TAG trains only hidden markov models, while  #AUTHOR_TAG trains only stochastic edit distance']","[', there is a stumbling block : where do the weights come from?', 'after all, statistical models require supervised or unsupervised training.', 'currently, finite - state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'for example, the forward - backward algorithm  #TAUTHOR_TAG trains only hidden markov models, while  #AUTHOR_TAG trains only stochastic edit distance']",0
"['desired.', 'such approaches have been tried recently in restricted cases  #TAUTHOR_TAG b ;  #AUTHOR_TAG']","['desired.', 'such approaches have been tried recently in restricted cases  #TAUTHOR_TAG b ;  #AUTHOR_TAG']","['desired.', 'such approaches have been tried recently in restricted cases  #TAUTHOR_TAG b ;  #AUTHOR_TAG']","[': e −→, and a : ae −→ share a contextual "" vowel - fronting "" feature, then their weights rise and fall together with the strength of that feature.', 'the resulting machine must be normalized, either per - state or globally, to obtain a joint or a conditional distribution as desired.', 'such approaches have been tried recently in restricted cases  #TAUTHOR_TAG b ;  #AUTHOR_TAG']",0
"['of total feature counts equals c, using improved iterative scaling ( della  #TAUTHOR_TAG ; chen and', 'for globally normalized, joint models, the predicted vector is ec f ( σ *, ∆ * ).', 'if the log - linear probabilities are conditioned on the state and / or the input, the predicted vector is harder to describe ( though usually much easier to compute ).']","['of total feature counts equals c, using improved iterative scaling ( della  #TAUTHOR_TAG ; chen and', 'for globally normalized, joint models, the predicted vector is ec f ( σ *, ∆ * ).', 'if the log - linear probabilities are conditioned on the state and / or the input, the predicted vector is harder to describe ( though usually much easier to compute ).']","['the predicted vector of total feature counts equals c, using improved iterative scaling ( della  #TAUTHOR_TAG ; chen and', 'for globally normalized, joint models, the predicted vector is ec f ( σ *, ∆ * ).', 'if the log - linear probabilities are conditioned on the state and / or the input, the predicted vector is harder to describe ( though usually much easier to compute ).']","['if arc probabilities ( or even λ, ν, [UNK], ρ ) have loglinear parameterization, then the e step must compute c = i ec f ( x i, y i ), where ec ( x, y ) denotes the expected vector of total feature counts along a random path in f θ whose ( input, output ) matches ( x, y ).', 'the m step then treats c as fixed, observed data and adjusts 0 until the predicted vector of total feature counts equals c, using improved iterative scaling ( della  #TAUTHOR_TAG ; chen and', 'for globally normalized, joint models, the predicted vector is ec f ( σ *, ∆ * ).', 'if the log - linear probabilities are conditioned on the state and / or the input, the predicted vector is harder to describe ( though usually much easier to compute ).']",5
"['of one or more conditional relations as in fig. 1  #TAUTHOR_TAG.', 'the general form is illustrated by 3 conceptually, the parameters']","['of one or more conditional relations as in fig. 1  #TAUTHOR_TAG.', 'the general form is illustrated by 3 conceptually, the parameters']","['central technique is to define a joint relation as a noisy - channel model, by composing a joint relation with a cascade of one or more conditional relations as in fig. 1  #TAUTHOR_TAG.', 'the general form is illustrated by 3 conceptually, the parameters']","['central technique is to define a joint relation as a noisy - channel model, by composing a joint relation with a cascade of one or more conditional relations as in fig. 1  #TAUTHOR_TAG.', 'the general form is illustrated by 3 conceptually, the parameters represent the probabilities of reading another a ( λ ) ; reading another b ( ν ) ; transducing b to p rather than q ( [UNK] ) ; starting to transduce p to rather than x ( ρ ). 4 to prove ( 1 ) ⇒ ( 3 ), express f as an fst and apply the well - known kleene - schutzenberger construction  #AUTHOR_TAG, taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'a full proof is straightforward, as are proofs of ( 3 ) ⇒ ( 2 ), ( 2 ) ⇒ ( 1 )']",0
"[' #TAUTHOR_TAG,']","['11 ], refseq  #TAUTHOR_TAG,']","['[ 11 ], refseq  #TAUTHOR_TAG,']","['system utilizes several large size biological databases including three ncbi databases ( genpept [ 11 ], refseq  #TAUTHOR_TAG, and entrez gene [ 13 ] ), psd database from protein information resources ( pir ) [ 14 ], and', 'additionally, several model organism databases or nomenclature databases were used.', 'correspondences among records from these databases are identified using the rich cross - reference information provided by the iproclass database of pir [ 14 ].', 'the following provides a brief description of each of the database']",5
"[')  #TAUTHOR_TAG,']","['( sgd )  #TAUTHOR_TAG,']","[')  #TAUTHOR_TAG,']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase [ 19 ], yeast saccharomyces genome database ( sgd )  #TAUTHOR_TAG, rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
"[')  #TAUTHOR_TAG, fly']","['( mgd )  #TAUTHOR_TAG, fly flybase [ 19 ], yeast saccharomyces genome']","[')  #TAUTHOR_TAG, fly flybase [ 19 ], yeast saccharomyces genome database ( sg']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd )  #TAUTHOR_TAG, fly flybase [ 19 ], yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
"['worm - - wormbase  #TAUTHOR_TAG, human nomenclature']","['worm - - wormbase  #TAUTHOR_TAG, human nomenclature']","['worm - - wormbase  #TAUTHOR_TAG, human nomenclature database ( hugo ) [ 23 ],']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase [ 19 ], yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase  #TAUTHOR_TAG, human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
"['nlm )  #TAUTHOR_TAG.', '']","['nlm )  #TAUTHOR_TAG.', '']","['nlm )  #TAUTHOR_TAG.', 'it contains three knowledge']","['umls - - the unified medical language system ( umls ) has been developed and maintained by national library of medicine ( nlm )  #TAUTHOR_TAG.', '']",0
"['the task of managing information recorded in free text more feasible  #TAUTHOR_TAG.', 'one requirement for nlp is the']","['the task of managing information recorded in free text more feasible  #TAUTHOR_TAG.', 'one requirement for nlp is the']","['the task of managing information recorded in free text more feasible  #TAUTHOR_TAG.', 'one requirement for nlp is the ability to']","['the use of computers in storing the explosive amount of biological information, natural language processing ( nlp ) approaches have been explored to make the task of managing information recorded in free text more feasible  #TAUTHOR_TAG.', 'one requirement for nlp is the ability to accurately recognize terms that represent biological entities in free text.', 'another requirement is the ability to associate these terms with corresponding biological entities ( i. e., records in biological databases ) in order to be used by other automated systems for literature mining.', 'such task is called biological entity tagging.', 'biological entity tagging is not a trivial task because of several characteristics associated with biological entity names, namely : synonymy ( i. e., different terms refer to the same entity ), ambiguity ( i. e., one term is associated with different entities ), and coverage ( i. e., entity terms or entities are not present in databases or knowledge bases )']",0
"['inheritance in man ( omim )  #TAUTHOR_TAG, and']","['inheritance in man ( omim )  #TAUTHOR_TAG, and']","['in man ( omim )  #TAUTHOR_TAG, and']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase [ 19 ], yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim )  #TAUTHOR_TAG, and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
['##100 sequences into clusters based on the cd - hit algorithm  #TAUTHOR_TAG such that each'],['built by clustering uniref100 sequences into clusters based on the cd - hit algorithm  #TAUTHOR_TAG such that each'],['##50 are built by clustering uniref100 sequences into clusters based on the cd - hit algorithm  #TAUTHOR_TAG such that each cluster is composed of sequences that have'],"['resources - there are three databases in pir : the protein sequence database ( psd ), iproclass, and pir - nref.', 'psd database includes functionally annotated protein sequences.', 'the iproclass database is a central point for exploration of protein information, which provides summary descriptions of protein family, function and structure for all protein sequences from pir, swiss - prot, and trembl ( now uniprot ).', 'additionally, it links to over 70 biological databases in the world.', 'the pir - nref database is a comprehensive database for sequence searching and protein identification.', 'it contains non - redundant protein sequences from psd, swiss - prot, trembl, refseq, genpept, and pdb.', 'three uniref tables uniref100, uniref90 and uniref50 ) are available for download : uniref100 combines identical sequences and sub - fragments into a single uniref entry ; and uniref90 and uniref50 are built by clustering uniref100 sequences into clusters based on the cd - hit algorithm  #TAUTHOR_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity, respectively, to the representative sequence.', 'ncbi resources - three data sources from ncbi were used in this study : genpept, refseq, and entrez gene.', 'genpept entries are those translated from the genbanknucleotide sequence database.', 'refseq is a comprehensive, integrated, non - redundant set of sequences, including genomic dna, transcript ( rna ), and protein products, for major research organisms.', ""entrez gene provides a unified query environment for genes defined by sequence and / or in ncbi's map viewer."", 'it records gene names, symbols, and many other attributes associated with genes and the products they encode']",5
"[')  #TAUTHOR_TAG, worm - - wormbase [ 22 ], human nomenclature']","['( rgd )  #TAUTHOR_TAG, worm - - wormbase [ 22 ], human nomenclature']","[')  #TAUTHOR_TAG, worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ],']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase [ 19 ], yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd )  #TAUTHOR_TAG, worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
"[' #TAUTHOR_TAG, and', 'additionally, several model organism']","[' #TAUTHOR_TAG, and', 'additionally, several model organism']","[' #TAUTHOR_TAG, and', 'additionally, several model organism']","['system utilizes several large size biological databases including three ncbi databases ( genpept [ 11 ], refseq [ 12 ], and entrez gene [ 13 ] ), psd database from protein information resources ( pir )  #TAUTHOR_TAG, and', 'additionally, several model organism databases or nomenclature databases were used.', 'correspondences among records from these databases are identified using the rich cross - reference information provided by the iproclass database of pir [ 14 ].', 'the following provides a brief description of each of the database']",5
"['##base  #TAUTHOR_TAG, yeast saccharomyces genome']","['( mgd ) [ 18 ], fly flybase  #TAUTHOR_TAG, yeast saccharomyces genome']","[') [ 18 ], fly flybase  #TAUTHOR_TAG, yeast saccharomyces genome database ( sg']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase  #TAUTHOR_TAG, yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
['##num )  #TAUTHOR_TAG'],['( ecnum )  #TAUTHOR_TAG'],['##num )  #TAUTHOR_TAG'],"['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase [ 19 ], yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum )  #TAUTHOR_TAG']",5
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],['of our prior work  #TAUTHOR_TAG'],"['our experiments, we use naive bayes as the learning algorithm.', 'the knowledge sources we use include parts - of - speech, local collocations, and surrounding words.', 'these knowledge sources were effectively used to build a state - of - the - art wsd program in one of our prior work  #TAUTHOR_TAG']",2
"['( see  #AUTHOR_TAG ),  #TAUTHOR_TAG speculate that deeper linguistic']","['( see  #AUTHOR_TAG ),  #TAUTHOR_TAG speculate that deeper linguistic']","['( see  #AUTHOR_TAG ),  #TAUTHOR_TAG speculate that deeper linguistic knowledge needs to']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG ).', 'while these approaches have been reasonably successful ( see  #AUTHOR_TAG ),  #TAUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g.,  #AUTHOR_TAG ), their semantic similarity as computed using wordnet ( e. g.,  #AUTHOR_TAG ) or wikipedia  #AUTHOR_TAG, and the contextual role played by an np ( see  #AUTHOR_TAG )']",0
"['. g.,  #TAUTHOR_TAG ; and for those that do report per - forma']","['on perfect mentions ( e. g.,  #TAUTHOR_TAG ; and for those that do report per - formance on automatically']","['. g.,  #TAUTHOR_TAG ; and for those that do report per - formance on automatically']","['ace participants have also adopted a corpus - based approach to sc deter - mination that is investigated as part of the mention detection ( md ) task ( e. g.,  #AUTHOR_TAG ).', 'briefly, the goal of md is to identify the boundary of a mention, its mention type ( e. g., pronoun, name ), and its semantic type ( e. g., person, location ).', 'un - like them, ( 1 ) we do not perform the full md task, as our goal is to investigate the role of sc knowl - edge in coreference resolution ; and ( 2 ) we do not use the ace training data for acquiring our sc clas - sifier ; instead, we use the bbn entity type corpus  #AUTHOR_TAG, which consists of all the penn treebank wall street journal articles with the ace mentions manually identified and an - notated with their scs.', 'this provides us with a train - ing set that is approximately five times bigger than that of ace.', 'more importantly, the ace participants do not evaluate the role of induced sc knowledge in coreference resolution : many of them evaluate coreference performance on perfect mentions ( e. g.,  #TAUTHOR_TAG ; and for those that do report per - formance on automatically extracted mentions, they do not explain whether or how the induced sc information is used in their coreference algorithms']",1
"['.,  #AUTHOR_TAG,  #TAUTHOR_TAG']","[',  #AUTHOR_TAG,  #TAUTHOR_TAG']","['.,  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'while these approaches have been reasonably']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g.,  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'while these approaches have been reasonably successful ( see  #AUTHOR_TAG ),  #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g.,  #AUTHOR_TAG ), their semantic similarity as computed using wordnet ( e. g.,  #AUTHOR_TAG ) or wikipedia  #AUTHOR_TAG, and the contextual role played by an np ( see  #AUTHOR_TAG )']",0
"['type corpus  #TAUTHOR_TAG,']","['type corpus  #TAUTHOR_TAG,']","['acquiring our sc classifier ; instead, we use the bbn entity type corpus  #TAUTHOR_TAG,']","['ace participants have also adopted a corpus - based approach to sc determination that is investigated as part of the mention detection ( md ) task ( e. g.,  #AUTHOR_TAG ).', 'briefly, the goal of md is to identify the boundary of a mention, its mention type ( e. g., pronoun, name ), and its semantic type ( e. g., person, location ).', 'unlike them, ( 1 ) we do not perform the full md task, as our goal is to investigate the role of sc knowledge in coreference resolution ; and ( 2 ) we do not use the ace training data for acquiring our sc classifier ; instead, we use the bbn entity type corpus  #TAUTHOR_TAG, which consists of all the penn treebank wall street journal articles with the ace mentions manually identified and annotated with their scs.', 'this provides us with a training set that is approximately five times bigger than that of ace.', 'more importantly, the ace participants do not evaluate the role of induced sc knowledge in coreference resolution : many of them evaluate coreference performance on perfect mentions ( e. g.,  #AUTHOR_TAG ) ; and for those that do report per - formance on automatically extracted mentions, they do not explain whether or how the induced sc information is used in their coreference algorithms']",5
"['( see  #TAUTHOR_TAG,  #AUTHOR_TAG speculate that deeper linguistic']","['( see  #TAUTHOR_TAG,  #AUTHOR_TAG speculate that deeper linguistic']","['( see  #TAUTHOR_TAG,  #AUTHOR_TAG speculate that deeper linguistic knowledge needs to']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG ).', 'while these approaches have been reasonably successful ( see  #TAUTHOR_TAG,  #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g.,  #AUTHOR_TAG ), their semantic similarity as computed using wordnet ( e. g.,  #AUTHOR_TAG ) or wikipedia  #AUTHOR_TAG, and the contextual role played by an np ( see  #AUTHOR_TAG )']",0
['package  #TAUTHOR_TAG ) on these'],['( using the libsvm package  #TAUTHOR_TAG ) on these'],['( using the libsvm package  #TAUTHOR_TAG ) on these'],"['addition, we train an svm classifier for sc determination by combining the output of five clas - sification methods : dl, 1 - nn, me, nb, and soon et al. s method as described in the introduction, 8 with the goal of examining whether sc classifica - tion accuracy can be improved by combining the output of individual classifiers in a supervised man - ner.', 'specifically, we ( 1 ) use 80 % of the instances generated from the bbn entity type corpus to train the four classifiers ; ( 2 ) apply the four classifiers and soon et al. s method to independently make predictions for the remaining 20 % of the instances ; and ( 3 ) train an svm classifier ( using the libsvm package  #TAUTHOR_TAG ) on these 20 % of the instances, where each instance, i, is represented by a per org gpe fac loc oth training test 19. 8 9. 6 11. 4 1. 6 1. 2 56. 3 19. 5 9. 0 9. 6 1. 8 1. 1 59. 0 set of 31 binary features.', 'more specifically, let l = i li ; li2 ; li3 ; li4 ; li5 be the set of predictions that we obtained for i in step ( 2 ).', 'to represent i, we generate one feature from each non - empty subset of li']",5
"[""4 ) ne : we use bbn's identifinder  #TAUTHOR_TAG, a muc - style ne""]","[""4 ) ne : we use bbn's identifinder  #TAUTHOR_TAG, a muc - style ne""]","[""4 ) ne : we use bbn's identifinder  #TAUTHOR_TAG, a muc - style ne recognizer to""]","[""4 ) ne : we use bbn's identifinder  #TAUTHOR_TAG, a muc - style ne recognizer to determine the ne type of npz."", 'if npi is determined to be a person or organization, we create an ne feature whose value is simply its muc ne type.', 'however, if npi is determined to be a location, we create a feature with value gpe ( because most of the muc loca - tion nes are ace gpe nes ).', 'otherwise, no ne feature will be created ( because we are not interested in the other muc ne types )']",5
"[', most frequent ) wordnet sense as its sc ( e. g.,  #AUTHOR_TAG,  #TAUTHOR_TAG.', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]","['( i. e., most frequent ) wordnet sense as its sc ( e. g.,  #AUTHOR_TAG,  #TAUTHOR_TAG.', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]","[', most frequent ) wordnet sense as its sc ( e. g.,  #AUTHOR_TAG,  #TAUTHOR_TAG.', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]","['type of semantic knowledge that has been employed by coreference resolvers is the semantic class ( sc ) of an np, which can be used to disallow coreference between semantically incompatible nps.', 'however, learning - based resolvers have not been able to benefit from having an sc agreement feature, presumably because the method used to compute the sc of an np is too simplistic : while the sc of a proper name is computed fairly accurately using a named entity ( ne ) recognizer, many resolvers simply assign to a common noun the first ( i. e., most frequent ) wordnet sense as its sc ( e. g.,  #AUTHOR_TAG,  #TAUTHOR_TAG.', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]",0
"['dl ) : we use the dl learner as described in  #TAUTHOR_TAG, motivated by its']","['dl ) : we use the dl learner as described in  #TAUTHOR_TAG, motivated by its']","['dl ) : we use the dl learner as described in  #TAUTHOR_TAG, motivated by its']","['algorithms.', 'we experiment with four learners commonly employed in language learning : decision list ( dl ) : we use the dl learner as described in  #TAUTHOR_TAG, motivated by its success in the related tasks of word sense disambiguation  #AUTHOR_TAG and ne classification  #AUTHOR_TAG.', 'we apply add - one smoothing to smooth the class posteriors']",5
"[' #TAUTHOR_TAG, and ( 2 )']","[' #TAUTHOR_TAG, and ( 2 )']","['on the 97 test texts.', 'we report performance in terms of two metrics : ( 1 ) the fmeasure score as computed by the commonly - used muc scorer  #TAUTHOR_TAG, and ( 2 )']","['in sc induction, we use the ace phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.', 'we report performance in terms of two metrics : ( 1 ) the fmeasure score as computed by the commonly - used muc scorer  #TAUTHOR_TAG, and ( 2 ) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', ' #AUTHOR_TAG, we consider an anaphoric reference, np i, correctly resolved if np i and its closest antecedent are in the same coreference chain in the resulting partition.', 'in all of our experiments, we use nps automatically extracted by an in - house np chunker and identifinder']",5
"['', 'following  #TAUTHOR_TAG, we select as']","['np in a test text.', 'following  #TAUTHOR_TAG, we select as']","['each np in a test text.', 'following  #TAUTHOR_TAG, we select as']","['training, the decision tree classifier is used to select an antecedent for each np in a test text.', 'following  #TAUTHOR_TAG, we select as the antecedent of each np, npj, the closest preceding np that is classified as coreferent with npj.', 'if no such np exists, no antecedent is selected for npj']",4
"['. g.,  #TAUTHOR_TAG.', 'briefly, the goal of md is to']","['( md ) task ( e. g.,  #TAUTHOR_TAG.', 'briefly, the goal of md is to']","['. g.,  #TAUTHOR_TAG.', 'briefly, the goal of md is to']","['ace participants have also adopted a corpus - based approach to sc determination that is investigated as part of the mention detection ( md ) task ( e. g.,  #TAUTHOR_TAG.', 'briefly, the goal of md is to identify the boundary of a mention, its mention type ( e. g., pronoun, name ), and its semantic type ( e. g., person, location ).', 'unlike them, ( 1 ) we do not perform the full md task, as our goal is to investigate the role of sc knowledge in coreference resolution ; and ( 2 ) we do not use the ace training data for acquiring our sc classifier ; instead, we use the bbn entity type corpus  #AUTHOR_TAG, which consists of all the penn treebank wall street journal articles with the ace mentions manually identified and annotated with their scs.', 'this provides us with a training set that is approximately five times bigger than that of ace.', 'more importantly, the ace participants do not evaluate the role of induced sc knowledge in coreference resolution : many of them evaluate coreference performance on perfect mentions ( e. g.,  #AUTHOR_TAG ) ; and for those that do report performance on automatically extracted mentions, they do not explain whether or how the induced sc information is used in their coreference algorithms']",1
"[' #AUTHOR_TAG and  #TAUTHOR_TAG, as described below']","[' #AUTHOR_TAG and  #TAUTHOR_TAG, as described below']","[' #AUTHOR_TAG and  #TAUTHOR_TAG, as described below']","['baseline coreference system uses the c4. 5 deci - sion tree learner  #AUTHOR_TAG to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g.,  #AUTHOR_TAG and  #AUTHOR_TAG ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi +, npi + 2,, npj positional features that have been employed by high - performing resolvers such as  #AUTHOR_TAG and  #TAUTHOR_TAG, as described below']",1
['learner  #TAUTHOR_TAG'],['learner  #TAUTHOR_TAG'],['baseline coreference system uses the c4. 5 decision tree learner  #TAUTHOR_TAG'],"['baseline coreference system uses the c4. 5 decision tree learner  #TAUTHOR_TAG to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g.,  #AUTHOR_TAG and  #AUTHOR_TAG ), we generate training instances as follows : a positive instance is created for each anaphoric np, np j, and its closest antecedent, np i ; and a negative instance is created for np j paired with each of the intervening nps, np i + 1, np i + 2,..', '., np j−1.', 'each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as  #AUTHOR_TAG and  #AUTHOR_TAG, as described below']",5
"[' #TAUTHOR_TAG and  #AUTHOR_TAG, as described below']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, as described below']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, as described below']","['baseline coreference system uses the c4. 5 decision tree learner  #AUTHOR_TAG to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g.,  #AUTHOR_TAG and  #AUTHOR_TAG ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi +, npi + 2,, npj.', 'each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high - performing resolvers such as  #TAUTHOR_TAG and  #AUTHOR_TAG, as described below']",1
"['.', 'following  #TAUTHOR_TAG, we consider an anaphoric reference, npi,']","['anaphoric references correctly resolved.', 'following  #TAUTHOR_TAG, we consider an anaphoric reference, npi,']","['.', 'following  #TAUTHOR_TAG, we consider an anaphoric reference, npi,']","['in sc induction, we use the ace phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.', 'we report performance in terms of two metrics : ( 1 ) the fmeasure score as computed by the commonly - used muc scorer  #AUTHOR_TAG, and ( 2 ) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', 'following  #TAUTHOR_TAG, we consider an anaphoric reference, npi, correctly resolved if npi and its closest antecedent are in the same coreference chain in the resulting partition.', 'in all of our experiments, we use nps automatically extracted by an in - house np chunker and identifinder']",5
"['( e. g.,  #AUTHOR_TAG ) or wikipedia  #AUTHOR_TAG, and the contextual role played by an np ( see  #TAUTHOR_TAG']","['( e. g.,  #AUTHOR_TAG ) or wikipedia  #AUTHOR_TAG, and the contextual role played by an np ( see  #TAUTHOR_TAG']","['( e. g.,  #AUTHOR_TAG ) or wikipedia  #AUTHOR_TAG, and the contextual role played by an np ( see  #TAUTHOR_TAG']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG ).', 'while these approaches have been reasonably successful ( see  #AUTHOR_TAG ),  #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g.,  #AUTHOR_TAG ), their semantic similarity as computed using wordnet ( e. g.,  #AUTHOR_TAG ) or wikipedia  #AUTHOR_TAG, and the contextual role played by an np ( see  #TAUTHOR_TAG']",0
"['. g.,  #TAUTHOR_TAG']","['( i. e., most frequent ) wordnet sense as its sc ( e. g.,  #TAUTHOR_TAG']","['. g.,  #TAUTHOR_TAG']","['type of semantic knowledge that has been employed by coreference resolvers is the semantic class ( sc ) of an np, which can be used to disallow coreference between semantically incompatible nps.', 'however, learning - based resolvers have not been able to benefit from having an sc agreement feature, presumably because the method used to compute the sc of an np is too simplistic : while the sc of a proper name is computed fairly accurately using a named entity ( ne ) recognizer, many resolvers simply assign to a common noun the first ( i. e., most frequent ) wordnet sense as its sc ( e. g.,  #TAUTHOR_TAG,  #AUTHOR_TAG ).', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]",0
"['. g.,  #TAUTHOR_TAG and  #AUTHOR_TAG ), we generate training instances as follows : a positive']","['( e. g.,  #TAUTHOR_TAG and  #AUTHOR_TAG ), we generate training instances as follows : a positive']","['. g.,  #TAUTHOR_TAG and  #AUTHOR_TAG ), we generate training instances as follows : a positive instance is created for each anaphoric np,']","['baseline coreference system uses the c4. 5 decision tree learner  #AUTHOR_TAG to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g.,  #TAUTHOR_TAG and  #AUTHOR_TAG ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi + 1, npi + 2,..., npj _ 1.', '., np j−1.', 'each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as  #AUTHOR_TAG and  #AUTHOR_TAG, as described below']",5
"[' #TAUTHOR_TAG and ne classification  #AUTHOR_TAG.', 'we apply add - one smoothing to smooth the class posteriors']","[' #TAUTHOR_TAG and ne classification  #AUTHOR_TAG.', 'we apply add - one smoothing to smooth the class posteriors']","['in the related tasks of word sense disambiguation  #TAUTHOR_TAG and ne classification  #AUTHOR_TAG.', 'we apply add - one smoothing to smooth the class posteriors']","['algorithms.', 'we experiment with four learners commonly employed in language learning : decision list ( dl ) : we use the dl learner as described in  #AUTHOR_TAG, motivated by its success in the related tasks of word sense disambiguation  #TAUTHOR_TAG and ne classification  #AUTHOR_TAG.', 'we apply add - one smoothing to smooth the class posteriors']",4
"['( e. g.,  #AUTHOR_TAG ) or wikipedia  #TAUTHOR_TAG, and the contextual role played by an np ( see  #AUTHOR_TAG )']","['( e. g.,  #AUTHOR_TAG ) or wikipedia  #TAUTHOR_TAG, and the contextual role played by an np ( see  #AUTHOR_TAG )']","['( e. g.,  #AUTHOR_TAG ) or wikipedia  #TAUTHOR_TAG, and the contextual role played by an np ( see  #AUTHOR_TAG )']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG ).', 'while these approaches have been reasonably successful ( see  #AUTHOR_TAG ),  #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g.,  #AUTHOR_TAG ), their semantic similarity as computed using wordnet ( e. g.,  #AUTHOR_TAG ) or wikipedia  #TAUTHOR_TAG, and the contextual role played by an np ( see  #AUTHOR_TAG )']",0
"['. g.,  #TAUTHOR_TAG, their semantic']","['between two nps ( e. g.,  #TAUTHOR_TAG, their semantic']","['. g.,  #TAUTHOR_TAG, their semantic similarity as computed using']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG ).', 'while these approaches have been reasonably successful ( see  #AUTHOR_TAG ),  #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g.,  #TAUTHOR_TAG, their semantic similarity as computed using wordnet ( e. g.,  #AUTHOR_TAG ) or wikipedia  #AUTHOR_TAG, and the contextual role played by an np ( see  #AUTHOR_TAG )']",0
"['distributionally similar nps ( see  #TAUTHOR_TAG a ) ).', 'motivated by this']","['distributionally similar nps ( see  #TAUTHOR_TAG a ) ).', 'motivated by this observation, we create']","['distributionally similar nps ( see  #TAUTHOR_TAG a ) ).', 'motivated by this observation, we create']","['', 'we then infer the sc of a common noun as follows : ( 1 ) we compute the probability that the common noun co - occurs with each of the eight ne types 6 based on the extracted appositive relations, and ( 2 ) if the most likely ne type has a co - occurrence probability above a certain threshold ( we set it to 0. 7 ), we create a induced class fea - ture for np i whose value is the most likely ne type.', '( 7 ) neighbor : research in lexical semantics suggests that the sc of an np can be inferred from its distributionally similar nps ( see  #TAUTHOR_TAG a ) ).', ""motivated by this observation, we create for each of np i's ten most semantically similar nps a neigh - bor feature whose value is the surface string of the np."", ""to determine the ten nearest neighbors, we use the semantic similarity values provided by lin's dependency - based thesaurus, which is constructed using a distributional approach combined with an information - theoretic definition of similarity""]",4
"['. g.,  #TAUTHOR_TAG.', 'given a large, unannotated corpus 3, we use identi -']","['by research in lexical semantics ( e. g.,  #TAUTHOR_TAG.', 'given a large, unannotated corpus 3, we use identi - finder']","['. g.,  #TAUTHOR_TAG.', 'given a large, unannotated corpus 3, we use identi -']","['3 ) verb obj : a verb obj feature is created in a similar fashion as subj verb if np i participates in a verb - object relation.', 'again, this represents our attempt to coarsely model subcategorization.', ""( 4 ) ne : we use bbn's identifinder  #AUTHOR_TAG ( 5 ) wn class : for each keyword w shown in the right column of table 1, we determine whether the head noun of np i is a hyponym of w in wordnet, using only the first wordnet sense of np i. 1 if so, we create a wn class feature with w as its value."", 'these keywords are potentially useful features because some of them are subclasses of the ace scs shown in the left column of table 1, while others appear to be correlated with these ace scs. 2 ( 6 ) induced class : since the first - sense heuristic used in the previous feature may not be accurate in capturing the sc of an np, we employ a corpusbased method for inducing scs that is motivated by research in lexical semantics ( e. g.,  #TAUTHOR_TAG.', 'given a large, unannotated corpus 3, we use identi - finder to label each ne with its ne type and mini - par to extract all the appositive relations.', '']",4
"['and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our']","['predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our']","['and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', ' #AUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', ' #AUTHOR_TAG and  #AUTHOR_TAG study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues  #AUTHOR_TAG and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in  #AUTHOR_TAG, must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
"['and annotated with anvil using a subset of the attributes defined in the mumin annotation scheme  #TAUTHOR_TAG.', 'the mu - min scheme is a general']","['and annotated with anvil using a subset of the attributes defined in the mumin annotation scheme  #TAUTHOR_TAG.', 'the mu - min scheme is a general']","['communicative head gestures in the videos were found and annotated with anvil using a subset of the attributes defined in the mumin annotation scheme  #TAUTHOR_TAG.', 'the mu - min scheme is a general framework for the study of gestures in interpersonal communication.', 'in this study, we do not deal with functional classification of the gestures in themselves, but rather with how gestures']","['communicative head gestures in the videos were found and annotated with anvil using a subset of the attributes defined in the mumin annotation scheme  #TAUTHOR_TAG.', 'the mu - min scheme is a general framework for the study of gestures in interpersonal communication.', 'in this study, we do not deal with functional classification of the gestures in themselves, but rather with how gestures contribute to the semantic interpretations of linguistic expressions.', 'therefore, only a subset of the mumin attributes has been used, i. e.', 'smile, laughter, scowl, faceother for facial expressions, and nod, jerk, tilt, sideturn, shake, waggle, other for head movements']",5
"['in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #TAUTHOR_TAG examine the']","['in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #TAUTHOR_TAG examine the']","['of map - task dialogues, also targeted in this paper.', ' #AUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #TAUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', ' #AUTHOR_TAG and  #AUTHOR_TAG study the relation between eye']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', ' #AUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #TAUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', ' #AUTHOR_TAG and  #AUTHOR_TAG study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues  #AUTHOR_TAG and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication  #AUTHOR_TAG.', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in  #AUTHOR_TAG, must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
"['and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our']","['predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our']","['and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', ' #AUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', ' #AUTHOR_TAG and  #AUTHOR_TAG study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues  #AUTHOR_TAG and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in  #AUTHOR_TAG, must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see  #TAUTHOR_TAG for an'],['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see  #TAUTHOR_TAG for an'],"['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see  #TAUTHOR_TAG for an overview ).', '']","['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see  #TAUTHOR_TAG for an overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example,  #AUTHOR_TAG and  #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements, while  #AUTHOR_TAG show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den  #AUTHOR_TAG and  #AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']",0
"[' #TAUTHOR_TAG 2.', 'an']","[' #TAUTHOR_TAG 2.', '']","[' #AUTHOR_TAG 1 and corrected kappa  #TAUTHOR_TAG 2.', 'an']","['general, dialogue act, agreement and turn anno - tations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'however, one dialogue was coded independently and in parallel by two expert annotators to measure inter - coder agreement.', 'a measure was derived for each annotated feature using the agreement analysis facility provided in anvil.', ""agreement between two annotation sets is calculated here in terms of cohen's kappa  #AUTHOR_TAG 1 and corrected kappa  #TAUTHOR_TAG 2."", 'anvil divides the annotations in slices and compares each slice.', 'we used slices of 0. 04 seconds.', 'the inter - coder agreement figures obtained for the three types of annotation are given in table 2']",5
"[', pauses and dialogue structure in annotated english map - task dialogues  #TAUTHOR_TAG and find correlations between']","[', pauses and dialogue structure in annotated english map - task dialogues  #TAUTHOR_TAG and find correlations between']","['of map - task dialogues, also targeted in this paper.', ' #AUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', ' #AUTHOR_TAG and  #AUTHOR_TAG study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues  #TAUTHOR_TAG and find correlations between']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', ' #AUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', ' #AUTHOR_TAG and  #AUTHOR_TAG study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues  #TAUTHOR_TAG and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication  #AUTHOR_TAG.', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in  #AUTHOR_TAG, must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
"['system  #TAUTHOR_TAG.', 'we experimented with various weka classifiers, comprising hidden naive']","['system  #TAUTHOR_TAG.', 'we experimented with various weka classifiers, comprising hidden naive bayes, smo, id3, ladtree and decision table.', 'the best results on most of our data were']","['automatic dialogue act classification, which was run in the weka system  #TAUTHOR_TAG.', 'we experimented with various weka classifiers, comprising hidden naive bayes, smo, id3, ladtree and decision table.', 'the best results on most of our data were obtained using hidden naive bayes ( hnb )  #AUTHOR_TAG.', 'therefore, here we show the results of this classifier.', 'ten - folds crossvalidation was applied throughout']","['multimodal data we obtained by combining the linguistic annotations from danpass with the gesture annotation created in anvil, resulted into two different groups of data, one containing all yes and no expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in table 4.', 'these two sets of data were used for automatic dialogue act classification, which was run in the weka system  #TAUTHOR_TAG.', 'we experimented with various weka classifiers, comprising hidden naive bayes, smo, id3, ladtree and decision table.', 'the best results on most of our data were obtained using hidden naive bayes ( hnb )  #AUTHOR_TAG.', 'therefore, here we show the results of this classifier.', 'ten - folds crossvalidation was applied throughout']",5
"['of map - task dialogues, also targeted in this paper.', ' #TAUTHOR_TAG obtain promising']","['of map - task dialogues, also targeted in this paper.', ' #TAUTHOR_TAG obtain promising']","['of map - task dialogues, also targeted in this paper.', ' #TAUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', ' #AUTHOR_TAG and  #AUTHOR_TAG study the relation between eye']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', ' #TAUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', ' #AUTHOR_TAG and  #AUTHOR_TAG study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues  #AUTHOR_TAG and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication  #AUTHOR_TAG.', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in  #AUTHOR_TAG, must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
[' #TAUTHOR_TAG 1 and corrected'],[' #TAUTHOR_TAG 1 and corrected'],[' #TAUTHOR_TAG 1 and corrected'],"['general, dialogue act, agreement and turn annotations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'however, one dialogue was coded independently and in parallel by two expert annotators to measure inter - coder agreement.', 'a measure was derived for each annotated feature using the agreement analysis facility provided in anvil.', ""agreement between two annotation sets is calculated here in terms of cohen's kappa  #TAUTHOR_TAG 1 and corrected kappa  #AUTHOR_TAG 2."", 'anvil divides the annotations in slices and compares each slice.', 'we used slices of 0. 04 seconds.', 'the inter - coder agreement figures obtained for the three types of annotation are given in table 2']",5
"['and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our']","['predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our']","['and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', ' #AUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', ' #AUTHOR_TAG and  #AUTHOR_TAG study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues  #AUTHOR_TAG and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in  #AUTHOR_TAG, must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
"['the resulting figures, see  #TAUTHOR_TAG, it is usually']","['the resulting figures, see  #TAUTHOR_TAG, it is usually']","['the resulting figures, see  #TAUTHOR_TAG, it is usually']","['should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'for example, a yes can be an answer to a question, an agree and a turnelicit at the same time, thus making the semantic classification very fine - grained.', ""table 1 although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see  #TAUTHOR_TAG, it is usually assumed that cohen's kappa figures over 60 are good while those over are excellent  #AUTHOR_TAG."", 'looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by anvil from the latest annotated element']",0
"['to recognise some of the functions of head movements, while  #AUTHOR_TAG show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den  #AUTHOR_TAG and  #TAUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']","['to recognise some of the functions of head movements, while  #AUTHOR_TAG show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den  #AUTHOR_TAG and  #TAUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']","['have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example,  #AUTHOR_TAG and  #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements, while  #AUTHOR_TAG show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den  #AUTHOR_TAG and  #TAUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']","['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see mc  #AUTHOR_TAG for an overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example,  #AUTHOR_TAG and  #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements, while  #AUTHOR_TAG show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den  #AUTHOR_TAG and  #TAUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']",0
"[',  #TAUTHOR_TAG and  #AUTHOR_TAG find that machine learning algorithms can be']","[',  #TAUTHOR_TAG and  #AUTHOR_TAG find that machine learning algorithms can be']","[',  #TAUTHOR_TAG and  #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements, while  #AUTHOR_TAG show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den  #AUTHOR_TAG and  #AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']","['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see mc  #AUTHOR_TAG for an overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example,  #TAUTHOR_TAG and  #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements, while  #AUTHOR_TAG show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den  #AUTHOR_TAG and  #AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']",0
"[',  #AUTHOR_TAG and  #TAUTHOR_TAG find that machine learning algorithms can be']","[',  #AUTHOR_TAG and  #TAUTHOR_TAG find that machine learning algorithms can be']","[',  #AUTHOR_TAG and  #TAUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements, while  #AUTHOR_TAG show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den  #AUTHOR_TAG and  #AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multi - modal corpus']","['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see mc  #AUTHOR_TAG for an overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example,  #AUTHOR_TAG and  #TAUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements, while  #AUTHOR_TAG show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den  #AUTHOR_TAG and  #AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multi - modal corpus']",0
"['figures over 60 are good while those over are excellent  #TAUTHOR_TAG.', 'looking at the cases of disagreement we could see that many of these']","['figures over 60 are good while those over are excellent  #TAUTHOR_TAG.', 'looking at the cases of disagreement we could see that many of these']","['figures over 60 are good while those over are excellent  #TAUTHOR_TAG.', 'looking at the cases of disagreement we could see that many of these']","['should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'for example, a yes can be an answer to a question, an agree and a turnelicit at the same time, thus making the semantic classification very fine - grained.', ""table 1 although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see  #AUTHOR_TAG, it is usually assumed that cohen's kappa figures over 60 are good while those over are excellent  #TAUTHOR_TAG."", 'looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by anvil from the latest annotated element']",0
"['in previous studies using the same annotation scheme  #TAUTHOR_TAG, but are still sat - isfactory given the high number of categories provided by the scheme']","['in previous studies using the same annotation scheme  #TAUTHOR_TAG, but are still sat - isfactory given the high number of categories provided by the scheme']","['in previous studies using the same annotation scheme  #TAUTHOR_TAG, but are still sat - isfactory given the high number of categories provided by the scheme']","['head gestures in the danpass data have been coded by non expert annotators ( one annotator per video ) and subsequently controlled by a second annotator, with the exception of one video which was annotated independently and in parallel by two annotators.', 'the annotations of this video were then used to measure inter - coder agreement in anvil as it was the case for the annotations on feedback expressions.', 'in the case of gestures we also measured agreement on gesture segmentation.', 'the figures obtained are given in table 3.', 'these results are slightly worse than those obtained in previous studies using the same annotation scheme  #TAUTHOR_TAG, but are still sat - isfactory given the high number of categories provided by the scheme']",1
"['obtained on a smaller dataset in  #TAUTHOR_TAG,']","['obtained on a smaller dataset in  #TAUTHOR_TAG,']","['obtained on a smaller dataset in  #TAUTHOR_TAG, must be seen in']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', ' #AUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', ' #AUTHOR_TAG and  #AUTHOR_TAG study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues  #AUTHOR_TAG and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication  #AUTHOR_TAG.', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in  #TAUTHOR_TAG, must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",1
"['using hidden naive bayes ( hnb )  #TAUTHOR_TAG.', 'therefore, here we show the results of this classifier.', 'ten - folds crossvalidation was applied throughout']","['using hidden naive bayes ( hnb )  #TAUTHOR_TAG.', 'therefore, here we show the results of this classifier.', 'ten - folds crossvalidation was applied throughout']","['automatic dialogue act classification, which was run in the weka system  #AUTHOR_TAG.', 'we experimented with various weka classifiers, comprising hidden naive bayes, smo, id3, ladtree and decision table.', 'the best results on most of our data were obtained using hidden naive bayes ( hnb )  #TAUTHOR_TAG.', 'therefore, here we show the results of this classifier.', 'ten - folds crossvalidation was applied throughout']","['multimodal data we obtained by combining the linguistic annotations from danpass with the gesture annotation created in anvil, resulted into two different groups of data, one containing all yes and no expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in table 4.', 'these two sets of data were used for automatic dialogue act classification, which was run in the weka system  #AUTHOR_TAG.', 'we experimented with various weka classifiers, comprising hidden naive bayes, smo, id3, ladtree and decision table.', 'the best results on most of our data were obtained using hidden naive bayes ( hnb )  #TAUTHOR_TAG.', 'therefore, here we show the results of this classifier.', 'ten - folds crossvalidation was applied throughout']",5
"['##l  #TAUTHOR_TAG.', 'to distinguish among the various functions']","['out using anvil  #TAUTHOR_TAG.', 'to distinguish among the various functions']","['them are marked with onset or offset hesitation, or both.', 'for this study, we added semantic labels - including dialogue acts - and gesture annotation.', 'both kinds of annotation were carried out using anvil  #TAUTHOR_TAG.', 'to distinguish among the various functions']","['already mentioned, all words in danpass are phonetically and prosodically annotated.', 'in the subset of the corpus considered here, 82 % of the feedback expressions bear stress or tone information, and 12 % are unstressed ; 7 % of them are marked with onset or offset hesitation, or both.', 'for this study, we added semantic labels - including dialogue acts - and gesture annotation.', 'both kinds of annotation were carried out using anvil  #TAUTHOR_TAG.', 'to distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging iso 24617 - 2 standard for semantic annotation of language resources.', 'this subset comprises the categories accept, decline, repeatrephrase and answer.', 'moreover, all feedback expressions were annotated with an agreement feature ( agree, nonagree ) where relevant.', 'finally, the two turn management categories turn - take and turnelicit were also coded']",5
"['expert annotator, for more information see grnnum ( 2006 ).', 'the praat tool was used  #TAUTHOR_TAG']","['expert annotator, for more information see grnnum ( 2006 ).', 'the praat tool was used  #TAUTHOR_TAG']","['upon version was produced with the supervision of an expert annotator, for more information see grnnum ( 2006 ).', 'the praat tool was used  #TAUTHOR_TAG']","['and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see grnnum ( 2006 ).', 'the praat tool was used  #TAUTHOR_TAG']",5
"[', based on  #TAUTHOR_TAG b ), outputs']","[', based on  #TAUTHOR_TAG b ), outputs']","[', based on  #TAUTHOR_TAG b ), outputs']","['', ""the diagnoser, based on  #TAUTHOR_TAG b ), outputs a diagnosis which consists of lists of correct, contradictory and non - mentioned objects and relations from the student's answer."", 'at present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to  #AUTHOR_TAG']",2
"['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
['surge  #TAUTHOR_TAG generation'],"[', and a fuf / surge  #TAUTHOR_TAG generation']","[', and a fuf / surge  #TAUTHOR_TAG generation system']","[""strategy decision made by the tutorial planner, together with relevant semantic content from the student's answer ( e. g., part of the answer to confirm ), is passed to content planning and generation."", 'the system uses a domain - specific content planner to produce input to the surface realizer based on the strategy decision, and a fuf / surge  #TAUTHOR_TAG generation system to produce the appropriate text.', 'templates are used to generate some stock phrases such as "" when you are ready, go on to the next slide.']",5
"['with higher learning gain  #TAUTHOR_TAG.', 'using a deep generator to automatically generate system feedback gives us a']","['with higher learning gain  #TAUTHOR_TAG.', 'using a deep generator to automatically generate system feedback gives us a']","['with higher learning gain  #TAUTHOR_TAG.', 'using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail']","['dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.', ""the analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example, using better terminology ) but doesn't explicitly explain the reason why different terminology is needed  #AUTHOR_TAG."", 'results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain  #TAUTHOR_TAG.', 'using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail']",3
"['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['dialogue manager which uses the informationstate approach  #TAUTHOR_TAG.', 'the dialogue state is represented by']","['dialogue manager which uses the informationstate approach  #TAUTHOR_TAG.', 'the dialogue state is represented by']","['between components is coordinated by the dialogue manager which uses the informationstate approach  #TAUTHOR_TAG.', 'the dialogue state is represented by']","['between components is coordinated by the dialogue manager which uses the informationstate approach  #TAUTHOR_TAG.', 'the dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not - yet - mentioned parts of the answer.', 'once the complete answer has been accumulated, the system accepts it and moves on.', 'tutor hints can contribute parts of the answer to the cumulative state as well, allowing the system to jointly construct the solution with the student']",5
"['##ing  #TAUTHOR_TAG.', 'however,']","['in humanhuman tutoring  #TAUTHOR_TAG.', 'however,']","['in developing tutorial dialogue systems that understand student explanations  #AUTHOR_TAG van  #AUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #TAUTHOR_TAG.', 'however,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations  #AUTHOR_TAG van  #AUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #TAUTHOR_TAG.', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['different terminology is needed  #TAUTHOR_TAG.', 'results from other systems show that measures']","['different terminology is needed  #TAUTHOR_TAG.', 'results from other systems show that measures']","['different terminology is needed  #TAUTHOR_TAG.', 'results from other systems show that measures']","['dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.', ""the analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example, using better terminology ) but doesn't explicitly explain the reason why different terminology is needed  #TAUTHOR_TAG."", 'results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain  #AUTHOR_TAG.', 'using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail']",3
"['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['##ing  #TAUTHOR_TAG.', 'however,']","['in humanhuman tutoring  #TAUTHOR_TAG.', 'however,']","['in developing tutorial dialogue systems that understand student explanations  #AUTHOR_TAG van  #AUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #TAUTHOR_TAG.', 'however,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations  #AUTHOR_TAG van  #AUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #TAUTHOR_TAG.', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['an error recovery policy  #TAUTHOR_TAG.', 'since']","['an error recovery policy  #TAUTHOR_TAG.', 'since']","[', the tutorial planner implements an error recovery policy  #TAUTHOR_TAG.', 'since']","['tutorial policy makes a high - level decision as to which strategy to use ( for example, "" acknowledge the correct part and give a high specificity hint "" ) based on the answer analysis and dialogue context.', 'at present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 in addition to a remediation policy, the tutorial planner implements an error recovery policy  #TAUTHOR_TAG.', 'since the system accepts unrestricted input, interpretation errors are unavoidable.', 'our recovery policy is modeled on the targetedhelp  #AUTHOR_TAG policy used in task - oriented dialogue.', '']",5
"['similar to  #TAUTHOR_TAG, and an']","['similar to  #TAUTHOR_TAG, and an']","['', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', 'the contextual interpreter then uses a reference resolution approach similar to  #TAUTHOR_TAG, and an ontology mapping mechanism  #AUTHOR_TAG a )']","['use the trips dialogue parser  #AUTHOR_TAG to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to  #TAUTHOR_TAG, and an ontology mapping mechanism  #AUTHOR_TAG a ) to produce a domain - specific semantic representation of the student's output."", '']",1
"['to overcome these limitations  #TAUTHOR_TAG.', '']","['to overcome these limitations  #TAUTHOR_TAG.', '']","['beetle ii system architecture is designed to overcome these limitations  #TAUTHOR_TAG.', 'it uses a deep parser']","['beetle ii system architecture is designed to overcome these limitations  #TAUTHOR_TAG.', 'it uses a deep parser and generator, together with a domain reasoner and a diagnoser, to produce detailed analyses of student utterances and generate feedback automatically.', 'this allows the system to consistently apply the same tutorial policy across a range of questions.', 'to some extent, this comes at the expense of being able to address individual student misconceptions.', ""however, the system's modular setup and extensibility make it a suitable testbed for both computational linguistics algorithms and more general questions about theories of learning""]",0
"['category, though in the future we may consider a classifier similar to  #TAUTHOR_TAG']","['category, though in the future we may consider a classifier similar to  #TAUTHOR_TAG']","['relations into the appropriate category, though in the future we may consider a classifier similar to  #TAUTHOR_TAG']","['', ""the diagnoser, based on  #AUTHOR_TAG b ), outputs a diagnosis which consists of lists of correct, contradictory and non - mentioned objects and relations from the student's answer."", 'at present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to  #TAUTHOR_TAG']",3
"['##ing  #TAUTHOR_TAG.', 'however,']","['in humanhuman tutoring  #TAUTHOR_TAG.', 'however,']","['in developing tutorial dialogue systems that understand student explanations  #AUTHOR_TAG van  #AUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #TAUTHOR_TAG.', 'however,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations  #AUTHOR_TAG van  #AUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #TAUTHOR_TAG.', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
[' #TAUTHOR_TAG a )'],[' #TAUTHOR_TAG a )'],"[' #AUTHOR_TAG, and an ontology mapping mechanism  #TAUTHOR_TAG a )']","['use the trips dialogue parser  #AUTHOR_TAG to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to  #AUTHOR_TAG, and an ontology mapping mechanism  #TAUTHOR_TAG a ) to produce a domain - specific semantic representation of the student's output."", '']",5
['factors such as student confidence could be considered as well  #TAUTHOR_TAG'],['factors such as student confidence could be considered as well  #TAUTHOR_TAG'],['factors such as student confidence could be considered as well  #TAUTHOR_TAG'],['factors such as student confidence could be considered as well  #TAUTHOR_TAG'],3
"['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
['is modeled on the targetedhelp  #TAUTHOR_TAG'],['is modeled on the targetedhelp  #TAUTHOR_TAG'],"['.', 'our recovery policy is modeled on the targetedhelp  #TAUTHOR_TAG policy used in task - oriented dialogue.', 'if']","['tutorial policy makes a high - level decision as to which strategy to use ( for example, "" acknowledge the correct part and give a high specificity hint "" ) based on the answer analysis and dialogue context.', 'at present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 in addition to a remediation policy, the tutorial planner implements an error recovery policy.', 'since the system accepts unrestricted input, interpretation errors are unavoidable.', 'our recovery policy is modeled on the targetedhelp  #TAUTHOR_TAG policy used in task - oriented dialogue.', '']",2
['use the trips dialogue parser  #TAUTHOR_TAG to'],['use the trips dialogue parser  #TAUTHOR_TAG to'],['use the trips dialogue parser  #TAUTHOR_TAG to'],"['use the trips dialogue parser  #TAUTHOR_TAG to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to  #AUTHOR_TAG, and an ontology mapping mechanism  #AUTHOR_TAG a ) to produce a domain - specific semantic representation of the student's output."", '']",5
['implemented in the km representation language  #TAUTHOR_TAG to represent'],['implemented in the km representation language  #TAUTHOR_TAG to represent'],"['implemented in the km representation language  #TAUTHOR_TAG to represent the state of the world.', 'at present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits']","['system uses a knowledge base implemented in the km representation language  #TAUTHOR_TAG to represent the state of the world.', 'at present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits']",5
"['applying logical inference  #AUTHOR_TAG, or adopting transformation - based techniques  #TAUTHOR_TAG, incorporate different types of lexical knowledge to support textual inference']","['applying logical inference  #AUTHOR_TAG, or adopting transformation - based techniques  #TAUTHOR_TAG, incorporate different types of lexical knowledge to support textual inference']","['applying logical inference  #AUTHOR_TAG, or adopting transformation - based techniques  #TAUTHOR_TAG, incorporate different types of lexical knowledge to support textual inference.', 'such information ranges']","['current approaches to monolingual te, either syntactically oriented  #AUTHOR_TAG, or applying logical inference  #AUTHOR_TAG, or adopting transformation - based techniques  #TAUTHOR_TAG, incorporate different types of lexical knowledge to support textual inference.', '']",0
"['in another language  #TAUTHOR_TAG.', 'there are several methods to build phrase tables.', 'the one adopted in this']","['in another language  #TAUTHOR_TAG.', 'there are several methods to build phrase tables.', 'the one adopted in this']","['in another language  #TAUTHOR_TAG.', 'there are several methods to build phrase tables.', 'the one adopted in this work consists in learning phrase alignments from a word - aligned bilingual corpus.', 'in']","['tables ( pht ) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'they are widely used in mt as a way to figure out how to translate input in one language into output in another language  #TAUTHOR_TAG.', 'there are several methods to build phrase tables.', 'the one adopted in this work consists in learning phrase alignments from a word - aligned bilingual corpus.', 'in order to build english - spanish phrase tables for our experiments, we used the freely available europarl v. 4, news commentary and united nations spanish - english parallel corpora released for the wmt10 1.', 'we run treetagger  #AUTHOR_TAG for tokenization, and used the giza + +  #AUTHOR_TAG to align the tokenized corpora at the word level.', 'subsequently, we extracted the bilingual phrase table from the aligned corpora using the moses toolkit  #AUTHOR_TAG.', 'since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'in addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0. 01, 0. 05, 0. 1, 0. 2, 0. 3, 0. 4 and 0. 5.', 'the resulting 1 http : / / www. statmt. org / wmt10 /', 'phrase tables range from 76 to 48 million entries, with an average of 3. 9 words per phrase']",0
"[' #TAUTHOR_TAG, verboce']","[', dirt  #TAUTHOR_TAG, verbocean  #AUTHOR_TAG, framenet  #AUTHOR_TAG, and wikipedia  #AUTHOR_TAG.', 'dirt is a']","[', dirt  #TAUTHOR_TAG, verboce']","[', dirt  #TAUTHOR_TAG, verbocean  #AUTHOR_TAG, framenet  #AUTHOR_TAG, and wikipedia  #AUTHOR_TAG.', 'dirt is a collection of sta - tistically learned inference rules, that is often inte - grated as a source of lexical paraphrases and entail - ment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are fre - quently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similar - ity / relatedness scores']",0
"['applying logical inference ( tatu and  #TAUTHOR_TAG, or adopting transformation - based techniques  #AUTHOR_TAG bar -  #AUTHOR_TAG, incorporate different types of lexical knowledge to']","['applying logical inference ( tatu and  #TAUTHOR_TAG, or adopting transformation - based techniques  #AUTHOR_TAG bar -  #AUTHOR_TAG, incorporate different types of lexical knowledge to']","['applying logical inference ( tatu and  #TAUTHOR_TAG, or adopting transformation - based techniques  #AUTHOR_TAG bar -  #AUTHOR_TAG, incorporate different types of lexical knowledge to']","['current approaches to monolingual te, either syntactically oriented  #AUTHOR_TAG, or applying logical inference ( tatu and  #TAUTHOR_TAG, or adopting transformation - based techniques  #AUTHOR_TAG bar -  #AUTHOR_TAG, incorporate different types of lexical knowledge to support textual inference.', '']",0
"['by  #TAUTHOR_TAG.', 'the method relies on translation -']","['by  #TAUTHOR_TAG.', 'the method relies on translation - validation cycles, defined as separate jobs']","['by  #TAUTHOR_TAG.', 'the method relies on translation -']","['dataset used for our experiments is an english - spanish entailment corpus obtained from the original rte3 dataset by translating the english hypothesis into spanish.', 'it consists of 1600 pairs derived from the rte3 development and test sets ( 800 + 800 ).', 'translations have been generated by the crowdflower3 channel to amazon mechanical turk4 ( mturk ), adopting the methodology proposed by  #TAUTHOR_TAG.', ""the method relies on translation - validation cycles, defined as separate jobs routed to mturk's workforce."", 'translation jobs return one spanish version for each hypothesis.', 'validation jobs ask multiple workers to check the correctness of each translation using the original english sentence as reference.', 'at each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the clte corpus, while wrong translations are sent back to workers in a new translation job.', 'although the quality of the results is enhanced by the possibility to automatically weed out untrusted workers using gold units, we performed a manual quality check on a subset of the acquired clte corpus.', 'the validation, carried out by a spanish native speaker on 100 randomly selected pairs after two translation - validation cycles, showed the good quality of the collected material, with only 3 minor "" errors "" consisting in controversial but substantially acceptable translations reflecting regional spanish variations']",5
['( clte ) has been proposed by  #TAUTHOR_TAG as an extension of textual'],['( clte ) has been proposed by  #TAUTHOR_TAG as an extension of textual'],['( clte ) has been proposed by  #TAUTHOR_TAG as an extension of'],"['- lingual textual entailment ( clte ) has been proposed by  #TAUTHOR_TAG as an extension of textual entailment  #AUTHOR_TAG that consists in deciding, given two texts t and h in different languages, if the meaning of h can be inferred from the meaning of t.', 'the task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'for instance, the reliance of cur - rent monolingual te systems on lexical resources ( e. g. wordnet, verbocean, framenet ) and deep processing components ( e. g. syntactic and semantic parsers, co - reference resolution tools, temporal ex - pressions recognizers and normalizers ) has to con - front, at the cross - lingual level, with the limited availability of lexical / semantic resources covering multiple languages, the limited coverage of the ex - isting ones, and the burden of integrating language - specific components into the same cross - lingual ar - chitecture']",0
"[' #TAUTHOR_TAG as a term of comparison, we experiment with different sources of multilingual lexical knowledge to address']","[' #TAUTHOR_TAG as a term of comparison, we experiment with different sources of multilingual lexical knowledge to address']","[' #TAUTHOR_TAG as a term of comparison, we experiment with different sources of multilingual lexical knowledge to address']","['paper investigates the idea, still unexplored, of a tighter integration of mt and te algorithms and techniques.', 'our aim is to embed cross - lingual processing techniques inside the te recognition process in order to avoid any dependency on external mt components, and eventually gain full control of the systems behaviour.', 'along this direction, we start from the acquisition and use of lexical knowl - edge, which represents the basic building block of any te system.', 'using the basic solution proposed by  #TAUTHOR_TAG as a term of comparison, we experiment with different sources of multilingual lexical knowledge to address the following questions']",1
"[', and used the giza + +  #TAUTHOR_TAG']","['tokenization, and used the giza + +  #TAUTHOR_TAG']","[', and used the giza + +  #TAUTHOR_TAG']","['tables ( pht ) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'they are widely used in mt as a way to figure out how to translate input in one language into output in another language  #AUTHOR_TAG.', 'there are several methods to build phrase tables.', 'the one adopted in this work consists in learning phrase alignments from a word - aligned bilingual corpus.', 'in order to build english - spanish phrase tables for our experiments, we used the freely available europarl v. 4, news commentary and united nations spanish - english parallel corpora released for the wmt10 1.', 'we run treetagger  #AUTHOR_TAG for tokenization, and used the giza + +  #TAUTHOR_TAG to align the tokenized corpora at the word level.', 'subsequently, we extracted the bilingual phrase table from the aligned corpora using the moses toolkit  #AUTHOR_TAG.', 'since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'in addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0. 01, 0. 05, 0. 1, 0. 2, 0. 3, 0. 4 and 0. 5.', 'the resulting 1 http : / / www. statmt. org / wmt10 /', 'phrase tables range from 76 to 48 million entries, with an average of 3. 9 words per phrase']",5
"[' #TAUTHOR_TAG, automatic']","[' #TAUTHOR_TAG, automatic']","[' #TAUTHOR_TAG, automatic evaluation']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation  #AUTHOR_TAG, multidocument summarization  #TAUTHOR_TAG, automatic evaluation of mt  #AUTHOR_TAG, and te  #AUTHOR_TAG.', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison -  #AUTHOR_TAG.', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques  #AUTHOR_TAG can be applied to increase the precision of the extracted paraphrases']",4
"['.', 'some previous works  #TAUTHOR_TAG indicate, as main limitations of the mentioned resources,']","['others.', 'some previous works  #TAUTHOR_TAG indicate, as main limitations of the mentioned resources,']","['.', 'some previous works  #TAUTHOR_TAG indicate, as main limitations of the mentioned resources,']","['the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'as emerges from the ablation tests reported in  #AUTHOR_TAG, even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'some previous works  #TAUTHOR_TAG indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words']",0
"['relative weights, we trained a support vector machine classifier, svmlight  #TAUTHOR_TAG, using each score as a feature']","['relative weights, we trained a support vector machine classifier, svmlight  #TAUTHOR_TAG, using each score as a feature']","['relative weights, we trained a support vector machine classifier, svmlight  #TAUTHOR_TAG, using each score as a feature']","['combine the phrasal matching scores obtained at each n - gram level, and optimize their relative weights, we trained a support vector machine classifier, svmlight  #TAUTHOR_TAG, using each score as a feature']",5
"['proved to be useful in a number of nlp applications such as natural language generation  #TAUTHOR_TAG, multi']","['proved to be useful in a number of nlp applications such as natural language generation  #TAUTHOR_TAG, multidocument']","['tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation  #TAUTHOR_TAG, multi']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation  #TAUTHOR_TAG, multidocument summarization ( mc  #AUTHOR_TAG, automatic evaluation of mt  #AUTHOR_TAG, and te  #AUTHOR_TAG.', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison -  #AUTHOR_TAG.', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques  #AUTHOR_TAG can be applied to increase the precision of the extracted paraphrases']",4
"[' #TAUTHOR_TAG, framenet  #AUTHOR_TAG, and wikipedia  #AUTHOR_TAG.', 'dirt is a']","[' #AUTHOR_TAG, verbocean  #TAUTHOR_TAG, framenet  #AUTHOR_TAG, and wikipedia  #AUTHOR_TAG.', 'dirt is a']","[' #AUTHOR_TAG, verbocean  #TAUTHOR_TAG, framenet  #AUTHOR_TAG, and wikipedia  #AUTHOR_TAG.', 'dirt is a collection of sta - tistically']","['wordnet, the rte literature documents the use of a variety of lexical information sources  #AUTHOR_TAG.', 'these include, just to mention the most popular ones, dirt  #AUTHOR_TAG, verbocean  #TAUTHOR_TAG, framenet  #AUTHOR_TAG, and wikipedia  #AUTHOR_TAG.', 'dirt is a collection of sta - tistically learned inference rules, that is often inte - grated as a source of lexical paraphrases and entail - ment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similar - ity / relatedness scores']",0
"['##net  #TAUTHOR_TAG ) have been created for several languages, with different degrees of coverage']","['multiwordnet  #TAUTHOR_TAG ) have been created for several languages, with different degrees of coverage']","['. multiwordnet  #TAUTHOR_TAG ) have been created for several languages, with different degrees of coverage.', 'as an example, the 57, 424 synsets of the spanish section of multi']","['', 'as regards the first issue, its worth noting that in the monolingual scenario simple bag of words ( or bag of n - grams ) approaches are per se sufficient to achieve results above baseline.', 'in contrast, their application in the cross - lingual setting is not a viable solution due to the impossibility to perform direct lex - ical matches between texts and hypotheses in different languages.', 'this situation makes the availability of multilingual lexical knowledge a necessary condition to bridge the language gap.', 'however, with the only exceptions represented by wordnet and wikipedia, most of the aforementioned resources are available only for english.', 'multilingual lexical databases aligned with the english wordnet ( e. g. multiwordnet  #TAUTHOR_TAG ) have been created for several languages, with different degrees of coverage.', 'as an example, the 57, 424 synsets of the spanish section of multiwordnet aligned to english cover just around 50 % of the wordnets synsets, thus making the coverage issue even more problematic than for te.', 'as regards wikipedia, the cross - lingual links between pages in different languages offer a possibility to extract lexical knowledge useful for clte.', 'however, due to their relatively small number ( especially for some languages ), bilingual lexicons extracted from wikipedia are still inadequate to provide acceptable coverage.', 'in addition, featuring a bias towards named entities, the information acquired through cross - lingual links can at most complement the lexical knowledge extracted from more generic multilingual resources ( e. g bilingual dictionaries )']",0
"['mt  #AUTHOR_TAG, and te  #TAUTHOR_TAG.', 'one of the proposed methods to']","['mt  #AUTHOR_TAG, and te  #TAUTHOR_TAG.', 'one of the proposed methods to']","['mt  #AUTHOR_TAG, and te  #TAUTHOR_TAG.', 'one of the proposed methods to']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation  #AUTHOR_TAG, multidocument summarization ( mc  #AUTHOR_TAG, automatic evaluation of mt  #AUTHOR_TAG, and te  #TAUTHOR_TAG.', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison -  #AUTHOR_TAG.', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques  #AUTHOR_TAG can be applied to increase the precision of the extracted paraphrases']",4
"['using phrase alignments in a bilingual parallel corpus  #TAUTHOR_TAG.', 'with this method, all the different phrases in one language that are aligned with the']","['using phrase alignments in a bilingual parallel corpus  #TAUTHOR_TAG.', 'with this method, all the different phrases in one language that are aligned with the']","['on a pivot - based approach using phrase alignments in a bilingual parallel corpus  #TAUTHOR_TAG.', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation  #AUTHOR_TAG, multidocument summarization ( mc  #AUTHOR_TAG, automatic evaluation of mt  #AUTHOR_TAG, and te  #AUTHOR_TAG.', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus  #TAUTHOR_TAG.', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques  #AUTHOR_TAG can be applied to increase the precision of the extracted paraphrases']",0
"[' #AUTHOR_TAG, framenet  #AUTHOR_TAG, and wikipedia  #TAUTHOR_TAG.', 'dirt is a']","[' #AUTHOR_TAG, verbocean  #AUTHOR_TAG, framenet  #AUTHOR_TAG, and wikipedia  #TAUTHOR_TAG.', 'dirt is a']","[' #AUTHOR_TAG, framenet  #AUTHOR_TAG, and wikipedia  #TAUTHOR_TAG.', 'dirt is a collection of statistically']","['wordnet, the rte literature documents the use of a variety of lexical information sources  #AUTHOR_TAG.', 'these include, just to mention the most popular ones, dirt  #AUTHOR_TAG, verbocean  #AUTHOR_TAG, framenet  #AUTHOR_TAG, and wikipedia  #TAUTHOR_TAG.', '']",0
"[' #TAUTHOR_TAG that consists in deciding, given two texts t and h in different languages, if the']","[' #TAUTHOR_TAG that consists in deciding, given two texts t and h in different languages, if the']","[' #TAUTHOR_TAG that consists in deciding, given two texts t and h in different languages, if the meaning']","['- lingual textual entailment ( clte ) has been proposed by  #AUTHOR_TAG as an extension of textual entailment  #TAUTHOR_TAG that consists in deciding, given two texts t and h in different languages, if the meaning of h can be inferred from the meaning of t.', 'the task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'for instance, the reliance of cur - rent monolingual te systems on lexical resources ( e. g. wordnet, verbocean, framenet ) and deep processing components ( e. g. syntactic and semantic parsers, co - reference resolution tools, temporal expressions recognizers and normalizers ) has to confront, at the cross - lingual level, with the limited availability of lexical / semantic resources covering multiple languages, the limited coverage of the existing ones, and the burden of integrating language - specific components into the same cross - lingual architecture']",0
"[' #AUTHOR_TAG, framenet  #TAUTHOR_TAG, and wikipedia  #AUTHOR_TAG.', 'dirt is a']","[' #AUTHOR_TAG, verbocean  #AUTHOR_TAG, framenet  #TAUTHOR_TAG, and wikipedia  #AUTHOR_TAG.', 'dirt is a']","[' #AUTHOR_TAG, framenet  #TAUTHOR_TAG, and wikipedia  #AUTHOR_TAG.', 'dirt is a collection of statistically']","['wordnet, the rte literature documents the use of a variety of lexical information sources  #AUTHOR_TAG.', 'these include, just to mention the most popular ones, dirt  #AUTHOR_TAG, verbocean  #AUTHOR_TAG, framenet  #TAUTHOR_TAG, and wikipedia  #AUTHOR_TAG.', '']",0
"[', pruning techniques  #TAUTHOR_TAG can be applied to increase the']","[', pruning techniques  #TAUTHOR_TAG can be applied to increase the']","['as paraphrases.', 'after the extraction, pruning techniques  #TAUTHOR_TAG can be applied to increase the precision of the extracted paraphrases']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation  #AUTHOR_TAG, multidocument summarization ( mc  #AUTHOR_TAG, automatic evaluation of mt  #AUTHOR_TAG, and te  #AUTHOR_TAG.', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison -  #AUTHOR_TAG.', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques  #TAUTHOR_TAG can be applied to increase the precision of the extracted paraphrases']",0
"['mt  #TAUTHOR_TAG, and te  #AUTHOR_TAG.', 'one of the proposed methods to']","['mt  #TAUTHOR_TAG, and te  #AUTHOR_TAG.', 'one of the proposed methods to']","['mt  #TAUTHOR_TAG, and te  #AUTHOR_TAG.', 'one of the proposed methods to']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation  #AUTHOR_TAG, multidocument summarization ( mc  #AUTHOR_TAG, automatic evaluation of mt  #TAUTHOR_TAG, and te  #AUTHOR_TAG.', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison -  #AUTHOR_TAG.', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques  #AUTHOR_TAG can be applied to increase the precision of the extracted paraphrases']",4
"['.', 'some previous works  #TAUTHOR_TAG indicate, as main limitations of the mentioned resources,']","['others.', 'some previous works  #TAUTHOR_TAG indicate, as main limitations of the mentioned resources,']","['.', 'some previous works  #TAUTHOR_TAG indicate, as main limitations of the mentioned resources,']","['the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'as emerges from the ablation tests reported in  #AUTHOR_TAG, even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'some previous works  #TAUTHOR_TAG indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words']",0
"['by  #TAUTHOR_TAG.', 'although']","['by  #TAUTHOR_TAG.', 'although']","['by  #TAUTHOR_TAG.', 'although it was presented as an approach to clte, the proposed method brings the problem back to']","[""the sake of completeness, we report in this section also the results obtained adopting the ` ` basic solution'' proposed by  #TAUTHOR_TAG."", 'although it was presented as an approach to clte, the proposed method brings the problem back to the monolingual case by translating h into the language of t. the comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive mt system ( google translate ) in the same scenario']",1
['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],"['by  #TAUTHOR_TAG.', 'another interesting direction']","['future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for te and clte.', 'on one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'one possible direction is to consider linguistically motivated approaches, such as the extraction of syntactic phrase tables as proposed by  #TAUTHOR_TAG.', '']",3
"[') over wikipedia using the jlsi tool  #TAUTHOR_TAG to measure the relatedness between words in the dataset.', 'then, we filtered']","['( lsa ) over wikipedia using the jlsi tool  #TAUTHOR_TAG to measure the relatedness between words in the dataset.', 'then, we filtered']","[') over wikipedia using the jlsi tool  #TAUTHOR_TAG to measure the relatedness between words in the dataset.', 'then, we filtered all the pairs with similarity lower than']","['( wiki ).', 'we performed latent semantic analysis ( lsa ) over wikipedia using the jlsi tool  #TAUTHOR_TAG to measure the relatedness between words in the dataset.', 'then, we filtered all the pairs with similarity lower than 0. 7 as proposed by  #AUTHOR_TAG.', 'in this way we obtained 13760 word pairs']",5
"['of  #TAUTHOR_TAG, the']","['of  #TAUTHOR_TAG, the']","['of  #TAUTHOR_TAG, the results']",[' #TAUTHOR_TAG'],1
"['aligned corpora using the moses toolkit  #TAUTHOR_TAG.', 'since']","['aligned corpora using the moses toolkit  #TAUTHOR_TAG.', 'since']","['aligned corpora using the moses toolkit  #TAUTHOR_TAG.', 'since']","['tables ( pht ) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'they are widely used in mt as a way to figure out how to translate input in one language into output in another language  #AUTHOR_TAG.', 'there are several methods to build phrase tables.', 'the one adopted in this work consists in learning phrase alignments from a word - aligned bilingual corpus.', 'in order to build english - spanish phrase tables for our experiments, we used the freely available europarl v. 4, news commentary and united nations spanish - english parallel corpora released for the wmt10 1.', 'we run treetagger  #AUTHOR_TAG for tokenization, and used the giza + +  #AUTHOR_TAG to align the tokenized corpora at the word level.', 'subsequently, we extracted the bilingual phrase table from the aligned corpora using the moses toolkit  #TAUTHOR_TAG.', 'since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'in addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0. 01, 0. 05, 0. 1, 0. 2, 0. 3, 0. 4 and 0. 5.', 'the resulting 1 http : / / www. statmt. org / wmt10 /', 'phrase tables range from 76 to 48 million entries, with an average of 3. 9 words per phrase']",5
"['lexical information sources  #TAUTHOR_TAG.', 'these include, just to mention the most popular']","['lexical information sources  #TAUTHOR_TAG.', 'these include, just to mention the most popular ones,']","['lexical information sources  #TAUTHOR_TAG.', 'these include, just to mention the most']","['wordnet, the rte literature documents the use of a variety of lexical information sources  #TAUTHOR_TAG.', 'these include, just to mention the most popular ones, dirt  #AUTHOR_TAG, verbocean  #AUTHOR_TAG, framenet  #AUTHOR_TAG, and  #AUTHOR_TAG.', '']",0
"['by designing appropriate priming experiments  #TAUTHOR_TAG or other lexical decision tasks.', 'the reaction time of the subjects']","['by designing appropriate priming experiments  #TAUTHOR_TAG or other lexical decision tasks.', 'the reaction time of the subjects']","['by designing appropriate priming experiments  #TAUTHOR_TAG or other lexical decision tasks.', 'the reaction time of the subjects']","['questions are typically answered by designing appropriate priming experiments  #TAUTHOR_TAG or other lexical decision tasks.', 'the reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain.', '( see sec. 2 for models of morphological organization and access and related experiments )']",0
['inter annotator agreement using the fleiss kappa  #TAUTHOR_TAG measure ( x ) where the agreement lies'],['inter annotator agreement using the fleiss kappa  #TAUTHOR_TAG measure ( x ) where the agreement lies'],"['of them and rest 500 are overlapping.', 'we measure the inter annotator agreement using the fleiss kappa  #TAUTHOR_TAG measure ( x ) where the agreement lies']","['', 'for example, "" [UNK] [UNK] "" ( getting up ) is a compound verb where individual words do not entirely reflects the meaning of the whole expression.', 'however, not all v1 + v2 combinations are cvs.', 'for example, expressions like, "" [UNK] [UNK] "" ( take and then go ) and "" [UNK] [UNK] [UNK] "" ( return back ) are the examples of verb sequences where meaning of the whole expression can be derived from the meaning of the individual component and thus, these verb sequences are not considered as cv.', 'the key question linguists are trying to identify for a long time and debating a lot is whether to consider cvs as a single lexical units or consider them as two separate units.', 'since linguistic rules fails to explain the process, we for the first time tried to perform cognitive experiments to understand the organization and processing of such verb sequences in the human mind.', 'a clear understanding about these phenomena may help us to classify or extract actual cvs from other verb sequences.', 'in order to do so, presently we have applied three different techniques to collect user data.', 'in the first technique, we annotated 4500 v1 + v2 sequences, along with their example sentences, using a group of three linguists ( the expert subjects ).', 'we asked the experts to classify the verb sequences into three classes namely, cv, not a cv and not sure.', 'each linguist has received 2000 verb pairs along with their respective example sentences.', 'out of this, 1500 verb sequences are unique to each of them and rest 500 are overlapping.', 'we measure the inter annotator agreement using the fleiss kappa  #TAUTHOR_TAG measure ( x ) where the agreement lies around 0. 79.', 'next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 v1 + v2 pairs and presented them to 36 native bangla speakers.', 'we ask each subjects to give a compositionality score of each verb sequences under 1 - 10 point scale, 10 being highly compositional and 1 for noncompositional.', 'we found an agreement of κ = 0. 69 among the subjects.', 'we also observe a continuum of compositionality score among the verb sequences.', 'this reflects that it is difficult to classify bangla verb sequences discretely into the classes of cv and not']",5
['proposed by  #TAUTHOR_TAG that points out'],['proposed by  #TAUTHOR_TAG that points out'],['by  #TAUTHOR_TAG that points out'],"['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', ' #AUTHOR_TAG considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', ' #AUTHOR_TAG argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', ' #AUTHOR_TAG tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by  #TAUTHOR_TAG that points out v1 and v2 are paired on the basis of their semantic compatibility, which is subject to syntactic constraints.', ' #AUTHOR_TAG tried to represent bangla cvs in terms of hpsg formalism.', 'she proposes that the selection of a v2 by a v1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'since none of the linguistic formalism could satisfactorily explain the unique phenomena of cv formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb - verb combinations in the ml and compare these responses with that of the existing models']",0
"['as discussed in  #TAUTHOR_TAG for english polymorphemic words. however,', 'rather than derived words, the']","['as discussed in  #TAUTHOR_TAG for english polymorphemic words. however,', 'rather than derived words, the']","['experimental procedure as discussed in  #TAUTHOR_TAG for english polymorphemic words. however,', 'rather than derived words, the subjects were shown a verb sequence and asked whether']","['the agreement lies around 0. 79. next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 v1 + v2 pairs and presented them to 36', 'native bangla speakers. we ask each subjects to give a compositionality score of', 'each verb sequences under 1 - 10 point scale, 10 being highly compositional and 1 for noncompositional. we found an agreement of κ =', '0. 69 among the subjects. we also observe a continuum of compositional', ""##ity score among the verb sequences. this reflects that it is difficult to classify bangla verb sequences discretely into the classes of cv and not a cv. we then, compare the compositionality score with that of the expert user's annotation. we found a significant correlation between the expert annotation and the"", 'compositionality score. we observe verb sequences that are annotated as cvs ( like, [UNK] [UNK], [UNK] [UNK], [UNK] [UNK] ) have got low compositionality score ( average score ranges between', '1 - 4 ) on the other hand high compositional values are in general tagged as not a cv ( [UNK] [UNK] ( come and get ), [UNK] [UNK] ( return back )', ', [UNK] [UNK] [UNK] ( kept ), [UNK] [UNK] ( roll on floor ) ). this reflects that verb', 'sequences which are not cv shows high degree of compositionality. in other words non cv verbs', 'can directly interpret from their constituent verbs. this leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be', 'greater than the non - compositional verbs which maps to a single expression of meaning. in order to validate such claim we', 'perform a lexical decision experiment using native bangla speakers with 92 different verb sequences', '. we followed the same experimental procedure as discussed in  #TAUTHOR_TAG for english polymorphemic words. however,', 'rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination. the reaction time', '( rt ) of each subject is recorded. our preliminarily observation from the rt analysis shows that as per our claim, rt of verb sequences having high compositionality value is significantly higher than the', 'rts for low or noncompositional verbs. this proves our hypothesis that bangla compound verbs that show less compositionality are stored as a hole in the mental lexicon', 'and thus follows the full - listing model whereas compositional verb phrases are individually', 'parsed. however, we do believe that our experiment is composed of', 'a very small set of data and it is premature to conclude anything concrete based only on the current experimental results']",5
"['argument structure.', ' #TAUTHOR_TAG tried to construct a semantic']","['argument structure.', ' #TAUTHOR_TAG tried to construct a semantic']","['the argument structure.', ' #TAUTHOR_TAG tried to construct a semantic analysis']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', ' #AUTHOR_TAG considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', ' #AUTHOR_TAG argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', ' #TAUTHOR_TAG tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by  #AUTHOR_TAG that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', ' #AUTHOR_TAG tried to represent bangla cvs in terms of hpsg formalism.', 'she proposes that the selection of a v2 by a v1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'since none of the linguistic formalism could satisfactorily explain the unique phenomena of cv formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb - verb combinations in the ml and compare these responses with that of the existing models']",0
[' #AUTHOR_TAG taft 1975 ;  #TAUTHOR_TAG where it'],[' #AUTHOR_TAG taft 1975 ;  #TAUTHOR_TAG where it'],[' #AUTHOR_TAG taft 1975 ;  #TAUTHOR_TAG where'],"['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', ' #AUTHOR_TAG with his experiment on english inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'similar observation for surface word frequency was also observed by  #AUTHOR_TAG taft 1975 ;  #TAUTHOR_TAG where it has been claimed that words having low surface frequency tends to decompose.', ' #AUTHOR_TAG proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'if it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts']",0
"['mental lexicon  #TAUTHOR_TAG.', '']","['mental lexicon  #TAUTHOR_TAG.', '']","['a whole in the human mental lexicon  #TAUTHOR_TAG.', '']","['the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.', 'most of the studies are designed to support one of the two mutually exclusive paradigms : the full - listing and the morphemic model.', 'the full - listing model claims that polymorphic words are represented as a whole in the human mental lexicon  #TAUTHOR_TAG.', 'on the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.', 'the affixes are stripped away from the root form, which in turn are used to access the mental lexicon  #AUTHOR_TAG mac  #AUTHOR_TAG.', 'intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.', 'for instance, the derived morphological forms are believed to be represented as a whole, whereas the representation of the inflected forms follows the morphemic model  #AUTHOR_TAG']",0
"[', dutch, and few other languages  #TAUTHOR_TAG.', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than']","[', dutch, and few other languages  #TAUTHOR_TAG.', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than']","[', dutch, and few other languages  #TAUTHOR_TAG.', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of english, hebrew, italian, french, dutch, and few other languages  #TAUTHOR_TAG.', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than many of their indo - european cousins.', 'moreover, indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'on the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent  #AUTHOR_TAG.', 'therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages']",0
"['.', ' #TAUTHOR_TAG argues cv formations in hindi and urdu are']","['auxiliaries.', ' #TAUTHOR_TAG argues cv formations in hindi and urdu are']","['as an aspectual complex comparable to the auxiliaries.', ' #TAUTHOR_TAG argues cv formations in hindi and urdu are']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', ' #AUTHOR_TAG considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', ' #TAUTHOR_TAG argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', ' #AUTHOR_TAG tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by  #AUTHOR_TAG that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', ' #AUTHOR_TAG tried to represent bangla cvs in terms of hpsg formalism.', 'she proposes that the selection of a v2 by a v1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'since none of the linguistic formalism could satisfactorily explain the unique phenomena of cv formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb - verb combinations in the ml and compare these responses with that of the existing models']",0
"['the different priming and other lexical decision experiments, described in literature  #TAUTHOR_TAG ; bentin, s. and  #AUTHOR_TAG specifically for derivation']","['the different priming and other lexical decision experiments, described in literature  #TAUTHOR_TAG ; bentin, s. and  #AUTHOR_TAG specifically for derivationally suffixed polymorphemic words and compound verbs of bangla.', 'our cross - modal and masked priming experiment on bangla derivationally suffixed words shows that morphological relatedness between lexical items triggers a significant priming effect, even when']","['to this, we apply the different priming and other lexical decision experiments, described in literature  #TAUTHOR_TAG ; bentin, s. and  #AUTHOR_TAG specifically for derivationally suffixed polymorphemic words and compound verbs of bangla.', 'our cross - modal and masked priming experiment on bangla derivationally suffixed words shows that morphological relatedness between lexical items triggers a significant priming effect, even when the forms are phonologically / orthographically unrelated.', 'these observations are similar to']","['respect to this, we apply the different priming and other lexical decision experiments, described in literature  #TAUTHOR_TAG ; bentin, s. and  #AUTHOR_TAG specifically for derivationally suffixed polymorphemic words and compound verbs of bangla.', 'our cross - modal and masked priming experiment on bangla derivationally suffixed words shows that morphological relatedness between lexical items triggers a significant priming effect, even when the forms are phonologically / orthographically unrelated.', 'these observations are similar to those reported for english and indicate that derivationally suffixed words in bangla are in general accessed through decomposition of the word into its constituent morphemes.', 'further, based on the experimental data we have developed a series of computational models that can be used to predict the decomposition of bangla polymorphemic words.', 'our evaluation result shows that decomposition of a polymorphemic word depends on several factors like, frequency, productivity of the suffix and the compositionality between the stem and the suffix']",5
"['modal priming and masked priming experiment discussed in  #TAUTHOR_TAG for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were']","['modal priming and masked priming experiment discussed in  #TAUTHOR_TAG for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were']","['apply two different priming experiments namely, the cross modal priming and masked priming experiment discussed in  #TAUTHOR_TAG for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were asked to']","['apply two different priming experiments namely, the cross modal priming and masked priming experiment discussed in  #TAUTHOR_TAG for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'the same target word is again probed but with a different audio or visual probe called the control word.', 'the control shows no relationship with the target.', 'for example, bayaska ( aged ) and bayasa ( age ) is a prime - target pair, for which the corresponding control - target pair could be nayana ( eye ) and bayasa ( age )']",5
"[', dutch, and few other languages  #TAUTHOR_TAG ; grain']","[', dutch, and few other languages  #TAUTHOR_TAG ; grainger, et al.,']","[', dutch, and few other languages  #TAUTHOR_TAG ; grain']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of english, hebrew, italian, french, dutch, and few other languages  #TAUTHOR_TAG ; grainger, et al., 1991 ;  #AUTHOR_TAG.', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than many of their indo - european cousins.', 'moreover, indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'on the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent  #AUTHOR_TAG.', 'therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages']",0
"['modal priming and masked priming experiment discussed in  #TAUTHOR_TAG for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were']","['modal priming and masked priming experiment discussed in  #TAUTHOR_TAG for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were']","['apply two different priming experiments namely, the cross modal priming and masked priming experiment discussed in  #TAUTHOR_TAG for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were asked to']","['apply two different priming experiments namely, the cross modal priming and masked priming experiment discussed in  #TAUTHOR_TAG for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'the same target word is again probed but with a different audio or visual probe called the control word.', 'the control shows no relationship with the target.', 'for example, bayaska ( aged ) and bayasa ( age ) is a prime - target pair, for which the corresponding control - target pair could be nayana ( eye ) and bayasa ( age )']",5
"['##net  #TAUTHOR_TAG and raises the questions like, how to store morphologically complex words, in a lexical']","['like wordnet  #TAUTHOR_TAG and raises the questions like, how to store morphologically complex words, in a lexical']","['##net  #TAUTHOR_TAG and raises the questions like, how to store morphologically complex words, in a lexical resource like']","['clear understanding of the structure and the processing mechanism of the mental lexicon will further our knowledge of how the human brain processes language.', 'further, these linguistically important and interesting questions are also highly significant for computational linguistics ( cl ) and natural language processing ( nlp ) applications.', 'their computational significance arises from the issue of their storage in lexical resources like wordnet  #TAUTHOR_TAG and raises the questions like, how to store morphologically complex words, in a lexical resource like wordnet keeping in mind the storage and access efficiency']",0
"['processing of morphologically complex words are not quite language independent  #TAUTHOR_TAG.', 'therefore, the findings from experiments in one language cannot be generalized to all languages making it important to']","['processing of morphologically complex words are not quite language independent  #TAUTHOR_TAG.', 'therefore, the findings from experiments in one language cannot be generalized to all languages making it important to']","['processing of morphologically complex words are not quite language independent  #TAUTHOR_TAG.', 'therefore, the findings from experiments in one language cannot be generalized to all languages making it important to']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of english, hebrew, italian, french, dutch, and few other languages ( marslen -  #AUTHOR_TAG.', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than many of their indo - european cousins.', 'moreover, indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'on the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent  #TAUTHOR_TAG.', 'therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages']",0
['of the inflected forms follows the morphemic model  #TAUTHOR_TAG'],['of the inflected forms follows the morphemic model  #TAUTHOR_TAG'],"['argues that different types of morphological forms are processed separately.', 'for instance, the derived morphological forms are believed to be represented as a whole, whereas the representation of the inflected forms follows the morphemic model  #TAUTHOR_TAG']","['the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.', 'most of the studies are designed to support one of the two mutually exclusive paradigms : the full - listing and the morphemic model.', 'the full - listing model claims that polymorphic words are represented as a whole in the human mental lexicon  #AUTHOR_TAG.', 'on the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.', 'the affixes are stripped away from the root form, which in turn are used to access the mental lexicon  #AUTHOR_TAG mac  #AUTHOR_TAG.', 'intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.', 'for instance, the derived morphological forms are believed to be represented as a whole, whereas the representation of the inflected forms follows the morphemic model  #TAUTHOR_TAG']",0
"['', ' #TAUTHOR_TAG considers']","['to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', ' #TAUTHOR_TAG considers']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', ' #TAUTHOR_TAG considers']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', ' #TAUTHOR_TAG considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', ' #AUTHOR_TAG argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', ' #AUTHOR_TAG tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by  #AUTHOR_TAG that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', ' #AUTHOR_TAG tried to represent bangla cvs in terms of hpsg formalism.', 'she proposes that the selection of a v2 by a v1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'since none of the linguistic formalism could satisfactorily explain the unique phenomena of cv formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb - verb combinations in the ml and compare these responses with that of the existing models']",0
[' #TAUTHOR_TAG taft 1975 ;  #AUTHOR_TAG where it'],[' #TAUTHOR_TAG taft 1975 ;  #AUTHOR_TAG where it'],[' #TAUTHOR_TAG taft 1975 ;  #AUTHOR_TAG where'],"['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', ' #AUTHOR_TAG with his experiment on english inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'similar observation for surface word frequency was also observed by  #TAUTHOR_TAG taft 1975 ;  #AUTHOR_TAG where it has been claimed that words having low surface frequency tends to decompose.', ' #AUTHOR_TAG proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'if it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts']",0
"['., 1991 ;  #TAUTHOR_TAG.', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than']","[', dutch, and few other languages ( marslen -  #AUTHOR_TAG grainger, et al., 1991 ;  #TAUTHOR_TAG.', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than']","['., 1991 ;  #TAUTHOR_TAG.', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of english, hebrew, italian, french, dutch, and few other languages ( marslen -  #AUTHOR_TAG grainger, et al., 1991 ;  #TAUTHOR_TAG.', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than many of their indo - european cousins.', 'moreover, indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'on the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent  #AUTHOR_TAG.', 'therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages']",0
"['based  #TAUTHOR_TAG c ).', 'the majority of indirect associations can be']","['based  #TAUTHOR_TAG c ).', 'the majority of indirect associations can be']","[' #TAUTHOR_TAG c ).', 'the majority of indirect associations can be filtered out by a simple competition heuristic : whenever several word tokens ui in one half of the bitext co - occur with']","['##we could just as easily use other symmetric "" association "" measures, such as ¢2 or the dice coefficient  #AUTHOR_TAG.', 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within their language.', 'then vk and uk + l will also co - occur more often than expected by chance.', 'the arrow connecting vk and u ~ + l in figure 1 represents an indirect association, since the association between vk and uk + z arises only by virtue of the association between each of them and uk.', 'models of translational equivalence that are ignorant of indirect associations have "" a tendency... to be confused by collocates ""  #AUTHOR_TAG.', 'fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based  #TAUTHOR_TAG c ).', ""the majority of indirect associations can be filtered out by a simple competition heuristic : whenever several word tokens ui in one half of the bitext co - occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood l ( u, v ) of translational equivalence is highest."", 'the competitive linking algorithm implements this heuristic']",0
"[' #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad']","[' #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation  #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computationally expensive to']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation  #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'table look - up using an explicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( church & i -  #AUTHOR_TAG, certain machine - assisted translation tools ( e. g.', ' #AUTHOR_TAG b ) ), concordancing for bilingual lexicography  #AUTHOR_TAG computerassisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard &  #AUTHOR_TAG']",0
"[""'  #TAUTHOR_TAG."", 'fortunately, indirect associations are usually not difficult to identify,']","[""confused by collocates''  #TAUTHOR_TAG."", 'fortunately, indirect associations are usually not difficult to identify,']","[""'  #TAUTHOR_TAG."", 'fortunately, indirect associations are usually not difficult to identify,']","['##we could just as easily use other symmetric "" association "" measures, such as ¢2 or the dice coefficient  #AUTHOR_TAG.', 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within their language.', 'then vk and uk + l will also co - occur more often than expected by chance.', 'the arrow connecting vk and u ~ + l in figure 1 represents an indirect association, since the association between vk and uk + z arises only by virtue of the association between each of them and uk.', ""models of translational equivalence that are ignorant of indirect associations have ` ` a tendency... to be confused by collocates''  #TAUTHOR_TAG."", 'fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based  #AUTHOR_TAG c ).', ""the majority of indirect associations can be filtered out by a simple competition heuristic : whenever several word tokens ui in one half of the bitext co - occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood l ( u, v ) of translational equivalence is highest."", 'the competitive linking algorithm implements this heuristic']",0
"['. g.  #TAUTHOR_TAG b ) ), concordan']","['machine - assisted translation tools ( e. g.  #TAUTHOR_TAG b ) ), concordancing']","['. g.  #TAUTHOR_TAG b ) ), concordan']","['the past decade, researchers at ibm have devel - oped a series of increasingly sophisticated statistical models for machine translation  #AUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computation - ally expensive to apply.', 'table look - up using an ex - plicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( church & i -  #AUTHOR_TAG, certain machine - assisted translation tools ( e. g.  #TAUTHOR_TAG b ) ), concordancing for bilingual lexicography  #AUTHOR_TAG, computer - assisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard &  #AUTHOR_TAG']",0
"['that has only one correct translation.', 'this imbalance foils thresholding strategies, clever as they might be  #TAUTHOR_TAG.', 'the likelihoods in the']","['that has only one correct translation.', 'this imbalance foils thresholding strategies, clever as they might be  #TAUTHOR_TAG.', 'the likelihoods in the word - to - word model remain unnormalized, so they do not compete']","['that has only one correct translation.', 'this imbalance foils thresholding strategies, clever as they might be  #TAUTHOR_TAG.', 'the likelihoods in the word - to - word model remain unnormalized, so they do not compete']","['', 'this imbalance foils thresholding strategies, clever as they might be  #TAUTHOR_TAG.', 'the likelihoods in the word - to - word model remain unnormalized, so they do not compete']",0
"['bilingual lexicography  #TAUTHOR_TAG, computer - assisted language']","['bilingual lexicography  #TAUTHOR_TAG, computer - assisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information']","['bilingual lexicography  #TAUTHOR_TAG, computer - assisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval (']","['the past decade, researchers at ibm have devel - oped a series of increasingly sophisticated statistical models for machine translation  #AUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computation - ally expensive to apply.', 'table look - up using an ex - plicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( church & i -  #AUTHOR_TAG, certain machine - assisted translation tools ( e. g.  #AUTHOR_TAG b ) ), concordancing for bilingual lexicography  #TAUTHOR_TAG, computer - assisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard &  #AUTHOR_TAG']",0
"['. g.  #TAUTHOR_TAG b ) ), concordan']","['machine - assisted translation tools ( e. g.  #TAUTHOR_TAG b ) ), concordancing']","['. g.  #TAUTHOR_TAG b ) ), concordan']","['the past decade, researchers at ibm have devel - oped a series of increasingly sophisticated statistical models for machine translation  #AUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computation - ally expensive to apply.', 'table look - up using an ex - plicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( church & i -  #AUTHOR_TAG, certain machine - assisted translation tools ( e. g.  #TAUTHOR_TAG b ) ), concordancing for bilingual lexicography  #AUTHOR_TAG, computer - assisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard &  #AUTHOR_TAG']",0
"['02  #AUTHOR_TAG or the dice coefficient  #TAUTHOR_TAG.', 'co - occur is called a direct association.', 'now, suppose that uk and uk +']","['02  #AUTHOR_TAG or the dice coefficient  #TAUTHOR_TAG.', 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within']","['02  #AUTHOR_TAG or the dice coefficient  #TAUTHOR_TAG.', 'co - occur is called a direct association.', 'now, suppose that uk and uk +']","['##we could just as easily use other symmetric "" association "" measures, such as 02  #AUTHOR_TAG or the dice coefficient  #TAUTHOR_TAG.', 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within their language.', 'then vk and uk + l will also co - occur more often than expected by chance.', 'the arrow connecting vk and u ~ + l in figure 1 represents an indirect association, since the association between vk and uk + z arises only by virtue of the association between each of them and uk.', 'models of translational equivalence that are ignorant of indirect associations have "" a tendency... to be confused by collocates ""  #AUTHOR_TAG.', 'fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based  #AUTHOR_TAG c ).', ""the majority of indirect associations can be filtered out by a simple competition heuristic : whenever several word tokens ui in one half of the bitext co - occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood l ( u, v ) of translational equivalence is highest."", 'the competitive linking algorithm implements this heuristic']",1
"[' #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad']","[' #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation  #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computationally expensive to']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation  #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'table look - up using an explicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( church & i -  #AUTHOR_TAG, certain machine - assisted translation tools ( e. g.', ' #AUTHOR_TAG b ) ), concordancing for bilingual lexicography  #AUTHOR_TAG computerassisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard &  #AUTHOR_TAG']",0
"[' #TAUTHOR_TAG a ).', 'then, two word tokens (']","[' #TAUTHOR_TAG a ).', 'then, two word tokens ( u, v ) are said to co - occur in']","[' #TAUTHOR_TAG a ).', 'then, two word tokens (']","['- occurrence with the exception of  #AUTHOR_TAG b ), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  #AUTHOR_TAG a ;  #AUTHOR_TAG.', 'a bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'word co - occurrence can be defined in various ways.', 'the most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments si and ti are translations of each other  #TAUTHOR_TAG a ).', '']",0
"['- to - word model of translational equivalence from 13 million words of the canadian hansards, aligned using the method in  #TAUTHOR_TAG']","['induced a two - class word - to - word model of translational equivalence from 13 million words of the canadian hansards, aligned using the method in  #TAUTHOR_TAG']","['induced a two - class word - to - word model of translational equivalence from 13 million words of the canadian hansards, aligned using the method in  #TAUTHOR_TAG']","['induced a two - class word - to - word model of translational equivalence from 13 million words of the canadian hansards, aligned using the method in  #TAUTHOR_TAG']",5
"['negligible values  #TAUTHOR_TAG.', 'to retain']","['negligible values  #TAUTHOR_TAG.', 'to retain']","['negligible values  #TAUTHOR_TAG.', 'to retain word type pairs that are']","['. discard all likelihood scores for word types deemed unlikely to be mutual translations, i. e. all l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values  #TAUTHOR_TAG.', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly']",0
"['weather reports.', ' #TAUTHOR_TAG reported that the translation lexicon']","['weather reports.', ' #TAUTHOR_TAG reported that the translation lexicon']","['english weather reports.', ' #TAUTHOR_TAG reported that the translation lexicon']","['', ' #TAUTHOR_TAG']",0
"['##s  #TAUTHOR_TAG a ;  #AUTHOR_TAG.', 'a bite']","[' #AUTHOR_TAG b ), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  #TAUTHOR_TAG a ;  #AUTHOR_TAG.', 'a bitext comprises a pair of texts in two languages, where']","['##s  #TAUTHOR_TAG a ;  #AUTHOR_TAG.', 'a bite']","['- occurrence with the exception of  #AUTHOR_TAG b ), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  #TAUTHOR_TAG a ;  #AUTHOR_TAG.', 'a bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'word co - occurrence can be defined in various ways.', 'the most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments si and ti are translations of each other  #AUTHOR_TAG a ).', '']",0
"['bilingual lexicography  #TAUTHOR_TAG, computerassisted language']","['bilingual lexicography  #TAUTHOR_TAG, computerassisted language']","['bilingual lexicography  #TAUTHOR_TAG, computerassisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingu']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation  #AUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'table look - up using an explicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web  #AUTHOR_TAG, certain machine - assisted translation tools ( e. g.  #AUTHOR_TAG b ) ), concordancing for bilingual lexicography  #TAUTHOR_TAG, computerassisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard &  #AUTHOR_TAG']",0
"[""` word'' to include multi - word lexical units  #TAUTHOR_TAG."", 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this']","['better accuracy can be achieved with a more fine - grained link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy.', ""another interesting extension is to broaden the definition of a ` ` word'' to include multi - word lexical units  #TAUTHOR_TAG."", 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account']","[""` word'' to include multi - word lexical units  #TAUTHOR_TAG."", 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account']","['better accuracy can be achieved with a more fine - grained link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy.', ""another interesting extension is to broaden the definition of a ` ` word'' to include multi - word lexical units  #TAUTHOR_TAG."", 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account for a wider range of translation phenomena']",3
"[') 1, following  #TAUTHOR_TAG 2.', 'when the']","[') 1, following  #TAUTHOR_TAG 2.', ""when the l ( u, v ) are re - estimated, the model's hidden parameters come into play""]","[') 1, following  #TAUTHOR_TAG 2.', 'when the']","['translation model consists of the hidden parameters a + and a -, and likelihood ratios l ( u, v ).', 'the two hidden parameters are the probabilities of the model generating true and false positives in the data.', 'l ( u, v ) represents the likelihood that u and v can be mutual translations.', 'for each co - occurring pair of word types u and v, these likelihoods are initially set proportional to their co - occurrence frequency ( a, v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1, following  #TAUTHOR_TAG 2.', ""when the l ( u, v ) are re - estimated, the model's hidden parameters come into play""]",5
"['##s  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'a bite']","[' #AUTHOR_TAG b ), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'a bitext comprises a pair of texts in two languages, where']","['##s  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'a bite']","['the exception of  #AUTHOR_TAG b ), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'a bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'word co - occurrence can be defined in various ways.', 'the most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments si and ti are translations of each other  #AUTHOR_TAG a ).', '']",0
"['negligible values  #TAUTHOR_TAG.', 'to retain']","['negligible values  #TAUTHOR_TAG.', 'to retain']","['negligible values  #TAUTHOR_TAG.', 'to retain word type pairs that are']","['. discard all likelihood scores for word types deemed unlikely to be mutual translations, i. e. all l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values  #TAUTHOR_TAG.', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly']",1
"['has no local maxima.', 'by using the em algorithm  #TAUTHOR_TAG, they can guarantee convergence']","['has no local maxima.', 'by using the em algorithm  #TAUTHOR_TAG, they can guarantee convergence']","['- to - word model is that their objective function has no local maxima.', 'by using the em algorithm  #TAUTHOR_TAG, they can guarantee convergence']","[""advantage that brown et al.'s model i has over our word - to - word model is that their objective function has no local maxima."", 'by using the em algorithm  #TAUTHOR_TAG, they can guarantee convergence towards the globally optimum parameter set.', 'in contrast, the dynamic nature of the competitive linking algorithm changes the pr ( datalmodel ) in a non - monotonic fashion.', 'we have adopted the simple heuristic that the model "" has converged "" when this probability stops increasing']",0
"['link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy  #TAUTHOR_TAG.', 'another interesting extension is to broaden the definition of a "" word "" to include multi - word lexical units  #AUTHOR_TAG.', 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this']","['better accuracy can be achieved with a more fine - grained link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy  #TAUTHOR_TAG.', 'another interesting extension is to broaden the definition of a "" word "" to include multi - word lexical units  #AUTHOR_TAG.', 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account']","['##ed link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy  #TAUTHOR_TAG.', 'another interesting extension is to broaden the definition of a "" word "" to include multi - word lexical units  #AUTHOR_TAG.', 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account']","['better accuracy can be achieved with a more fine - grained link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy  #TAUTHOR_TAG.', 'another interesting extension is to broaden the definition of a "" word "" to include multi - word lexical units  #AUTHOR_TAG.', 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account for a wider range of translation phenomena']",3
"['- to - word model, the hidden parameters a + and a - depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'more accurate models can be induced by taking into account various features of the linked tokens.', 'for example, frequent words are translated less consistently than rare words  #TAUTHOR_TAG']","['the basic word - to - word model, the hidden parameters a + and a - depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'more accurate models can be induced by taking into account various features of the linked tokens.', 'for example, frequent words are translated less consistently than rare words  #TAUTHOR_TAG']","['the basic word - to - word model, the hidden parameters a + and a - depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'more accurate models can be induced by taking into account various features of the linked tokens.', 'for example, frequent words are translated less consistently than rare words  #TAUTHOR_TAG']","['the basic word - to - word model, the hidden parameters a + and a - depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'more accurate models can be induced by taking into account various features of the linked tokens.', 'for example, frequent words are translated less consistently than rare words  #TAUTHOR_TAG']",0
"['negligible values  #TAUTHOR_TAG.', 'to retain']","['negligible values  #TAUTHOR_TAG.', 'to retain']","['negligible values  #TAUTHOR_TAG.', 'to retain word type pairs that are']","['. discard all likelihood scores for word types deemed unlikely to be mutual translations, i. e. all l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values  #TAUTHOR_TAG.', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly']",1
"['02  #TAUTHOR_TAG or the dice coefficient  #AUTHOR_TAG.', 'co - occur is called a direct association.', 'now, suppose that uk and uk +']","['02  #TAUTHOR_TAG or the dice coefficient  #AUTHOR_TAG.', 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within']","['02  #TAUTHOR_TAG or the dice coefficient  #AUTHOR_TAG.', 'co - occur is called a direct association.', 'now, suppose that uk and uk +']","[""##we could just as easily use other symmetric ` ` association'' measures, such as 02  #TAUTHOR_TAG or the dice coefficient  #AUTHOR_TAG."", 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within their language.', 'then vk and uk + l will also co - occur more often than expected by chance.', 'the arrow connecting vk and u ~ + l in figure 1 represents an indirect association, since the association between vk and uk + z arises only by virtue of the association between each of them and uk.', 'models of translational equivalence that are ignorant of indirect associations have "" a tendency... to be confused by collocates ""  #AUTHOR_TAG.', 'fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based  #AUTHOR_TAG c ).', ""the majority of indirect associations can be filtered out by a simple competition heuristic : whenever several word tokens ui in one half of the bitext co - occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood l ( u, v ) of translational equivalence is highest."", 'the competitive linking algorithm implements this heuristic']",1
"[' #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad']","[' #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation  #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computationally expensive to']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation  #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'table look - up using an explicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( church & i -  #AUTHOR_TAG, certain machine - assisted translation tools ( e. g.', ' #AUTHOR_TAG b ) ), concordancing for bilingual lexicography  #AUTHOR_TAG computerassisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard &  #AUTHOR_TAG']",0
"['that has only one correct translation.', 'this imbalance foils thresholding strategies, clever as they might be  #TAUTHOR_TAG.', 'the likelihoods in the']","['that has only one correct translation.', 'this imbalance foils thresholding strategies, clever as they might be  #TAUTHOR_TAG.', 'the likelihoods in the word - to - word model remain unnormalized, so they do not compete']","['that has only one correct translation.', 'this imbalance foils thresholding strategies, clever as they might be  #TAUTHOR_TAG.', 'the likelihoods in the word - to - word model remain unnormalized, so they do not compete']","['', 'this imbalance foils thresholding strategies, clever as they might be  #TAUTHOR_TAG.', 'the likelihoods in the word - to - word model remain unnormalized, so they do not compete']",0
"['models  #TAUTHOR_TAG b ).', 'for']","['models  #TAUTHOR_TAG b ).', 'for']","['models  #TAUTHOR_TAG b ).', 'for']","['account for this difference, we can estimate separate values of x + and a - for different ranges of n ( u, v ).', 'similarly, the hidden parameters can be conditioned on the linked parts of speech.', 'word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences.', 'just as easily, we can model links that coincide with entries in a pre - existing translation lexicon separately from those that do not.', 'this method of incorporating dictionary information seems simpler than the method proposed by brown et al. for their models  #TAUTHOR_TAG b ).', 'for their models  #AUTHOR_TAG b ).', 'when the hidden parameters are conditioned on different link classes, the estimation method does not change ; it is just repeated for each link class']",1
"['by  #TAUTHOR_TAG, who trained brown et']","['by  #TAUTHOR_TAG, who trained brown et']","['detailed evaluation of link tokens to date was performed by  #TAUTHOR_TAG, who trained brown et']","['', 'for many applications, this is the desired behavior.', ""the most detailed evaluation of link tokens to date was performed by  #TAUTHOR_TAG, who trained brown et al.'s model 2 on 74 million words of the canadian hansards."", 'these authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'we generated links in the same 51 sentences using our two - class word - to - word model, and manually evaluated the content - word links from both models.', 'the ibm models are directional ; i. e. they posit the english words that gave rise to each french word, but ignore the distribution of the english words.', 'therefore, we ignored english words that were linked to nothing']",1
"['in a grammar that would be too large. 11', '11 from  #TAUTHOR_TAG, we find that the']","['in a grammar that would be too large. 11', '11 from  #TAUTHOR_TAG, we find that the']","['in a grammar that would be too large. 11', '11 from  #TAUTHOR_TAG, we find that the performance of samt system is similar with the method of labeling scfg rules with pos tags.', 'thus, to be convenient, we only']","['the u - trees, we run the gibbs sampler for 1000 iterations on the whole corpus.', 'the sampler uses 1, 087s per iteration, on average, using a single core, 2. 3 ghz intel xeon machine.', 'for the hyperparameters, we set i to 0. 1 and p expand = 1 / 3 to give a preference to the rules with small fragments.', 'we built an s2t translation system with the achieved u - trees after the 1000th iteration.', 'we only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 from  #TAUTHOR_TAG, we find that the performance of samt system is similar with the method of labeling scfg rules with pos tags.', 'thus, to be convenient, we only conduct experiments with the samt system']",4
"['', 'differently,  #TAUTHOR_TAG designed a sampler to infer an stsg by fixing the tree structure and exploring']","['of tree structures in our sampler.', 'differently,  #TAUTHOR_TAG designed a sampler to infer an stsg by fixing the tree structure and exploring']","['', 'differently,  #TAUTHOR_TAG designed a sampler to infer an stsg by fixing the tree structure and']","['', 'in addition, it should be noted that the word alignment is fixed 8, and we only explore the entire space of tree structures in our sampler.', 'differently,  #TAUTHOR_TAG designed a sampler to infer an stsg by fixing the tree structure and exploring the space of alignment.', 'we believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'this subject will be one of our future work topics']",4
"['system  #TAUTHOR_TAG.', ' #AUTHOR_TAG employed a bayesian method']","['system  #TAUTHOR_TAG.', ' #AUTHOR_TAG employed a bayesian method']","['hierarchical phrase - based system  #TAUTHOR_TAG.', ' #AUTHOR_TAG employed a bayesian method']",[' #TAUTHOR_TAG'],1
"['encoded into the model more freely and effectively.', ' #TAUTHOR_TAG, 2009, 2010 ) utilized bayesian methods']","['encoded into the model more freely and effectively.', ' #TAUTHOR_TAG, 2009, 2010 ) utilized bayesian methods']","['into the model more freely and effectively.', ' #TAUTHOR_TAG, 2009, 2010 ) utilized bayesian methods']","['unsupervised tree structure induction, de  #AUTHOR_TAG adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work  #AUTHOR_TAG designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', ' #TAUTHOR_TAG, 2009, 2010 ) utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', '']",1
"[').', 'the system is implemented based on  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'in the system, we extract both the minimal ghkm rules  #AUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']","['( abbreviated as s2t ).', 'the system is implemented based on  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'in the system, we extract both the minimal ghkm rules  #AUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']","[').', 'the system is implemented based on  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'in the system, we extract both the minimal ghkm rules  #AUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']","['translation system used for testing the effectiveness of our u - trees is our in - house stringto - tree system ( abbreviated as s2t ).', 'the system is implemented based on  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'in the system, we extract both the minimal ghkm rules  #AUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']",5
[' #TAUTHOR_TAG designed an embased method'],[' #TAUTHOR_TAG designed an embased method'],"['syntactic pre - reordering.', 'our previous work  #TAUTHOR_TAG designed an embased method']","['unsupervised tree structure induction, de  #AUTHOR_TAG adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work  #TAUTHOR_TAG designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', ' #AUTHOR_TAG utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', '']",1
[' #TAUTHOR_TAG and use the resulting binary parse trees to'],[' #TAUTHOR_TAG and use the resulting binary parse trees to'],"['build the above s2t system, we first use the parse tree, which is generated by parsing the english side of the bilingual data with the berkeley parser  #AUTHOR_TAG.', 'then, we binarize the english parse trees using the head binarization approach  #TAUTHOR_TAG and use the resulting binary parse trees to']","['build the above s2t system, we first use the parse tree, which is generated by parsing the english side of the bilingual data with the berkeley parser  #AUTHOR_TAG.', 'then, we binarize the english parse trees using the head binarization approach  #TAUTHOR_TAG and use the resulting binary parse trees to build another s2t system']",5
"['to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #AUTHOR_TAG 2006 ;  #TAUTHOR_TAG b )']","['to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #AUTHOR_TAG 2006 ;  #TAUTHOR_TAG b )']","['remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #AUTHOR_TAG 2006 ;  #TAUTHOR_TAG b )']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #AUTHOR_TAG 2006 ;  #TAUTHOR_TAG b )']",0
"['to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #AUTHOR_TAG 2006 ;  #TAUTHOR_TAG b )']","['to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #AUTHOR_TAG 2006 ;  #TAUTHOR_TAG b )']","['remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #AUTHOR_TAG 2006 ;  #TAUTHOR_TAG b )']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #AUTHOR_TAG 2006 ;  #TAUTHOR_TAG b )']",0
"['', ' #TAUTHOR_TAG adopted a bayesian method']","['alignment.', ' #TAUTHOR_TAG adopted a bayesian method']","['', ' #TAUTHOR_TAG adopted a bayesian method']","['', ' #TAUTHOR_TAG adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', '']",1
"['source string str in the model, we follow  #TAUTHOR_TAG']","['source string str in the model, we follow  #TAUTHOR_TAG']","['tree fragment frag and a source string str in the model, we follow  #TAUTHOR_TAG']","['base distribution 0 ( | ) p r n is designed to assign prior probabilities to the stsg production rules.', 'because each rule r consists of a target tree fragment frag and a source string str in the model, we follow  #TAUTHOR_TAG and decompose the prior probability p0 ( r | n ) into two factors as follows']",5
"['', ' #TAUTHOR_TAG further labeled']","['by extended syntactic categories.', ' #TAUTHOR_TAG further labeled']","['by extended syntactic categories.', ' #TAUTHOR_TAG further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs']","['', ' #AUTHOR_TAG utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', ' #AUTHOR_TAG substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', ' #TAUTHOR_TAG further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['nodes  #TAUTHOR_TAG.', '']","['nodes  #TAUTHOR_TAG.', '']","['of frontier nodes  #TAUTHOR_TAG.', 'frontier nodes are the tree nodes that can map onto contiguous substrings on']","['the initial target u - trees, source sentences and word alignment, we extract minimal ghkm translation rules7 in terms of frontier nodes  #TAUTHOR_TAG.', 'frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', '']",5
"['', ' #TAUTHOR_TAG']","['on parse trees.', ' #TAUTHOR_TAG']","['on parse trees.', ' #TAUTHOR_TAG']","['', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', ' #AUTHOR_TAG and  #AUTHOR_TAG focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', ' #TAUTHOR_TAG']",1
"['- 4 with the shortest length penalty.', 'the statistical significance test is performed by the re - sampling approach  #TAUTHOR_TAG']","['by case - insensitive bleu - 4 with the shortest length penalty.', 'the statistical significance test is performed by the re - sampling approach  #TAUTHOR_TAG']","['by case - insensitive bleu - 4 with the shortest length penalty.', 'the statistical significance test is performed by the re - sampling approach  #TAUTHOR_TAG']","['experiments are conducted on chinese - to - english translation.', 'the training data are the fbis corpus with approximately 7. 1 million chinese words and 9. 2 million english words.', 'we obtain the bidirectional word alignment with giza + +, and then adopt the grow - diag - final - and strategy to obtain the final symmetric alignment.', 'we train a 5gram language model on the xinhua portion of the english gigaword corpus and the english part of the training data.', 'for tuning and testing, we use the nist mt 2003 evaluation data as the development set, and use the nist mt04 and mt05 data as the test set.', 'we use mert  #AUTHOR_TAG to tune parameters.', 'since mert is prone to search errors, we run mert 5 times and select the best tuning parameters in the tuning set.', 'the translation quality is evaluated by case - insensitive bleu - 4 with the shortest length penalty.', 'the statistical significance test is performed by the re - sampling approach  #TAUTHOR_TAG']",5
"[').', 'the system is implemented based on  #TAUTHOR_TAG and ( marcu et al. 2006 ).', 'in the system, we extract both the minimal ghkm rules  #AUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']","['( abbreviated as s2t ).', 'the system is implemented based on  #TAUTHOR_TAG and ( marcu et al. 2006 ).', 'in the system, we extract both the minimal ghkm rules  #AUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']","[').', 'the system is implemented based on  #TAUTHOR_TAG and ( marcu et al. 2006 ).', 'in the system, we extract both the minimal ghkm rules  #AUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']","['translation system used for testing the effectiveness of our u - trees is our in - house stringto - tree system ( abbreviated as s2t ).', 'the system is implemented based on  #TAUTHOR_TAG and ( marcu et al. 2006 ).', 'in the system, we extract both the minimal ghkm rules  #AUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']",5
"['system  #AUTHOR_TAG.', ' #TAUTHOR_TAG employed a bayesian method']","['system  #AUTHOR_TAG.', ' #TAUTHOR_TAG employed a bayesian method']","['hierarchical phrase - based system  #AUTHOR_TAG.', ' #TAUTHOR_TAG employed a bayesian method']","['', ' #TAUTHOR_TAG employed a bayesian method to learn discontinuous scfg rules.', '']",1
"['.', ' #TAUTHOR_TAG and  #AUTHOR_TAG focused on joint']","['models than scfg.', ' #TAUTHOR_TAG and  #AUTHOR_TAG focused on joint']","['.', ' #TAUTHOR_TAG and  #AUTHOR_TAG focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank']","['', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', ' #TAUTHOR_TAG and  #AUTHOR_TAG focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', '']",1
"['', '9 we only use the minimal ghkm rules  #TAUTHOR_TAG here to reduce']","['in future.', '9 we only use the minimal ghkm rules  #TAUTHOR_TAG here to reduce']","['', '9 we only use the minimal ghkm rules  #TAUTHOR_TAG here to reduce the complexity of the sampler']","['', 'our final experiments verify this point and we will conduct a much detailed analysis in future.', '9 we only use the minimal ghkm rules  #TAUTHOR_TAG here to reduce the complexity of the sampler']",5
"[').', 'the system is implemented based on  #AUTHOR_TAG and ).', 'in the system, we extract both the minimal ghkm rules  #TAUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']","['( abbreviated as s2t ).', 'the system is implemented based on  #AUTHOR_TAG and ).', 'in the system, we extract both the minimal ghkm rules  #TAUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']","[').', 'the system is implemented based on  #AUTHOR_TAG and ).', 'in the system, we extract both the minimal ghkm rules  #TAUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']","['translation system used for testing the effectiveness of our u - trees is our in - house stringto - tree system ( abbreviated as s2t ).', 'the system is implemented based on  #AUTHOR_TAG and ).', 'in the system, we extract both the minimal ghkm rules  #TAUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']",5
"['algorithm  #TAUTHOR_TAG, we']","['would define two different u - trees.', 'using the ghkm algorithm  #TAUTHOR_TAG, we']","['would define two different u - trees.', 'using the ghkm algorithm  #TAUTHOR_TAG,']","['', 'otherwise, we change its state to the right state o, and transform the u - tree to figure 3 obviously, towards an s - node for sampling, the two values of o would define two different u - trees.', 'using the ghkm algorithm  #TAUTHOR_TAG, we can get two different stsg derivations from the two u - trees based on the fixed word alignment.', 'each derivation carries a set of stsg rules ( i. e., minimal ghkm translation rules ) of its own.', '']",5
"['to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #TAUTHOR_TAG, 2006 ;  #AUTHOR_TAG b )']","['to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #TAUTHOR_TAG, 2006 ;  #AUTHOR_TAG b )']","['remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #TAUTHOR_TAG, 2006 ;  #AUTHOR_TAG b )']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #TAUTHOR_TAG, 2006 ;  #AUTHOR_TAG b )']",0
"['to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #TAUTHOR_TAG 2006 ;  #AUTHOR_TAG b )']","['to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #TAUTHOR_TAG 2006 ;  #AUTHOR_TAG b )']","['remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #TAUTHOR_TAG 2006 ;  #AUTHOR_TAG b )']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #TAUTHOR_TAG 2006 ;  #AUTHOR_TAG b )']",0
"['hpb ) system, and a syntax - augmented mt ( samt ) 11 system  #TAUTHOR_TAG respectively']","['hpb ) system, and a syntax - augmented mt ( samt ) 11 system  #TAUTHOR_TAG respectively']","['hpb ) system, and a syntax - augmented mt ( samt ) 11 system  #TAUTHOR_TAG respectively']","['create the baseline system, we use the opensource joshua 4. 0 system  #AUTHOR_TAG to build a hierarchical phrase - based ( hpb ) system, and a syntax - augmented mt ( samt ) 11 system  #TAUTHOR_TAG respectively']",5
"['with the berkeley parser  #TAUTHOR_TAG.', 'then, we binari']","['with the berkeley parser  #TAUTHOR_TAG.', 'then, we binarize']","[', which is generated by parsing the english side of the bilingual data with the berkeley parser  #TAUTHOR_TAG.', 'then, we binarize the english parse trees using the head binarization approach  #AUTHOR_TAG and use the resulting binary parse trees to']","['build the above s2t system, we first use the parse tree, which is generated by parsing the english side of the bilingual data with the berkeley parser  #TAUTHOR_TAG.', 'then, we binarize the english parse trees using the head binarization approach  #AUTHOR_TAG and use the resulting binary parse trees to build another s2t system']",5
['the binary structure has been verified to be very effective for tree - based translation  #TAUTHOR_TAG a )'],['the binary structure has been verified to be very effective for tree - based translation  #TAUTHOR_TAG a )'],['the binary structure has been verified to be very effective for tree - based translation  #TAUTHOR_TAG a )'],"['the probability of producing the target tree fragment frag.', 'to generate frag, used a geometric prior to decide how many child nodes to assign each node.', 'differently, we require that each multi - word non - terminal node must have two child nodes.', 'this is because the binary structure has been verified to be very effective for tree - based translation  #TAUTHOR_TAG a )']",4
"['to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #TAUTHOR_TAG, 2009 ;  #AUTHOR_TAG 2006 ;  #AUTHOR_TAG b )']","['to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #TAUTHOR_TAG, 2009 ;  #AUTHOR_TAG 2006 ;  #AUTHOR_TAG b )']","['remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #TAUTHOR_TAG, 2009 ;  #AUTHOR_TAG 2006 ;  #AUTHOR_TAG b )']","['recent years, tree - based translation models1 are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #TAUTHOR_TAG, 2009 ;  #AUTHOR_TAG 2006 ;  #AUTHOR_TAG b )']",0
['training tree - based translation models  #TAUTHOR_TAG'],['training tree - based translation models  #TAUTHOR_TAG'],['training tree - based translation models  #TAUTHOR_TAG'],"[', for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of tree - bank resources for training.', '2 ) parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'this indicates that parse trees are usually not the optimal choice for training tree - based translation models  #TAUTHOR_TAG']",0
"['.', ' #AUTHOR_TAG and  #TAUTHOR_TAG focused on joint']","['models than scfg.', ' #AUTHOR_TAG and  #TAUTHOR_TAG focused on joint']","['.', ' #AUTHOR_TAG and  #TAUTHOR_TAG focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank']","['', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', ' #AUTHOR_TAG and  #TAUTHOR_TAG focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', '']",1
"['', ' #TAUTHOR_TAG utilized a transformation - based method']","['alignment.', ' #TAUTHOR_TAG utilized a transformation - based method']","['', ' #TAUTHOR_TAG utilized a transformation - based method']","['', ' #TAUTHOR_TAG utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', ' #AUTHOR_TAG substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', ' #AUTHOR_TAG further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['( 4 ) is the probability of generating the source string, which contains several source words and variables.', 'inspired by  #AUTHOR_TAG and  #TAUTHOR_TAG, we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['( 4 ) is the probability of generating the source string, which contains several source words and variables.', 'inspired by  #AUTHOR_TAG and  #TAUTHOR_TAG, we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['( 4 ) is the probability of generating the source string, which contains several source words and variables.', 'inspired by  #AUTHOR_TAG and  #TAUTHOR_TAG, we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['( str | frag ) in equation ( 4 ) is the probability of generating the source string, which contains several source words and variables.', 'inspired by  #AUTHOR_TAG and  #TAUTHOR_TAG, we define p ( str | frag ) as follows : where csw is the number of words in the source string']",4
"['( 4 ) is the probability of generating the source string, which contains several source words and variables.', 'inspired by  #TAUTHOR_TAG and  #AUTHOR_TAG, we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['( 4 ) is the probability of generating the source string, which contains several source words and variables.', 'inspired by  #TAUTHOR_TAG and  #AUTHOR_TAG, we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['( 4 ) is the probability of generating the source string, which contains several source words and variables.', 'inspired by  #TAUTHOR_TAG and  #AUTHOR_TAG, we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['( str | frag ) in equation ( 4 ) is the probability of generating the source string, which contains several source words and variables.', 'inspired by  #TAUTHOR_TAG and  #AUTHOR_TAG, we define p ( str | frag ) as follows : where csw is the number of words in the source string']",4
"['grammar  #TAUTHOR_TAG 5.', 'this would indicate that the scf']","['the heuristic scfg grammar  #TAUTHOR_TAG 5.', 'this would indicate that the scfg - based derivation tree as by - product is also not such good for tree - based translation models.', 'considering the above reasons, we believe that the stsg - based learning procedure would result in a better translation grammar for tree - based models']","['the heuristic scfg grammar  #TAUTHOR_TAG 5.', 'this would indicate that the scfg - based derivation tree as by - product is also not such good for tree']","['addition, we find that the bayesian scfg grammar can not even significantly outperform the heuristic scfg grammar  #TAUTHOR_TAG 5.', 'this would indicate that the scfg - based derivation tree as by - product is also not such good for tree - based translation models.', 'considering the above reasons, we believe that the stsg - based learning procedure would result in a better translation grammar for tree - based models']",1
"['', ' #TAUTHOR_TAG substituted']","['models.', ' #TAUTHOR_TAG substituted']","['', ' #TAUTHOR_TAG substituted the non - terminal x in']","['', ' #AUTHOR_TAG utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', ' #TAUTHOR_TAG substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', ' #AUTHOR_TAG further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
[' #TAUTHOR_TAG using'],[' #TAUTHOR_TAG using'],['in the forward search  #TAUTHOR_TAG using'],"['hypotheses.', 'as the recogn / fion engine, either voicerex, developed by nti ""  #AUTHOR_TAG, or htk from entropic research can be used.', 'acoustic models for htk is trained with the continuous speech database of the acoustical society of japan  #AUTHOR_TAG.', 'this recognizer incrementally outputs word hypotheses as soon as they are found in the best - scored path in the forward search  #TAUTHOR_TAG using the istar ( incremental structure transmitter and receiver ) protocol, which conveys word graph information as well as word hypotheses.', 'this incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real - time responses are possible.', '']",0
"['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']",0
"['generating responses from a frame representation of user requests  #TAUTHOR_TAG.', 'incorporating such techniques would deo cre']","['generating responses from a frame representation of user requests  #TAUTHOR_TAG.', 'incorporating such techniques would deo crease the system developer workload.', 'however, there has been no']","['generating responses from a frame representation of user requests  #TAUTHOR_TAG.', 'incorporating such techniques would deo crease the system developer workload.', 'however, there has been no work on domain - independent response generation for robust spoken dialogue systems that can deal with utterances']","['have been several efforts aimed at developing a domain - independent method for generating responses from a frame representation of user requests  #TAUTHOR_TAG.', 'incorporating such techniques would deo crease the system developer workload.', 'however, there has been no work on domain - independent response generation for robust spoken dialogue systems that can deal with utterances that might include pauses in the middle of a sentence, which wit handles well.', 'therefore incorporating those techniques remains as a future work']",3
"['current dialogue state.', 'they can also shift the dialogue 2the notion of the initiative in this paper is different from that of the dialogue initiative of  #TAUTHOR_TAG']","['current dialogue state.', 'they can also shift the dialogue 2the notion of the initiative in this paper is different from that of the dialogue initiative of  #TAUTHOR_TAG']","['based on the current dialogue state.', 'they can also shift the dialogue 2the notion of the initiative in this paper is different from that of the dialogue initiative of  #TAUTHOR_TAG']","['defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'they can also shift the dialogue 2the notion of the initiative in this paper is different from that of the dialogue initiative of  #TAUTHOR_TAG']",1
"['acoustical society of japan  #TAUTHOR_TAG.', '']","['acoustical society of japan  #TAUTHOR_TAG.', '']","['the acoustical society of japan  #TAUTHOR_TAG.', '']","['hypotheses.', 'as the recogn / fion engine, either voicerex, developed by nti ""  #AUTHOR_TAG, or htk from entropic research can be used.', 'acoustic models for htk is trained with the continuous speech database of the acoustical society of japan  #TAUTHOR_TAG.', 'this recognizer incrementally outputs word hypotheses as soon as they are found in the best - scored path in the forward search  #AUTHOR_TAG using the istar ( incremental structure transmitter and receiver ) protocol, which conveys word graph information as well as word hypotheses.', 'this incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real - time responses are possible.', 'this module continuously runs and outputs recognition results when it detects a speech interval.', 'this enables the language generation module to react immediately to user interruptions while the system is speaking.', 'the language model for speech recognition is a network ( regular ) grammar, and it allows each speech interval to be an arbitrary number of phrases.', 'a phrase is a sequence of words, which is to be defined in a domain - dependent way.', 'sentences can be decomposed into a couple of phrases.', 'the reason we use a repetition of phrases instead of a sentence grammar for the language model is that the speech recognition module of a robust spoken dialogue system sometimes has to recognize spontaneously spoken utterances, which include self - repairs and repetition.', 'in japanese, bunsetsu is appropriate for defining phrases.', 'a bunsetsu consists of one content word and a number ( possibly zero ) of function words.', 'in the meeting room reservation system we have developed, examples of defined phrases are bunsetsu to specify the room to be reserved and the time of the reservation and bunsetsu to express affirmation and negation']",5
"['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']",0
"['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']",0
"['system  #TAUTHOR_TAG b ),']","['system  #TAUTHOR_TAG b ),']","['has been implemented in common lisp and c on unix, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system  #TAUTHOR_TAG b ), a video - recording programming system, a schedule management system  #AUTHOR_TAG a ), and a weather infomiation system  #AUTHOR_TAG.', 'the meeting room reservation system has vocabulary']","['has been implemented in common lisp and c on unix, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system  #TAUTHOR_TAG b ), a video - recording programming system, a schedule management system  #AUTHOR_TAG a ), and a weather infomiation system  #AUTHOR_TAG.', 'the meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'a sample dialogue between this system and a naive user is shown in figure 2.', 'this system employs htk as the speech recognition engine.', ""the weather information system can answer the user's questions about weather forecasts in japan."", 'the vocabulary size is around 500, and the number of phrase structure rules is 31.', 'the number of attributes in the semantic flame is 11, and the number of the files of the pre - recorded speech is about 13, 000']",2
"['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']",0
"['grammar  #TAUTHOR_TAG.', 'the language generation']","['of unification grammar  #TAUTHOR_TAG.', 'the language generation']","['with in the framework of unification grammar  #TAUTHOR_TAG.', 'the language generation module features common lisp functions, so there is no limitation on the description.', 'some of the systems we have developed feature a generation method based on hierarchical planning  #AUTHOR_TAG.', 'it is also possible to build a simple finite - state - model - based dialogue system using wit.', 'states can be represented by dialogue phases in wit']","['previous finite - state - model - based toolkits place many severe restrictions on domain descriptions, wit has enough descriptive power to build a variety of dialogue systems.', 'although the dialogue state is represented by a simple attributevalue matrix, since there is no limitation on the number of attributes, it can hold more complicated information.', 'for example, it is possible to represent a discourse stack whose depth is limited.', 'recording some dialogue history is also possible.', 'since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered.', 'for example, speech repairs, particle omission, and fillers can be dealt with in the framework of unification grammar  #TAUTHOR_TAG.', 'the language generation module features common lisp functions, so there is no limitation on the description.', 'some of the systems we have developed feature a generation method based on hierarchical planning  #AUTHOR_TAG.', 'it is also possible to build a simple finite - state - model - based dialogue system using wit.', 'states can be represented by dialogue phases in wit']",3
"[', utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking  #TAUTHOR_TAG a ).', 'thus']","[', utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking  #TAUTHOR_TAG a ).', 'thus']","['the language generation module works in parallel with the language understanding module, utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking  #TAUTHOR_TAG a ).', '']","['the language generation module works in parallel with the language understanding module, utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking  #TAUTHOR_TAG a ).', 'thus the system can respond immediately after user pauses when the user has the initiative.', 'when the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way  #AUTHOR_TAG']",0
"['system  #TAUTHOR_TAG.', 'the meeting room reservation']","['system  #TAUTHOR_TAG.', 'the meeting room reservation']","[', and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system  #AUTHOR_TAG b ), a video - recording programming system, a schedule management system  #AUTHOR_TAG a ), and a weather infomiation system  #TAUTHOR_TAG.', 'the meeting room reservation system has vocabulary']","['has been implemented in common lisp and c on unix, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system  #AUTHOR_TAG b ), a video - recording programming system, a schedule management system  #AUTHOR_TAG a ), and a weather infomiation system  #TAUTHOR_TAG.', 'the meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'a sample dialogue between this system and a naive user is shown in figure 2.', 'this system employs htk as the speech recognition engine.', ""the weather information system can answer the user's questions about weather forecasts in japan."", 'the vocabulary size is around 500, and the number of phrase structure rules is 31.', 'the number of attributes in the semantic flame is 11, and the number of the files of the pre - recorded speech is about 13, 000']",2
"['generation, and speech output.', 'wit features an incremental understanding method  #TAUTHOR_TAG b ) that']","['generation, and speech output.', 'wit features an incremental understanding method  #TAUTHOR_TAG b ) that']","['generation, and speech output.', 'wit features an incremental understanding method  #TAUTHOR_TAG b ) that']","['paper presents wit 1, which is a toolkit iwit is an acronym of workable spoken dialogue lnter - for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output.', 'wit features an incremental understanding method  #TAUTHOR_TAG b ) that makes it possible to build a robust and real - time system.', 'in addition, wit compiles domain - dependent system specifications into internal knowledge sources so that building systems is easier.', 'although wit requires more domaindependent specifications than finite - state - modelbased toolkits, wit - based systems are capable of taking full advantage of language processing technology.', 'wit has been implemented and used to build several spoken dialogue systems']",5
"[' #TAUTHOR_TAG.', 'when a phrase boundary is detected, the feature structure for a phrase']","['consists of a unification - based lexicon and phrase structure rules.', 'disjunctive feature descriptions are also possible ; wit incorporates an efficient method for handling disjunctions  #TAUTHOR_TAG.', 'when a phrase boundary is detected, the feature structure for a phrase']","['domain - dependent knowledge used in this module consists of a unification - based lexicon and phrase structure rules.', 'disjunctive feature descriptions are also possible ; wit incorporates an efficient method for handling disjunctions  #TAUTHOR_TAG.', 'when a phrase boundary is detected, the feature structure for a phrase']","['domain - dependent knowledge used in this module consists of a unification - based lexicon and phrase structure rules.', 'disjunctive feature descriptions are also possible ; wit incorporates an efficient method for handling disjunctions  #TAUTHOR_TAG.', 'when a phrase boundary is detected, the feature structure for a phrase is computed using some built - in rules from the feature structure rules for the words in the phrase.', 'the phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.', ""two kinds of sentenees can be considered ; domain - related ones that express the user's intention about the reser - vafion and dialogue - related ones that express the user's attitude with respect to the progress of the dialogue, such as confirmation and denial."", 'considering the meeting room reservation system, examples of domain - related sentences are "" i need to book room 2 on wednesday "", "" i need to book room 2 "", and "" room 2 "" and dialogue - related ones are "" yes "", "" no "", and "" okay ""']",5
"['incremental understanding method  #TAUTHOR_TAG b ).', 'when the command']","['incremental understanding method  #TAUTHOR_TAG b ).', 'when the command']","['disambiguating interpretation in the incremental understanding method  #TAUTHOR_TAG b ).', 'when the command']","['roles are similar to dcg  #AUTHOR_TAG rules ; they can include logical variables and these variables can be bound when these rules are applied.', 'it is possible to add to the rules constraints that stipulate relationships that must hold among variables ( nakano, 199 i ), but we do not explain these constraints in detail in this paper.', 'the priorities are used for disambiguating interpretation in the incremental understanding method  #TAUTHOR_TAG b ).', ""when the command on the right - hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state."", 'the command is one of the following']",5
"['building spoken dialogue systems have been developed  #TAUTHOR_TAG.', 'one is the cslu toolkit  #AUTHOR_TAG, which']","['building spoken dialogue systems have been developed  #TAUTHOR_TAG.', 'one is the cslu toolkit  #AUTHOR_TAG, which']","['building spoken dialogue systems have been developed  #TAUTHOR_TAG.', 'one is the cslu toolkit  #AUTHOR_TAG, which']","['this end, several toolkits for building spoken dialogue systems have been developed  #TAUTHOR_TAG.', 'one is the cslu toolkit  #AUTHOR_TAG, which enables rapid prototyping of a spoken dialogue system that incorporates a finite - state dialogue model.', 'it decreases the amount of the effort required in building a spoken dialogue system in a user - defined task domain.', 'however, it limits system functions ; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics.', 'another is galaxy - ii ( seneffet al., 1998 ), * mikio nakano is currently a visiting scientist at mit laboratory for computer science. which enables modules in a dialogue system to communicate with each other.', 'it consists of the hub and several servers, such as the speech recognition server and the natural language server, and the hub communicates with these servers.', 'although it requires more specifications than finitestate - model - based toolkits, it places less limitations on system functions']",0
"['sequence search )  #TAUTHOR_TAG b ), which is an integrated parsing and discourse processing method.', 'isss']","['utilizes isss ( incremental significant - utterance sequence search )  #TAUTHOR_TAG b ), which is an integrated parsing and discourse processing method.', 'isss']","[', attribute - value pairs ).', 'the understanding module utilizes isss ( incremental significant - utterance sequence search )  #TAUTHOR_TAG b ), which is an integrated parsing and discourse processing method.', 'isss']","['language understanding : module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame ( i. e., attribute - value pairs ).', 'the understanding module utilizes isss ( incremental significant - utterance sequence search )  #TAUTHOR_TAG b ), which is an integrated parsing and discourse processing method.', 'isss enables the incremental understanding of user utterances that are not segmented into sentences prior to pars - ing by incrementally finding the most plausible sequence of sentences ( or significant utterances in the isss terms ) out of the possible sentence sequences for the input word sequence.', 'isss also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time']",5
"['.  #TAUTHOR_TAG.', 'while word sense']","['is term translation, e. g., via a bilingual lexicon.  #TAUTHOR_TAG.', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations']","['common approach is term translation, e. g., via a bilingual lexicon.  #TAUTHOR_TAG.', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations']","['common approach is term translation, e. g., via a bilingual lexicon.  #TAUTHOR_TAG.', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiernstra and de  #AUTHOR_TAG.', ' #AUTHOR_TAG studied the issue of disarnbiguation for mono - lingual ir']",1
"['.', 'there are several variations of such a method  #TAUTHOR_TAG.', 'one such method is to treat']","['whole query.', 'there are several variations of such a method  #TAUTHOR_TAG.', 'one such method is to treat']","['and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method  #TAUTHOR_TAG.', 'one such method is to treat different translations']","['second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'this approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method  #TAUTHOR_TAG.', 'one such method is to treat different translations of the same term as synonyms.', 'ballesteros, for example, used the inquery  #AUTHOR_TAG synonym operator to group translations of different query terms.', 'however, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'by contrast, our hmm approach supports translation probabilities.', ""the synonym approach is equivalent to changing all non - zero translation probabilities p ( w ~ [ wy )'s to 1 in our retrieyal function."", 'even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations']",1
['- test  #TAUTHOR_TAG at significance'],"['a small number of queries.', 'the one - sided t - test  #TAUTHOR_TAG at significance']","['', 'the one - sided t - test  #TAUTHOR_TAG at significance level 0. 05 indicated that the improvement on tre']","['results in table 4 show that manual disambiguation improves performance by 17 % on trec5c, 4 % on trec4s, but not at all on trec6c.', 'furthermore, the improvement on trec5c appears to be caused by big improvements for a small number of queries.', 'the one - sided t - test  #TAUTHOR_TAG at significance level 0. 05 indicated that the improvement on trec5c is not statistically significant']",5
"['##sm ),  #TAUTHOR_TAG.', 'we believe our']","['general vector space model ( gvsm ),  #TAUTHOR_TAG.', 'we believe our']","['##sm ),  #TAUTHOR_TAG.', 'we believe our']","['third approach to cross - lingual retrieval is to map queries and documents to some intermediate representation, e. g latent semantic indexing ( lsi )  #AUTHOR_TAG, or the general vector space model ( gvsm ),  #TAUTHOR_TAG.', 'we believe our approach is computationally less costly than ( lsi and gvsm ) and assumes less resources ( wordnet in  #AUTHOR_TAG']",1
['##¢ the transition probability a is 0. 7 using the em algorithm  #TAUTHOR_TAG on the trec4 ad - hoc query set'],['##¢ the transition probability a is 0. 7 using the em algorithm  #TAUTHOR_TAG on the trec4 ad - hoc query set'],['##¢ the transition probability a is 0. 7 using the em algorithm  #TAUTHOR_TAG on the trec4 ad - hoc query set'],['##¢ the transition probability a is 0. 7 using the em algorithm  #TAUTHOR_TAG on the trec4 ad - hoc query set'],5
"['##nstra and de  #AUTHOR_TAG.', ' #TAUTHOR_TAG studied the issue of disambiguation for mono - lingual m']","['value of disambiguation for cross - lingual ir include hiernstra and de  #AUTHOR_TAG.', ' #TAUTHOR_TAG studied the issue of disambiguation for mono - lingual m']","['##nstra and de  #AUTHOR_TAG.', ' #TAUTHOR_TAG studied the issue of disambiguation for mono - lingual m']","['common approach is term translation, e. g., via a bilingual lexicon.', ' #AUTHOR_TAG.', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. other studies on the value of disambiguation for cross - lingual ir include hiernstra and de  #AUTHOR_TAG.', ' #TAUTHOR_TAG studied the issue of disambiguation for mono - lingual m']",0
"['incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiemstra and de  #TAUTHOR_TAG.', ' #AUTHOR_TAG studied the issue of disarnbiguation for mono - lingual ir']","['incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiemstra and de  #TAUTHOR_TAG.', ' #AUTHOR_TAG studied the issue of disarnbiguation for mono - lingual ir']","['the incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiemstra and de  #TAUTHOR_TAG.', ' #AUTHOR_TAG studied the issue of disarnbiguation for mono - lingual ir']","['common approach is term translation, e. g., via a bilingual lexicon.', ' #AUTHOR_TAG.', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiemstra and de  #TAUTHOR_TAG.', ' #AUTHOR_TAG studied the issue of disarnbiguation for mono - lingual ir']",0
['to the language of the queries  #TAUTHOR_TAG'],['to the language of the queries  #TAUTHOR_TAG'],"['to the language of the queries  #TAUTHOR_TAG.', 'for most languages, there are no mt systems at all']","['approaches to cross - lingual ir have been published.', 'one common approach is using machine translation ( mt ) to translate the queries to the language of the documents or translate documents to the language of the queries  #TAUTHOR_TAG.', 'for most languages, there are no mt systems at all.', 'our focus is on languages where no mt exists, but a bilingual dictionary may exist or may be derived']",1
"['.', 'there are several variations of such a method  #TAUTHOR_TAG ; hull 1997 ).', 'one such method is to treat different translations']","['whole query.', 'there are several variations of such a method  #TAUTHOR_TAG ; hull 1997 ).', 'one such method is to treat different translations']","['and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method  #TAUTHOR_TAG ; hull 1997 ).', 'one such method is to treat different translations']","['second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'this approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method  #TAUTHOR_TAG ; hull 1997 ).', 'one such method is to treat different translations of the same term as synonyms.', 'ballesteros, for example, used the inquery  #AUTHOR_TAG synonym operator to group translations of different query terms.', 'however, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'by contrast, our hmm approach supports translation probabilities.', ""the synonym approach is equivalent to changing all non - zero translation probabilities p ( w ~ [ wy )'s to 1 in our retrieyal function."", 'even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations']",1
['- occur  #TAUTHOR_TAG'],['terms are more likely to co - occur  #TAUTHOR_TAG'],['- occur  #TAUTHOR_TAG'],"['', 'since all terms are treated as equal in the translated query, this gives terms with more translations ( potentially the more common terms ) more credit in retrieval, even though such terms should potentially be given less credit if they are more common.', 'also, a document matching different translations of one term in the original query may be ranked higher than a document that matches translations of different terms in the original query.', 'that is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at andbl.', 'however, the second document is more likely to be relevant since correct translations of the query terms are more likely to co - occur  #TAUTHOR_TAG']",0
"['.', 'a cooccurrence based stemmer  #TAUTHOR_TAG was used to stem spanish words.', 'one difference from the']","['translations on average.', 'a cooccurrence based stemmer  #TAUTHOR_TAG was used to stem spanish words.', 'one difference from the']","['.', 'a cooccurrence based stemmer  #TAUTHOR_TAG was used to stem spanish words.', 'one difference from the treatment of chinese is to include the english word as one of its own translations in addition to its spanish translations in the lexicon.', 'this is useful']","['', 'a cooccurrence based stemmer  #TAUTHOR_TAG was used to stem spanish words.', 'one difference from the treatment of chinese is to include the english word as one of its own translations in addition to its spanish translations in the lexicon.', 'this is useful for translating proper nouns, which often have identical spellings in english and spanish but are routinely excluded from a lexicon']",5
"['generation process include  #TAUTHOR_TAG.', 'our']","['generation process include  #TAUTHOR_TAG.', 'our']","['studies which view lr as a query generation process include  #TAUTHOR_TAG.', 'our']","['studies which view lr as a query generation process include  #TAUTHOR_TAG.', 'our work has focused on cross - lingual retrieval']",1
"['generation process include  #TAUTHOR_TAG.', 'our']","['generation process include  #TAUTHOR_TAG.', 'our']","['studies which view lr as a query generation process include  #TAUTHOR_TAG.', 'our']","['studies which view lr as a query generation process include  #TAUTHOR_TAG.', 'our work has focused on cross - lingual retrieval']",1
"['of translations from parallel or non - parallel corpora  #TAUTHOR_TAG.', 'since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement']","['of translations from parallel or non - parallel corpora  #TAUTHOR_TAG.', 'since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement']","['technique is automatic discovery of translations from parallel or non - parallel corpora  #TAUTHOR_TAG.', 'since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement']","['technique is automatic discovery of translations from parallel or non - parallel corpora  #TAUTHOR_TAG.', 'since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus - specific vocabularies']",0
"[' #TAUTHOR_TAG, the ir']","[' #TAUTHOR_TAG, the ir']","[' #TAUTHOR_TAG, the ir system ranks']","[' #TAUTHOR_TAG, the ir system ranks documents according to the probability that a document d is relevant given the query q, p ( d is r iq ).', 'using bayes rule, and the fact that p ( q ) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to p ( q [ d is r ) is the same as ranking them according to p ( d is riq ).', 'the approach therefore estimates the probability that a query q is generated, given the document d is relevant.', '( a glossary of symbols used appears below.']",5
"['of full parsers  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been']","['of full parsers  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been']","['of full parsers  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #AUTHOR_TAG']","['', 'would be chunked as follows  #AUTHOR_TAG : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #AUTHOR_TAG']",0
"['- 2000 ( tjong kim  #TAUTHOR_TAG.', 'in this case, a full parse']","['( tjong kim  #TAUTHOR_TAG.', 'in this case, a full parse']","['- 2000 ( tjong kim  #TAUTHOR_TAG.', 'in this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'the goal in this case is therefore to accurately predict a collection of ¢ £ ¢ different types of phrases.', 'the chunk types are based on the syntactic category part of the bracket label in the treebank.', 'roughly, a chunk contains everything to the left of']","['first is the one used in the chunking competition in conll - 2000 ( tjong kim  #TAUTHOR_TAG.', 'in this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'the goal in this case is therefore to accurately predict a collection of ¢ £ ¢ different types of phrases.', 'the chunk types are based on the syntactic category part of the bracket label in the treebank.', 'roughly, a chunk contains everything to the left of and including the syntactic head of the constituent of the same name.', 'the phrases are : adjective phrase ( adjp ), adverb phrase ( advp ), conjunction phrase ( conjp ), interjection phrase ( intj ), list marker ( lst ), noun phrase ( np ), preposition phrase ( pp ), particle ( prt ), subordinated clause ( sbar ), unlike coordinated phrase ( ucp ), verb phrase ( vp ).', '( see details in  #AUTHOR_TAG.']",5
"['of the syntactic structures in a text  #TAUTHOR_TAG.', 'a']","['of the syntactic structures in a text  #TAUTHOR_TAG.', 'a']","['- sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text  #TAUTHOR_TAG.', 'a lot']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text  #TAUTHOR_TAG.', 'a lot of recent work on shallow parsing has been influenced by abney\'s work  #AUTHOR_TAG, who has suggested to "" chunk "" sentences to base level phrases.', '']",0
"['of full parsers  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been']","['of full parsers  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been']","['of full parsers  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #AUTHOR_TAG']","['', 'would be chunked as follows  #AUTHOR_TAG : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #AUTHOR_TAG']",0
['of accuracy and processing time  #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #TAUTHOR_TAG'],['of accuracy and processing time  #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #TAUTHOR_TAG'],['of accuracy and processing time  #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #TAUTHOR_TAG'],"[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'however, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time  #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #TAUTHOR_TAG']",0
"['michael collins  #TAUTHOR_TAG - - one of the most accurate full parsers around.', '']","['michael collins  #TAUTHOR_TAG - - one of the most accurate full parsers around.', '']","['michael collins  #TAUTHOR_TAG - - one of the most accurate full parsers around.', 'it represents']","['perform our comparison using two state - ofthe - art parsers.', 'for the full parser, we use the one developed by michael collins  #TAUTHOR_TAG - - one of the most accurate full parsers around.', 'it represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'after that, it will choose the candidate parse tree with the highest probability as output.', 'the experiments use the version that was trained ( by collins ) on sections 02 - 21 of the penn treebank.', 'the reported results for the full parse tree ( on section 23 ) are recall / precision of 88. 1 / 87. 5  #AUTHOR_TAG']",5
['of accuracy and processing time  #TAUTHOR_TAG b ;  #AUTHOR_TAG a ;  #AUTHOR_TAG'],['of accuracy and processing time  #TAUTHOR_TAG b ;  #AUTHOR_TAG a ;  #AUTHOR_TAG'],['of accuracy and processing time  #TAUTHOR_TAG b ;  #AUTHOR_TAG a ;  #AUTHOR_TAG'],"[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'however, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time  #TAUTHOR_TAG b ;  #AUTHOR_TAG a ;  #AUTHOR_TAG']",0
"['parser  #AUTHOR_TAG.', 'snow  #TAUTHOR_TAG is a multi - class']","['parser  #AUTHOR_TAG.', 'snow  #TAUTHOR_TAG is a multi - class']","['shallow parser used is the snow - based cscl parser  #AUTHOR_TAG.', 'snow  #TAUTHOR_TAG is a multi - class classifier']","['shallow parser used is the snow - based cscl parser  #AUTHOR_TAG.', 'snow  #TAUTHOR_TAG is a multi - class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large, of which nlp is a principal example.', 'it works by learning a sparse network of linear functions over a pre - defined or incrementally learned feature space.', 'typically, snow is used as a classifier, and predicts using a winner - take - all mechanism over the activation value of the target classes.', 'however, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'indeed, in cscl ( constraint satisfaction with classifiers ), snow is used to learn several different classifiers - each detects the beginning or end of a phrase of some type ( noun phrase, verb phrase, etc. ).', ""the outcomes of these classifiers are then combined in a way that satisfies some constraints - non - overlapping constraints in this case - using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes""]",5
['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],"['of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']","['', 'would be chunked as follows  #AUTHOR_TAG : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']",0
['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],"['of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']","['', 'would be chunked as follows  #AUTHOR_TAG : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']",0
"['of the syntactic structures in a text  #TAUTHOR_TAG.', 'a']","['of the syntactic structures in a text  #TAUTHOR_TAG.', 'a']","['- sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text  #TAUTHOR_TAG.', 'a lot']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text  #TAUTHOR_TAG.', 'a lot of recent work on shallow parsing has been influenced by abney\'s work  #AUTHOR_TAG, who has suggested to "" chunk "" sentences to base level phrases.', '']",0
"['of the syntactic structures in a text  #TAUTHOR_TAG.', 'a']","['of the syntactic structures in a text  #TAUTHOR_TAG.', 'a']","['- sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text  #TAUTHOR_TAG.', 'a lot']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text  #TAUTHOR_TAG.', 'a lot of recent work on shallow parsing has been influenced by abney\'s work  #AUTHOR_TAG, who has suggested to "" chunk "" sentences to base level phrases.', '']",0
['and text summarization  #TAUTHOR_TAG'],['and text summarization  #TAUTHOR_TAG'],"[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization  #TAUTHOR_TAG.', 'second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'if all that is available is a collection of sentences annotated for nps, it can be used to']","[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization  #TAUTHOR_TAG.', 'second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'if all that is available is a collection of sentences annotated for nps, it can be used to produce this level of analysis.', 'this can be augmented later if more information is available.', 'finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low - sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes']",0
['##ong kim  #TAUTHOR_TAG to compare'],['under the exact conditions of conll - 2000 ( tjong kim  #TAUTHOR_TAG to compare'],['##ong kim  #TAUTHOR_TAG to compare'],"['earlier versions of the snow based cscl were used only to identify single phrases  #AUTHOR_TAG and never to identify a collection of several phrases at the same time, as we do here, we also trained and tested it under the exact conditions of conll - 2000 ( tjong kim  #TAUTHOR_TAG to compare it to other shallow parsers.', 'table 1 shows that it ranks among the top shallow parsers evaluated there 1']",5
['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],"['of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']","['', 'would be chunked as follows  #AUTHOR_TAG : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']",0
['of accuracy and processing time  #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #TAUTHOR_TAG'],['of accuracy and processing time  #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #TAUTHOR_TAG'],['of accuracy and processing time  #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #TAUTHOR_TAG'],"[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'however, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time  #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #TAUTHOR_TAG']",0
"['parser  #TAUTHOR_TAG.', 'snow  #AUTHOR_TAG is a multi - class']","['parser  #TAUTHOR_TAG.', 'snow  #AUTHOR_TAG is a multi - class']","['shallow parser used is the snow - based cscl parser  #TAUTHOR_TAG.', 'snow  #AUTHOR_TAG is a multi - class classifier']","['shallow parser used is the snow - based cscl parser  #TAUTHOR_TAG.', 'snow  #AUTHOR_TAG is a multi - class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large, of which nlp is a principal example.', 'it works by learning a sparse network of linear functions over a pre - defined or incrementally learned feature space.', 'typically, snow is used as a classifier, and predicts using a winner - take - all mechanism over the activation value of the target classes.', 'however, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'indeed, in cscl ( constraint satisfaction with classifiers ), snow is used to learn several different classifiers - each detects the beginning or end of a phrase of some type ( noun phrase, verb phrase, etc. ).', ""the outcomes of these classifiers are then combined in a way that satisfies some constraints - non - overlapping constraints in this case - using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes""]",5
"['2 shows the results on identifying all phrases - - chunking in conll2000 ( tjong kim  #TAUTHOR_TAG terminology.', 'the results show that']","['2 shows the results on identifying all phrases - - chunking in conll2000 ( tjong kim  #TAUTHOR_TAG terminology.', 'the results show that']","['2 shows the results on identifying all phrases - - chunking in conll2000 ( tjong kim  #TAUTHOR_TAG terminology.', 'the results show that']","['start by reporting the results in which we compare the full parser and the shallow parser on the "" clean "" wsj data.', 'table 2 shows the results on identifying all phrases - - chunking in conll2000 ( tjong kim  #TAUTHOR_TAG terminology.', 'the results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.', 'next, we compared the performance of the parsers on the task of identifying atomic phrases 2.', 'here, again, the shallow parser exhibits significantly better performance.', 'table 3 shows the results of extracting atomic phrases']",5
['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],"['of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']","['', 'would be chunked as follows  #AUTHOR_TAG : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']",0
['single phrases  #TAUTHOR_TAG and never to identify a collection of several phrases at'],['single phrases  #TAUTHOR_TAG and never to identify a collection of several phrases at'],"['single phrases  #TAUTHOR_TAG and never to identify a collection of several phrases at the same time, as we do here, we also trained']","['earlier versions of the snow based cscl were used only to identify single phrases  #TAUTHOR_TAG and never to identify a collection of several phrases at the same time, as we do here, we also trained and tested it under the exact conditions of conll - 2000  #AUTHOR_TAG to compare it to other shallow parsers.', 'table 1 shows that it ranks among the top shallow parsers evaluated there 1']",2
['on shallow parsing was inspired by psycholinguistics arguments  #TAUTHOR_TAG that suggest'],['on shallow parsing was inspired by psycholinguistics arguments  #TAUTHOR_TAG that suggest'],['on shallow parsing was inspired by psycholinguistics arguments  #TAUTHOR_TAG that suggest'],"['on shallow parsing was inspired by psycholinguistics arguments  #TAUTHOR_TAG that suggest that in many scenarios ( e. g., conversational ) full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint']",0
"['michael collins  #TAUTHOR_TAG - - one of the most accurate full parsers around.', '']","['michael collins  #TAUTHOR_TAG - - one of the most accurate full parsers around.', '']","['michael collins  #TAUTHOR_TAG - - one of the most accurate full parsers around.', 'it represents']","['perform our comparison using two state - ofthe - art parsers.', 'for the full parser, we use the one developed by michael collins  #TAUTHOR_TAG - - one of the most accurate full parsers around.', 'it represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'after that, it will choose the candidate parse tree with the highest probability as output.', 'the experiments use the version that was trained ( by collins ) on sections 02 - 21 of the penn treebank.', 'the reported results for the full parse tree ( on section 23 ) are recall / precision of 88. 1 / 87. 5  #AUTHOR_TAG']",5
['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],"['of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']","['', 'would be chunked as follows  #AUTHOR_TAG : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']",0
"['parser  #TAUTHOR_TAG.', 'snow  #AUTHOR_TAG is a multi - class']","['parser  #TAUTHOR_TAG.', 'snow  #AUTHOR_TAG is a multi - class']","['shallow parser used is the snow - based cscl parser  #TAUTHOR_TAG.', 'snow  #AUTHOR_TAG is a multi - class classifier']","['shallow parser used is the snow - based cscl parser  #TAUTHOR_TAG.', 'snow  #AUTHOR_TAG is a multi - class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large, of which nlp is a principal example.', 'it works by learning a sparse network of linear functions over a pre - defined or incrementally learned feature space.', 'typically, snow is used as a classifier, and predicts using a winner - take - all mechanism over the activation value of the target classes.', 'however, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'indeed, in cscl ( constraint satisfaction with classifiers ), snow is used to learn several different classifiers - each detects the beginning or end of a phrase of some type ( noun phrase, verb phrase, etc. ).', ""the outcomes of these classifiers are then combined in a way that satisfies some constraints - non - overlapping constraints in this case - using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes""]",5
"['parser  #AUTHOR_TAG.', 'snow  #TAUTHOR_TAG is a multi - class']","['parser  #AUTHOR_TAG.', 'snow  #TAUTHOR_TAG is a multi - class']","['shallow parser used is the snow - based cscl parser  #AUTHOR_TAG.', 'snow  #TAUTHOR_TAG is a multi - class classifier']","['shallow parser used is the snow - based cscl parser  #AUTHOR_TAG.', 'snow  #TAUTHOR_TAG is a multi - class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large, of which nlp is a principal example.', 'it works by learning a sparse network of linear functions over a pre - defined or incrementally learned feature space.', 'typically, snow is used as a classifier, and predicts using a winner - take - all mechanism over the activation value of the target classes.', 'however, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'indeed, in cscl ( constraint satisfaction with classifiers ), snow is used to learn several different classifiers - each detects the beginning or end of a phrase of some type ( noun phrase, verb phrase, etc. ).', ""the outcomes of these classifiers are then combined in a way that satisfies some constraints - non - overlapping constraints in this case - using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes""]",5
['as follows ( tjong kim  #TAUTHOR_TAG :'],['as follows ( tjong kim  #TAUTHOR_TAG :'],['as follows ( tjong kim  #TAUTHOR_TAG :'],"['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text  #AUTHOR_TAG.', 'a lot of recent work on shallow parsing has been influenced by abneys work  #AUTHOR_TAG, who has suggested to chunk sentences to base level phrases.', 'for example, the sentence he reckons the current account deficit will narrow to only $ 1. 8 billion in september.', 'would be chunked as follows ( tjong kim  #TAUTHOR_TAG : [ np he ] [ vp reckons ] [ np the current account deficit ] [ vp will narrow ] [ pp to ] [ np only $ 1. 8 billion ] [ pp in ] [ np september ]']",0
['and text summarization  #TAUTHOR_TAG'],['and text summarization  #TAUTHOR_TAG'],"[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization  #TAUTHOR_TAG.', 'second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'if all that is available is a collection of sentences annotated for nps, it can be used to']","[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization  #TAUTHOR_TAG.', 'second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'if all that is available is a collection of sentences annotated for nps, it can be used to produce this level of analysis.', 'this can be augmented later if more information is available.', 'finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low - sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes']",0
['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],"['of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']","['', 'would be chunked as follows  #AUTHOR_TAG : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']",0
['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],"['of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']","['', 'would be chunked as follows  #AUTHOR_TAG : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']",0
"['of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #TAUTHOR_TAG, significant progress has been']","['of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #TAUTHOR_TAG, significant progress has been']","['of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #TAUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #AUTHOR_TAG']","['', 'would be chunked as follows  #AUTHOR_TAG : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #TAUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #AUTHOR_TAG']",0
['single phrases  #TAUTHOR_TAG and never to identify a collection of several phrases at'],['single phrases  #TAUTHOR_TAG and never to identify a collection of several phrases at'],"['single phrases  #TAUTHOR_TAG and never to identify a collection of several phrases at the same time, as we do here, we also trained']","['earlier versions of the snow based cscl were used only to identify single phrases  #TAUTHOR_TAG and never to identify a collection of several phrases at the same time, as we do here, we also trained and tested it under the exact conditions of conll - 2000  #AUTHOR_TAG to compare it to other shallow parsers.', 'table 1 shows that it ranks among the top shallow parsers evaluated there 1']",2
"[' #TAUTHOR_TAG wall street journal data, sections 02 - 21.', 'to train the cscl shallow parser we had first to convert the wsj data to a flat format that directly provides the phrase annotations.', 'this is done using the "" chunklink']","['was done on the penn treebank  #TAUTHOR_TAG wall street journal data, sections 02 - 21.', 'to train the cscl shallow parser we had first to convert the wsj data to a flat format that directly provides the phrase annotations.', 'this is done using the "" chunklink "" program provided for']","['was done on the penn treebank  #TAUTHOR_TAG wall street journal data, sections 02 - 21.', 'to train the cscl shallow parser we had first to convert the wsj data to a flat format that directly provides the phrase annotations.', 'this is done using the "" chunklink "" program provided for']","['was done on the penn treebank  #TAUTHOR_TAG wall street journal data, sections 02 - 21.', 'to train the cscl shallow parser we had first to convert the wsj data to a flat format that directly provides the phrase annotations.', 'this is done using the "" chunklink "" program provided for conll - 2000 ( tjong kim sang and  #AUTHOR_TAG']",5
"['##ag into hpsg  #TAUTHOR_TAG.', 'however, their method depended on translators intuitive analy - sis of']","['ltag into hpsg  #TAUTHOR_TAG.', 'however, their method depended on translators intuitive analy - sis of']","['##ag into hpsg  #TAUTHOR_TAG.', 'however, their method depended on translators intuitive analy - sis of the original grammar.', 'thus the transla - tion was manual and grammar dependent.', 'the manual translation demanded considerable efforts']","['##eisi et al. also translated ltag into hpsg  #TAUTHOR_TAG.', 'however, their method depended on translators intuitive analy - sis of the original grammar.', 'thus the transla - tion was manual and grammar dependent.', 'the manual translation demanded considerable efforts from the translator, and obscures the equiva - lence between the original and obtained gram - mars.', 'other works  #AUTHOR_TAG convert hpsg grammars into ltag grammars.', 'however, given the greater ex - pressive power of hpsg, it is impossible to con - vert an arbitrary hpsg grammar into an ltag grammar.', 'therefore, a conversion from hpsg into ltag often requires some restrictions on the hpsg grammar to suppress its generative capac - ity.', 'thus, the conversion loses the equivalence of the grammars, and we cannot gain the above ad - vantages']",1
"['models  #TAUTHOR_TAG, and ones on programming / grammar - development environ -  #AUTHOR_TAG.', 'these works are restricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['disambiguation models  #TAUTHOR_TAG, and ones on programming / grammar - development environ -  #AUTHOR_TAG.', 'these works are restricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['disambiguation models  #TAUTHOR_TAG, and ones on programming / grammar - development environ -  #AUTHOR_TAG.', 'these works are restricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['have been many studies on parsing techniques  #AUTHOR_TAG, ones on disambiguation models  #TAUTHOR_TAG, and ones on programming / grammar - development environ -  #AUTHOR_TAG.', 'these works are restricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']",0
"['ltag parser  #TAUTHOR_TAG.', 'this']","['ltag parser  #TAUTHOR_TAG.', 'this']","['the xtag english grammar ( the xtag  #AUTHOR_TAG, which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser  #TAUTHOR_TAG.', 'this result implies that parsing techniques']","['this paper, we show that the strongly equivalent grammars enable the sharing of "" parsing techniques "", which are dependent on each computational framework and have never been shared among hpsg and ltag communities.', 'we apply our system to the latest version of the xtag english grammar ( the xtag  #AUTHOR_TAG, which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser  #TAUTHOR_TAG.', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the grammar conversion enables us to share hpsg parsing techniques in ltag parsing']",1
"['parser  #TAUTHOR_TAG, c + + implementation of the two - phase']","['hpsg parser  #TAUTHOR_TAG, c + + implementation of the two - phase']","['feature unification ( phase 2 ).', 'tnt refers to the hpsg parser  #TAUTHOR_TAG, c + + implementation of the two - phase parsing algorithm']","['2 shows the average parsing time with the ltag and hpsg parsers.', 'in table 2, lem refers to the ltag parser, ansi c implementation of the two - phase parsing algorithm that performs the head corner parsing ( van  #AUTHOR_TAG without features ( phase 1 ), and then executes feature unification ( phase 2 ).', 'tnt refers to the hpsg parser  #TAUTHOR_TAG, c + + implementation of the two - phase parsing algorithm that performs filtering with a compiled cfg ( phase 1 ) and then executes feature unification ( phase 2 ).', '']",1
"['( hpsg )  #TAUTHOR_TAG by a method of grammar conversion.', 'the rental system automatically']","['( hpsg )  #TAUTHOR_TAG by a method of grammar conversion.', 'the rental system automatically']","['head - driven phrase structure grammar ( hpsg )  #TAUTHOR_TAG by a method of grammar conversion.', 'the rental system automatically']","['paper describes an approach for sharing resources in various grammar formalisms such as feature - based lexicalized tree adjoining grammar ( fb - ltag1 ) ( vijay -  #AUTHOR_TAG vijay -  #AUTHOR_TAG and head - driven phrase structure grammar ( hpsg )  #TAUTHOR_TAG by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a strongly equivalent hpsg - style grammar.', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the ltag grammars and lexicons in hpsg applications.', 'our system can reduce considerable workload to develop a huge resource ( grammars and lexicons ) from scratch']",0
['##ag  #TAUTHOR_TAG is an extension of the ltag formalism'],"['symbol u ( figure 4 ).', 'fbltag  #TAUTHOR_TAG is an extension of the ltag formalism']","['##ag  #TAUTHOR_TAG is an extension of the ltag formalism.', 'in fb - ltag,']","['', 'fbltag  #TAUTHOR_TAG is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag  #AUTHOR_TAG.', 'the xtag group  #AUTHOR_TAG at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', 'development of a large - scale french grammar ( abeille and  #AUTHOR_TAG has also started at the university of pennsylvania and is expanded at university of paris 7']",0
"[' #TAUTHOR_TAG.', 'these works are']","[' #TAUTHOR_TAG.', 'these works are re - stricted to each closed community, and the rela - tion between them is not well discussed.', 'investi - gating the relation will be apparently valuable for both communities']","[' #TAUTHOR_TAG.', 'these works are']","[' #TAUTHOR_TAG.', 'these works are re - stricted to each closed community, and the rela - tion between them is not well discussed.', 'investi - gating the relation will be apparently valuable for both communities']",0
['##1 )  #TAUTHOR_TAG'],[')  #TAUTHOR_TAG'],['##1 )  #TAUTHOR_TAG'],"['paper describes an approach for sharing resources in various grammar formalisms such as feature - based lexicalized tree adjoining grammar ( fb - ltag1 )  #TAUTHOR_TAG and head - driven phrase structure grammar ( hpsg )  #AUTHOR_TAG by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a strongly equivalent hpsg - style grammar.', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the ltag grammars and lexicons in hpsg applications.', 'our system can reduce considerable workload to develop a huge resource ( grammars and lexicons ) from scratch']",0
['##ag  #TAUTHOR_TAG is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - figure 6 : parsing with an hpsg'],['##ag  #TAUTHOR_TAG is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - figure 6 : parsing with an hpsg'],['##ag  #TAUTHOR_TAG is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - figure 6 : parsing with an hpsg'],['##ag  #TAUTHOR_TAG is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - figure 6 : parsing with an hpsg'],0
"['verbmobil project  #TAUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #AUTHOR_TAG, which is used in a high -']","['verbmobil project  #TAUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #AUTHOR_TAG, which is used in a high - accuracy japanese dependency analyzer  #AUTHOR_TAG']","['in the verbmobil project  #TAUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #AUTHOR_TAG, which is used in a high - accuracy japanese dependency analyzer  #AUTHOR_TAG']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project  #AUTHOR_TAG.', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project  #TAUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #AUTHOR_TAG, which is used in a high - accuracy japanese dependency analyzer  #AUTHOR_TAG']",0
"['verbmobil project  #AUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #TAUTHOR_TAG, which is used in a high - accuracy japanese dependency analyzer  #AUTHOR_TAG']","['verbmobil project  #AUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #TAUTHOR_TAG, which is used in a high - accuracy japanese dependency analyzer  #AUTHOR_TAG']","['in the verbmobil project  #AUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #TAUTHOR_TAG, which is used in a high - accuracy japanese dependency analyzer  #AUTHOR_TAG']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project  #AUTHOR_TAG.', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project  #AUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #TAUTHOR_TAG, which is used in a high - accuracy japanese dependency analyzer  #AUTHOR_TAG']",0
"['', 'other works  #TAUTHOR_TAG convert hpsg grammars into ltag grammars.', 'however, given the greater expressive power of hpsg, it is impossible to convert an arbitrary hpsg grammar into an ltag grammar.', 'therefore, a conversion from hpsg into ltag often requires some restrictions on the hpsg grammar to suppress its generative capacity.', '']","['obscures the equivalence between the original and obtained grammars.', 'other works  #TAUTHOR_TAG convert hpsg grammars into ltag grammars.', 'however, given the greater expressive power of hpsg, it is impossible to convert an arbitrary hpsg grammar into an ltag grammar.', 'therefore, a conversion from hpsg into ltag often requires some restrictions on the hpsg grammar to suppress its generative capacity.', 'thus, the']","['', 'other works  #TAUTHOR_TAG convert hpsg grammars into ltag grammars.', 'however, given the greater expressive power of hpsg, it is impossible to convert an arbitrary hpsg grammar into an ltag grammar.', 'therefore, a conversion from hpsg into ltag often requires some restrictions on the hpsg grammar to suppress its generative capacity.', '']","['', 'the derivation translator module takes hpsg parse  #AUTHOR_TAG.', ""however, their method depended on translator's intuitive analysis of the original grammar."", 'thus the translation was manual and grammar dependent.', 'the manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars.', 'other works  #TAUTHOR_TAG convert hpsg grammars into ltag grammars.', 'however, given the greater expressive power of hpsg, it is impossible to convert an arbitrary hpsg grammar into an ltag grammar.', 'therefore, a conversion from hpsg into ltag often requires some restrictions on the hpsg grammar to suppress its generative capacity.', 'thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages']",1
['japanese dependency analyzer  #TAUTHOR_TAG'],"['verbmobil project  #AUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #AUTHOR_TAG, which is used in a high - accuracy japanese dependency analyzer  #TAUTHOR_TAG']","['in the verbmobil project  #AUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #AUTHOR_TAG, which is used in a high - accuracy japanese dependency analyzer  #TAUTHOR_TAG']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project  #AUTHOR_TAG.', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project  #AUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #AUTHOR_TAG, which is used in a high - accuracy japanese dependency analyzer  #TAUTHOR_TAG']",0
"[', each of which is described with typed feature structures  #TAUTHOR_TAG.', 'a']","[', each of which is described with typed feature structures  #TAUTHOR_TAG.', 'a']","['and id grammar rules, each of which is described with typed feature structures  #TAUTHOR_TAG.', '']","['hpsg grammar consists of lexical entries and id grammar rules, each of which is described with typed feature structures  #TAUTHOR_TAG.', 'a lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category.', 'an id grammar rule represents a relation between a mother and its daughters, and is independent of lexical characteristics.', 'figure 6 illustrates an example of bottom - up parsing with an hpsg grammar.', 'first, lexical entries for "" can "" and "" run "" are unified respectively with the daughter feature structures']",0
"['xtag research  #TAUTHOR_TAG, which is a large -']","['xtag research  #TAUTHOR_TAG, which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hpsg']","['the xtag english grammar ( the xtag research  #TAUTHOR_TAG, which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser.', 'this result implies that parsing techniques']","['this paper, we show that the strongly equivalent grammars enable the sharing of "" parsing techniques "", which are dependent on each computational framework and have never been shared among hpsg and ltag communities.', 'we apply our system to the latest version of the xtag english grammar ( the xtag research  #TAUTHOR_TAG, which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser.', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the grammar conversion enables us to share hpsg parsing techniques in ltag parsing']",5
"['', 'the xtag group  #TAUTHOR_TAG']","['', 'the xtag group  #TAUTHOR_TAG']","['', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag  #AUTHOR_TAG.', 'the xtag group  #TAUTHOR_TAG']","['', 'fb - ltag ( vijay -  #AUTHOR_TAG vijay -  #AUTHOR_TAG is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag  #AUTHOR_TAG.', 'the xtag group  #TAUTHOR_TAG at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', 'development of a large - scale french grammar ( abeille and  #AUTHOR_TAG has also started at the university of pennsylvania and is expanded at university of paris 7']",0
"['hpsg - style grammar  #TAUTHOR_TAG.', 'strong']","['hpsg - style grammar  #TAUTHOR_TAG.', 'strong']","['a strongly equivalent hpsg - style grammar  #TAUTHOR_TAG.', 'strong equivalence means that both grammars']","['paper describes an approach for sharing resources in various grammar formalisms such as feature - based lexicalized tree adjoin - ing grammar ( fb - ltag 1 ) ( vijay -  #AUTHOR_TAG vijay -  #AUTHOR_TAG and head - driven phrase structure grammar ( hpsg )  #AUTHOR_TAG by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a strongly equivalent hpsg - style grammar  #TAUTHOR_TAG.', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the ltag grammars and lexicons in hpsg applications.', 'our system can reduce considerable workload to develop a huge resource ( grammars and lexicons ) from scratch']",0
"['ltag parser  #TAUTHOR_TAG, ansi c implementation of the two - phase']","['ltag parser  #TAUTHOR_TAG, ansi c implementation of the two - phase']","['2 shows the average parsing time with the ltag and hpsg parsers.', 'in table 2, lem refers to the ltag parser  #TAUTHOR_TAG, ansi c implementation of the two - phase parsing algorithm']","['2 shows the average parsing time with the ltag and hpsg parsers.', 'in table 2, lem refers to the ltag parser  #TAUTHOR_TAG, ansi c implementation of the two - phase parsing algorithm that performs the head corner parsing ( van  #AUTHOR_TAG without features ( phase 1 ), and then executes feature unification ( phase 2 ).', 'tnt refers to the hpsg parser, c + + implementation of the two - phase parsing algorithm that performs filtering with a compiled cfg ( phase 1 ) and then executes feature unification ( phase 2 ).', '']",1
['##ag  #TAUTHOR_TAG is an extension of the ltag formalism'],"['symbol u ( figure 4 ).', 'fbltag  #TAUTHOR_TAG is an extension of the ltag formalism']","['##ag  #TAUTHOR_TAG is an extension of the ltag formalism.', 'in fb - ltag,']","['', 'fbltag  #TAUTHOR_TAG is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag  #AUTHOR_TAG.', 'the xtag group  #AUTHOR_TAG at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', 'development of a large - scale french grammar ( abeille and  #AUTHOR_TAG has also started at the university of pennsylvania and is expanded at university of paris 7']",0
"[') project  #TAUTHOR_TAG.', 'in practical context, german, english, and japanese hpsg - based grammars are developed']","['lingo ) project  #TAUTHOR_TAG.', 'in practical context, german, english, and japanese hpsg - based grammars are developed']","[') project  #TAUTHOR_TAG.', 'in practical context, german, english, and japanese hpsg - based grammars are developed']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project  #TAUTHOR_TAG.', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project  #AUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #AUTHOR_TAG, which is used in a high - accuracy japanese dependency analyzer  #AUTHOR_TAG']",0
"['models  #AUTHOR_TAG, and ones on programming / grammar - development environment  #TAUTHOR_TAG.', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['disambiguation models  #AUTHOR_TAG, and ones on programming / grammar - development environment  #TAUTHOR_TAG.', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['disambiguation models  #AUTHOR_TAG, and ones on programming / grammar - development environment  #TAUTHOR_TAG.', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['concern is, however, not limited to the sharing of grammars and lexicons.', 'strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques  #AUTHOR_TAG, ones on disambiguation models  #AUTHOR_TAG, and ones on programming / grammar - development environment  #TAUTHOR_TAG.', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']",0
"['processing feature structure logic, and efficient hpsg parsers have already been built on this system  #TAUTHOR_TAG.', 'we applied our system to']","['processing feature structure logic, and efficient hpsg parsers have already been built on this system  #TAUTHOR_TAG.', 'we applied our system to']","['processing feature structure logic, and efficient hpsg parsers have already been built on this system  #TAUTHOR_TAG.', 'we applied our system to the xtag english grammar ( the xtag  #AUTHOR_TAG 3, which is a large - scale fb - ltag grammar for english.', 'the original and the obtained grammar generated exactly']","['rental system is implemented in lil - fes  #AUTHOR_TAG 2.', 'lilfes is one of the fastest inference engines for processing feature structure logic, and efficient hpsg parsers have already been built on this system  #TAUTHOR_TAG.', 'we applied our system to the xtag english grammar ( the xtag  #AUTHOR_TAG 3, which is a large - scale fb - ltag grammar for english.', 'the original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the atis corpus  #AUTHOR_TAG 6 ( the average length is 6. 32 words ).', 'this result empirically attested the strong equivalence of our algorithm']",0
"['share hpsg parsing techniques in ltag parsing.', 'another paper  #TAUTHOR_TAG describes the detailed analysis on the factor of the difference of parsing performance']","['share hpsg parsing techniques in ltag parsing.', 'another paper  #TAUTHOR_TAG describes the detailed analysis on the factor of the difference of parsing performance']","['share hpsg parsing techniques in ltag parsing.', 'another paper  #TAUTHOR_TAG describes the detailed analysis on the factor of the difference of parsing performance']","['', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the grammar conversion enables us to share hpsg parsing techniques in ltag parsing.', 'another paper  #TAUTHOR_TAG describes the detailed analysis on the factor of the difference of parsing performance']",0
['##7 sentences from the atis corpus  #TAUTHOR_TAG 6 ( the average length is 6'],"['number of derivation trees in the parsing experiment with 457 sentences from the atis corpus  #TAUTHOR_TAG 6 ( the average length is 6. 32 words ).', 'this result empirically attested the strong equivalence of our algorithm']",['##7 sentences from the atis corpus  #TAUTHOR_TAG 6 ( the average length is 6'],"['rental system is implemented in lil - fes  #AUTHOR_TAG 2.', 'lilfes is one of the fastest inference engines for processing feature structure logic, and efficient hpsg parsers have already been built on this system  #AUTHOR_TAG.', 'we applied our system to the xtag english grammar ( the xtag  #AUTHOR_TAG 3, which is a large - scale fb - ltag grammar for english.', 'the original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the atis corpus  #TAUTHOR_TAG 6 ( the average length is 6. 32 words ).', 'this result empirically attested the strong equivalence of our algorithm']",5
"[' #TAUTHOR_TAG 2.', '']","[' #TAUTHOR_TAG 2.', '']","['##lfes  #TAUTHOR_TAG 2.', '']","['rental system is implemented in lilfes  #TAUTHOR_TAG 2.', 'lilfes is one of the fastest inference engines for processing feature structure logic, and efficient hpsg parsers have already been built on this system  #AUTHOR_TAG.', 'we applied our system to the xtag english grammar ( the xtag  #AUTHOR_TAG 3, which is a large - scale fb - ltag grammar for english.', 'the original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the atis corpus  #AUTHOR_TAG 6 ( the average length is 6. 32 words ).', 'this result empirically attested the strong equivalence of our algorithm']",5
['##1 )  #TAUTHOR_TAG'],[')  #TAUTHOR_TAG'],['##1 )  #TAUTHOR_TAG'],"['paper describes an approach for sharing resources in various grammar formalisms such as feature - based lexicalized tree adjoining grammar ( fb - ltag1 )  #TAUTHOR_TAG and head - driven phrase structure grammar ( hpsg )  #AUTHOR_TAG by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a strongly equivalent hpsg - style grammar.', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the ltag grammars and lexicons in hpsg applications.', 'our system can reduce considerable workload to develop a huge resource ( grammars and lexicons ) from scratch']",0
"['models  #AUTHOR_TAG, and ones on programming / grammar - development  #TAUTHOR_TAG.', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['disambiguation models  #AUTHOR_TAG, and ones on programming / grammar - development  #TAUTHOR_TAG.', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['disambiguation models  #AUTHOR_TAG, and ones on programming / grammar - development  #TAUTHOR_TAG.', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['concern is, however, not limited to the sharing of grammars and lexicons.', 'strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques  #AUTHOR_TAG, ones on disambiguation models  #AUTHOR_TAG, and ones on programming / grammar - development  #TAUTHOR_TAG.', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']",0
"['formalism.', 'there have been many studies on parsing techniques  #TAUTHOR_TAG,']","['formalism.', 'there have been many studies on parsing techniques  #TAUTHOR_TAG,']","['concern is, however, not limited to the sharing of grammars and lexicons.', 'strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques  #TAUTHOR_TAG,']","['concern is, however, not limited to the sharing of grammars and lexicons.', 'strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques  #TAUTHOR_TAG, ones on disambiguation models  #AUTHOR_TAG, and ones on programming / grammar - development environment  #AUTHOR_TAG.', 'these works are restricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']",0
['grammar conversion from ltag to hpsg  #TAUTHOR_TAG is the core portion of the rental system'],['grammar conversion from ltag to hpsg  #TAUTHOR_TAG is the core portion of the rental system'],['grammar conversion from ltag to hpsg  #TAUTHOR_TAG is the core portion of the rental system'],['grammar conversion from ltag to hpsg  #TAUTHOR_TAG is the core portion of the rental system'],0
"['for english ( the xtag research  #TAUTHOR_TAG.', 'the xtag group  #AUTHOR_TAG']","['for english ( the xtag research  #TAUTHOR_TAG.', 'the xtag group  #AUTHOR_TAG']","['', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag research  #TAUTHOR_TAG.', 'the xtag group  #AUTHOR_TAG']","['', 'fb - ltag ( vijay -  #AUTHOR_TAG vijay -  #AUTHOR_TAG is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag research  #TAUTHOR_TAG.', 'the xtag group  #AUTHOR_TAG at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', 'development of a large - scale french grammar ( abeille and  #AUTHOR_TAG has also started at the university of pennsylvania and is expanded at university of paris 7']",0
"['xtag research  #TAUTHOR_TAG 3, which is a large -']","['xtag research  #TAUTHOR_TAG 3, which is a large - scale fb - ltag']","['( the xtag research  #TAUTHOR_TAG 3, which is a large - scale fb - ltag grammar for english.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser.', 'this result implies that parsing techniques']","['this paper, we show that the strongly equivalent grammars enable the sharing of "" parsing techniques "", which are dependent on each computational framework and have never been shared among hpsg and ltag communities.', 'we applied our system to the xtag english grammar ( the xtag research  #TAUTHOR_TAG 3, which is a large - scale fb - ltag grammar for english.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser.', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the grammar conversion enables us to share hpsg parsing techniques in ltag parsing']",5
"['document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some']","['document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some']","['document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries']","['has been some controversy, at least for simple stemmers  #AUTHOR_TAG, about the effectiveness of morphological analysis for document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories  #AUTHOR_TAG']",0
['of dederivation and decomposition  #TAUTHOR_TAG'],['of dederivation and decomposition  #TAUTHOR_TAG'],"['specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #TAUTHOR_TAG']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1'¡'denotes the string concatenation operator."", 'mers  #AUTHOR_TAG demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ;  #AUTHOR_TAG ekmekcioglu et al., 1995 ;  #AUTHOR_TAG.', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #TAUTHOR_TAG']",0
"['string concatenation operator.', 'mers  #TAUTHOR_TAG demonstr']","['string concatenation operator.', 'mers  #TAUTHOR_TAG demonstrably']","[""limited number of inflection patterns, lexicon - free general - purpose stem - 1'¡'denotes the string concatenation operator."", 'mers  #TAUTHOR_TAG demonstrably']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1'¡'denotes the string concatenation operator."", 'mers  #TAUTHOR_TAG demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ;  #AUTHOR_TAG ekmekcioglu et al., 1995 ;  #AUTHOR_TAG.', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #AUTHOR_TAG']",0
"['document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some']","['document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some']","['document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries']","['has been some controversy, at least for simple stemmers  #AUTHOR_TAG, about the effectiveness of morphological analysis for document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories  #AUTHOR_TAG']",0
"['string concatenation operator.', 'mers  #TAUTHOR_TAG demonstr']","['string concatenation operator.', 'mers  #TAUTHOR_TAG demonstrably']","[""limited number of inflection patterns, lexicon - free general - purpose stem - 1'¡'denotes the string concatenation operator."", 'mers  #TAUTHOR_TAG demonstrably']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1'¡'denotes the string concatenation operator."", 'mers  #TAUTHOR_TAG demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ;  #AUTHOR_TAG ekmekcioglu et al., 1995 ;  #AUTHOR_TAG.', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #AUTHOR_TAG']",0
"['[UNK] o, 1988 ;  #TAUTHOR_TAG,']","['[UNK] o, 1988 ;  #TAUTHOR_TAG,']","['##t [UNK] o, 1988 ;  #TAUTHOR_TAG,']","['alterations of a search term have a negative impact on the recall performance of an information retrieval ( ir ) system  #AUTHOR_TAG j [UNK] appinen and niemist [UNK] o, 1988 ;  #TAUTHOR_TAG, since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved.', ""in order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection ( e. g.,'search ed ','search ing') 1, derivation ( e. g.,'search er'or'search able') and composition ( e. g., german'blut hoch druck'['high blood pressure'] )."", ""the goal is to map all occurring morphological variants to some canonical base forme. g.,'search'in the examples from above""]",0
['of dederivation and decomposition  #TAUTHOR_TAG'],['of dederivation and decomposition  #TAUTHOR_TAG'],"['specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #TAUTHOR_TAG']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1'¡'denotes the string concatenation operator."", 'mers  #AUTHOR_TAG demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ;  #AUTHOR_TAG ekmekcioglu et al., 1995 ;  #AUTHOR_TAG.', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #TAUTHOR_TAG']",0
"['for simple stemmers  #TAUTHOR_TAG, about the effectiveness of morphological analysis']","['for simple stemmers  #TAUTHOR_TAG, about the effectiveness of morphological analysis']","['for simple stemmers  #TAUTHOR_TAG, about the effectiveness of morphological analysis']","['has been some controversy, at least for simple stemmers  #TAUTHOR_TAG, about the effectiveness of morphological analysis for document retrieval  #AUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories  #AUTHOR_TAG']",0
"[', but at high ones its precision decreases almost dramatically.', 'unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in  #TAUTHOR_TAG ) one can not really recommend this method']","[', but at high ones its precision decreases almost dramatically.', 'unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in  #TAUTHOR_TAG ) one can not really recommend this method']","[', but at high ones its precision decreases almost dramatically.', 'unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in  #TAUTHOR_TAG ) one can not really recommend this method']","[', but at high ones its precision decreases almost dramatically.', 'unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in  #TAUTHOR_TAG ) one can not really recommend this method']",1
['of dederivation and decomposition  #TAUTHOR_TAG'],['of dederivation and decomposition  #TAUTHOR_TAG'],"['specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #TAUTHOR_TAG']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1'¡'denotes the string concatenation operator."", 'mers  #AUTHOR_TAG demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ;  #AUTHOR_TAG ekmekcioglu et al., 1995 ;  #AUTHOR_TAG.', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #TAUTHOR_TAG']",0
['without access to such lexical repositories  #TAUTHOR_TAG'],['without access to such lexical repositories  #TAUTHOR_TAG'],['without access to such lexical repositories  #TAUTHOR_TAG'],"['has been some controversy, at least for simple stemmers  #AUTHOR_TAG, about the effectiveness of morphological analysis for document retrieval  #AUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories  #TAUTHOR_TAG']",0
"['relies on the vector space model  #TAUTHOR_TAG, with the cosine measure expressing the similarity']","['relies on the vector space model  #TAUTHOR_TAG, with the cosine measure expressing the similarity']","['assigns salience weights to terms and documents based on a simple tf - idf metric.', 'the retrieval process relies on the vector space model  #TAUTHOR_TAG, with the cosine measure expressing the similarity']","['unbiased evaluation of our approach, we used a home - grown search engine ( implemented in the python script language ).', 'it crawls text / html files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf - idf metric.', 'the retrieval process relies on the vector space model  #TAUTHOR_TAG, with the cosine measure expressing the similarity between a query and a document.', 'the search engine produces a ranked output of documents']",5
"['for simple stemmers  #TAUTHOR_TAG, about the effectiveness of morphological analysis']","['for simple stemmers  #TAUTHOR_TAG, about the effectiveness of morphological analysis']","['for simple stemmers  #TAUTHOR_TAG, about the effectiveness of morphological analysis']","['has been some controversy, at least for simple stemmers  #TAUTHOR_TAG, about the effectiveness of morphological analysis for document retrieval  #AUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories  #AUTHOR_TAG']",0
['attempt to cope adequately with medical free - texts in an ir setting  #TAUTHOR_TAG'],['attempt to cope adequately with medical free - texts in an ir setting  #TAUTHOR_TAG'],"['to as neo - classical compounding ( mc -  #AUTHOR_TAG.', 'while this is simply irrelevant for general - purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free - texts in an ir setting  #TAUTHOR_TAG']","[', medical terminology is characterized by a typical mix of latin and greek roots with the corresponding host language ( e. g., german ), often referred to as neo - classical compounding ( mc -  #AUTHOR_TAG.', 'while this is simply irrelevant for general - purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free - texts in an ir setting  #TAUTHOR_TAG']",0
"['( mesh,  #TAUTHOR_TAG ) are incorporated into our system.', 'alternatively, we']","['mappings of our synonym identifiers to a large medical thesaurus ( mesh,  #TAUTHOR_TAG ) are incorporated into our system.', 'alternatively, we']","['mappings of our synonym identifiers to a large medical thesaurus ( mesh,  #TAUTHOR_TAG ) are incorporated into our system.', 'alternatively, we may think of user - centered comparative studies  #AUTHOR_TAG']","['', 'it would be interesting to evaluate the retrieval effectiveness ( in terms of precision and recall ) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'this will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( mesh,  #TAUTHOR_TAG ) are incorporated into our system.', 'alternatively, we may think of user - centered comparative studies  #AUTHOR_TAG']",3
"['document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some']","['document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some']","['document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries']","['has been some controversy, at least for simple stemmers  #AUTHOR_TAG, about the effectiveness of morphological analysis for document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories  #AUTHOR_TAG']",0
"['[UNK] o, 1988 ;  #TAUTHOR_TAG ; ekmekc [UNK] ioglu']","['( j [UNK] appinen and niemist [UNK] o, 1988 ;  #TAUTHOR_TAG ; ekmekc [UNK] ioglu']","['retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( j [UNK] appinen and niemist [UNK] o, 1988 ;  #TAUTHOR_TAG ; ekmekc [UNK] ioglu et al.,']","['efforts required for performing morphologi - cal analysis vary from language to language.', 'for english, known for its limited number of inflection patterns, lexicon - free general - purpose stemmers  #AUTHOR_TAG demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( j [UNK] appinen and niemist [UNK] o, 1988 ;  #TAUTHOR_TAG ; ekmekc [UNK] ioglu et al., 1995 ;  #AUTHOR_TAG.', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #AUTHOR_TAG']",0
['of dederivation and decomposition  #TAUTHOR_TAG'],['of dederivation and decomposition  #TAUTHOR_TAG'],"['specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #TAUTHOR_TAG']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1'¡'denotes the string concatenation operator."", 'mers  #AUTHOR_TAG demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ;  #AUTHOR_TAG ekmekcioglu et al., 1995 ;  #AUTHOR_TAG.', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #TAUTHOR_TAG']",0
['of dederivation and decomposition  #TAUTHOR_TAG'],['of dederivation and decomposition  #TAUTHOR_TAG'],"['specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #TAUTHOR_TAG']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1'¡'denotes the string concatenation operator."", 'mers  #AUTHOR_TAG demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ;  #AUTHOR_TAG ekmekcioglu et al., 1995 ;  #AUTHOR_TAG.', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #TAUTHOR_TAG']",0
['think of user - centered comparative studies  #TAUTHOR_TAG'],['think of user - centered comparative studies  #TAUTHOR_TAG'],"['.', 'alternatively, we may think of user - centered comparative studies  #TAUTHOR_TAG']","['', 'it would be interesting to evaluate the retrieval effectiveness ( in terms of precision and recall ) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'this will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( mesh, ( nlm, 2001 ) ) are incorporated into our system.', 'alternatively, we may think of user - centered comparative studies  #TAUTHOR_TAG']",3
['without access to such lexical repositories  #TAUTHOR_TAG'],['without access to such lexical repositories  #TAUTHOR_TAG'],['without access to such lexical repositories  #TAUTHOR_TAG'],"['has been some controversy, at least for simple stemmers  #AUTHOR_TAG, about the effectiveness of morphological analysis for document retrieval  #AUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories  #TAUTHOR_TAG']",0
"['as proposed for french  #TAUTHOR_TAG, turns out to be infeasible, at']","['as proposed for french  #TAUTHOR_TAG, turns out to be infeasible, at']","['as proposed for french  #TAUTHOR_TAG, turns out to be infeasible, at']","['one may argue that single - word compounds are quite rare in english ( which is not the case in the medical domain either ), this is certainly not true for german and other basically agglutinative languages known for excessive single - word nominal compounding.', 'this problem becomes even more pressing for technical sublanguages, such as medical german ( e. g., blut druck mess gera _ _ t translates to device for measuring blood pressure ).', 'the problem one faces from an ir point of view is that besides fairly standardized nominal compounds, which already form a regular part of the sublanguage proper, a myriad of ad hoc compounds are formed on the fly which cannot be anticipated when formulating a retrieval query though they appear in relevant documents.', 'hence, enumerating morphological variants in a semi - automatically generated lexicon, such as proposed for french  #TAUTHOR_TAG, turns out to be infeasible, at least for german and related languages']",0
"[';  #TAUTHOR_TAG.', 'when']","[';  #TAUTHOR_TAG.', 'when']","[';  #TAUTHOR_TAG.', 'when it comes']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1'¡'denotes the string concatenation operator."", 'mers  #AUTHOR_TAG demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( j [UNK] appinen and niemist [UNK] o, 1988 ;  #AUTHOR_TAG ekmekc [UNK] ioglu et al., 1995 ;  #TAUTHOR_TAG.', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #AUTHOR_TAG']",0
['ir ) system  #TAUTHOR_TAG ; j [UNK] appinen and nie'],['ir ) system  #TAUTHOR_TAG ; j [UNK] appinen and'],['ir ) system  #TAUTHOR_TAG ; j [UNK] appinen and nie'],"['alterations of a search term have a negative impact on the recall performance of an information retrieval ( ir ) system  #TAUTHOR_TAG ; j [UNK] appinen and niemist [UNK] o, 1988 ;  #AUTHOR_TAG, since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved.', ""in order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection ( e. g.,'search ed ','search ing') 1, derivation ( e. g.,'search er'or'search able') and composition ( e. g., german'blut hoch druck'['high blood pressure'] )."", ""the goal is to map all occurring morphological variants to some canonical base forme. g.,'search'in the examples from above""]",0
"['##ing  #TAUTHOR_TAG.', 'while this is simply irrelevant for general - purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free - texts in an ir setting  #AUTHOR_TAG']","['to as neo - classical compounding  #TAUTHOR_TAG.', 'while this is simply irrelevant for general - purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free - texts in an ir setting  #AUTHOR_TAG']","['to as neo - classical compounding  #TAUTHOR_TAG.', 'while this is simply irrelevant for general - purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free - texts in an ir setting  #AUTHOR_TAG']","[', medical terminology is characterized by a typical mix of latin and greek roots with the corresponding host language ( e. g., german ), often referred to as neo - classical compounding  #TAUTHOR_TAG.', 'while this is simply irrelevant for general - purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free - texts in an ir setting  #AUTHOR_TAG']",0
"[',  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et']","[',  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","[',  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']",0
"[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG']",0
"['( sstc )  #TAUTHOR_TAG will be introduced to capture a natural language text,']","['( sstc )  #TAUTHOR_TAG will be introduced to capture a natural language text,']","['( sstc )  #TAUTHOR_TAG will be introduced to capture a natural language text,']","['this paper, a flexible annotation schema called structured string - tree correspondence ( sstc )  #TAUTHOR_TAG will be introduced to capture a natural language text, its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two.', 'the correspondence between the string and its associated representation tree structure is defined in terms of the sub - correspondence between parts of the string ( substrings ) and parts of the tree structure ( subtrees ), which can be interpreted for both analysis and generation.', 'such correspondence is defined in a way that is able to handle some non - standard cases ( e. g.', 'non - projective correspondence )']",0
"['. g. crossed dependencies  #TAUTHOR_TAG.', 'crossed dependencies  #AUTHOR_TAG']","['the treatment of linguistic phenomena, which are non - standard, e. g. crossed dependencies  #TAUTHOR_TAG.', 'crossed dependencies  #AUTHOR_TAG']","['. g. crossed dependencies  #TAUTHOR_TAG.', 'crossed dependencies  #AUTHOR_TAG']","['sstc is a general structure that can associate an arbitrary tree structure to string in a language as desired by the annotator to be the interpretation structure of the string, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be nonprojective  #AUTHOR_TAG.', 'these features are very much desired in the design of an annotation scheme, in particular for the treatment of linguistic phenomena, which are non - standard, e. g. crossed dependencies  #TAUTHOR_TAG.', 'crossed dependencies  #AUTHOR_TAG']",0
['( sstc ) was introduced in  #TAUTHOR_TAG to record'],['( sstc ) was introduced in  #TAUTHOR_TAG to record'],"['( sstc ) was introduced in  #TAUTHOR_TAG to record the string of terms,']","['this section, we stress on the fact that in order to describe natural language ( nl ) in a natural manner, three distinct components need to be expressed by the linguistic formalisms ; namely, the text, its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two.', 'actually, nl is not only a correspondence between different representation levels, as stressed by mtt postulates, but also a sub - correspondence between them.', 'for instance, between the string in a language and its representation tree structure, it is important to specify the sub - correspondences between parts of the string ( substrings ) and parts of the tree structure ( subtrees ), which can be interpreted for both analysis and generation in nlp.', 'it is well known that many linguistic constructions are not projective ( e. g.', 'scrambling, cross serial dependencies, etc. ).', 'hence, it is very much desired to define the correspondence in a way to be able to handle the non - standard cases ( e. g.', 'non - projective correspondence ), see figure 1.', 'towards this aim, a flexible annotation structure called structured string - tree correspondence ( sstc ) was introduced in  #TAUTHOR_TAG to record the string of terms, its associated representation structure and the mapping between the two, which is expressed by the sub - correspondences recorded as part of a sstc']",0
"['the ebmt applications.', ' #TAUTHOR_TAG presented an approach for constructing a bkb based on the s - sstc']","['the ebmt applications.', ' #TAUTHOR_TAG presented an approach for constructing a bkb based on the s - sstc']","['the ebmt applications.', ' #TAUTHOR_TAG presented an approach for constructing a bkb based on the s - sstc']","[', what has so far been lacking is a schema or a framework to annotate and express such extracted lexical and structural correspondences in a flexible and powerful manner.', ""the proposed s - sstc annotation schema can fulfill this need, and it is flexible enough to handle different type of relations that may happen between different languages'structures."", 's - sstc very well suited for the construction of a bkb, which is needed for the ebmt applications.', ' #TAUTHOR_TAG presented an approach for constructing a bkb based on the s - sstc']",0
"[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, ( kaji et']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']",0
"['of a non - tal  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'as a result,  #AUTHOR_TAG propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages,']","['of a non - tal  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'as a result,  #AUTHOR_TAG propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages,']","['( tag ) introduced by  #AUTHOR_TAG to characterize correspondences between tree adjoining languages.', 'considering the original definition of s - tags, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'it allows the construction of a non - tal  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'as a result,  #AUTHOR_TAG propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages,']","['', 's - tag is a variant of tree adjoining grammar ( tag ) introduced by  #AUTHOR_TAG to characterize correspondences between tree adjoining languages.', 'considering the original definition of s - tags, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'it allows the construction of a non - tal  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'as a result,  #AUTHOR_TAG propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages, which cannot be expressed by is - tag, figure 3 illustrates some examples  #AUTHOR_TAG']",0
"['as a correspondence between meanings and texts  #TAUTHOR_TAG.', 'the mtt']","['as a correspondence between meanings and texts  #TAUTHOR_TAG.', 'the mtt']","[') 1 point of view, natural language ( nl ) is considered as a correspondence between meanings and texts  #TAUTHOR_TAG.', 'the mtt point of']","['the meaning - text theory ( mtt ) 1 point of view, natural language ( nl ) is considered as a correspondence between meanings and texts  #TAUTHOR_TAG.', 'the mtt point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community']",0
"['., 1992 ), and example - base machine translation ebmt3  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']",0
"['problem of using synchronous formalisms to define translations between languages ( e. g.  #TAUTHOR_TAG cases ).', ' #AUTHOR_TAG cases ).', '']","['problem of using synchronous formalisms to define translations between languages ( e. g.  #TAUTHOR_TAG cases ).', ' #AUTHOR_TAG cases ).', '']","['for synchronized formalisms.', 'in this section, we will describe some example cases, which are drawn from the problem of using synchronous formalisms to define translations between languages ( e. g.  #TAUTHOR_TAG cases ).', ' #AUTHOR_TAG cases ).', '']","['mentioned earlier, there are some non - standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'in this section, we will describe some example cases, which are drawn from the problem of using synchronous formalisms to define translations between languages ( e. g.  #TAUTHOR_TAG cases ).', ' #AUTHOR_TAG cases ).', 'due to lack of space we will only brief on some of these non - standard cases without going into the details']",0
"[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, ( kaji et']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']",0
"['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures  #TAUTHOR_TAG, such as']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures  #TAUTHOR_TAG, such as']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures  #TAUTHOR_TAG, such as']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures  #TAUTHOR_TAG, such as the relation between syntax and semantic']",0
"[',  #AUTHOR_TAG, ( al  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, etc., where s - sstc can be used to represent the entries of the bkb or']","[',  #AUTHOR_TAG, ( al  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, etc., where s - sstc can be used to represent the entries of the bkb or']","[',  #AUTHOR_TAG, ( al  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, etc., where s - sstc can be used to represent the entries of the bkb or']","['are governed by the following constraints :.', 'this means allowing one - to - one, one - to - many and many - to - many, but the mappings do not overlap.', 'note that these constraints can be used to license only the linguistically meaningful synchronous correspondences between the two sstcs of the s - sstc ( i. e. between the two languages ).', ""for instance, when building translation units in ebmt approaches  #AUTHOR_TAG,  #AUTHOR_TAG, ( al  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, etc., where s - sstc can be used to represent the entries of the bkb or when s - sstc used as an annotation schema to find the translation correspondences ( lexical and structural correspondences ) for transferrules'extraction from parallel parsed""]",0
"['of transfer grammar learning  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et']","['of transfer grammar learning  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","['of transfer grammar learning  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e.', 'synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']",0
"['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures  #TAUTHOR_TAG']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures  #TAUTHOR_TAG']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures  #TAUTHOR_TAG']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures  #TAUTHOR_TAG']",0
"[', see  #TAUTHOR_TAG']","['"" he picks the box up "".', 'for more details on the proprieties of sstc, see  #TAUTHOR_TAG']","[', see  #TAUTHOR_TAG']","['case depicted in figure 2, describes how the sstc structure treats some non - standard linguistic phenomena.', 'the particle "" up "" is featurised into the verb "" pick "" and in discontinuous manner ( e. g.', '"" up "" ( 4 - 5 ) in "" pick - up "" ( 1 - 2 + 4 - 5 ) ) in the sentence "" he picks the box up "".', 'for more details on the proprieties of sstc, see  #TAUTHOR_TAG']",0
"['to snode of 2 these definitions are based on the discussion in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'and its dependency']","['substring to snode of 2 these definitions are based on the discussion in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'and its dependency']","['to snode of 2 these definitions are based on the discussion in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'and its dependency tree together with the correspondences between substrings of the sentence and']","['2 illustrates the sentence "" john picks the box up "" with its corresponding sstc.', 'it contains a nonprojective correspondence.', 'an interval is assigned to each word in the sentence, i. e. ( 0 - 1 ) for "" john "", ( 1 - 2 ) for "" picks "", ( 2 - 3 ) for "" the "", ( 3 - 4 ) for "" box "" and ( 4 - 5 ) for "" up "".', 'a substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to snode of 2 these definitions are based on the discussion in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree']",5
"['., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, ( al -  #AUTHOR_TAG']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, ( al -  #AUTHOR_TAG']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, ( al -  #AUTHOR_TAG']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, ( al -  #AUTHOR_TAG']",0
"['- tal  #TAUTHOR_TAG,  #AUTHOR_TAG.', 'as a result,  #AUTHOR_TAG propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages,']","['of a non - tal  #TAUTHOR_TAG,  #AUTHOR_TAG.', 'as a result,  #AUTHOR_TAG propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages,']","['the construction of a non - tal  #TAUTHOR_TAG,  #AUTHOR_TAG.', 'as a result,  #AUTHOR_TAG propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages,']","['', 's - tag is a variant of tree adjoining grammar ( tag ) introduced by  #AUTHOR_TAG to characterize correspondences between tree adjoining languages.', 'considering the original definition of s - tags, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'it allows the construction of a non - tal  #TAUTHOR_TAG,  #AUTHOR_TAG.', 'as a result,  #AUTHOR_TAG propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages, which cannot be expressed by is - tag, figure 3 illustrates some examples  #AUTHOR_TAG']",0
"['., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']",0
"['to snode of 2 these definitions are based on the discussion in  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'and its dependency']","['substring to snode of 2 these definitions are based on the discussion in  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'and its dependency']","['to snode of 2 these definitions are based on the discussion in  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'and its dependency tree together with the correspondences between substrings of the sentence and']","['2 illustrates the sentence "" john picks the box up "" with its corresponding sstc.', 'it contains a nonprojective correspondence.', 'an interval is assigned to each word in the sentence, i. e. ( 0 - 1 ) for "" john "", ( 1 - 2 ) for "" picks "", ( 2 - 3 ) for "" the "", ( 3 - 4 ) for "" box "" and ( 4 - 5 ) for "" up "".', 'a substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to snode of 2 these definitions are based on the discussion in  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree']",5
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['g. the english gigaword corpus ( linguistic data  #TAUTHOR_TAG.', 'recent work  #AUTHOR_TAG has suggested that some tasks will benefit from using significantly more data']","[', the greatest increase is in the amount of raw text available to be processed, e. g. the english gigaword corpus ( linguistic data  #TAUTHOR_TAG.', 'recent work  #AUTHOR_TAG has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']",0
"['systems for dialogue systems  #TAUTHOR_TAG and speech recognition  #AUTHOR_TAG.', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['of standardised descriptions of services with wsdl and uddi.', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems  #TAUTHOR_TAG and speech recognition  #AUTHOR_TAG.', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['.', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems  #TAUTHOR_TAG and speech recognition  #AUTHOR_TAG.', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['final interface we intend to implement is a collection of web services for nlp.', 'a web service provides a remote procedure that can be called using xml based encodings ( xmlrpc or soap ) of function names, arguments and results transmitted via internet protocols such as http.', 'systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with wsdl and uddi.', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems  #TAUTHOR_TAG and speech recognition  #AUTHOR_TAG.', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']",0
"['example active learning  #AUTHOR_TAG and co - training  #TAUTHOR_TAG.', 'processing text needs to be extremely efficient']","['example active learning  #AUTHOR_TAG and co - training  #TAUTHOR_TAG.', 'processing text needs to be extremely efficient']","['example active learning  #AUTHOR_TAG and co - training  #TAUTHOR_TAG.', 'processing text needs to be extremely efficient']","['discussed earlier, there are two main requirements of the system that are covered by "" high performance "" : speed and state of the art accuracy.', 'efficiency is required both in training and processing.', 'efficient training is required because the amount of data available for training will increase significantly.', 'also, advanced methods often require many training iterations, for example active learning  #AUTHOR_TAG and co - training  #TAUTHOR_TAG.', 'processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly']",0
[')  #TAUTHOR_TAG and'],[')  #TAUTHOR_TAG and'],[')  #TAUTHOR_TAG and the alembic workbench  #AUTHOR_TAG )'],"['are a number of generalised nlp systems in the literature.', 'many provide graphical user interfaces ( gui ) for manual annotation ( e. g. general architecture for text engineering ( gate )  #TAUTHOR_TAG and the alembic workbench  #AUTHOR_TAG ) as well as nlp tools and resources that can be manipulated from the gui.', 'for instance, gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors  #AUTHOR_TAG.', 'gate goes beyond earlier systems by using a component - based infrastructure  #AUTHOR_TAG which the gui is built on top of.', 'this allows components to be highly configurable and simplifies the addition of new components to the system']",0
"['systems for dialogue systems  #AUTHOR_TAG and speech recognition  #TAUTHOR_TAG.', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems  #AUTHOR_TAG and speech recognition  #TAUTHOR_TAG.', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems  #AUTHOR_TAG and speech recognition  #TAUTHOR_TAG.', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems  #AUTHOR_TAG and speech recognition  #TAUTHOR_TAG.', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']",0
['mxpost pos tagger  #TAUTHOR_TAG will simply involve composing'],['mxpost pos tagger  #TAUTHOR_TAG will simply involve composing'],"['to be rapidly composed from many elemental components.', 'for instance, implementing an efficient version of the mxpost pos tagger  #TAUTHOR_TAG will simply involve composing']","['generative programming approach to nlp infrastructure development will allow tools such as sentence boundary detectors, pos taggers, chunkers and named entity recognisers to be rapidly composed from many elemental components.', 'for instance, implementing an efficient version of the mxpost pos tagger  #TAUTHOR_TAG will simply involve composing and configuring the appropriate text file reading component, with the sequential tagging component, the collection of feature extraction components and the maximum entropy model component']",3
"['implementation has been inspired by experience in extracting information from very large corpora  #AUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #TAUTHOR_TAG.', 'we have already implemented a pos tag']","['implementation has been inspired by experience in extracting information from very large corpora  #AUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #TAUTHOR_TAG.', 'we have already implemented a pos tagger, chunker, ccg supertagger']","['implementation has been inspired by experience in extracting information from very large corpora  #AUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #TAUTHOR_TAG.', 'we have already implemented a pos tagger, chunker, ccg supertagger']","['implementation has been inspired by experience in extracting information from very large corpora  #AUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #TAUTHOR_TAG.', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt, the fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing  #AUTHOR_TAG.', 'we expect even faster training times when we move to conjugate gradient methods']",4
"['implementation has been inspired by experience in extracting information from very large corpora  #TAUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #AUTHOR_TAG.', 'we have already implemented a p']","['implementation has been inspired by experience in extracting information from very large corpora  #TAUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #AUTHOR_TAG.', 'we have already implemented a p']","['implementation has been inspired by experience in extracting information from very large corpora  #TAUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #AUTHOR_TAG.', 'we have already implemented a p o s tagger, chunker, c c g supertagger']","['implementation has been inspired by experience in extracting information from very large corpora  #TAUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #AUTHOR_TAG.', 'we have already implemented a p o s tagger, chunker, c c g supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training ma - terials and tag faster than t n t, the fastest existing p o s tagger.', 'these tools use a highly optimised g i s imple - mentation and provide sophisticated gaussian smoothing  #AUTHOR_TAG.', 'we expect even faster train - ing times when we move to conjugate gradient methods']",4
['iterative estimation algorithms used by  #TAUTHOR_TAG'],['iterative estimation algorithms used by  #TAUTHOR_TAG'],"['corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by  #TAUTHOR_TAG']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by  #TAUTHOR_TAG that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly  #AUTHOR_TAG.', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit  #AUTHOR_TAG which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging  #AUTHOR_TAG.', 'the tnt pos tagger  #AUTHOR_TAG has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']",0
"['configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines  #TAUTHOR_TAG.', 'however, the source code for these tools is not freely available, so']","['configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines  #TAUTHOR_TAG.', 'however, the source code for these tools is not freely available, so']","['configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines  #TAUTHOR_TAG.', 'however, the source code for these tools is not freely available, so they cannot be extended']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools  #AUTHOR_TAG perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines  #TAUTHOR_TAG.', 'however, the source code for these tools is not freely available, so they cannot be extended']",0
"['english gigaword corpus  #AUTHOR_TAG.', 'recent work  #TAUTHOR_TAG has suggested that some tasks will benefit from using significantly more data']","['english gigaword corpus  #AUTHOR_TAG.', 'recent work  #TAUTHOR_TAG has suggested that some tasks will benefit from using significantly more data']","['g. the english gigaword corpus  #AUTHOR_TAG.', 'recent work  #TAUTHOR_TAG has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting']","[', the greatest increase is in the amount of raw text available to be processed, e. g. the english gigaword corpus  #AUTHOR_TAG.', 'recent work  #TAUTHOR_TAG has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']",0
"[' #TAUTHOR_TAG, currently']","['penn treebank  #TAUTHOR_TAG, currently']","['electronic text available.', 'some of this new data will be manually annotated.', 'for example, 10 million words of the american national corpus  #AUTHOR_TAG will have manually corrected pos tags, a tenfold increase over the penn treebank  #TAUTHOR_TAG, currently']","['##p is experiencing an explosion in the quantity of electronic text available.', 'some of this new data will be manually annotated.', 'for example, 10 million words of the american national corpus  #AUTHOR_TAG will have manually corrected pos tags, a tenfold increase over the penn treebank  #TAUTHOR_TAG, currently used for training pos taggers.', 'this will require more efficient learning algorithms and implementations']",0
"['ideal for experimentation.', 'it has already been used to implement a framework for teaching nlp  #TAUTHOR_TAG']","['ideal for experimentation.', 'it has already been used to implement a framework for teaching nlp  #TAUTHOR_TAG']","['ideal for experimentation.', 'it has already been used to implement a framework for teaching nlp  #TAUTHOR_TAG']","['c + + is extremely efficient, it is not suitable for rapidly gluing components together to form new tools.', 'to overcome this problem we have implemented an interface to the infrastructure in the python scripting language.', 'python has a number of advantages over other options, such as java and perl.', 'python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation.', 'it has already been used to implement a framework for teaching nlp  #TAUTHOR_TAG']",2
['alembic workbench  #TAUTHOR_TAG )'],['alembic workbench  #TAUTHOR_TAG )'],[')  #AUTHOR_TAG and the alembic workbench  #TAUTHOR_TAG )'],"['are a number of generalised nlp systems in the literature.', 'many provide graphical user interfaces ( gui ) for manual annotation ( e. g. general architecture for text engineering ( gate )  #AUTHOR_TAG and the alembic workbench  #TAUTHOR_TAG ) as well as nlp tools and resources that can be manipulated from the gui.', 'for instance, gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors  #AUTHOR_TAG.', 'gate goes beyond earlier systems by using a component - based infrastructure  #AUTHOR_TAG which the gui is built on top of.', 'this allows components to be highly configurable and simplifies the addition of new components to the system']",0
"['ontology editors  #TAUTHOR_TAG.', '']","['ontology editors  #TAUTHOR_TAG.', '']","['ontology editors  #TAUTHOR_TAG.', '']","['are a number of generalised nlp systems in the literature.', 'many provide graphical user interfaces ( gui ) for manual annotation ( e. g.', 'general architecture for text engineering ( gate )  #AUTHOR_TAG and the alembic workbench  #AUTHOR_TAG ) as well as nlp tools and resources that can be manipulated from the gui.', 'for instance, gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors  #TAUTHOR_TAG.', 'gate goes beyond earlier systems by using a component - based infrastructure  #AUTHOR_TAG which the gui is built on top of.', 'this allows components to be highly configurable and simplifies the addition of new components to the system']",0
['engineering research on generative programming  #TAUTHOR_TAG attempts to solve these problems by focusing on'],['engineering research on generative programming  #TAUTHOR_TAG attempts to solve these problems by focusing on'],"['engineering research on generative programming  #TAUTHOR_TAG attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems.', 'our infrastructure for nlp will provide high performance 1 components inspired by generative programming principles']","['engineering research on generative programming  #TAUTHOR_TAG attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems.', 'our infrastructure for nlp will provide high performance 1 components inspired by generative programming principles']",0
"['american national corpus  #TAUTHOR_TAG will have manually corrected pos tags, a']","['american national corpus  #TAUTHOR_TAG will have manually corrected pos tags, a tenfold']","['', 'for example, 10 million words of the american national corpus  #TAUTHOR_TAG will have manually corrected pos tags,']","['##p is experiencing an explosion in the quantity of electronic text available.', 'some of this new data will be manually annotated.', 'for example, 10 million words of the american national corpus  #TAUTHOR_TAG will have manually corrected pos tags, a tenfold increase over the penn treebank  #AUTHOR_TAG, currently used for training pos taggers.', 'this will require more efficient learning algorithms and implementations']",0
['of lt tools  #TAUTHOR_TAG perform token'],"['of lt tools  #TAUTHOR_TAG perform tokenization, tagging and chunking on xml marked - up text directly.', 'these']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools  #TAUTHOR_TAG perform tokenization, tagging and']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools  #TAUTHOR_TAG perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines  #AUTHOR_TAG.', 'however, the source code for these tools is not freely available, so they cannot be extended']",0
"['static version of the code we will use policy templates  #TAUTHOR_TAG, and for']","['static version of the code we will use policy templates  #TAUTHOR_TAG, and for']","['will also provide dynamic versions ( using inheritance ), which will be slower but accessible from scripting languages and user interfaces.', 'to provide the required configurability in the static version of the code we will use policy templates  #TAUTHOR_TAG, and for the dynamic version we will use configuration classes']","['infrastructure will be implemented in c / c + +.', 'templates will be used heavily to provide generality without significantly impacting on efficiency.', 'however, because templates are a static facility we will also provide dynamic versions ( using inheritance ), which will be slower but accessible from scripting languages and user interfaces.', 'to provide the required configurability in the static version of the code we will use policy templates  #TAUTHOR_TAG, and for the dynamic version we will use configuration classes']",5
"['english gigaword corpus  #AUTHOR_TAG.', 'recent work  #TAUTHOR_TAG has suggested that some tasks will benefit from using significantly more data']","['english gigaword corpus  #AUTHOR_TAG.', 'recent work  #TAUTHOR_TAG has suggested that some tasks will benefit from using significantly more data']","['g. the english gigaword corpus  #AUTHOR_TAG.', 'recent work  #TAUTHOR_TAG has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting']","[', the greatest increase is in the amount of raw text available to be processed, e. g. the english gigaword corpus  #AUTHOR_TAG.', 'recent work  #TAUTHOR_TAG has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']",0
['of lt tools  #TAUTHOR_TAG perform token'],"['of lt tools  #TAUTHOR_TAG perform tokenization, tagging and chunking on xml marked - up text directly.', 'these']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools  #TAUTHOR_TAG perform tokenization, tagging and']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools  #TAUTHOR_TAG perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines  #AUTHOR_TAG.', 'however, the source code for these tools is not freely available, so they cannot be extended']",0
['( tbl )  #TAUTHOR_TAG and memory - based'],['( tbl )  #TAUTHOR_TAG and memory - based'],"[': transformation - based learning ( tbl )  #TAUTHOR_TAG and memory - based learning ( mbl )  #AUTHOR_TAG have been applied to many different problems, so a single interchangeable component should be used to represent each method.', 'we will base these components on the design of weka  #AUTHOR_TAG']","['learning methods should be interchangeable : transformation - based learning ( tbl )  #TAUTHOR_TAG and memory - based learning ( mbl )  #AUTHOR_TAG have been applied to many different problems, so a single interchangeable component should be used to represent each method.', 'we will base these components on the design of weka  #AUTHOR_TAG']",4
[') toolkit  #TAUTHOR_TAG which dramatically'],[') toolkit  #TAUTHOR_TAG which dramatically'],"[') toolkit  #TAUTHOR_TAG which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by  #AUTHOR_TAG that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly  #AUTHOR_TAG.', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit  #TAUTHOR_TAG which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging  #AUTHOR_TAG.', 'the tnt pos tagger  #AUTHOR_TAG has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']",0
"['used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines  #TAUTHOR_TAG.', 'an example of using the python tagger interface is shown in figure 1']","['used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines  #TAUTHOR_TAG.', 'an example of using the python tagger interface is shown in figure 1']","['used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines  #TAUTHOR_TAG.', 'an example of using the python tagger interface is shown in figure 1']","['basic python reflection has already been implemented and used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines  #TAUTHOR_TAG.', 'an example of using the python tagger interface is shown in figure 1']",0
"['.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing  #TAUTHOR_TAG.', 'we expect even faster training times when we move to conjugate gradient methods']","['fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing  #TAUTHOR_TAG.', 'we expect even faster training times when we move to conjugate gradient methods']","['##t, the fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing  #TAUTHOR_TAG.', 'we expect even faster training times when we move to conjugate gradient methods']","['implementation has been inspired by experience in extracting information from very large corpora  #AUTHOR_TAG and performing experiments on maximum entropy sequence tagging ( curran and.', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt, the fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing  #TAUTHOR_TAG.', 'we expect even faster training times when we move to conjugate gradient methods']",5
"['very fast tagging  #TAUTHOR_TAG.', 'the tnt pos tagger  #AUTHOR_TAG has also been designed to train']","['very fast tagging  #TAUTHOR_TAG.', 'the tnt pos tagger  #AUTHOR_TAG has also been designed to train']","['very fast tagging  #TAUTHOR_TAG.', 'the tnt pos tagger  #AUTHOR_TAG has also been designed to train']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by  #AUTHOR_TAG that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly  #AUTHOR_TAG.', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit  #AUTHOR_TAG which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging  #TAUTHOR_TAG.', 'the tnt pos tagger  #AUTHOR_TAG has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']",0
"[', the natural language toolkit ( nltk ) is a package of nlp components implemented in python  #TAUTHOR_TAG.', 'python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple']","[', the natural language toolkit ( nltk ) is a package of nlp components implemented in python  #TAUTHOR_TAG.', 'python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple']","[', the natural language toolkit ( nltk ) is a package of nlp components implemented in python  #TAUTHOR_TAG.', 'python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple']","[', the natural language toolkit ( nltk ) is a package of nlp components implemented in python  #TAUTHOR_TAG.', 'python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple']",0
"['implementation has been inspired by experience in extracting information from very large corpora  #AUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #TAUTHOR_TAG.', 'we have already implemented a pos tag']","['implementation has been inspired by experience in extracting information from very large corpora  #AUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #TAUTHOR_TAG.', 'we have already implemented a pos tagger, chunker, ccg supertagger']","['implementation has been inspired by experience in extracting information from very large corpora  #AUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #TAUTHOR_TAG.', 'we have already implemented a pos tagger, chunker, ccg supertagger']","['implementation has been inspired by experience in extracting information from very large corpora  #AUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #TAUTHOR_TAG.', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt, the fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing  #AUTHOR_TAG.', 'we expect even faster training times when we move to conjugate gradient methods']",4
"['converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly  #TAUTHOR_TAG.', 'other attempts to address efficiency include the fast transformation based']","['converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly  #TAUTHOR_TAG.', 'other attempts to address efficiency include the fast transformation based']","['converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly  #TAUTHOR_TAG.', 'other attempts to address efficiency include the fast transformation based']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by  #AUTHOR_TAG that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly  #TAUTHOR_TAG.', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit  #AUTHOR_TAG which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging  #AUTHOR_TAG.', 'the tnt pos tagger  #AUTHOR_TAG has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']",0
"['used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines  #TAUTHOR_TAG.', 'an example of using the python tagger interface is shown in figure 1']","['used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines  #TAUTHOR_TAG.', 'an example of using the python tagger interface is shown in figure 1']","['used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines  #TAUTHOR_TAG.', 'an example of using the python tagger interface is shown in figure 1']","['basic python reflection has already been implemented and used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines  #TAUTHOR_TAG.', 'an example of using the python tagger interface is shown in figure 1']",0
[' #TAUTHOR_TAG has also been designed to train'],"['very fast tagging  #AUTHOR_TAG.', 'the tnt pos tagger  #TAUTHOR_TAG has also been designed to train']","['very fast tagging  #AUTHOR_TAG.', 'the tnt pos tagger  #TAUTHOR_TAG has also been designed to train']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by  #AUTHOR_TAG that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly  #AUTHOR_TAG.', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit  #AUTHOR_TAG which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging  #AUTHOR_TAG.', 'the tnt pos tagger  #TAUTHOR_TAG has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']",0
"['by using a component - based infrastructure  #TAUTHOR_TAG which the gui is built on top of.', 'this allows components to be highly configurable and simplifies the addition of new components to the system']","['by using a component - based infrastructure  #TAUTHOR_TAG which the gui is built on top of.', 'this allows components to be highly configurable and simplifies the addition of new components to the system']","['earlier systems by using a component - based infrastructure  #TAUTHOR_TAG which the gui is built on top of.', 'this allows components to be highly configurable and simplifies the addition of new components to the system']","['are a number of generalised nlp systems in the literature.', 'many provide graphical user interfaces ( gui ) for manual annotation ( e. g.', 'general architecture for text engineering ( gate )  #AUTHOR_TAG and the alembic workbench  #AUTHOR_TAG ) as well as nlp tools and resources that can be manipulated from the gui.', 'for instance, gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors  #AUTHOR_TAG.', 'gate goes beyond earlier systems by using a component - based infrastructure  #TAUTHOR_TAG which the gui is built on top of.', 'this allows components to be highly configurable and simplifies the addition of new components to the system']",0
"['semantics of natural language.', 'thus for instance,  #TAUTHOR_TAG describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction']","['semantics of natural language.', 'thus for instance,  #TAUTHOR_TAG describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction']","['semantic grammars "" already exist which describe not only the syntax but also the semantics of natural language.', 'thus for instance,  #TAUTHOR_TAG describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction']","['semantic grammars "" already exist which describe not only the syntax but also the semantics of natural language.', 'thus for instance,  #TAUTHOR_TAG describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and  #AUTHOR_TAG show how to equip lexical functional grammar ( lfg ) with a glue semantics']",0
"['for french are available  #TAUTHOR_TAG.', 'multi word expressions however remain a problem as']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available  #TAUTHOR_TAG.', 'multi word expressions however remain a problem as']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available  #TAUTHOR_TAG.', 'multi word expressions however remain a problem as']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available  #TAUTHOR_TAG.', 'multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'for these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity  #AUTHOR_TAG.', 'techniques']",0
"['to be close in meaning.', 'similarly,  #AUTHOR_TAG and  #TAUTHOR_TAG learn']","['to be close in meaning.', 'similarly,  #AUTHOR_TAG and  #TAUTHOR_TAG learn']","['have similar arguments are taken to be close in meaning.', 'similarly,  #AUTHOR_TAG and  #TAUTHOR_TAG learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and  #AUTHOR_TAG use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['recently, work in information extraction ( ie ) and question answering ( qa ) has triggered a renewed research interest in paraphrases as ie and qa systems typically need to be able to recognise various verbalisations of the content.', 'because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'for instance,  #AUTHOR_TAG acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly,  #AUTHOR_TAG and  #TAUTHOR_TAG learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and  #AUTHOR_TAG use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']",0
"['of news articles stemming from different news source.', 'and  #TAUTHOR_TAG use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['of news articles stemming from different news source.', 'and  #TAUTHOR_TAG use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['have similar arguments are taken to be close in meaning.', 'similarly,  #AUTHOR_TAG and  #AUTHOR_TAG learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and  #TAUTHOR_TAG use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['recently, work in information extraction ( ie ) and question answering ( qa ) has triggered a renewed research interest in paraphrases as ie and qa systems typically need to be able to recognise various verbalisations of the content.', 'because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'for instance,  #AUTHOR_TAG acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly,  #AUTHOR_TAG and  #AUTHOR_TAG learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and  #TAUTHOR_TAG use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']",0
"['to do this in a general way while not overgeneralising.', 'to address this problem, we are currently working on developing a metagrammar in the sense of  #TAUTHOR_TAG.', 'this metagrammar allows us to factorise both syntactic and semantic information.', 'syntactic information is factorised in the usual way']","['to do this in a general way while not overgeneralising.', 'to address this problem, we are currently working on developing a metagrammar in the sense of  #TAUTHOR_TAG.', 'this metagrammar allows us to factorise both syntactic and semantic information.', 'syntactic information is factorised in the usual way.', 'for instance, there will be a']","['to do this in a general way while not overgeneralising.', 'to address this problem, we are currently working on developing a metagrammar in the sense of  #TAUTHOR_TAG.', 'this metagrammar allows us to factorise both syntactic and semantic information.', 'syntactic information is factorised in the usual way.', 'for instance, there will be a class novn1 which groups together all the initial trees representing the possible syntactic configurations in which']","['intercategorial synonymic links.', ""a first investigation of anne abeille's tag for french suggests that modeling the synonymic relations across syntactic constructs is reasonably straightforward."", 'for instance, as figures 3, 4 and 5 show, the ftag trees assigned on syntactic grounds by anne abeille ftag to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above.', 'generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising.', 'to address this problem, we are currently working on developing a metagrammar in the sense of  #TAUTHOR_TAG.', 'this metagrammar allows us to factorise both syntactic and semantic information.', 'syntactic information is factorised in the usual way.', 'for instance, there will be a class novn1 which groups together all the initial trees representing the possible syntactic configurations in which a transitive verb with two nominal arguments can occur.', 'but additionnally there will be semantic classes such as, "" binary predicate of semantic type x "" which will be associated with the relevant syntactic classes for instance, novn1 ( the class of transitive verbs with nominal arguments ), binary npred ( the class of binary predicative nouns ), novsupnn1, the class of support verb constructions taking two nominal arguments.', 'by further associating semantic units ( e. g., "" cost "" ) with the appropriate semantic classes ( e. g., "" binary predicate of semantic type x "" ), we can in this way capture both intra and intercategorial paraphrasing links in a general way']",3
"['to be close in meaning.', 'similarly,  #TAUTHOR_TAG and  #AUTHOR_TAG learn sentence level paraphrase templates from a']","['to be close in meaning.', 'similarly,  #TAUTHOR_TAG and  #AUTHOR_TAG learn sentence level paraphrase templates from a']","['have similar arguments are taken to be close in meaning.', 'similarly,  #TAUTHOR_TAG and  #AUTHOR_TAG learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and  #AUTHOR_TAG use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['recently, work in information extraction ( ie ) and question answering ( qa ) has triggered a renewed research interest in paraphrases as ie and qa systems typically need to be able to recognise various verbalisations of the content.', 'because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'for instance,  #AUTHOR_TAG acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly,  #TAUTHOR_TAG and  #AUTHOR_TAG learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and  #AUTHOR_TAG use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']",0
"['##net inventory of frames and frame elements ( c.  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', '']","['represent the semantics of predicative units, we use framenet inventory of frames and frame elements ( c.  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'framenet is an online lexical resource for english based on the principles of frame semantics.', 'in this approach,']","['##net inventory of frames and frame elements ( c.  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', '']","['represent the semantics of predicative units, we use framenet inventory of frames and frame elements ( c.  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'framenet is an online lexical resource for english based on the principles of frame semantics.', 'in this approach, a word evokes a frame i. e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a given role in the frame.', 'finally each frame is associated with a set of target words, the words that evoke that frame']",5
"['is based on automatic learning techniques.', 'for instance,  #TAUTHOR_TAG acquire two - argument']","['is based on automatic learning techniques.', 'for instance,  #TAUTHOR_TAG acquire two - argument']","['is based on automatic learning techniques.', 'for instance,  #TAUTHOR_TAG acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in']","['recently, work in information extraction ( ie ) and question answering ( qa ) has triggered a renewed research interest in paraphrases as ie and qa systems typically need to be able to recognise various verbalisations of the content.', 'because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'for instance,  #TAUTHOR_TAG acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly,  #AUTHOR_TAG and  #AUTHOR_TAG learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and  #AUTHOR_TAG use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']",0
"['on distributional similarity  #TAUTHOR_TAG.', 'techniques']","['on distributional similarity  #TAUTHOR_TAG.', 'techniques']","['on distributional similarity  #TAUTHOR_TAG.', 'techniques']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available  #AUTHOR_TAG.', 'multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'for these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity  #TAUTHOR_TAG.', 'techniques']",3
['according to linguistics phenomena  #TAUTHOR_TAG ;'],['according to linguistics phenomena  #TAUTHOR_TAG ;'],['grouping them according to linguistics phenomena  #TAUTHOR_TAG ;'],"['on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar ( does it generate all the sentences of the described language? )', 'and its degree of overgeneration ( does it generate only the sentences of the described language? )', 'while corpus driven efforts along the par - seval lines  #AUTHOR_TAG are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'for english, there is for instance the 15 year old hewlettpackard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena  #TAUTHOR_TAG ; and more recently, the much more sophisticated tsnlp ( test suite for natural language processing ) which includes some 9500 test items for english, french and german, each of them being annotated with syntactic and application related information  #AUTHOR_TAG.', 'yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'to remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.', 'in such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.', 'the construction and annotation of the paraphrases reflects the paraphrase typology.', 'in a first phase, we concentrate on simple, non - recursive predicate / argument structure.', 'given such a structure, the construction and annotation of a test item proceeds as follows']",1
"['of  #TAUTHOR_TAG.', 'however']","['of  #TAUTHOR_TAG.', 'however']","['language chosen for semantic representation is a flat semantics along the line of  #TAUTHOR_TAG.', 'however']","['language chosen for semantic representation is a flat semantics along the line of  #TAUTHOR_TAG.', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate / arguments and modifiers / modified relationships.', 'thus the semantic representations we assume are simply set of literals of the form p n ( x 1,...', ', x n ) where p n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing']",1
"[')', 'while corpus driven efforts along the parseval lines  #TAUTHOR_TAG']","['', 'while corpus driven efforts along the parseval lines  #TAUTHOR_TAG']","['it generate only the sentences of the described language? )', 'while corpus driven efforts along the parseval lines  #TAUTHOR_TAG']","['', 'while corpus driven efforts along the parseval lines  #TAUTHOR_TAG are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'for english, there is for instance the 15 year old hewlett - packard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena  #AUTHOR_TAG ; and more recently, the much more sophisticated tsnlp ( test suite for natural language processing ) which includes some 9500 test items for english, french and german, each of them being annotated with syntactic and application related information  #AUTHOR_TAG']",0
"['of  #TAUTHOR_TAG.', 'however']","['of  #TAUTHOR_TAG.', 'however']","['language chosen for semantic representation is a flat semantics along the line of  #TAUTHOR_TAG.', 'however']","['language chosen for semantic representation is a flat semantics along the line of  #TAUTHOR_TAG.', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate / arguments and modifiers / modified relationships.', 'thus the semantic representations we assume are simply set of literals of the form p n ( x 1,...', ', x n ) where p n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing']",1
"['to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular,  #TAUTHOR_TAG lists the converses']","['converse constructions, the ladl tables  #AUTHOR_TAG can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular,  #TAUTHOR_TAG lists the converses']","['converse constructions, the ladl tables  #AUTHOR_TAG can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular,  #TAUTHOR_TAG lists the converses']","['shuffling paraphrases, french alternations are partially described in ( saint -  #AUTHOR_TAG and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', 'for complementing this database and for converse constructions, the ladl tables  #AUTHOR_TAG can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular,  #TAUTHOR_TAG lists the converses of some 3 500 predicative nouns']",3
['of a meta - grammar  #TAUTHOR_TAG'],['of a meta - grammar  #TAUTHOR_TAG'],"['we shall briefly discuss in section 4, the grammar is developed with the help of a meta - grammar  #TAUTHOR_TAG thus ensuring an additional level of abstraction.', 'the metagrammar is an abstract specification of the linguistic properties ( phrase structure, valency, realisation of grammatical functions etc. ) encoded in the grammar basic units.', 'this specification is then compiled to automatically']","['we shall briefly discuss in section 4, the grammar is developed with the help of a meta - grammar  #TAUTHOR_TAG thus ensuring an additional level of abstraction.', 'the metagrammar is an abstract specification of the linguistic properties ( phrase structure, valency, realisation of grammatical functions etc. ) encoded in the grammar basic units.', 'this specification is then compiled to automatically produce a specific grammar']",5
['##l tables  #TAUTHOR_TAG can furthermore be resort'],"['complementing this database and for converse constructions, the ladl tables  #TAUTHOR_TAG can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular,  #AUTHOR_TAG lists the converses']","['complementing this database and for converse constructions, the ladl tables  #TAUTHOR_TAG can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular,  #AUTHOR_TAG lists the converses']","['shuffling paraphrases, french alternations are partially described in ( saint -  #AUTHOR_TAG and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', 'for complementing this database and for converse constructions, the ladl tables  #TAUTHOR_TAG can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular,  #AUTHOR_TAG lists the converses of some 3 500 predicative nouns']",3
"['being annotated with syntactic and application related information  #TAUTHOR_TAG.', 'yet']","['being annotated with syntactic and application related information  #TAUTHOR_TAG.', 'yet']","['of them being annotated with syntactic and application related information  #TAUTHOR_TAG.', 'yet']","['', 'while corpus driven efforts along the par - seval lines  #AUTHOR_TAG are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'for english, there is for instance the 15 year old hewlettpackard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena  #AUTHOR_TAG ; and more recently, the much more sophisticated tsnlp ( test suite for natural language processing ) which includes some 9500 test items for english, french and german, each of them being annotated with syntactic and application related information  #TAUTHOR_TAG.', 'yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'to remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.', 'in such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.', 'the construction and annotation of the paraphrases reflects the paraphrase typology.', 'in a first phase, we concentrate on simple, non - recursive predicate / argument structure.', 'given such a structure, the construction and annotation of a test item proceeds as follows']",0
"['on distributional similarity  #TAUTHOR_TAG.', 'techniques']","['on distributional similarity  #TAUTHOR_TAG.', 'techniques']","['on distributional similarity  #TAUTHOR_TAG.', 'techniques']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available  #AUTHOR_TAG.', 'multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'for these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity  #TAUTHOR_TAG.', 'techniques']",3
['derived ) tree and of a semantic representation and  #TAUTHOR_TAG show how to equip lexical functional grammar ( lf'],['derived ) tree and of a semantic representation and  #TAUTHOR_TAG show how to equip lexical functional grammar ( lfg ) with a glue semantics'],['derived ) tree and of a semantic representation and  #TAUTHOR_TAG show how to equip lexical functional grammar ( lf'],"['semantic grammars "" already exist which describe not only the syntax but also the semantics of natural language.', 'thus for instance,  #AUTHOR_TAG describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and  #TAUTHOR_TAG show how to equip lexical functional grammar ( lfg ) with a glue semantics']",0
[' #TAUTHOR_TAG rather than - - as is more common in tag - - from'],[' #TAUTHOR_TAG rather than - - as is more common in tag - - from'],['derived tree  #TAUTHOR_TAG rather than - - as is more common in tag - - from'],"['construction proceeds from the derived tree  #TAUTHOR_TAG rather than - - as is more common in tag - - from the derivation tree.', 'this is done by associating each elementary tree with a semantic representation and by decorating relevant tree nodes with unification variables and constants occuring in associated semantic representation.', 'the association between tree nodes and unification variables encodes the syntax / semantics interface - it specifies which node in the tree provides the value for which variable in the final semantic representation']",0
"['semantics of natural language.', 'thus for instance,  #TAUTHOR_TAG describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction']","['semantics of natural language.', 'thus for instance,  #TAUTHOR_TAG describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction']","['semantic grammars "" already exist which describe not only the syntax but also the semantics of natural language.', 'thus for instance,  #TAUTHOR_TAG describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction']","['semantic grammars "" already exist which describe not only the syntax but also the semantics of natural language.', 'thus for instance,  #TAUTHOR_TAG describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and  #AUTHOR_TAG show how to equip lexical functional grammar ( lfg ) with a glue semantics']",0
"['of  #TAUTHOR_TAG.', 'however']","['of  #TAUTHOR_TAG.', 'however']","['language chosen for semantic representation is a flat semantics along the line of  #TAUTHOR_TAG.', 'however']","['language chosen for semantic representation is a flat semantics along the line of  #TAUTHOR_TAG.', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate / arguments and modifiers / modified relationships.', 'thus the semantic representations we assume are simply set of literals of the form p n ( x 1,...', ', x n ) where p n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing']",1
"['shuffling paraphrases, french alternations are partially described in  #TAUTHOR_TAG and a']","['shuffling paraphrases, french alternations are partially described in  #TAUTHOR_TAG and a']","['shuffling paraphrases, french alternations are partially described in  #TAUTHOR_TAG and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', '']","['shuffling paraphrases, french alternations are partially described in  #TAUTHOR_TAG and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', 'for complementing this database and for converse constructions, the ladl tables  #AUTHOR_TAG can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular,  #AUTHOR_TAG lists the converses of some 3 500 predicative nouns']",0
"['algorithms  #TAUTHOR_TAG, with a']","['algorithms  #TAUTHOR_TAG, with a']","['canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in  #AUTHOR_TAG.', 'it compares favorably to other stemming or root extraction algorithms  #TAUTHOR_TAG, with a performance']","['root extraction process is concerned with the transformation of all arabic word derivatives to their single common root or canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in  #AUTHOR_TAG.', 'it compares favorably to other stemming or root extraction algorithms  #TAUTHOR_TAG, with a performance of over 97 % for extracting the correct root in web documents, and it addresses the challenge of the arabic broken plural and hollow verbs.', 'in the remainder of this paper, we will use the term "" root "" and "" term "" interchangeably to refer to canonical forms obtained through this root extraction process']",4
"['algorithms  #TAUTHOR_TAG, with a']","['algorithms  #TAUTHOR_TAG, with a']","['canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in  #AUTHOR_TAG.', 'it compares favorably to other stemming or root extraction algorithms  #TAUTHOR_TAG, with a performance']","['root extraction process is concerned with the transformation of all arabic word derivatives to their single common root or canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in  #AUTHOR_TAG.', 'it compares favorably to other stemming or root extraction algorithms  #TAUTHOR_TAG, with a performance of over 97 % for extracting the correct root in web documents, and it addresses the challenge of the arabic broken plural and hollow verbs.', 'in the remainder of this paper, we will use the term "" root "" and "" term "" interchangeably to refer to canonical forms obtained through this root extraction process']",4
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig )  #TAUTHOR_TAG, minimum description length principal  #AUTHOR_TAG, and the x2 statistic.', ' #AUTHOR_TAG has found strong correlations between df, ig and the χ 2 statistic for a term.', 'on the other hand,  #AUTHOR_TAG reports the χ 2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in  #AUTHOR_TAG.', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval  #AUTHOR_TAG.', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']",0
"['initiated in  #TAUTHOR_TAG, which reports an overall nb classification']","['initiated in  #TAUTHOR_TAG, which reports an overall nb classification']","['initiated in  #TAUTHOR_TAG, which reports an overall nb classification correctness']","['arabic, one automatic categorizer has been reported to have been put under operational use to classify arabic documents ; it is referred to as "" sakhr\'s categorizer ""  #AUTHOR_TAG.', 'unfortunately, there is no technical documentation or specification concerning this arabic categorizer.', ""sakhr's marketing literature claims that this categorizer is based on arabic morphology and some research that has been carried out on natural language processing."", 'the present work evaluates the performance on arabic documents of the naive bayes algorithm ( nb ) - one of the simplest algorithms applied to english document categorization  #AUTHOR_TAG.', 'the aim of this work is to gain some insight as to whether arabic document categorization ( using nb ) is sensitive to the root extraction algorithm used or to different data sets.', 'this work is a continuation of that initiated in  #TAUTHOR_TAG, which reports an overall nb classification correctness of 75. 6 %, in cross validation experiments, on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different arabic portals ).', 'a 50 % overall classification accuracy is also reported when testing with a separately collected evaluation set ( 3 documents for each of the 12 categories ).', 'the present work expands the work in  #AUTHOR_TAG by experimenting with the use of a better root extraction algorithm  #AUTHOR_TAG for document preprocessing, and using a different data set, collected from the largest arabic site on the web : aljazeera. net']",2
"['algorithm from those used in  #TAUTHOR_TAG.', 'in this']","['algorithm from those used in  #TAUTHOR_TAG.', 'in this work, the']","['arabic documents using the nb algorithm, with the use of a different data set, a different number of categories, and a different root extraction algorithm from those used in  #TAUTHOR_TAG.', 'in this']","['sum up, this work has been carried out to automatically classify arabic documents using the nb algorithm, with the use of a different data set, a different number of categories, and a different root extraction algorithm from those used in  #TAUTHOR_TAG.', '']",1
"['good study comparing document categorization algorithms can be found in  #TAUTHOR_TAG.', 'more recently,  #AUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['good study comparing document categorization algorithms can be found in  #TAUTHOR_TAG.', 'more recently,  #AUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['good study comparing document categorization algorithms can be found in  #TAUTHOR_TAG.', 'more recently,  #AUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['good study comparing document categorization algorithms can be found in  #TAUTHOR_TAG.', 'more recently,  #AUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']",0
"['language  #TAUTHOR_TAG, and that']","['to the fact that arabic is a non - concatenative language  #TAUTHOR_TAG, and that']","['to the fact that arabic is a non - concatenative language  #TAUTHOR_TAG, and that the stem / infix']","['arabic, however, the use of stems will not yield satisfactory categorization.', 'this is mainly due to the fact that arabic is a non - concatenative language  #TAUTHOR_TAG, and that the stem / infix obtained by suppression of infix and prefix add - ons is not the same for words derived from the same origin called the root.', 'the infix form ( or stem ) needs further to be processed in order to obtain the root.', 'this processing is not straightforward : it necessitates expert knowledge in arabic language word morphology ( al -  #AUTHOR_TAG.', 'as an example, two close roots ( i. e., roots made of the same letters ), but semantically different, can yield the same infix form thus creating ambiguity']",0
"['minimum description length principal  #AUTHOR_TAG, and the χ 2 statistic.', ' #TAUTHOR_TAG has found strong']","['minimum description length principal  #AUTHOR_TAG, and the χ 2 statistic.', ' #TAUTHOR_TAG has found strong']","['minimum description length principal  #AUTHOR_TAG, and the χ 2 statistic.', ' #TAUTHOR_TAG has found strong correlations']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig )  #AUTHOR_TAG, minimum description length principal  #AUTHOR_TAG, and the χ 2 statistic.', ' #TAUTHOR_TAG has found strong correlations between df, ig and the x2 statistic for a term.', 'on the other hand,  #AUTHOR_TAG reports the χ 2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in  #AUTHOR_TAG.', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval  #AUTHOR_TAG.', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']",0
"['good study comparing document categorization algorithms can be found in  #AUTHOR_TAG.', 'more recently,  #TAUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['good study comparing document categorization algorithms can be found in  #AUTHOR_TAG.', 'more recently,  #TAUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['good study comparing document categorization algorithms can be found in  #AUTHOR_TAG.', 'more recently,  #TAUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['good study comparing document categorization algorithms can be found in  #AUTHOR_TAG.', 'more recently,  #TAUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']",0
"[' #TAUTHOR_TAG.', 'such']","[' #TAUTHOR_TAG.', 'such']",[' #TAUTHOR_TAG'],"['', 'automatic text categorization has been used in search engines, digital library systems, and document management systems  #TAUTHOR_TAG.', 'such applications have included electronic email filtering, newsgroups classification, and survey data grouping.', 'barq for instance uses automatic categorization to provide similar documents feature  #AUTHOR_TAG.', 'in this paper, nb which is a statistical machine learning algorithm is used to learn to classify non - vocalized 1 arabic web text documents']",0
"['in  #TAUTHOR_TAG.', 'tf - idf ( term']","['in  #TAUTHOR_TAG.', 'tf - idf ( term']","['in  #TAUTHOR_TAG.', 'tf - idf (']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig )  #AUTHOR_TAG, minimum description length principal  #AUTHOR_TAG, and the χ 2 statistic.', ' #AUTHOR_TAG has found strong correlations between df, ig and the χ 2 statistic for a term.', 'on the other hand,  #AUTHOR_TAG reports the χ 2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in  #TAUTHOR_TAG.', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval  #AUTHOR_TAG.', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']",1
"['df t is the number of documents containing the term t.  #TAUTHOR_TAG proposed the combination of tf and idf as weighting schemes, and it has been shown that their product gave better performance.', 'thus, the weight of each term']","['df t is the number of documents containing the term t.  #TAUTHOR_TAG proposed the combination of tf and idf as weighting schemes, and it has been shown that their product gave better performance.', 'thus, the weight of each term / root in a']","['df t is the number of documents containing the term t.  #TAUTHOR_TAG proposed the combination of tf and idf as weighting schemes, and it has been shown that their product gave better performance.', 'thus, the weight of each term / root in a document is given by w d, t = tf d, t * idf t']","['the tf measurement concerns the importance of a term in a given document, idf seeks to measure the relative importance of a term in a collection of documents.', 'the importance of each term is assumed to be inversely proportional to the number of documents that contain that term.', 'tf is given by tf d, t, and it denotes frequency of term t in document d. idf is given by idf t = log ( n / df t ), where n is the number of documents in the collection, and df t is the number of documents containing the term t.  #TAUTHOR_TAG proposed the combination of tf and idf as weighting schemes, and it has been shown that their product gave better performance.', 'thus, the weight of each term / root in a document is given by w d, t = tf d, t * idf t']",4
"[' #AUTHOR_TAG, minimum description length principal  #TAUTHOR_TAG, and the x2 statistic.', ' #AUTHOR_TAG has found strong']","[' #AUTHOR_TAG, minimum description length principal  #TAUTHOR_TAG, and the x2 statistic.', ' #AUTHOR_TAG has found strong']","[' #AUTHOR_TAG, minimum description length principal  #TAUTHOR_TAG, and the x2 statistic.', ' #AUTHOR_TAG has found strong correlations']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig )  #AUTHOR_TAG, minimum description length principal  #TAUTHOR_TAG, and the x2 statistic.', ' #AUTHOR_TAG has found strong correlations between df, ig and the χ 2 statistic for a term.', 'on the other hand,  #AUTHOR_TAG reports the χ 2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in  #AUTHOR_TAG.', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval  #AUTHOR_TAG.', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']",0
"[', early such works may be found in  #TAUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG, respectively']","[', early such works may be found in  #TAUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG, respectively']","[', early such works may be found in  #TAUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG, respectively']","['machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and bayesian learning, nearest neighbor learning, and artificial neural networks, early such works may be found in  #TAUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG, respectively']",0
"[') is one of the widely used feature selection techniques in information retrieval  #TAUTHOR_TAG.', 'specifically,']","[') is one of the widely used feature selection techniques in information retrieval  #TAUTHOR_TAG.', 'specifically,']","[') is one of the widely used feature selection techniques in information retrieval  #TAUTHOR_TAG.', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig )  #AUTHOR_TAG, minimum description length principal  #AUTHOR_TAG, and the χ 2 statistic.', ' #AUTHOR_TAG has found strong correlations between df, ig and the χ 2 statistic for a term.', 'on the other hand,  #AUTHOR_TAG reports the χ 2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in  #AUTHOR_TAG.', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval  #TAUTHOR_TAG.', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']",0
"['good study comparing document categorization algorithms can be found in  #AUTHOR_TAG.', 'more recently,  #AUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['good study comparing document categorization algorithms can be found in  #AUTHOR_TAG.', 'more recently,  #AUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['good study comparing document categorization algorithms can be found in  #AUTHOR_TAG.', 'more recently,  #AUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['good study comparing document categorization algorithms can be found in  #AUTHOR_TAG.', 'more recently,  #AUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']",0
"['has been devoted to cope with automatic categorization of english and latin character documents.', 'for example,  #TAUTHOR_TAG discusses the evaluation of two different text categorization strategies with several variations of their feature spaces']","['has been devoted to cope with automatic categorization of english and latin character documents.', 'for example,  #TAUTHOR_TAG discusses the evaluation of two different text categorization strategies with several variations of their feature spaces']","['bulk of the text categorization work has been devoted to cope with automatic categorization of english and latin character documents.', 'for example,  #TAUTHOR_TAG discusses the evaluation of two different text categorization strategies with several variations of their feature spaces']","['bulk of the text categorization work has been devoted to cope with automatic categorization of english and latin character documents.', 'for example,  #TAUTHOR_TAG discusses the evaluation of two different text categorization strategies with several variations of their feature spaces']",0
"[', on a combination of the two  #TAUTHOR_TAG, for example ).', 'it is worth noting that although these techniques']","[', on a combination of the two  #TAUTHOR_TAG, for example ).', 'it is worth noting that although these techniques']","[', on a combination of the two  #TAUTHOR_TAG, for example ).', 'it is worth noting that although these techniques']","['the other hand, other work has been carried out in order to acquire collocations.', 'most of these endeavours have focused on purely statistical acquisition techniques  #AUTHOR_TAG, on linguisitic acquisition ( by the use of part - of - speech filters hand - crafted by a linguist )  #AUTHOR_TAG or, more frequently, on a combination of the two  #TAUTHOR_TAG, for example ).', 'it is worth noting that although these techniques are able to identify n - v pairs, they do not specify the relationship between n and v, nor are they capable of focusing on a subset of n - v pairs.', 'the original acquisition methodology we present in the next section will allow us to overcome this limitation']",1
"['articles  #TAUTHOR_TAG.', 'note that to prevent any bias in the results,']","['of newspaper articles  #TAUTHOR_TAG.', 'note that to prevent any bias in the results,']","['of newspaper articles  #TAUTHOR_TAG.', 'note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '( they were removed from the example set.']","['construct this test set, we have focused our attention on ten domain - specific terms : commande ( command ), configuration, fichier ( file ), internet, logiciel ( software ), option, ordinateur ( computer ), serveur ( server ), systme ( system ), utilisateur ( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by  #AUTHOR_TAG and called termostat.', 'the ten most specific nouns have been produced by comparing our corpus of computing to the french corpus le monde, composed of newspaper articles  #TAUTHOR_TAG.', 'note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '( they were removed from the example set.']",5
"['semantic relatedness  #TAUTHOR_TAG, for example ), does not specify the relationship itself.', 'hence, synonyms, co - hyponyms, hyperonyms, etc. are not dierent']","['semantic relatedness  #TAUTHOR_TAG, for example ), does not specify the relationship itself.', 'hence, synonyms, co - hyponyms, hyperonyms, etc. are not dierentiated']","['semantic relatedness  #TAUTHOR_TAG, for example ), does not specify the relationship itself.', 'hence, synonyms, co - hyponyms, hyperonyms, etc. are not dierent']","['number of applications have relied on distributional analysis  #AUTHOR_TAG in order to build classes of semantically related terms.', 'this approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness  #TAUTHOR_TAG, for example ), does not specify the relationship itself.', 'hence, synonyms, co - hyponyms, hyperonyms, etc. are not dierentiated']",0
"['between terms :  #TAUTHOR_TAG uses derivational morphology ;  #AUTHOR_TAG use, as']","['between terms :  #TAUTHOR_TAG uses derivational morphology ;  #AUTHOR_TAG use, as']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms :  #TAUTHOR_TAG uses derivational morphology ;  #AUTHOR_TAG use, as a starting point, a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms :  #TAUTHOR_TAG uses derivational morphology ;  #AUTHOR_TAG use, as a starting point, a number of identical characters']",0
"['( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by  #TAUTHOR_TAG and called ter1vlostat.', 'the ten most speci c nouns have been produced by comparing our corpus of computing to the french corpus']","['( computer ), serveur ( server ), systme ( system ), utilisateur ( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by  #TAUTHOR_TAG and called ter1vlostat.', 'the ten most speci c nouns have been produced by comparing our corpus of computing to the french corpus']","['( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by  #TAUTHOR_TAG and called ter1vlostat.', 'the ten most speci c nouns have been produced by comparing our corpus of computing to the french corpus le monde,']","['construct this test set, we have focused our attention on ten domain - speci c terms : commande ( command ), con guration, chier ( le ), internet, logiciel ( software ), option, ordinateur ( computer ), serveur ( server ), systme ( system ), utilisateur ( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by  #TAUTHOR_TAG and called ter1vlostat.', 'the ten most speci c nouns have been produced by comparing our corpus of computing to the french corpus le monde, composed of newspaper articles  #AUTHOR_TAG.', 'note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '( they were removed from the example set.']",5
"['between terms :  #AUTHOR_TAG uses derivational morphology ;  #TAUTHOR_TAG use, as a starting point, a number of identical characters']","['between terms :  #AUTHOR_TAG uses derivational morphology ;  #TAUTHOR_TAG use, as a starting point, a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms :  #AUTHOR_TAG uses derivational morphology ;  #TAUTHOR_TAG use, as a starting point, a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms :  #AUTHOR_TAG uses derivational morphology ;  #TAUTHOR_TAG use, as a starting point, a number of identical characters']",0
"[')  #TAUTHOR_TAG or, more']","['of these endeavours have focused on purely statistical acquisition techniques  #AUTHOR_TAG, on linguisitic acquisition ( by the use of part - of - speech filters hand - crafted by a linguist )  #TAUTHOR_TAG or, more frequently, on']","['by a linguist )  #TAUTHOR_TAG or, more frequently, on a combination']","['the other hand, other work has been carried out in order to acquire collocations.', 'most of these endeavours have focused on purely statistical acquisition techniques  #AUTHOR_TAG, on linguisitic acquisition ( by the use of part - of - speech filters hand - crafted by a linguist )  #TAUTHOR_TAG or, more frequently, on a combination of the two  #AUTHOR_TAG kilgarri and  #AUTHOR_TAG for example ).', 'it is worth noting that although these techniques are able to identify n - v pairs, they do not specify the relationship between n and v, nor are they capable of focusing on a subset of n - v pairs.', 'the original acquisition methodology we present in the next section will allow us to overcome this limitation']",1
"['technique, inductive logic programming ( ilp )  #TAUTHOR_TAG, which']","['technique, inductive logic programming ( ilp )  #TAUTHOR_TAG, which']","['##res is based on a machine learning technique, inductive logic programming ( ilp )  #TAUTHOR_TAG, which']","['##res is based on a machine learning technique, inductive logic programming ( ilp )  #TAUTHOR_TAG, which infers general morpho - syntactic patterns from a set of examples ( this set is noted e + hereafter ) and counter - examples ( e a ) of the elements one wants to acquire and their context.', 'the contextual patterns produced can then be applied to the corpus in order to retrieve new elements.', 'the acquisition process can be summarized in 3 steps']",0
"['based on ` ` internal or ` ` external methods  #TAUTHOR_TAG, i. e. methods that rely on']","['based on ` ` internal or ` ` external methods  #TAUTHOR_TAG, i. e. methods that rely on']","[', most strategies are based on ` ` internal or ` ` external methods  #TAUTHOR_TAG, i. e. methods that rely on the form of terms or on the information gathered from contexts.', '(']","[', most strategies are based on ` ` internal or ` ` external methods  #TAUTHOR_TAG, i. e. methods that rely on the form of terms or on the information gathered from contexts.', '( in some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identification process. )', 'the work reported here infers specific semantic relationships based on sets of examples and counterexamples']",1
"['combinations in which verbs convey a meaning of realization.', 'the work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information  #TAUTHOR_TAG']","['combinations in which verbs convey a meaning of realization.', 'the work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information  #TAUTHOR_TAG']","['verb combinations in which verbs convey a meaning of realization.', 'the work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information  #TAUTHOR_TAG']","['this paper, the method is applied to a french corpus on computing to and noun - verb combinations in which verbs convey a meaning of realization.', 'the work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information  #TAUTHOR_TAG']",4
"['addition to its explanatory capacity, this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques  #TAUTHOR_TAG']","['addition to its explanatory capacity, this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques  #TAUTHOR_TAG']","['addition to its explanatory capacity, this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques  #TAUTHOR_TAG']","['addition to its explanatory capacity, this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques  #TAUTHOR_TAG']",4
"[' #TAUTHOR_TAG and called qualia relations  #AUTHOR_TAG.', 'here, we propose to use asares in a quite similar way to']","[' #TAUTHOR_TAG and called qualia relations  #AUTHOR_TAG.', 'here, we propose to use asares in a quite similar way to']","['of word pairs sharing semantic relations defined in the generative lexicon framework  #TAUTHOR_TAG and called qualia relations  #AUTHOR_TAG.', 'here, we propose to use asares in a quite similar way to']","['##res has been previously applied to the acquisition of word pairs sharing semantic relations defined in the generative lexicon framework  #TAUTHOR_TAG and called qualia relations  #AUTHOR_TAG.', 'here, we propose to use asares in a quite similar way to retrieve our valid n - v pairs.', 'however, the n - v combinations sought are more specific than those that were identified in these previous experiments']",0
['by  #TAUTHOR_TAG with wordnet relations )'],['by  #TAUTHOR_TAG with wordnet relations )'],['by  #TAUTHOR_TAG with wordnet relations )'],"['though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval.', 'indeed, such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by  #TAUTHOR_TAG with wordnet relations )']",1
"['qualia relations  #TAUTHOR_TAG.', 'here, we propose to use asares in a quite similar way to']","['qualia relations  #TAUTHOR_TAG.', 'here, we propose to use asares in a quite similar way to']","['qualia relations  #TAUTHOR_TAG.', 'here, we propose to use asares in a quite similar way to']","['##res has been previously applied to the acquisition of word pairs sharing semantic relations defined in the generative lexicon framework  #AUTHOR_TAG and called qualia relations  #TAUTHOR_TAG.', 'here, we propose to use asares in a quite similar way to retrieve our valid n - v pairs.', 'however, the n - v combinations sought are more specific than those that were identified in these previous experiments']",0
"['main benefits of this acquisition technique lie in the inferred patterns.', 'indeed, contrary to the more classical statistical methods ( mutual information, loglike..., see below ) used for collocation acquisition ( see  #TAUTHOR_TAG for a review ), these patterns allow']","['main benefits of this acquisition technique lie in the inferred patterns.', 'indeed, contrary to the more classical statistical methods ( mutual information, loglike..., see below ) used for collocation acquisition ( see  #TAUTHOR_TAG for a review ), these patterns allow']","['main benefits of this acquisition technique lie in the inferred patterns.', 'indeed, contrary to the more classical statistical methods ( mutual information, loglike..., see below ) used for collocation acquisition ( see  #TAUTHOR_TAG for a review ), these patterns allow']","['main benefits of this acquisition technique lie in the inferred patterns.', 'indeed, contrary to the more classical statistical methods ( mutual information, loglike..., see below ) used for collocation acquisition ( see  #TAUTHOR_TAG for a review ), these patterns allow']",0
"['.', 'asares is presented in detail in  #TAUTHOR_TAG.', 'we simply give a short account of its basic principles herein']","['method used for the acquisition of n - v pairs relies mainly on asares, a pattern inference tool.', 'asares is presented in detail in  #TAUTHOR_TAG.', 'we simply give a short account of its basic principles herein']","['method used for the acquisition of n - v pairs relies mainly on asares, a pattern inference tool.', 'asares is presented in detail in  #TAUTHOR_TAG.', 'we simply give a short account of its basic principles herein']","['method used for the acquisition of n - v pairs relies mainly on asares, a pattern inference tool.', 'asares is presented in detail in  #TAUTHOR_TAG.', 'we simply give a short account of its basic principles herein']",5
['number of applications have relied on distributional analysis  #TAUTHOR_TAG in'],['number of applications have relied on distributional analysis  #TAUTHOR_TAG in'],['number of applications have relied on distributional analysis  #TAUTHOR_TAG in'],"['number of applications have relied on distributional analysis  #TAUTHOR_TAG in order to build classes of semantically related terms.', 'this approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness  #AUTHOR_TAG for example ), does not specify the relationship itself.', 'hence, synonyms, co - hyponyms, hyperonyms, etc. are not differentiated']",0
['arabic language processing challenging  #TAUTHOR_TAG'],['arabic language processing challenging  #TAUTHOR_TAG'],['arabic language processing challenging  #TAUTHOR_TAG'],"['separate the edr task into two parts : a mention detection step, which identifies and classifies all the mentions in a text - and a coreference resolution step, which combinines the detected mentions into groups that refer to the same object.', 'in its entirety, the edr task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non - named mentions ( nominal and pronominal ) and the requirement of grouping mentions into entities.', 'this is particularly true for arabic where nominals and pronouns are also attached to the word they modify.', 'in fact, most arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form arabic surface forms ( blank - delimited words ).', 'in addition to the different forms of the arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the arabic surface word.', 'it is these orthographic variations and complex morphological structure that make arabic language processing challenging  #TAUTHOR_TAG']",0
"['recent investigations  #TAUTHOR_TAG, and have been']","['recent investigations  #TAUTHOR_TAG, and have been']","['several recent investigations  #TAUTHOR_TAG, and have been']",[' #TAUTHOR_TAG'],0
['program  #TAUTHOR_TAG : we will call'],['program  #TAUTHOR_TAG : we will call'],[' #TAUTHOR_TAG : we will call'],"['', 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program  #TAUTHOR_TAG : we will call the instances of textual references to objects / abstractions mentions, which can be either named ( e. g. john mayor ), nominal ( the president ) or pronominal ( she, it ).', 'john mayor ), nominal ( the president ) or pronominal ( she, it ).', 'an entity is the aggregate of all the mentions ( of any level ) which refer to one conceptual entity.', 'for instance, in the sentence president john smith said he has no comments there are two mentions ( named and pronomial ) but only one entity, formed by the set { john smith, he }']",5
"['chinese segmentation  #TAUTHOR_TAG.', 'for these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.', 'here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of']","['chinese segmentation  #TAUTHOR_TAG.', 'for these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.', 'here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of']","['chinese segmentation  #TAUTHOR_TAG.', 'for these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.', 'here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of']","['addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n - grams, similar to those used for chinese segmentation  #TAUTHOR_TAG.', 'for these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.', 'here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of their appearance in n - gram character contexts that appear in the training data.', 'the character based model alone achieves a 94. 5 % exact match segmentation accuracy, considerably less accurate then the dictionary based model']",1
"[', as shown through experiments on the  #TAUTHOR_TAG arabic data.', 'the experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the']","[', as shown through experiments on the  #TAUTHOR_TAG arabic data.', 'the experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'in addition, we also report results on the official test data']","['coreference resolution performance, as shown through experiments on the  #TAUTHOR_TAG arabic data.', 'the experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'in addition, we also report results on the official test data']","['types of features result in an improvement in both the mention detection and coreference resolution performance, as shown through experiments on the  #TAUTHOR_TAG arabic data.', 'the experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'in addition, we also report results on the official test data']",5
"['punctuation symbol is considered a separate token.', 'character classes, such as punctuation, are defined according to the unicode standard  #TAUTHOR_TAG']","['punctuation symbol is considered a separate token.', 'character classes, such as punctuation, are defined according to the unicode standard  #TAUTHOR_TAG']","['more alphabetic or numeric characters.', 'each punctuation symbol is considered a separate token.', 'character classes, such as punctuation, are defined according to the unicode standard  #TAUTHOR_TAG']","['2 ).', 'we define a token and introduce whitespace boundaries between every span of one or more alphabetic or numeric characters.', 'each punctuation symbol is considered a separate token.', 'character classes, such as punctuation, are defined according to the unicode standard  #TAUTHOR_TAG']",5
['arabic language processing challenging  #TAUTHOR_TAG'],['arabic language processing challenging  #TAUTHOR_TAG'],['arabic language processing challenging  #TAUTHOR_TAG'],"['separate the edr task into two parts : a mention detection step, which identifies and classifies all the mentions in a text - and a coreference resolution step, which combinines the detected mentions into groups that refer to the same object.', 'in its entirety, the edr task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non - named mentions ( nominal and pronominal ) and the requirement of grouping mentions into entities.', 'this is particularly true for arabic where nominals and pronouns are also attached to the word they modify.', 'in fact, most arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form arabic surface forms ( blank - delimited words ).', 'in addition to the different forms of the arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the arabic surface word.', 'it is these orthographic variations and complex morphological structure that make arabic language processing challenging  #TAUTHOR_TAG']",0
['coreference system system is similar to the bell tree algorithm as described by  #TAUTHOR_TAG'],['coreference system system is similar to the bell tree algorithm as described by  #TAUTHOR_TAG'],['coreference system system is similar to the bell tree algorithm as described by  #TAUTHOR_TAG'],['coreference system system is similar to the bell tree algorithm as described by  #TAUTHOR_TAG'],1
"['##s  #TAUTHOR_TAG.', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the']","['has two kinds of plurals : broken plurals and sound plurals  #TAUTHOR_TAG.', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the']","['##s and sound plurals  #TAUTHOR_TAG.', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example,']","['has two kinds of plurals : broken plurals and sound plurals  #TAUTHOR_TAG.', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the plural form of the noun ( man ) is ( men ), which is formed by inserting the infix.', 'the plural form of the noun ( book ) is ( books ), which is formed by deleting the infix.', 'the plural form and the singular form may also be completely different ( e. g. for woman, but for women ).', 'the sound plurals are formed by adding plural suffixes to singular nouns ( e. g., meaning researcher ) : the plural suffix is for feminine nouns in grammatical cases ( e. g.']",0
"['recent investigations  #TAUTHOR_TAG, and have been']","['recent investigations  #TAUTHOR_TAG, and have been']","['several recent investigations  #TAUTHOR_TAG, and have been']",[' #TAUTHOR_TAG'],0
"['( evidence ), the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model )  #TAUTHOR_TAG.', 'one big advantage of this']","['( evidence ), the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model )  #TAUTHOR_TAG.', 'one big advantage of this']","['( evidence ), the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model )  #TAUTHOR_TAG.', 'one big advantage of this approach is that it can combine arbitrary and diverse types of information in']","['formulate the mention detection problem as a classification problem, which takes as input segmented arabic text.', 'we assign to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions.', 'we use a maximum entropy markov model ( memm ) classifier.', 'the principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ), the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model )  #TAUTHOR_TAG.', 'one big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision']",0
['- category of the mention type  #TAUTHOR_TAG ('],"['mention sub - type, which is a sub - category of the mention type  #TAUTHOR_TAG ( e. g. orggovernmental, facilitypath, etc. )']","['mention sub - type, which is a sub - category of the mention type  #TAUTHOR_TAG (']","['mention sub - type, which is a sub - category of the mention type  #TAUTHOR_TAG ( e. g. orggovernmental, facilitypath, etc. )']",5
"['recent investigations  #TAUTHOR_TAG, and have been']","['recent investigations  #TAUTHOR_TAG, and have been']","['several recent investigations  #TAUTHOR_TAG, and have been']",[' #TAUTHOR_TAG'],0
['demonstrates a technique'],['demonstrates a technique'],['demonstrates a technique'],"['demonstrates a technique for segmenting arabic text and uses it as a morphological processing step in machine translation.', 'a trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules']",5
"['described in.', 'both systems are built around from the maximum - entropy technique  #TAUTHOR_TAG.', 'we']","['described in.', 'both systems are built around from the maximum - entropy technique  #TAUTHOR_TAG.', 'we']","['in.', 'both systems are built around from the maximum - entropy technique  #TAUTHOR_TAG.', 'we formulate the mention detection task as a sequence classification problem.', 'while this approach is language independent, it must be modified to accomodate the particulars of the arabic language.', 'the arabic words may be composed of zero']","['', 'both systems are built around from the maximum - entropy technique  #TAUTHOR_TAG.', 'we formulate the mention detection task as a sequence classification problem.', 'while this approach is language independent, it must be modified to accomodate the particulars of the arabic language.', 'the arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'we begin with a segmentation of the written text before starting the classification.', 'this segmentation process consists of separating the normal whitespace delimited words into ( hypothesized ) prefixes, stems, and suffixes, which become the subject of analysis ( tokens ).', 'the resulting granularity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label ( for instance, in the case of nominal and pronominal mentions ).', 'additionally, because the prefixes and suffixes are quite frequent, directly processing unsegmented words results in significant data sparseness']",5
['presented in  #TAUTHOR_TAG and the coreference resolution system is similar to'],['presented in  #TAUTHOR_TAG and the coreference resolution system is similar to'],['presented in  #TAUTHOR_TAG and the coreference resolution system is similar to'],"['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in  #TAUTHOR_TAG and the coreference resolution system is similar to the one described in  #AUTHOR_TAG.', 'both systems are built around from the maximum - entropy technique  #AUTHOR_TAG.', 'we formulate the mention detection task as a sequence classi cation problem.', 'while this approach is language independent, it must be modified to accomodate the particulars of the arabic language.', 'the arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'we begin with a segmentation of the written text before starting the classification.', 'this segmentation process consists of separating the normal whitespace delimited words into ( hypothesized ) prefixes, stems, and suffixes, which become the subject of analysis ( tokens ).', 'the resulting granulariity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label ( for instance, in the case of nominal and pronominal mentions ).', 'additionally, because the pre xes and su _ xes are quite frequent, directly processing unsegmented words results in signi cant data sparseness']",1
"['not  #TAUTHOR_TAG.', 'we']","['not  #TAUTHOR_TAG.', 'we']","['not  #TAUTHOR_TAG.', 'we denote these features as backward token']","['context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not  #TAUTHOR_TAG.', 'we denote these features as backward token tri - grams and forward token tri - grams for the previous and next context of t i respectively.', 'for a token t i, the backward token n - gram feature will contains the previous n − 1 tokens in the history ( t i−n + 1,...', 't i−1 ) and the forward token n - gram feature will contains the next n − 1 tokens ( t i + 1,...', 't i + n−1 )']",0
"['using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution  #TAUTHOR_TAG.', 'for arabic, since words are morphologically derived from a list of roots ( stems ), we expected that a feature based on the right and left stems would lead to improvement in system accuracy']","['using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution  #TAUTHOR_TAG.', 'for arabic, since words are morphologically derived from a list of roots ( stems ), we expected that a feature based on the right and left stems would lead to improvement in system accuracy']","['using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution  #TAUTHOR_TAG.', 'for arabic, since words are morphologically derived from a list of roots ( stems ), we expected that a feature based on the right and left stems would lead to improvement in system accuracy']","['using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution  #TAUTHOR_TAG.', 'for arabic, since words are morphologically derived from a list of roots ( stems ), we expected that a feature based on the right and left stems would lead to improvement in system accuracy']",5
"['recent investigations  #TAUTHOR_TAG, and have been']","['recent investigations  #TAUTHOR_TAG, and have been']","['several recent investigations  #TAUTHOR_TAG, and have been']",[' #TAUTHOR_TAG'],0
"['to generalize to improve the dictionary based model.', 'as in  #TAUTHOR_TAG,']","['to generalize to improve the dictionary based model.', 'as in  #TAUTHOR_TAG,']","['do not appear in the training data.', 'we seeked to exploit this ability to generalize to improve the dictionary based model.', 'as in  #TAUTHOR_TAG, we used unsupervised training data which is automatically segmented']","[', an analysis of the errors indicated that the character based model is more effective at segmenting words that do not appear in the training data.', 'we seeked to exploit this ability to generalize to improve the dictionary based model.', 'as in  #TAUTHOR_TAG, we used unsupervised training data which is automatically segmented to discover previously unseen stems.', 'in our case, the character n - gram model is used to segment a portion of the arabic gigaword corpus.', 'from this, we create a vocabulary of stems and affixes by requiring that tokens appear more than twice in the supervised training data or more than ten times in the unsupervised, segmented corpus']",5
"['recent investigations  #TAUTHOR_TAG, and have been']","['recent investigations  #TAUTHOR_TAG, and have been']","['several recent investigations  #TAUTHOR_TAG, and have been']",[' #TAUTHOR_TAG'],0
"['system is trained on the arabic ace 2003 and part of the 2004 data.', 'we introduce here a clearly defined and replicable split of the  #TAUTHOR_TAG data, so that future investigations can accurately and correctly compare against the results presented here']","['system is trained on the arabic ace 2003 and part of the 2004 data.', 'we introduce here a clearly defined and replicable split of the  #TAUTHOR_TAG data, so that future investigations can accurately and correctly compare against the results presented here']","['system is trained on the arabic ace 2003 and part of the 2004 data.', 'we introduce here a clearly defined and replicable split of the  #TAUTHOR_TAG data, so that future investigations can accurately and correctly compare against the results presented here']","['system is trained on the arabic ace 2003 and part of the 2004 data.', 'we introduce here a clearly defined and replicable split of the  #TAUTHOR_TAG data, so that future investigations can accurately and correctly compare against the results presented here']",5
"[' #TAUTHOR_TAG based backoff language model.', 'differing from  #AUTHOR_TAG, we have also introduced an explicit model for un - known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty.', 'using 0. 5m words from the combined arabic treebanks 1']","['trigram language model, specifically a kneser - ney  #TAUTHOR_TAG based backoff language model.', 'differing from  #AUTHOR_TAG, we have also introduced an explicit model for un - known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty.', 'using 0. 5m words from the combined arabic treebanks 1v2, 2v2 and 3v1, the dictionary based segmenter achieves a exact word match 97. 8 % correct segmentation']","['identifiers corresponding to dictionary entries.', 'the final machine is a trigram language model, specifically a kneser - ney  #TAUTHOR_TAG based backoff language model.', 'differing from  #AUTHOR_TAG, we have also introduced an explicit model for un - known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty.', 'using 0. 5m words from the combined arabic treebanks 1v2, 2v2 and 3v1, the dictionary based segmenter achieves a exact word match 97. 8 % correct segmentation']","['our latest implementation of this algorithm, we have recast this segmentation strategy as the composition of three distinct finite state machines.', 'the first machine, illustrated in figure 1 encodes the prefix and suffix expansion rules, producing a lattice of possible segmentations.', 'the second machine is a dictionary that accepts characters and produces identifiers corresponding to dictionary entries.', 'the final machine is a trigram language model, specifically a kneser - ney  #TAUTHOR_TAG based backoff language model.', 'differing from  #AUTHOR_TAG, we have also introduced an explicit model for un - known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty.', 'using 0. 5m words from the combined arabic treebanks 1v2, 2v2 and 3v1, the dictionary based segmenter achieves a exact word match 97. 8 % correct segmentation']",5
"['recent investigations  #TAUTHOR_TAG, and have been']","['recent investigations  #TAUTHOR_TAG, and have been']","['several recent investigations  #TAUTHOR_TAG, and have been']",[' #TAUTHOR_TAG'],0
"['recent investigations  #TAUTHOR_TAG, and have been']","['recent investigations  #TAUTHOR_TAG, and have been']","['several recent investigations  #TAUTHOR_TAG, and have been']",[' #TAUTHOR_TAG'],0
['- f is an entity - constrained mention fmeasure ( cfxxx  #TAUTHOR_TAG for how'],"['and ace - value.', 'ecm - f is an entity - constrained mention fmeasure ( cfxxx  #TAUTHOR_TAG for how']",['- f is an entity - constrained mention fmeasure ( cfxxx  #TAUTHOR_TAG for how'],"['this section, we present the coreference results on the devtest defined earlier.', 'first, to see the effect of stem matching features, we compare two coreference systems : one with the stem features, the other with - out.', 'we test the two systems on both "" true "" and system mentions of the devtest set.', 'true mentions mean that input to the coreference system are mentions marked by human, while system mentions are output from the mention detection system.', 'we report results with two metrics : ecm - f and ace - value.', 'ecm - f is an entity - constrained mention fmeasure ( cfxxx  #TAUTHOR_TAG for how ecm - f is computed ), and ace - value is the official ace evaluation metric.', 'the result is shown in table 4 : the baseline numbers without stem features are listed under \\ base, "" and the results of the coreference system with stem features are listed under \\ base + stem.']",5
"['##s  #TAUTHOR_TAG.', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the']","['has two kinds of plurals : broken plurals and sound plurals  #TAUTHOR_TAG.', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the']","['##s and sound plurals  #TAUTHOR_TAG.', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example,']","['has two kinds of plurals : broken plurals and sound plurals  #TAUTHOR_TAG.', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the plural form of the noun ( man ) is ( men ), which is formed by inserting the infix.', 'the plural form of the noun ( book ) is ( books ), which is formed by deleting the infix.', 'the plural form and the singular form may also be completely different ( e. g. for woman, but for women ).', 'the sound plurals are formed by adding plural suffixes to singular nouns ( e. g., meaning researcher ) : the plural suffix is for feminine nouns in grammatical cases ( e. g.']",0
"['described in  #TAUTHOR_TAG.', 'both systems are built around from the maximum - entropy technique  #AUTHOR_TAG.', 'we']","['described in  #TAUTHOR_TAG.', 'both systems are built around from the maximum - entropy technique  #AUTHOR_TAG.', 'we']","['in  #TAUTHOR_TAG.', 'both systems are built around from the maximum - entropy technique  #AUTHOR_TAG.', 'we formulate the mention detection task as a sequence classification problem.', 'while this approach is language independent, it must be modified to accomodate']",[' #TAUTHOR_TAG'],1
[' #TAUTHOR_TAG where the system will'],[' #TAUTHOR_TAG where the system will'],"['', ""as stated before, the experiments are run in the ace'04 framework  #TAUTHOR_TAG where the system will identify mentions""]","['want to investigate the usefulness of stem n - gram features in the mention detection system.', ""as stated before, the experiments are run in the ace'04 framework  #TAUTHOR_TAG where the system will identify mentions and will label them ( cfxxx section 4 ) with a type ( person, organization, etc ), a sub - type ( orgcommercial, orggovernmental, etc ), a mention level ( named, nominal, etc ), and a class ( specific, generic, etc )."", 'detecting the mention boundaries ( set of consecutive tokens ) and their main type is one of the important steps of our mention detection system.', 'the score that the ace community uses ( ace value ) attributes a higher importance ( outlined by its weight ) to the main type compared to other sub - tasks, such as the mention level and the class.', 'hence, to build our mention detection system we spent a lot of effort in improving the rst step : detecting the mention boundary and their main type.', 'in this paper, we report the results in terms of precision, recall, and f - measure3']",5
"['e, mk, m ) is an exponential or maximum entropy model  #TAUTHOR_TAG']","['e, mk, m ) is an exponential or maximum entropy model  #TAUTHOR_TAG']","['mk is one mention in entity e, and the basic model building block pl ( l = 1 | e, mk, m ) is an exponential or maximum entropy model  #TAUTHOR_TAG']","['mk is one mention in entity e, and the basic model building block pl ( l = 1 | e, mk, m ) is an exponential or maximum entropy model  #TAUTHOR_TAG']",5
"['were not supervised during the experiment.', ' #TAUTHOR_TAG observed that']","['were not supervised during the experiment.', ' #TAUTHOR_TAG observed that']","['automatically generated concept pair.', 'test subjects were invited via email to participate in the experiment.', 'thus, they were not supervised during the experiment.', ' #TAUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness.', '']","['developed a web - based interface to obtain human judgments of semantic relatedness for each automatically generated concept pair.', 'test subjects were invited via email to participate in the experiment.', 'thus, they were not supervised during the experiment.', ' #TAUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness.', 'their results differed particularly in cases of antonymy or distributionally related pairs.', 'we created a manual with a detailed introduction to sr stressing the crucial points.', 'the manual was presented to the subjects before the experiment and could be re - accessed at any time.', 'during the experiment, one concept pair at a time was presented to the test subjects in random ordering.', 'subjects had to assign a discrete relatedness value { 0, 1, 2, 3, 4 } to each pair.', ""figure 2 shows the system's gui""]",4
"['document ( e. g.', 'words and their surrounding context ), words or concepts  #TAUTHOR_TAG. 2 linguistic distance between words is inverse to their semantic similarity or relatedness']","['document ( e. g.', 'words and their surrounding context ), words or concepts  #TAUTHOR_TAG. 2 linguistic distance between words is inverse to their semantic similarity or relatedness']","['##ing correction.', 'it is defined on different kinds of textual units, e. g.', 'documents, parts of a document ( e. g.', 'words and their surrounding context ), words or concepts  #TAUTHOR_TAG. 2 linguistic distance between words is inverse to their semantic similarity or relatedness']","['distance plays an important role in many applications like information retrieval, word sense disambiguation, text summarization or spelling correction.', 'it is defined on different kinds of textual units, e. g.', 'documents, parts of a document ( e. g.', 'words and their surrounding context ), words or concepts  #TAUTHOR_TAG. 2 linguistic distance between words is inverse to their semantic similarity or relatedness']",0
"['experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im  #TAUTHOR_TAG. table 1 : comparison of previous experiments.', 'r']","['experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im  #TAUTHOR_TAG. table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein,']","['experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im  #TAUTHOR_TAG. table 1 : comparison of previous experiments.', 'r']","['comprehensive evaluation of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', ' #AUTHOR_TAG replicated the experiment of rubenstein and goodenough with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in  #AUTHOR_TAG.', ' #AUTHOR_TAG annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im  #TAUTHOR_TAG. table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein, gur = gurevych, z / g = zesch and gurevych similarity from "" not similar "" to "" synonymous "".', 'this elaborate process is not feasible for a larger dataset or if domain - specific test sets should be compiled quickly.', 'therefore, we automatically create word pairs using a corpus - based approach']",0
"['to  #TAUTHOR_TAG, there are three prevalent approaches']","['to  #TAUTHOR_TAG, there are three prevalent approaches']","['to  #TAUTHOR_TAG, there are three prevalent approaches']","['to  #TAUTHOR_TAG, there are three prevalent approaches for evaluating sr measures : mathematical analysis, applicationspecific evaluation and comparison with human judgments']",0
"['remarkably well and can be used to quickly create balanced test datasets.', ' #TAUTHOR_TAG pointed out that distribution plots of']","['remarkably well and can be used to quickly create balanced test datasets.', ' #TAUTHOR_TAG pointed out that distribution plots of']","['concepts.', 'to create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain - ing could be used.', 'however, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', ' #TAUTHOR_TAG pointed out that distribution plots of']","['distribution of averaged human judgments on the whole test set ( see figure 3 ) is almost balanced with a slight underrepresentation of highly related concepts.', 'to create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain - ing could be used.', 'however, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', ' #TAUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by rubenstein and goodenough display an empty horizontal band that could be used to separate related and unrelated pairs.', 'this empty band is not observed here.', 'however, figure 4 shows the distribution of averaged judgments with the highest agreement between annotators ( standard deviation < 0. 8 ).', 'the plot clearly shows an empty horizontal band with no judgments.', 'the connection between averaged judgments and standard deviation is plotted in figure 5']",1
"['with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in  #TAUTHOR_TAG.', ' #AUTHOR_TAG annotated a larger set of word pairs (']","['rubenstein and goodenough with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in  #TAUTHOR_TAG.', ' #AUTHOR_TAG annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about']","['with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in  #TAUTHOR_TAG.', ' #AUTHOR_TAG annotated a larger set of word pairs (']","['comprehensive evaluation of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', ' #AUTHOR_TAG replicated the experiment of rubenstein and goodenough with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in  #TAUTHOR_TAG.', ' #AUTHOR_TAG annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im  #AUTHOR_TAG table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein, gur = gurevych, z / g = zesch and gurevych similarity from "" not similar "" to "" synonymous "".', 'this elaborate process is not feasible for a larger dataset or if domain - specific test sets should be compiled quickly.', 'therefore, we automatically create word pairs using a corpus - based approach']",0
['by  #TAUTHOR_TAG with 10'],['by  #TAUTHOR_TAG with 10'],"['by  #TAUTHOR_TAG with 10 subjects.', 'table']","['the seminal work by  #AUTHOR_TAG, similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'test subjects were instructed to order the cards according to the "" similarity of meaning "" and then assign a continuous similarity value ( 0. 0 - 4. 0 ) to each card.', ' #AUTHOR_TAG replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'this experiment was again replicated by  #TAUTHOR_TAG with 10 subjects.', 'table']",0
"[""''project ( sir  #TAUTHOR_TAG systematically investigates the use of lexical - semantic relations between words or concepts""]","[""''project ( sir  #TAUTHOR_TAG systematically investigates the use of lexical - semantic relations between words or concepts""]","['extracted word pairs from three different domain - specific corpora ( see table 2 ).', 'this is motivated by the aim to enable research in information retrieval incorporating sr measures.', ""in particular, the ` ` semantic information retrieval'' project ( sir  #TAUTHOR_TAG systematically investigates the use of lexical - semantic relations between words or concepts""]","['extracted word pairs from three different domain - specific corpora ( see table 2 ).', 'this is motivated by the aim to enable research in information retrieval incorporating sr measures.', ""in particular, the ` ` semantic information retrieval'' project ( sir  #TAUTHOR_TAG systematically investigates the use of lexical - semantic relations between words or concepts for improving the performance of information retrieval systems""]",4
"[' #AUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #TAUTHOR_TAG']","[' #AUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #TAUTHOR_TAG']","[' #AUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #TAUTHOR_TAG.', 'the knowledge sources used']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based  #AUTHOR_TAG, ontology - based  #AUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #TAUTHOR_TAG.', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['expanded to concept pairs using germanet  #TAUTHOR_TAG, the']","['expanded to concept pairs using germanet  #TAUTHOR_TAG, the']","['noun pairs, 15 % noun - verb pairs and so on.', 'word pairs containing polysemous words are expanded to concept pairs using germanet  #TAUTHOR_TAG, the german equivalent to']","['implemented a set of filters for word pairs.', 'one group of filters removed unwanted word pairs.', 'word pairs are filtered if they contain at least one word that a ) has less than three letters b ) contains only uppercase letters ( mostly acronyms ) or c ) can be found in a stoplist.', 'another filter enforced a specified fraction of combinations of nouns ( n ), verbs ( v ) and adjectives ( a ) to be present in the result set.', 'we used the following parameters : were noun - noun pairs, 15 % noun - verb pairs and so on.', 'word pairs containing polysemous words are expanded to concept pairs using germanet  #TAUTHOR_TAG, the german equivalent to wordnet, as a sense inventory for each word.', 'it is the most complete resource of this type for german']",5
"['. g. whether a measure is a metric  #TAUTHOR_TAG. 4', 'however, mathematical analysis cannot tell us']","['to some formal properties, e. g. whether a measure is a metric  #TAUTHOR_TAG. 4', 'however, mathematical analysis cannot tell us']","['. g. whether a measure is a metric  #TAUTHOR_TAG. 4', 'however, mathematical analysis cannot tell us']","['analysis can assess a measure with respect to some formal properties, e. g. whether a measure is a metric  #TAUTHOR_TAG. 4', 'however, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application']",0
"['.', ' #TAUTHOR_TAG reported an intra -']","['r =. 647.', ' #TAUTHOR_TAG reported an intra - subject correlation']","['.', ' #TAUTHOR_TAG reported an intra - subject correlation']","['our experiment, intra - subject correlation was r =. 670 for the first and r =. 623 for the second individual who repeated the experiment, yielding a summarized intra - subject correlation of r =. 647.', ' #TAUTHOR_TAG reported an intra - subject correlation of r =. 85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs.', 'the values may again not be compared directly.', 'furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low']",1
"['our experiment, we annotated a high number of pairs similar in size to the test sets by  #AUTHOR_TAG and  #AUTHOR_TAG.', 'we used the revised experimental setup  #TAUTHOR_TAG, based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity']","['our experiment, we annotated a high number of pairs similar in size to the test sets by  #AUTHOR_TAG and  #AUTHOR_TAG.', 'we used the revised experimental setup  #TAUTHOR_TAG, based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity']","['our experiment, we annotated a high number of pairs similar in size to the test sets by  #AUTHOR_TAG and  #AUTHOR_TAG.', 'we used the revised experimental setup  #TAUTHOR_TAG, based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity']","['our experiment, we annotated a high number of pairs similar in size to the test sets by  #AUTHOR_TAG and  #AUTHOR_TAG.', 'we used the revised experimental setup  #TAUTHOR_TAG, based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity and included also non noun - noun pairs.', 'additionally, our corpusbased approach includes domain - specific technical terms and enables evaluation of the robustness of a measure']",5
"['our experiment, we annotated a high number of pairs similar in size to the test sets by  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'we used the revised experimental setup  #AUTHOR_TAG, based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity']","['our experiment, we annotated a high number of pairs similar in size to the test sets by  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'we used the revised experimental setup  #AUTHOR_TAG, based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity']","['our experiment, we annotated a high number of pairs similar in size to the test sets by  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'we used the revised experimental setup  #AUTHOR_TAG, based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity']","['our experiment, we annotated a high number of pairs similar in size to the test sets by  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'we used the revised experimental setup  #AUTHOR_TAG, based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity and included also non noun - noun pairs.', 'additionally, our corpusbased approach includes domain - specific technical terms and enables evaluation of the robustness of a measure']",1
"['- based  #TAUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #AUTHOR_TAG']","['concepts have been proposed, e. g. dictionary - based  #AUTHOR_TAG, ontology - based  #TAUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #AUTHOR_TAG']","['- based  #TAUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #AUTHOR_TAG.', 'the knowledge sources used']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g. dictionary - based  #AUTHOR_TAG, ontology - based  #TAUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #AUTHOR_TAG.', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['are antonyms ( high - low ).', 'many nlp applications require knowledge about semantic relatedness rather than just similarity  #TAUTHOR_TAG']","['are antonyms ( high - low ).', 'many nlp applications require knowledge about semantic relatedness rather than just similarity  #TAUTHOR_TAG']","['they are antonyms ( high - low ).', 'many nlp applications require knowledge about semantic relatedness rather than just similarity  #TAUTHOR_TAG']","['similarity is typically defined via the lexical relations of synonymy ( automobile - car ) and hypernymy ( vehicle - car ), while semantic relatedness ( sr ) is defined to cover any kind of lexical or functional association that may exist be - tween two words  #AUTHOR_TAG. 3', 'dissimilar words can be semantically related, e. g.', 'via functional relationships ( night - dark ) or when they are antonyms ( high - low ).', 'many nlp applications require knowledge about semantic relatedness rather than just similarity  #TAUTHOR_TAG']",0
"['by  #TAUTHOR_TAG, similarity']","['by  #TAUTHOR_TAG, similarity']","['the seminal work by  #TAUTHOR_TAG, similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'test subjects were instructed to order the cards according to']","['the seminal work by  #TAUTHOR_TAG, similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'test subjects were instructed to order the cards according to the "" similarity of meaning "" and then assign a continuous similarity value ( 0. 0 - 4. 0 ) to each card.', ' #AUTHOR_TAG replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'this experiment was again replicated by  #AUTHOR_TAG with 10 subjects.', 'table']",0
"['. g. word sense disambiguation  #TAUTHOR_TAG or malapropism detection  #AUTHOR_TAG.  #AUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses']","[', e. g. word sense disambiguation  #TAUTHOR_TAG or malapropism detection  #AUTHOR_TAG.  #AUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses']","['. g. word sense disambiguation  #TAUTHOR_TAG or malapropism detection  #AUTHOR_TAG.  #AUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses']","['latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e. g. word sense disambiguation  #TAUTHOR_TAG or malapropism detection  #AUTHOR_TAG.  #AUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'a certain measure may work well in one application, but not in another.', 'application - based evaluation can only state the fact, but give little explanation about the reasons']",0
"['. g.', 'word sense disambiguation  #AUTHOR_TAG or malapropism detection  #AUTHOR_TAG.', ' #TAUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses']","['application, e. g.', 'word sense disambiguation  #AUTHOR_TAG or malapropism detection  #AUTHOR_TAG.', ' #TAUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses']","['', 'word sense disambiguation  #AUTHOR_TAG or malapropism detection  #AUTHOR_TAG.', ' #TAUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses']","['latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e. g.', 'word sense disambiguation  #AUTHOR_TAG or malapropism detection  #AUTHOR_TAG.', ' #TAUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'a certain measure may work well in one application, but not in another.', 'application - based evaluation can only state the fact, but give little explanation about the reasons']",0
['- weighting scheme  #TAUTHOR_TAG'],['8 tf. idf - weighting scheme  #TAUTHOR_TAG'],['8 tf. idf - weighting scheme  #TAUTHOR_TAG'],"['three preprocessing steps ( tokenization, pos - tagging, lemmatization ) are performed using treetagger  #AUTHOR_TAG.', ""the resulting list of pos - tagged lemmas is weighted using the smart ` ltc'8 tf. idf - weighting scheme  #TAUTHOR_TAG""]",5
"['of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', ' #AUTHOR_TAG did not report inter - subject correlation for their larger dataset.', ' #TAUTHOR_TAG reported a']","['of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', ' #AUTHOR_TAG did not report inter - subject correlation for their larger dataset.', ' #TAUTHOR_TAG reported a']","['of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', ' #AUTHOR_TAG did not report inter - subject correlation for their larger dataset.', ' #TAUTHOR_TAG reported a correlation of r =. 69.', 'test subjects were trained students of computational linguistics, and word pairs were selected analytically']","['summarized inter - subject correlation between 21 subjects was r =. 478 ( cf. is statistically significant at p <. 05.', 'this correlation coefficient is an upper bound of performance for automatic sr measures applied on the same dataset.', ' #AUTHOR_TAG reported a correlation of r =. 9026. 10', 'the results are not directly comparable, because he only used noun - noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', ' #AUTHOR_TAG did not report inter - subject correlation for their larger dataset.', ' #TAUTHOR_TAG reported a correlation of r =. 69.', 'test subjects were trained students of computational linguistics, and word pairs were selected analytically']",1
"['brother : ` ` brother, male sibling vs. ` ` brother, comrade, friend  #TAUTHOR_TAG.', 'we removed words which had more than three senses']","['brother : ` ` brother, male sibling vs. ` ` brother, comrade, friend  #TAUTHOR_TAG.', 'we removed words which had more than three senses']","['brother : ` ` brother, male sibling vs. ` ` brother, comrade, friend  #TAUTHOR_TAG.', 'we removed words which had more than three senses']","['##et contains only a few conceptual glosses.', 'as they are required to enable test subjects to distinguish between senses, we use artificial glosses composed from synonyms and hypernyms as a surrogate, e. g. for brother : ` ` brother, male sibling vs. ` ` brother, comrade, friend  #TAUTHOR_TAG.', 'we removed words which had more than three senses']",5
"['are aware of.', 'automatic corpus - based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical - semantic relations.', ' #TAUTHOR_TAG pointed out that many relations between words in a text are non - classical ( i. e. other than typical taxonomic relations like synonym']","['are aware of.', 'automatic corpus - based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical - semantic relations.', ' #TAUTHOR_TAG pointed out that many relations between words in a text are non - classical ( i. e. other than typical taxonomic relations like synonymy or hypernymy ) and']","['to select only highly related pairs connected by relations they are aware of.', 'automatic corpus - based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical - semantic relations.', ' #TAUTHOR_TAG pointed out that many relations between words in a text are non - classical ( i. e. other than typical taxonomic relations like synonym']","[', manually selected word pairs are often biased towards highly related pairs  #AUTHOR_TAG, because human annotators tend to select only highly related pairs connected by relations they are aware of.', 'automatic corpus - based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical - semantic relations.', ' #TAUTHOR_TAG pointed out that many relations between words in a text are non - classical ( i. e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity']",0
"['with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in  #AUTHOR_TAG.', ' #TAUTHOR_TAG annotated a larger set of word pairs (']","['rubenstein and goodenough with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in  #AUTHOR_TAG.', ' #TAUTHOR_TAG annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about']","['with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in  #AUTHOR_TAG.', ' #TAUTHOR_TAG annotated a larger set of word pairs (']","['comprehensive evaluation of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', ' #AUTHOR_TAG replicated the experiment of rubenstein and goodenough with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in  #AUTHOR_TAG.', ' #TAUTHOR_TAG annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im  #AUTHOR_TAG table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein, gur = gurevych, z / g = zesch and gurevych similarity from "" not similar "" to "" synonymous "".', 'this elaborate process is not feasible for a larger dataset or if domain - specific test sets should be compiled quickly.', 'therefore, we automatically create word pairs using a corpus - based approach']",0
"['using treetagger  #TAUTHOR_TAG.', ""the resulting list of pos - tagged lemmas is weighted using the smart'ltc'8 tf. id""]","['using treetagger  #TAUTHOR_TAG.', ""the resulting list of pos - tagged lemmas is weighted using the smart'ltc'8 tf. idf - weighting scheme  #AUTHOR_TAG""]","['lemmatization ) are performed using treetagger  #TAUTHOR_TAG.', ""the resulting list of pos - tagged lemmas is weighted using the smart'ltc'8 tf. idf - weighting scheme  #AUTHOR_TAG""]","['three preprocessing steps ( tokenization, pos - tagging, lemmatization ) are performed using treetagger  #TAUTHOR_TAG.', ""the resulting list of pos - tagged lemmas is weighted using the smart'ltc'8 tf. idf - weighting scheme  #AUTHOR_TAG""]",5
"['- based  #TAUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #AUTHOR_TAG']","['concepts have been proposed, e. g.', 'dictionary - based  #AUTHOR_TAG, ontology - based  #TAUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #AUTHOR_TAG']","['- based  #TAUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #AUTHOR_TAG.', 'the knowledge sources used']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based  #AUTHOR_TAG, ontology - based  #TAUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #AUTHOR_TAG.', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['- based  #AUTHOR_TAG, information - based  #TAUTHOR_TAG or distributional  #AUTHOR_TAG']","['computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based  #AUTHOR_TAG, ontology - based  #AUTHOR_TAG, information - based  #TAUTHOR_TAG or distributional  #AUTHOR_TAG']","['- based  #AUTHOR_TAG, information - based  #TAUTHOR_TAG or distributional  #AUTHOR_TAG.', 'the knowledge sources used']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based  #AUTHOR_TAG, ontology - based  #AUTHOR_TAG, information - based  #TAUTHOR_TAG or distributional  #AUTHOR_TAG.', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['g.', 'dictionary - based  #TAUTHOR_TAG,']","['concepts have been proposed, e. g.', 'dictionary - based  #TAUTHOR_TAG, ontology - based  #AUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #AUTHOR_TAG']","['concepts have been proposed, e. g.', 'dictionary - based  #TAUTHOR_TAG,']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based  #TAUTHOR_TAG, ontology - based  #AUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #AUTHOR_TAG.', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['differences in meaning between senses are very fine - grained, distinguishing between them is hard even for humans  #TAUTHOR_TAG. 6', 'pairs containing such words are not suitable for evaluation.', 'to limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'words with a number of senses above the threshold are removed from the list']","['differences in meaning between senses are very fine - grained, distinguishing between them is hard even for humans  #TAUTHOR_TAG. 6', 'pairs containing such words are not suitable for evaluation.', 'to limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'words with a number of senses above the threshold are removed from the list']","['differences in meaning between senses are very fine - grained, distinguishing between them is hard even for humans  #TAUTHOR_TAG. 6', 'pairs containing such words are not suitable for evaluation.', 'to limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'words with a number of senses above the threshold are removed from the list']","['differences in meaning between senses are very fine - grained, distinguishing between them is hard even for humans  #TAUTHOR_TAG. 6', 'pairs containing such words are not suitable for evaluation.', 'to limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'words with a number of senses above the threshold are removed from the list']",0
"['pairs  #TAUTHOR_TAG, because human annotators']","['pairs  #TAUTHOR_TAG, because human annotators']","['pairs  #TAUTHOR_TAG, because human annotators']","['assume that due to lexical - semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'resulting from our corpus - based approach, test sets will also contain domain - specific terms.', 'previous studies only included general terms as opposed to domain - specific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domain - specific or technical terms.', 'this is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms ( porsche ) rather than general ones ( car ).', 'furthermore, manually selected word pairs are often biased towards highly related pairs  #TAUTHOR_TAG, because human annotators tend to select only highly related pairs connected by relations they are aware of.', 'automatic corpus - based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical - semantic relations.', ' #AUTHOR_TAG pointed out that many relations between words in a text are non - classical ( i. e.', 'other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity']",0
"['of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', ' #TAUTHOR_TAG did not report inter - subject correlation for their larger dataset.', ' #AUTHOR_TAG reported a']","['of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', ' #TAUTHOR_TAG did not report inter - subject correlation for their larger dataset.', ' #AUTHOR_TAG reported a']","['of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', ' #TAUTHOR_TAG did not report inter - subject correlation for their larger dataset.', ' #AUTHOR_TAG reported a correlation of']","['summarized inter - subject correlation between 21 subjects was r =. 478 ( cf. is statistically significant at p <. 05.', 'this correlation coefficient is an upper bound of performance for automatic sr measures applied on the same dataset.', ' #AUTHOR_TAG reported a correlation of r =. 9026. 10', 'the results are not directly comparable, because he only used noun - noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', ' #TAUTHOR_TAG did not report inter - subject correlation for their larger dataset.', ' #AUTHOR_TAG reported a correlation of r =. 69.', 'test subjects were trained students of computational linguistics, and word pairs were selected analytically']",1
"['. 10', ' #TAUTHOR_TAG reported a']","['r =. 9026. 10', ' #TAUTHOR_TAG reported a']","['. 10', ' #TAUTHOR_TAG reported a correlation']","['summarized inter - subject correlation between 21 subjects was r =. 478 ( cf. is statistically significant at p <. 05.', 'this correlation coefficient is an upper bound of performance for automatic sr measures applied on the same dataset.', ' #AUTHOR_TAG reported a correlation of r =. 9026. 10', ' #TAUTHOR_TAG reported a correlation of r =. 9026. 10 the results are not directly comparable, because he only used noun - noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', ' #AUTHOR_TAG did not report inter - subject correlation for their larger dataset.', ' #AUTHOR_TAG reported a correlation of r =. 69.', 'test subjects were trained students of computational linguistics, and word pairs were selected analytically']",1
"['.', ' #TAUTHOR_TAG replicated the']","['of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', ' #TAUTHOR_TAG replicated the']","['##ed continuous values is felt to overstrain the test subjects.', ' #TAUTHOR_TAG replicated the experiment']","['comprehensive evaluation of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', ' #TAUTHOR_TAG replicated the experiment of rubenstein and goodenough with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in  #AUTHOR_TAG.', ' #AUTHOR_TAG annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im  #AUTHOR_TAG table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein, gur = gurevych, z / g = zesch and gurevych similarity from "" not similar "" to "" synonymous "".', 'this elaborate process is not feasible for a larger dataset or if domain - specific test sets should be compiled quickly.', 'therefore, we automatically create word pairs using a corpus - based approach']",0
['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],"['coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations ( like cooccurrence of words ) and is thus a more complicated task.', 'judgments for such relations strongly depend on experience and cultural background of the test subjects.', 'while most people may agree 10 note that resnik used the averaged correlation coefficient.', 'we computed the summarized correlation coefficient using a fisher z - value transformation.', 'that ( car - vehicle ) are highly related, a strong connection between ( parts - speech ) may only be established by a certain group.', 'due to the corpusbased approach, many domain - specific concept pairs are introduced into the test set.', 'therefore, inter - subject correlation is lower than the results obtained by  #TAUTHOR_TAG']",1
"['- based  #AUTHOR_TAG, information - based  #TAUTHOR_TAG or distributional  #AUTHOR_TAG']","['computing semantic relatedness of words or concepts have been proposed, e. g. dictionary - based  #AUTHOR_TAG, ontology - based  #AUTHOR_TAG, information - based  #TAUTHOR_TAG or distributional  #AUTHOR_TAG']","['- based  #AUTHOR_TAG, information - based  #TAUTHOR_TAG or distributional  #AUTHOR_TAG.', 'the knowledge sources used']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g. dictionary - based  #AUTHOR_TAG, ontology - based  #AUTHOR_TAG, information - based  #TAUTHOR_TAG or distributional  #AUTHOR_TAG.', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
[' #TAUTHOR_TAG b ) and shingling techniques described by  #AUTHOR_TAG'],[' #TAUTHOR_TAG b ) and shingling techniques described by  #AUTHOR_TAG'],[' #TAUTHOR_TAG b ) and shingling techniques described by  #AUTHOR_TAG'],"['site based corpus annotation - in which the user can specify a web site to annotate • domain based corpus annotation - in which the user specifies a content domain ( with the use of keywords ) to annotate • crawler based corpus annotation - more general web based corpus annotation in which crawlers are used to locate web pages from a computational linguistic view, the framework will also need to take into account the granularity of the unit ( for example, pos tagging requires sentence - units, but anaphoric annotation needs paragraphs or larger ).', 'secondly, we need to investigate techniques for identifying identical documents, virtually identical documents and highly repetitive documents, such as those pioneered by  #TAUTHOR_TAG b ) and shingling techniques described by  #AUTHOR_TAG']",3
"["" #AUTHOR_TAG a ) and the linguist's search engine  #TAUTHOR_TAG""]","["" #AUTHOR_TAG a ) and the linguist's search engine  #TAUTHOR_TAG""]","["" #AUTHOR_TAG a ) and the linguist's search engine  #TAUTHOR_TAG""]","['', ' #AUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp  #AUTHOR_TAG, kwicfinder  #AUTHOR_TAG a ) and the linguist's search engine  #TAUTHOR_TAG""]",0
"['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible  #AUTHOR_TAG : 56 ) unless the web is used as a corpus  #TAUTHOR_TAG.', 'increasingly, corpus researchers are tapping the web to overcome the sparse']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible  #AUTHOR_TAG : 56 ) unless the web is used as a corpus  #TAUTHOR_TAG.', 'increasingly, corpus researchers are tapping the web to overcome the sparse']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible  #AUTHOR_TAG : 56 ) unless the web is used as a corpus  #TAUTHOR_TAG.', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem  #AUTHOR_TAG.', 'this topic generated intense interest']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible  #AUTHOR_TAG : 56 ) unless the web is used as a corpus  #TAUTHOR_TAG.', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem  #AUTHOR_TAG.', '']",0
"['escience  #TAUTHOR_TAG.', 'while such an approach promises much in']","['escience  #TAUTHOR_TAG.', 'while such an approach promises much in']","['escience  #TAUTHOR_TAG.', 'while such an approach promises much in']","['the areas of natural language processing ( nlp ) and computational linguistics, proposals have been made for using the computational grid for data - intensive nlp and text - mining for escience  #TAUTHOR_TAG.', 'while such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a p2p approach.', 'in simple terms, p2p is a technology that takes advantage of the resources and services available at the edge of the internet  #AUTHOR_TAG.', 'better known for file - sharing and instant messenger applications, p2p has increasingly been applied in distributed computational systems.', 'examples include seti @ home ( looking for radio evidence of extraterrestrial life ), climateprediction. net ( studying climate change ), predictor @ home ( investigating protein - related diseases ) and einstein @ home ( searching for gravitational signals )']",0
"["" #AUTHOR_TAG a ) and the linguist's search engine  #TAUTHOR_TAG""]","["" #AUTHOR_TAG a ) and the linguist's search engine  #TAUTHOR_TAG""]","["" #AUTHOR_TAG a ) and the linguist's search engine  #TAUTHOR_TAG""]","['', ' #AUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp  #AUTHOR_TAG, kwicfinder  #AUTHOR_TAG a ) and the linguist's search engine  #TAUTHOR_TAG""]",0
"["" #TAUTHOR_TAG a ) and the linguist's search""]","["" #TAUTHOR_TAG a ) and the linguist's search""]","[""##er  #TAUTHOR_TAG a ) and the linguist's search engine  #AUTHOR_TAG""]","['', ' #AUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp  #AUTHOR_TAG, kwicfinder  #TAUTHOR_TAG a ) and the linguist's search engine  #AUTHOR_TAG""]",0
"['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG, 2004b ) and received a special']","['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG, 2004b ) and received a special']","['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG, 2004b ) and received a special issue']","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times  #TAUTHOR_TAG, 2004b ) and received a special issue of the journal computational linguistics  #AUTHOR_TAG.', 'studies have used several different methods to mine web data.', '']",0
"['are inconsistent and unreliable  #TAUTHOR_TAG.', 'tools based on static corpora do not suffer from this problem,']","['are inconsistent and unreliable  #TAUTHOR_TAG.', 'tools based on static corpora do not suffer from this problem, e. g.', 'bncweb']","['key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies.', 'word frequency counts in internet search engines are inconsistent and unreliable  #TAUTHOR_TAG.', 'tools based on static corpora do not suffer from this problem, e. g.', 'bn']","['key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies.', 'word frequency counts in internet search engines are inconsistent and unreliable  #TAUTHOR_TAG.', 'tools based on static corpora do not suffer from this problem, e. g.', 'bncweb 7, developed at the university of zurich, and view 8 ( variation in english words and phrases, developed at brigham young university ) are both based on the british national corpus.', 'both bncweb and view enable access to annotated corpora and facilitate searching on part - ofspeech tags.', 'in addition, pie 9 ( phrases in english ), developed at usna, which performs searches on n - grams ( based on words, parts - ofspeech and characters ), is currently restricted to the british national corpus as well, although other static corpora are being added to its database.', 'in contrast, little progress has been made toward annotating sizable sample corpora from the web']",0
['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special'],['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special'],['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special issue'],"['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two ap - proaches e. g. automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws4, amal - gam5, connexor6 ) are available, for example, in the application of part - of - speech tags to corpora.', 'existing tagging systems are small scale and typically impose some limitation to prevent over - load ( e. g. restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin - gle - server systems.', 'this corpus annotation bot - tleneck becomes even more problematic for vo - luminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special issue of the journal computational linguistics  #AUTHOR_TAG.', 'studies have used several different methods to mine web data.', ' #AUTHOR_TAG extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', ' #AUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp  #AUTHOR_TAG, kwicfinder  #AUTHOR_TAG a ) and the linguists search en - gine  #AUTHOR_TAG']",0
"['are well documented  #TAUTHOR_TAG.', '']","['are well documented  #TAUTHOR_TAG.', '']","['##to in april 2006.', 'in addition, the advantages of using linguistically annotated data over raw data are well documented  #TAUTHOR_TAG.', '']","['', 'in addition, the advantages of using linguistically annotated data over raw data are well documented  #TAUTHOR_TAG.', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega - corpus']",0
"['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible  #TAUTHOR_TAG : 56 ) unless the web is used as a corpus  #AUTHOR_TAG.', 'increasingly, corpus researchers are tapping the web to overcome the sparse']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible  #TAUTHOR_TAG : 56 ) unless the web is used as a corpus  #AUTHOR_TAG.', 'increasingly, corpus researchers are tapping the web to overcome the sparse']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible  #TAUTHOR_TAG : 56 ) unless the web is used as a corpus  #AUTHOR_TAG.', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem  #AUTHOR_TAG.', 'this topic generated intense interest']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible  #TAUTHOR_TAG : 56 ) unless the web is used as a corpus  #AUTHOR_TAG.', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem  #AUTHOR_TAG.', '']",0
"['tested with the development of plug - ins supporting instant messaging, distributed video encoding  #TAUTHOR_TAG, distributed virtual worlds  #AUTHOR_TAG and digital library management  #AUTHOR_TAG.', 'it is our intention to implement our distributed corpus annotation']","['tested with the development of plug - ins supporting instant messaging, distributed video encoding  #TAUTHOR_TAG, distributed virtual worlds  #AUTHOR_TAG and digital library management  #AUTHOR_TAG.', 'it is our intention to implement our distributed corpus annotation']","['has been successfully tested with the development of plug - ins supporting instant messaging, distributed video encoding  #TAUTHOR_TAG, distributed virtual worlds  #AUTHOR_TAG and digital library management  #AUTHOR_TAG.', 'it is our intention to implement our distributed corpus annotation framework as a plug - in.', 'this will involve implementing new functionality and integrating this with our existing annotation tools (']","['second stage of our work will involve im - plementing the framework within a p2p environment.', ""we have already developed a prototype of an object - oriented application environment to support p2p system development using jxta ( sun's p2p api )."", 'we have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'this system has been successfully tested with the development of plug - ins supporting instant messaging, distributed video encoding  #TAUTHOR_TAG, distributed virtual worlds  #AUTHOR_TAG and digital library management  #AUTHOR_TAG.', 'it is our intention to implement our distributed corpus annotation framework as a plug - in.', 'this will involve implementing new functionality and integrating this with our existing annotation tools ( such as claws11 ).', 'the development environment is also flexible enough to utilise the boinc platform, and such support will be built into it']",0
"['internet  #TAUTHOR_TAG.', 'better known for']","['internet  #TAUTHOR_TAG.', 'better known for file - sharing']","['the internet  #TAUTHOR_TAG.', 'better known for']","['the areas of natural language processing ( nlp ) and computational linguistics, proposals have been made for using the computational grid for data - intensive nlp and text - mining for e - science  #AUTHOR_TAG.', 'while such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a p2p approach.', 'in simple terms, p2p is a technology that takes advantage of the resources and services available at the edge of the internet  #TAUTHOR_TAG.', 'better known for file - sharing and instant messenger applications, p2p has increasingly been applied in distributed computational systems.', 'examples include seti @ home ( looking for radio evidence of extraterrestrial life ), climateprediction. net ( studying climate change ), predictor @ home ( investigating protein - related diseases ) and einstein @ home ( searching for gravitational signals )']",0
"['digital library management  #TAUTHOR_TAG.', 'it is our intention to implement our distributed corpus annotation']","['digital library management  #TAUTHOR_TAG.', 'it is our intention to implement our distributed corpus annotation']","['digital library management  #TAUTHOR_TAG.', 'it is our intention to implement our distributed corpus annotation framework as a plugin.', 'this will involve implementing new functionality and integrating this with our existing annotation tools (']","['second stage of our work will involve implementing the framework within a p2p environment.', ""we have already developed a prototype of an object - oriented application environment to support p2p system development using jxta ( sun's p2p api )."", 'we have designed this environment so that specific application functionality can be captured within plug - ins that can then integrate with the environment and utilise its functionality.', 'this system has been successfully tested with the development of plug - ins supporting instant messaging, distributed video encoding  #AUTHOR_TAG, distributed virtual worlds  #AUTHOR_TAG and digital library management  #TAUTHOR_TAG.', 'it is our intention to implement our distributed corpus annotation framework as a plugin.', 'this will involve implementing new functionality and integrating this with our existing annotation tools ( such as claws 11 ).', 'the development environment is also flexible enough to utilise the boinc platform, and such support will be built into it']",0
['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special'],['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special'],['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special issue'],"['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special issue of the journal computational linguistics  #AUTHOR_TAG.', 'studies have used several different methods to mine web data.', '']",0
"['the journal computational linguistics  #TAUTHOR_TAG.', 'studies have used several different methods to mine web data.', ' #AUTHOR_TAG extracts word co - occurrence probabilities from unlabelled']","['the journal computational linguistics  #TAUTHOR_TAG.', 'studies have used several different methods to mine web data.', ' #AUTHOR_TAG extracts word co - occurrence probabilities from unlabelled']","['the journal computational linguistics  #TAUTHOR_TAG.', 'studies have used several different methods to mine web data.', ' #AUTHOR_TAG extracts word co - occurrence probabilities from unlabelled text']","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times  #AUTHOR_TAG 2004b ) and received a special issue of the journal computational linguistics  #TAUTHOR_TAG.', 'studies have used several different methods to mine web data.', ' #AUTHOR_TAG extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', ' #AUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp  #AUTHOR_TAG, kwicfinder  #AUTHOR_TAG a ) and the linguist's search engine  #AUTHOR_TAG""]",0
"['', ' #TAUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : web']","['', ' #TAUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp  #AUTHOR_TAG,']","['', ' #TAUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp  #AUTHOR_TAG, kwic']","['', ' #TAUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp  #AUTHOR_TAG, kwicfinder  #AUTHOR_TAG a ) and the linguist's search engine  #AUTHOR_TAG""]",0
"['##up of these formats with tools like hyppia - bte, tidy and parcels3  #TAUTHOR_TAG']","['cleanup of these formats with tools like hyppia - bte, tidy and parcels3  #TAUTHOR_TAG']","['##up of these formats with tools like hyppia - bte, tidy and parcels3  #TAUTHOR_TAG']","['key aspect of our case study research will be to investigate extending corpus collection to new document types.', 'most web - derived corpora have exploited raw text or html pages, so efforts have focussed on boilerplate removal and cleanup of these formats with tools like hyppia - bte, tidy and parcels3  #TAUTHOR_TAG']",0
"['problem  #TAUTHOR_TAG.', 'this topic generated intense']","['problem  #TAUTHOR_TAG.', 'this topic generated intense']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible  #AUTHOR_TAG : 56 ) unless the web is used as a corpus  #AUTHOR_TAG.', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem  #TAUTHOR_TAG.', 'this topic generated intense interest']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible  #AUTHOR_TAG : 56 ) unless the web is used as a corpus  #AUTHOR_TAG.', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem  #TAUTHOR_TAG.', '']",0
[' #TAUTHOR_TAG also selects sentences by syntactic criteria from large on - line'],[' #TAUTHOR_TAG also selects sentences by syntactic criteria from large on - line'],"['the lse completes the annotation process, which may entail significant delay with multiple users of the lse server.', 'the gsearch system  #TAUTHOR_TAG also selects sentences by syntactic criteria from large on - line text collections.', 'gs']","['', 'a new collection does not become available for analysis until the lse completes the annotation process, which may entail significant delay with multiple users of the lse server.', 'the gsearch system  #TAUTHOR_TAG also selects sentences by syntactic criteria from large on - line text collections.', 'gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre - existing syntactic mark - up.', 'in contrast, the sketch engine system to assist lexicographers to construct dictionary entries requires large pre - annotated corpora.', ""a word sketch is an automatic one - page corpus - derived summary of a word's grammatical and collocational behaviour."", 'word sketches were first used to prepare the macmillan english dictionary for  #AUTHOR_TAG, edited by michael rundell ).', 'they have also served as the starting point for high - accuracy word sense disambiguation.', 'more recently, the sketch engine was used to develop the new edition of the oxford thesaurus of  #AUTHOR_TAG, edited by maurice waite )']",0
['word sense tagging  #TAUTHOR_TAG and in the'],['word sense tagging  #TAUTHOR_TAG and in the long - standing'],['word sense tagging  #TAUTHOR_TAG and in the long - standing use of part -'],"['annotation of corpora contributes crucially to the study of language at several levels : morphology, syntax, semantics, and discourse.', 'its significance is reflected both in the growing interest in annotation software for word sense tagging  #TAUTHOR_TAG and in the long - standing use of part - of - speech taggers, parsers and morphological analysers for data from english and many other languages']",0
['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special'],['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special'],['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special issue'],"['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g. automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws4, amal - gam5, connexor6 ) are available, for example, in the application of part - of - speech tags to corpora.', 'existing tagging systems are small scale and typically impose some limitation to prevent over - load ( e. g. restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special issue of the journal computational linguistics  #AUTHOR_TAG.', 'studies have used several different methods to mine web data.', ' #AUTHOR_TAG extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', ' #AUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp  #AUTHOR_TAG, kwicfinder  #AUTHOR_TAG a ) and the linguists search en - gine  #AUTHOR_TAG']",0
"['', ' #TAUTHOR_TAG extracts word']","['the journal computational linguistics  #AUTHOR_TAG.', 'studies have used several different methods to mine web data.', ' #TAUTHOR_TAG extracts word co - occurrence probabilities from unlabelled']","['the journal computational linguistics  #AUTHOR_TAG.', 'studies have used several different methods to mine web data.', ' #TAUTHOR_TAG extracts word co - occurrence probabilities from unlabelled text']","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times  #AUTHOR_TAG fletcher,, 2004b and received a special issue of the journal computational linguistics  #AUTHOR_TAG.', 'studies have used several different methods to mine web data.', ' #TAUTHOR_TAG']",0
"['to legal concerns over copyright and redistribution of web data, issues considered at length by  #TAUTHOR_TAG a ).', 'other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here']","['to legal concerns over copyright and redistribution of web data, issues considered at length by  #TAUTHOR_TAG a ).', 'other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here']","['to legal concerns over copyright and redistribution of web data, issues considered at length by  #TAUTHOR_TAG a ).', 'other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here']","['will explore techniques to make the resulting annotated web corpus data available in static form to enable replication and verification of corpus studies based on such data.', 'the initial solution will be to store the resulting reference 11 http : / / www. comp. lancs. ac. uk / ucrel / claws / corpus in the sketch engine.', 'we will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web - based corpus studies based in general.', 'current practise elsewhere includes the distribution of url lists, but given the dynamic nature of the web, this is not sufficiently robust.', 'other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data, issues considered at length by  #TAUTHOR_TAG a ).', 'other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here']",0
"['are well documented  #TAUTHOR_TAG.', '']","['are well documented  #TAUTHOR_TAG.', '']","['##to in april 2006.', 'in addition, the advantages of using linguistically annotated data over raw data are well documented  #TAUTHOR_TAG.', '']","['', 'in addition, the advantages of using linguistically annotated data over raw data are well documented  #TAUTHOR_TAG.', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega - corpus']",0
[''],[''],[''],[''],0
"['do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve  #TAUTHOR_TAG.', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture  #AUTHOR_TAG']","['do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve  #TAUTHOR_TAG.', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture  #AUTHOR_TAG']","['believe that ownership has an important role and we do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve  #TAUTHOR_TAG.', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture  #AUTHOR_TAG']","['believe that ownership has an important role and we do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve  #TAUTHOR_TAG.', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture  #AUTHOR_TAG']",5
"['nobody of us had the opportunity to try it  #TAUTHOR_TAG.', 'storyspace is currently distributed by  #AUTHOR_TAG, and we have used']","['nobody of us had the opportunity to try it  #TAUTHOR_TAG.', 'storyspace is currently distributed by  #AUTHOR_TAG, and we have used']","['nobody of us had the opportunity to try it  #TAUTHOR_TAG.', 'storyspace is currently distributed by  #AUTHOR_TAG, and we have used']","['from wikis, blogs, and cognitive mapping, we were also inspired by the experiences of early hypertext writing tools, in particular intermedia and storyspace.', 'in fact, they were used expecially in academic writing with some success.', 'intermedia is no more developed and nobody of us had the opportunity to try it  #TAUTHOR_TAG.', 'storyspace is currently distributed by  #AUTHOR_TAG, and we have used it for a time.', ""however, in our opinion storyspace is a product of its time and in fact it isn't a web application."", 'although it is possible to label links, it lacks a lot of features we need.', 'moreover, no hypertext writing tool available is released under an open source licence.', 'we hope that novelle will bridge this gap - we will choose the exact licence when our first public release is ready']",0
"['the example of  #TAUTHOR_TAG, we will']","['the example of  #TAUTHOR_TAG, we will']","['the example of  #TAUTHOR_TAG, we will call the autonomous units']","[""the example of  #TAUTHOR_TAG, we will call the autonomous units of a hypertext lexias ( from ` lexicon'), a word coined by  #AUTHOR_TAG."", 'consequently, a hypertext is a set of lexias.', 'in hypertexts transitions from one lexia to another are not necessarily sequential, but navigational.', 'the main problems of hypertexts, acknowledged since the beginning, have been traced as follows  #AUTHOR_TAG']",5
"['written page  #TAUTHOR_TAG.', 'moreover,']","['written page  #TAUTHOR_TAG.', 'moreover,']","['written page  #TAUTHOR_TAG.', 'moreover,']","['. 1 hypertext as a  #AUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""terms as'chapter ','page'or'footnote'simply become meaningless in the new texts, or they highly change their meaning."", 'when gutenberg invented the printing press and aldo manuzio invented the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - i. e."", 'literary criticism - unlike in the previous times  #AUTHOR_TAG.', 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web ( berners -  #AUTHOR_TAG.', ""for example, a ` web page'is more similar to an infinite canvas than a written page  #TAUTHOR_TAG."", 'moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""from a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an'opera aperta'( open work ), as eco would define it ( 1962 )."", 'from a more pessimistic one, an author may feel to have lost power in this openness.', 'henceforth the collaborative traits of blogs and wikis ( mc  #AUTHOR_TAG emphasize annotation, comment, and strong editing.', 'they give more power to readers, eventually filling the gap - the so - called active readers become authors as well.', 'this situation could make new problems rise up : who owns the text?', 'which role is suitable for authors?', 'we have to analyse them before presenting the architecture of novelle']",0
"['do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve  #AUTHOR_TAG.', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture  #TAUTHOR_TAG']","['do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve  #AUTHOR_TAG.', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture  #TAUTHOR_TAG']","['believe that ownership has an important role and we do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve  #AUTHOR_TAG.', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture  #TAUTHOR_TAG']","['believe that ownership has an important role and we do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve  #AUTHOR_TAG.', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture  #TAUTHOR_TAG']",0
"['is not owned by a single author e. g.  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'now personal wiki tools are arising for brainstorming and mind mapping.', 'see section 4 for further aspects']","['is not owned by a single author e. g.  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'now personal wiki tools are arising for brainstorming and mind mapping.', 'see section 4 for further aspects']","['is not owned by a single author e. g.  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'now personal wiki tools are arising for brainstorming and mind mapping.', 'see section 4 for further aspects']","['emphasis on narrativity takes into account the use of blogs as public diaries on the web, that is still the main current interpretation of this literary genre, or metagenre ( mc  #AUTHOR_TAG.', 'furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', 'so blogs are a literary metagenre which started as authored personal diaries or journals.', ""now they try to collect themselves in so - called'blogspheres '."", 'on the other side, wikis started as collective works where each entry is not owned by a single author e. g.  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'now personal wiki tools are arising for brainstorming and mind mapping.', 'see section 4 for further aspects']",0
"['2 ).', 'authors may choose this right with the no - deriv option of the creative commons licences  #TAUTHOR_TAG.', 'if']","['2 ).', 'authors may choose this right with the no - deriv option of the creative commons licences  #TAUTHOR_TAG.', 'if nobody']","['and a new document history timeline will start ( see figure 2 ).', 'authors may choose this right with the no - deriv option of the creative commons licences  #TAUTHOR_TAG.', 'if']","['a user lets others edit some lexias, he has the right to retain or refuse the attribution when other users have edited it.', 'in the first instance, the edited version simply moves ahead the document history.', 'in the second one, the last user, who has edited the lexia, may claim the attribution for himself.', 'the lexia will be marked as a derivative work from the original one, and a new document history timeline will start ( see figure 2 ).', 'authors may choose this right with the no - deriv option of the creative commons licences  #TAUTHOR_TAG.', '']",0
"['main source of novelle are wikis and blogs.', 'while wikis have spread from a detailed design  #TAUTHOR_TAG, unfortunately blogs have not been designed under a model.', 'so we have tested']","['main source of novelle are wikis and blogs.', 'while wikis have spread from a detailed design  #TAUTHOR_TAG, unfortunately blogs have not been designed under a model.', 'so we have tested']","['main source of novelle are wikis and blogs.', 'while wikis have spread from a detailed design  #TAUTHOR_TAG, unfortunately blogs have not been designed under a model.', 'so we have tested']","['main source of novelle are wikis and blogs.', 'while wikis have spread from a detailed design  #TAUTHOR_TAG, unfortunately blogs have not been designed under a model.', 'so we have tested and compared the most used tools available for blogging : bloggers, wordpress, movabletype and livejournal']",0
"['our typology of links, we aim to solve the framing problem as defined in section 1. 2.', 'we want to model views as dynamic objects - the creation of context will be still arbitrary, but changes are very easily.', 'we would also provide a user facility for choosing the right licence for every lexia, following the model of creative commons licences  #TAUTHOR_TAG']","['our typology of links, we aim to solve the framing problem as defined in section 1. 2.', 'we want to model views as dynamic objects - the creation of context will be still arbitrary, but changes are very easily.', 'we would also provide a user facility for choosing the right licence for every lexia, following the model of creative commons licences  #TAUTHOR_TAG']","['our typology of links, we aim to solve the framing problem as defined in section 1. 2.', 'we want to model views as dynamic objects - the creation of context will be still arbitrary, but changes are very easily.', 'we would also provide a user facility for choosing the right licence for every lexia, following the model of creative commons licences  #TAUTHOR_TAG']","['our typology of links, we aim to solve the framing problem as defined in section 1. 2.', 'we want to model views as dynamic objects - the creation of context will be still arbitrary, but changes are very easily.', 'we would also provide a user facility for choosing the right licence for every lexia, following the model of creative commons licences  #TAUTHOR_TAG']",5
"['speaking, we find that the personal public diary metaphor behind blogs  #TAUTHOR_TAG may bring to an unsatisfactory representation of the context.', 'the only way to retrieve information is through a search']","['speaking, we find that the personal public diary metaphor behind blogs  #TAUTHOR_TAG may bring to an unsatisfactory representation of the context.', 'the only way to retrieve information is through a search']","['speaking, we find that the personal public diary metaphor behind blogs  #TAUTHOR_TAG may bring to an unsatisfactory representation of the context.', ""the only way to retrieve information is through a search engine or a calendar, i. e. the date of the'post'- a lexia in the jargon of bloggers""]","['speaking, we find that the personal public diary metaphor behind blogs  #TAUTHOR_TAG may bring to an unsatisfactory representation of the context.', ""the only way to retrieve information is through a search engine or a calendar, i. e. the date of the'post'- a lexia in the jargon of bloggers""]",0
"['##re  #TAUTHOR_TAG.', 'furthermore we noticed that blogs and wikis are currently']","['main current interpretation of this literary genre, or metagenre  #TAUTHOR_TAG.', 'furthermore we noticed that blogs and wikis are currently']","['##re  #TAUTHOR_TAG.', 'furthermore we noticed that blogs and wikis are currently']","['emphasis on narrativity takes into account the use of blogs as public diaries on the web, that is still the main current interpretation of this literary genre, or metagenre  #TAUTHOR_TAG.', 'furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', 'so blogs are a literary metagenre which started as authored personal diaries or journals.', ""now they try to collect themselves in so - called'blogspheres '."", 'on the other side, wikis started as collective works where each entry is not owned by a single author - e. g.', ' #AUTHOR_TAG.', 'now personal wiki tools are arising for brainstorming and mind mapping.', 'see section 4 for further aspects']",0
"['times  #TAUTHOR_TAG.', 'nowadays']","['times  #TAUTHOR_TAG.', 'nowadays']","[' #TAUTHOR_TAG.', 'nowadays the use of computers']","['was the first scholar who stressed the impact of the digital revolution to the medium of writing.', 'terms as chapter, page or foot - note simply become meaningless in the new texts, or they highly change their meaning.', 'when gutenberg invented the printing press and aldo manuzio invented the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - - i. e. literary criticism - - unlike in the previous times  #TAUTHOR_TAG."", 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web ( berners -  #AUTHOR_TAG.', 'for example, a web page is more similar to an infinite canvas than a written page ( mc  #AUTHOR_TAG.', 'moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', 'from a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an opera aperta ( open work ), as eco would define it ( 1962 ).', 'from a more pessimistic one, an author may feel to have lost power in this openness.', 'henceforth the collaborative traits of blogs and wikis ( mc  #AUTHOR_TAG emphasize annotation, comment, and strong editing.', 'they give more power to readers, eventually filling the gap - the so - called active readers become authors as well.', 'this situation could make new problems rise up : who owns the text? which role is suitable for authors? we have to analyse them before presenting the architecture of novelle']",0
"['1 hypertext as a new writing space  #TAUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""terms as'chapter ','page""]","['. 1 hypertext as a new writing space  #TAUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""terms as'chapter ','page'or'footnote'simply become meaningless in the new texts, or""]","['1 hypertext as a new writing space  #TAUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""terms as'chapter ','page""]","['. 1 hypertext as a new writing space  #TAUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""terms as'chapter ','page'or'footnote'simply become meaningless in the new texts, or they highly change their meaning."", 'when gutenberg invented the printing press and aldo manuzio invented the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - i. e."", 'literary criticism - unlike in the previous times  #AUTHOR_TAG.', 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web ( berners -  #AUTHOR_TAG.', ""for example, a'web page'is more similar to an infinite canvas than a written page ( mc  #AUTHOR_TAG."", 'moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""from a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an'opera aperta'( open work ), as eco would define it ( 1962 )."", 'from a more pessimistic one, an author may feel to have lost power in this openness.', 'henceforth the collaborative traits of blogs and wikis ( mc  #AUTHOR_TAG emphasize annotation, comment, and strong editing.', 'they give more power to readers, eventually filling the gap - the so - called active readers become authors as well.', 'this situation could make new problems rise up : who owns the text?', 'which role is suitable for authors?', 'we have to analyse them before presenting the architecture of novelle']",0
['##httprequest object  #TAUTHOR_TAG'],"['dom ), the xmlhttprequest object  #TAUTHOR_TAG']","['dom ), the xmlhttprequest object  #TAUTHOR_TAG']","['##ax is not a technology in itself but a term that refers to the use of a group of technologies together, in particular javascript and xml.', 'in other words ajax is a web development technique for creating interactive web applications using a combination of xhtml and css, document object model ( or dom ), the xmlhttprequest object  #TAUTHOR_TAG']",0
['. e. arcs are arrows  #TAUTHOR_TAG'],"['arc always has a definite direction, i. e. arcs are arrows  #TAUTHOR_TAG']",['. e. arcs are arrows  #TAUTHOR_TAG'],"[""mapping has been used at least in education for over thirty years, in particular at the cornell university, where piaget's ideas gave the roots to the assimilation theory by david ausubel."", 'very briefly, concept maps show the relationships between concepts labelling both nodes and arcs.', 'every arc always has a definite direction, i. e. arcs are arrows  #TAUTHOR_TAG']",0
['ruby on  #TAUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes'],['ruby on  #TAUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes'],['ruby on  #TAUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes'],['ruby on  #TAUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes'],5
"['history  #TAUTHOR_TAG.', 'figure 1 shows the model.', 'history snapshots of']","['history  #TAUTHOR_TAG.', 'figure 1 shows the model.', 'history snapshots of']","['not cause a change in the history  #TAUTHOR_TAG.', 'figure 1 shows the model.', 'history snapshots of']","['wikis every document keeps track of its own history : creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.', 'moreover, a sandbox is a temporary view of a document itself i. e. a sandbox can not cause a change in the history  #TAUTHOR_TAG.', 'figure 1 shows the model.', 'history snapshots of the timeline may be considered as permanent views, i. e. views with a timestamp.', 'consequently, except in the case of sandboxes, every change in the document cannot be erased.', 'this model will have a strong impact on the role of links and on the underpinning structure of novelle itself']",0
"['the collaborative traits of blogs and wikis  #TAUTHOR_TAG emphasize annotation, comment, and strong editing.', '']","['the collaborative traits of blogs and wikis  #TAUTHOR_TAG emphasize annotation, comment, and strong editing.', '']","['the collaborative traits of blogs and wikis  #TAUTHOR_TAG emphasize annotation, comment, and strong editing.', 'they give more power to readers, eventually filling the gap - the']","['. 1 hypertext as a  #AUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""terms as'chapter ','page'or'footnote'simply become meaningless in the new texts, or they highly change their meaning."", 'when gutenberg invented the printing press and aldo manuzio invented the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - i. e."", 'literary criticism - unlike in the previous times  #AUTHOR_TAG.', 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web ( berners -  #AUTHOR_TAG.', ""for example, a'web page'is more similar to an infinite canvas than a written page ( mc  #AUTHOR_TAG."", 'moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""from a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an'opera aperta'( open work ), as eco would define it ( 1962 )."", 'from a more pessimistic one, an author may feel to have lost power in this openness.', 'henceforth the collaborative traits of blogs and wikis  #TAUTHOR_TAG emphasize annotation, comment, and strong editing.', 'they give more power to readers, eventually filling the gap - the so - called active readers become authors as well.', 'this situation could make new problems rise up : who owns the text?', 'which role is suitable for authors?', 'we have to analyse them before presenting the architecture of novelle']",0
"['set of historical versions of the document itself.', 'generally, people avoid commenting, preferring to edit each document.', 'the paradigm is "" write many, read many ""  #TAUTHOR_TAG']","['set of historical versions of the document itself.', 'generally, people avoid commenting, preferring to edit each document.', 'the paradigm is "" write many, read many ""  #TAUTHOR_TAG']","['the contrary, in wikis no lexia is authored and there is no hierarchy between lexias.', 'in fact a document is still a set of lexias, but every document is only the set of historical versions of the document itself.', 'generally, people avoid commenting, preferring to edit each document.', 'the paradigm is "" write many, read many ""  #TAUTHOR_TAG']","['the contrary, in wikis no lexia is authored and there is no hierarchy between lexias.', 'in fact a document is still a set of lexias, but every document is only the set of historical versions of the document itself.', 'generally, people avoid commenting, preferring to edit each document.', 'the paradigm is "" write many, read many ""  #TAUTHOR_TAG']",0
['##s the communication works asyncronously between a client and a server through a set of messages based on http protocol and xml  #TAUTHOR_TAG'],"['used the asyncronous javascript and xml ( or ajax ) paradigm to create the graphical user interface.', 'ajax function lets the communication works asyncronously between a client and a server through a set of messages based on http protocol and xml  #TAUTHOR_TAG']",['##s the communication works asyncronously between a client and a server through a set of messages based on http protocol and xml  #TAUTHOR_TAG'],"['used the asyncronous javascript and xml ( or ajax ) paradigm to create the graphical user interface.', 'ajax function lets the communication works asyncronously between a client and a server through a set of messages based on http protocol and xml  #TAUTHOR_TAG']",0
"['.', 'this is noticeable for german  #AUTHOR_TAG and portuguese  #TAUTHOR_TAG, which still have high']","['roots.', 'this is noticeable for german  #AUTHOR_TAG and portuguese  #TAUTHOR_TAG, which still have high']","['.', 'this is noticeable for german  #AUTHOR_TAG and portuguese  #TAUTHOR_TAG, which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous']","['second observation is that a high proportion of non - projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'this is noticeable for german  #AUTHOR_TAG and portuguese  #TAUTHOR_TAG, which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for czech ( b [UNK] ohmov [UNK] a et al., 2003 ), dutch ( van der  #AUTHOR_TAG and slovene ( dezeroski et al., 2006 ), where root precision drops more drastically to about 69 %, 71 % and 41 %, respectively, and root recall is also affected negatively.', 'on the other hand, all three languages behave like high - accuracy languages with respect to attachment score.', 'a very similar pattern is found for spanish ( civit torruella and marti antonin, 2002 ), although this cannot be explained by a high proportion of non - projective structures.', 'one possible explanation in this case may be the fact that dependency graphs in the spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features']",1
['recovering nonprojective structures  #TAUTHOR_TAG'],['recovering nonprojective structures  #TAUTHOR_TAG'],['recovering nonprojective structures  #TAUTHOR_TAG'],"['a deterministic algorithm for building labeled projective dependency graphs  #AUTHOR_TAG.', '• history - based feature models for predicting the next parser action  #AUTHOR_TAG.', '• support vector machines for mapping histories to parser actions  #AUTHOR_TAG.', 'a¢ graph transformations for recovering nonprojective structures  #TAUTHOR_TAG']",5
"['.', 'this is noticeable for german  #TAUTHOR_TAG and portuguese  #AUTHOR_TAG, which still have high']","['roots.', 'this is noticeable for german  #TAUTHOR_TAG and portuguese  #AUTHOR_TAG, which still have high']","['.', 'this is noticeable for german  #TAUTHOR_TAG and portuguese  #AUTHOR_TAG, which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous']","['second observation is that a high proportion of non - projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'this is noticeable for german  #TAUTHOR_TAG and portuguese  #AUTHOR_TAG, which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for czech ( b [UNK] ohmov [UNK] a et al., 2003 ), dutch ( van der  #AUTHOR_TAG and slovene ( dezeroski et al., 2006 ), where root precision drops more drastically to about 69 %, 71 % and 41 %, respectively, and root recall is also affected negatively.', 'on the other hand, all three languages behave like high - accuracy languages with respect to attachment score.', 'a very similar pattern is found for spanish ( civit torruella and marti antonin, 2002 ), although this cannot be explained by a high proportion of non - projective structures.', 'one possible explanation in this case may be the fact that dependency graphs in the spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features']",1
['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],['parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by  #AUTHOR_TAG and extended to labeled dependency parsing by  #TAUTHOR_TAG'],5
"['danish  #AUTHOR_TAG, and swedish  #TAUTHOR_TAG']","['danish  #AUTHOR_TAG, and swedish  #TAUTHOR_TAG']","['danish  #AUTHOR_TAG, and swedish  #TAUTHOR_TAG.', 'japanese  #AUTHOR_TAG, despite a very high accuracy, is different in that attachment score']","['overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 detailed specifications of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian  #AUTHOR_TAG, chinese  #AUTHOR_TAG, danish  #AUTHOR_TAG, and swedish  #TAUTHOR_TAG.', 'japanese  #AUTHOR_TAG, despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']",0
"['', 'typical examples are bulgarian  #TAUTHOR_TAG']","['arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian  #TAUTHOR_TAG']","['', 'typical examples are bulgarian  #TAUTHOR_TAG']","['overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 detailed specifications of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian  #TAUTHOR_TAG, chinese  #AUTHOR_TAG, danish  #AUTHOR_TAG, and swedish  #AUTHOR_TAG.', 'japanese  #AUTHOR_TAG, despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']",0
"['', 'typical examples are bulgarian  #AUTHOR_TAG, chinese  #TAUTHOR_TAG']","['arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian  #AUTHOR_TAG, chinese  #TAUTHOR_TAG']","['', 'typical examples are bulgarian  #AUTHOR_TAG, chinese  #TAUTHOR_TAG']","['overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 detailed specifications of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian  #AUTHOR_TAG, chinese  #TAUTHOR_TAG, danish  #AUTHOR_TAG, and swedish  #AUTHOR_TAG.', 'japanese  #AUTHOR_TAG, despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']",0
"['predicting the next parser action  #TAUTHOR_TAG.', '• support vector machines']","['predicting the next parser action  #TAUTHOR_TAG.', '• support vector machines']","['predicting the next parser action  #TAUTHOR_TAG.', '• support vector machines']","['a deterministic algorithm for building labeled projective dependency graphs  #AUTHOR_TAG.', 'a¢ history - based feature models for predicting the next parser action  #TAUTHOR_TAG.', '• support vector machines for mapping histories to parser actions  #AUTHOR_TAG.', '• graph transformations for recovering nonprojective structures']",5
"['experiments have been performed using maltparser  #TAUTHOR_TAG, version 0. 4, which is']","['experiments have been performed using maltparser  #TAUTHOR_TAG, version 0. 4, which is']","['experiments have been performed using maltparser  #TAUTHOR_TAG, version 0. 4, which is made available together with the suite of programs used for preand post - processing.']","['experiments have been performed using maltparser  #TAUTHOR_TAG, version 0. 4, which is made available together with the suite of programs used for preand post - processing.']",5
"['vector representing the history.', 'more specifically, we use libsvm  #TAUTHOR_TAG with a quadratic']","['vector representing the history.', 'more specifically, we use libsvm  #TAUTHOR_TAG with a quadratic']","['to predict the next parser action from a feature vector representing the history.', 'more specifically, we use libsvm  #TAUTHOR_TAG with a quadratic kernel k ( xz, xj ) =']","['use support vector machines to predict the next parser action from a feature vector representing the history.', 'more specifically, we use libsvm  #TAUTHOR_TAG with a quadratic kernel k ( xz, xj ) = ( - yxt xj + r ) 2 and the built - in one - versus - all strategy for multi - class classification.', 'symbolic features are converted to numerical features using the standard technique of binarization, and we split values of the feats field into its atomic components. 4', 'or some languages, we divide the training data into smaller sets, based on some feature s ( normally the cpos or pos of the next input token ), which may reduce training times without a significant loss in accuracy  #AUTHOR_TAG.', 'to avoid too small training sets, we pool together categories that have a frequency below a certain threshold t']",5
['##the analysis is reminiscent of the treatment of coordination in the collins parser  #TAUTHOR_TAG'],['##the analysis is reminiscent of the treatment of coordination in the collins parser  #TAUTHOR_TAG'],['##the analysis is reminiscent of the treatment of coordination in the collins parser  #TAUTHOR_TAG'],['##the analysis is reminiscent of the treatment of coordination in the collins parser  #TAUTHOR_TAG'],1
"['the parser only derives projective graphs, the fact that graphs are labeled allows non - projective dependencies to be captured using the pseudoprojective approach of  #TAUTHOR_TAG']","['the parser only derives projective graphs, the fact that graphs are labeled allows non - projective dependencies to be captured using the pseudoprojective approach of  #TAUTHOR_TAG']","['the parser only derives projective graphs, the fact that graphs are labeled allows non - projective dependencies to be captured using the pseudoprojective approach of  #TAUTHOR_TAG']","['the parser only derives projective graphs, the fact that graphs are labeled allows non - projective dependencies to be captured using the pseudoprojective approach of  #TAUTHOR_TAG']",0
"['training times without a significant loss in accuracy  #TAUTHOR_TAG.', 'to avoid too small training sets, we pool together categories that have a frequency below']","['training times without a significant loss in accuracy  #TAUTHOR_TAG.', 'to avoid too small training sets, we pool together categories that have a frequency below']","['some feature s ( normally the cpos or pos of the next input token ), which may reduce training times without a significant loss in accuracy  #TAUTHOR_TAG.', 'to avoid too small training sets, we pool together categories that have a frequency below a certain threshold t']","['some languages, we divide the training data into smaller sets, based on some feature s ( normally the cpos or pos of the next input token ), which may reduce training times without a significant loss in accuracy  #TAUTHOR_TAG.', 'to avoid too small training sets, we pool together categories that have a frequency below a certain threshold t']",0
"['', 'typical examples are bulgarian  #TAUTHOR_TAG']","['arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian  #TAUTHOR_TAG']","['', 'typical examples are bulgarian  #TAUTHOR_TAG']","['overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 detailed specifications of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian  #TAUTHOR_TAG, chinese  #AUTHOR_TAG, danish  #AUTHOR_TAG, and swedish  #AUTHOR_TAG.', 'japanese  #AUTHOR_TAG, despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']",0
"['chinese  #AUTHOR_TAG, danish  #AUTHOR_TAG, and swedish.', 'japanese  #TAUTHOR_TAG, despite a very']","['chinese  #AUTHOR_TAG, danish  #AUTHOR_TAG, and swedish.', 'japanese  #TAUTHOR_TAG, despite a very']","['chinese  #AUTHOR_TAG, danish  #AUTHOR_TAG, and swedish.', 'japanese  #TAUTHOR_TAG, despite a very high accuracy, is different in that attachment score']","['overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 detailed specifications of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian  #AUTHOR_TAG, chinese  #AUTHOR_TAG, danish  #AUTHOR_TAG, and swedish.', 'japanese  #TAUTHOR_TAG, despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']",1
['actions  #TAUTHOR_TAG'],['actions  #TAUTHOR_TAG'],"['actions  #TAUTHOR_TAG.', '• graph transformations']","['a deterministic algorithm for building labeled projective dependency graphs  #AUTHOR_TAG.', '• history - based feature models for predicting the next parser action  #AUTHOR_TAG.', 'support vector machines for mapping histories to parser actions  #TAUTHOR_TAG.', '• graph transformations for recovering nonprojective structures']",5
"['length 2 ).', 'by contrast, turkish  #TAUTHOR_TAG exhibits high']","['length 2 ).', 'by contrast, turkish  #TAUTHOR_TAG exhibits high']","['length 2 ).', 'by contrast, turkish  #TAUTHOR_TAG exhibits']","['results for arabic ( hajic et al., 2004 ; smrz et al., 2002 ) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length ( from about 93 % for length 1 to 67 % for length 2 ).', 'by contrast, turkish  #TAUTHOR_TAG exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ).', 'it is noteworthy that arabic and turkish, being "" typological outliers "", show patterns that are different both from each other and from most of the other languages']",1
"['', ' #TAUTHOR_TAG have previously examined the task of categorizing sentences in']","['studying the role of semantics in various tasks.', ' #TAUTHOR_TAG have previously examined the task of categorizing sentences in']","['studying the role of semantics in various tasks.', ' #TAUTHOR_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of  #AUTHOR_TAG in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', ' #AUTHOR_TAG.', 'although our results were not obtained from the same exact collection as those used']","['', 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls )  #AUTHOR_TAG, and the availability of software that leverages this knowledge - metamap  #AUTHOR_TAG for concept identification and semrep  #AUTHOR_TAG for relation extraction - provide a foundation for studying the role of semantics in various tasks.', ' #TAUTHOR_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of  #AUTHOR_TAG in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', ' #AUTHOR_TAG.', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"[',  #TAUTHOR_TAG experimented with abstracts and']","[',  #TAUTHOR_TAG experimented with abstracts and']","[',  #TAUTHOR_TAG experimented with abstracts and']",[' #TAUTHOR_TAG'],0
"['the hmm states ).', ' #TAUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences )']","['the hmm states ).', ' #TAUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences )']","['the hmm states ).', ' #TAUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences )']","['an attempt to further boost performance, we employed linear discriminant analysis ( lda ) to find a linear projection of the four - dimensional vec - tors that maximizes the separation of the gaussians ( corresponding to the hmm states ).', ' #TAUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the lda transform matrix, which entails computing the withinand between - covariance matrices of the classes, and using singular value decomposition ( svd ) to compute the eigenvectors of the new space.', 'each sentence / vector is then mul - tiplied by this matrix, and new hmm models are re - computed from the projected data']",5
['system  #TAUTHOR_TAG or'],['system  #TAUTHOR_TAG or'],['system  #TAUTHOR_TAG or'],"['exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process.', 'in truth, our crossvalidation experiments do not correspond to any meaningful naturally - occurring task - structured abstracts are, after all, already appropriately labeled.', 'the true utility of content models is to structure abstracts that have no structure to begin with.', 'thus, our exploratory experiments in applying content models trained with structured rcts on unstructured rcts is a closer approximation of an extrinsically - valid measure of performance.', 'such a component would serve as the first stage of a clinical question answering system  #TAUTHOR_TAG or summarization system ( mc  #AUTHOR_TAG.', 'we chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured']",3
"[',  #TAUTHOR_TAG.', 'however,']","[',  #TAUTHOR_TAG.', 'however,']","[',  #TAUTHOR_TAG.', 'however,']","['approaches ( especially svms ) have been shown to be very effective for many supervised classification tasks ; see, for example,  #TAUTHOR_TAG.', 'however, their high computational complexity ( quadratic in the number of training samples ) renders them prohibitive for massive data processing.', 'under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'since hmms are very wellsuited to modeling sequences, our discourse modeling task lends itself naturally to this particular generative approach.', 'in fact, we demonstrate that hmms are competitive with svms, with the added advantage of lower computational complexity.', 'in addition, generative models can be directly applied to tackle certain classes of problems, such as sentence ordering, in ways that discriminative approaches cannot readily.', 'in the context of machine learning, we see our work as contributing to the ongoing debate between generative and discriminative approacheswe provide a case study in an interesting domain that begins to explore some of these tradeoffs']",0
"['and  #TAUTHOR_TAG, we employed hidden markov models to model the']","['and  #TAUTHOR_TAG, we employed hidden markov models to model the']","['and  #TAUTHOR_TAG, we employed hidden markov models to model']","['and  #TAUTHOR_TAG, we employed hidden markov models to model the discourse structure of medline abstracts.', 'the four states in our hmms correspond to the information that characterizes each section ( "" introduction "", "" methods "", "" results "", and "" conclusions "" ) and state transitions capture the discourse flow from section to section']",5
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],0
"['gaussians mixtures with diagonal covariance matrices.', 'using the section labels, the hmm was trained using the htk toolkit  #TAUTHOR_TAG, which efficiently']","['vectors.', 'the transition probability matrix of the hmm was initialized with uniform probabilities over a fully connected graph.', 'the output probabilities were modeled as four - dimensional gaussians mixtures with diagonal covariance matrices.', 'using the section labels, the hmm was trained using the htk toolkit  #TAUTHOR_TAG, which efficiently']","['then built a four - state hidden markov model that outputs these four - dimensional vectors.', 'the transition probability matrix of the hmm was initialized with uniform probabilities over a fully connected graph.', 'the output probabilities were modeled as four - dimensional gaussians mixtures with diagonal covariance matrices.', 'using the section labels, the hmm was trained using the htk toolkit  #TAUTHOR_TAG, which efficiently']","['then built a four - state hidden markov model that outputs these four - dimensional vectors.', 'the transition probability matrix of the hmm was initialized with uniform probabilities over a fully connected graph.', 'the output probabilities were modeled as four - dimensional gaussians mixtures with diagonal covariance matrices.', 'using the section labels, the hmm was trained using the htk toolkit  #TAUTHOR_TAG, which efficiently performs the forward - backward algorithm and baumwelch estimation.', 'for testing, we performed a viterbi ( maximum likelihood ) estimation of the label of each test sentence / vector ( also using the htk toolkit )']",5
"['conclusions ""  #TAUTHOR_TAG ; ore']","['"" conclusions ""  #TAUTHOR_TAG ; oreasan, 2001 )']","['conclusions ""  #TAUTHOR_TAG ; ore']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions ""  #TAUTHOR_TAG ; oreasan, 2001 ).', 'the ability to explicitly identify these sections in un - structured text could play an important role in applications such as document summarization  #AUTHOR_TAG, information retrieval  #AUTHOR_TAG, information extraction  #AUTHOR_TAG, and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example,  #AUTHOR_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'demner -  #AUTHOR_TAG found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts']",0
"['in  #TAUTHOR_TAG.', 'this technique provides two important advantages']","['in  #TAUTHOR_TAG.', 'this technique provides two important advantages.', 'first, gaussian modeling adds an extra degree of freedom during training, by capturing second - order statistics.', 'this is not possible when modeling word sequences, where only the probability of a sentence is actually used in the hmm training.', 'second, using continuous distributions']","['in  #TAUTHOR_TAG.', 'this technique provides two important advantages.', 'first, gaussian modeling adds an extra degree of freedom during training, by capturing second - order statistics.', 'this is not possible when modeling word sequences, where only the probability of a sentence is actually used in the hmm training.', 'second, using continuous distributions']","['interesting aspect of our generative approach is that we model hmm outputs as gaussian vectors ( log probabilities of observing entire sentences based on our language models ), as opposed to sequences of terms, as done in  #TAUTHOR_TAG.', 'this technique provides two important advantages.', 'first, gaussian modeling adds an extra degree of freedom during training, by capturing second - order statistics.', 'this is not possible when modeling word sequences, where only the probability of a sentence is actually used in the hmm training.', 'second, using continuous distributions allows us to leverage a variety of tools ( e. g., lda ) that have been shown to be successful in other fields, such as speech recognition  #AUTHOR_TAG']",1
"[',  #TAUTHOR_TAG.', 'however,']","[',  #TAUTHOR_TAG.', 'however,']","[',  #TAUTHOR_TAG.', 'however,']","['approaches ( especially svms ) have been shown to be very effective for many supervised classification tasks ; see, for example,  #TAUTHOR_TAG.', 'however, their high computational complexity ( quadratic in the number of training samples ) renders them prohibitive for massive data processing.', 'under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'since hmms are very wellsuited to modeling sequences, our discourse modeling task lends itself naturally to this particular generative approach.', 'in fact, we demonstrate that hmms are competitive with svms, with the added advantage of lower computational complexity.', 'in addition, generative models can be directly applied to tackle certain classes of problems, such as sentence ordering, in ways that discriminative approaches cannot readily.', 'in the context of machine learning, we see our work as contributing to the ongoing debate between generative and discriminative approacheswe provide a case study in an interesting domain that begins to explore some of these tradeoffs']",0
['- metamap  #TAUTHOR_TAG'],['- - metamap  #TAUTHOR_TAG'],['- metamap  #TAUTHOR_TAG'],"['', 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls )  #AUTHOR_TAG, and the availability of software that leverages this knowledge - - metamap  #TAUTHOR_TAG for concept identification and semrep  #AUTHOR_TAG for relation extraction - - provide a foundation for studying the role of semantics in various tasks.', 'mc  #AUTHOR_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of  #AUTHOR_TAG in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', ' #AUTHOR_TAG.', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['attempts to characterize text in terms of domainindependent rhetorical elements  #TAUTHOR_TAG.', 'our task is closer to the']","['attempts to characterize text in terms of domainindependent rhetorical elements  #TAUTHOR_TAG.', 'our task is closer to the']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements  #TAUTHOR_TAG.', 'our task is closer to the work of  #AUTHOR_TAG, who looked at the problem of intellectual attribution in scientific texts']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements  #TAUTHOR_TAG.', 'our task is closer to the work of  #AUTHOR_TAG, who looked at the problem of intellectual attribution in scientific texts']",1
"['has been well studied  #TAUTHOR_TAG.', '']","['has been well studied  #TAUTHOR_TAG.', '']","['has been well studied  #TAUTHOR_TAG.', 'retrieval techniques can have a large impact on']",[' #TAUTHOR_TAG'],0
"['of  #TAUTHOR_TAG, who looked at the problem of intellectual attribution in scientific texts']","['of  #TAUTHOR_TAG, who looked at the problem of intellectual attribution in scientific texts']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( mc  #AUTHOR_TAG.', 'our task is closer to the work of  #TAUTHOR_TAG, who looked at the problem of intellectual attribution in scientific texts']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( mc  #AUTHOR_TAG.', 'our task is closer to the work of  #TAUTHOR_TAG, who looked at the problem of intellectual attribution in scientific texts']",1
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process.', 'in truth, our crossvalidation experiments do not correspond to any meaningful naturally - occurring task - structured abstracts are, after all, already appropriately labeled.', 'the true utility of content models is to structure abstracts that have no structure to begin with.', 'thus, our exploratory experiments in applying content models trained with structured rcts on unstructured rcts is a closer approximation of an extrinsically - valid measure of performance.', 'such a component would serve as the first stage of a clinical question answering system ( demner -  #AUTHOR_TAG or summarization system  #TAUTHOR_TAG.', 'we chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured']",3
['( b ) again reproduces the results from  #TAUTHOR_TAG ( 2003 ) for a comparable task on a different subset of 206 unstructured abstracts'],['( b ) again reproduces the results from  #TAUTHOR_TAG ( 2003 ) for a comparable task on a different subset of 206 unstructured abstracts'],['( b ) again reproduces the results from  #TAUTHOR_TAG ( 2003 ) for a comparable task on a different subset of 206 unstructured abstracts'],['( b ) again reproduces the results from  #TAUTHOR_TAG ( 2003 ) for a comparable task on a different subset of 206 unstructured abstracts'],1
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[', information extraction  #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],0
"['has been well studied  #TAUTHOR_TAG.', '']","['has been well studied  #TAUTHOR_TAG.', '']","['has been well studied  #TAUTHOR_TAG.', 'retrieval techniques can have a large impact on']",[' #TAUTHOR_TAG'],0
"[' #TAUTHOR_TAG, information']","[' #TAUTHOR_TAG, information']","[' #TAUTHOR_TAG, information extraction']",[' #TAUTHOR_TAG'],0
['also presents the closest comparable experimental results reported by  #TAUTHOR_TAG. 1 mcknight and'],['also presents the closest comparable experimental results reported by  #TAUTHOR_TAG. 1 mcknight and'],"['', 'the table also presents the closest comparable experimental results reported by  #TAUTHOR_TAG. 1 mcknight and']","['results of our second set of experiments ( with rcts only ) are shown in tables 2 ( a ) and 2 ( b ). table 2 ( a ) reports the multi - way classification error rate ; once again, applying the markov assumption to model discourse transitions improves performance, and using lda further reduces error rate.', 'table 2 ( b ) reports accuracy, precision, recall, and f - measure for four separate binary classifiers ( hmm with lda ) specifically trained for each of the sections ( one per row in the table ).', 'the table also presents the closest comparable experimental results reported by  #TAUTHOR_TAG. 1 mcknight and srinivasan ( henceforth, m & s ) created a test collection consisting of 37, 151 rcts from approximately 12 million medline abstracts dated between 1976 and 2001.', 'this collection has significantly more training examples than our corpus of 27, 075 abstracts, which could be a source of performance differences.', 'furthermore, details regarding their procedure for mapping structured abstract headings to one of the four general labels was not discussed in their paper.', 'nevertheless, our hmm - based approach is at least competitive with svms, perhaps better in some cases']",1
"['attempts to characterize text in terms of domainindependent rhetorical elements  #TAUTHOR_TAG.', 'our task is closer to the']","['attempts to characterize text in terms of domainindependent rhetorical elements  #TAUTHOR_TAG.', 'our task is closer to the']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements  #TAUTHOR_TAG.', 'our task is closer to the work of  #AUTHOR_TAG, who looked at the problem of intellectual attribution in scientific texts']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements  #TAUTHOR_TAG.', 'our task is closer to the work of  #AUTHOR_TAG, who looked at the problem of intellectual attribution in scientific texts']",1
"['discourse structure of medline abstracts using hidden markov models ( hmms ) ; cfxxx  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'although our results were not obtained from the']","['discourse structure of medline abstracts using hidden markov models ( hmms ) ; cfxxx  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'although our results were not obtained from the']","[', we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cfxxx  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'although our results were not obtained from the same exact collection as those used']","['', 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls )  #AUTHOR_TAG, and the availability of software that leverages this knowledge - metamap  #AUTHOR_TAG for concept identification and semrep  #AUTHOR_TAG for relation extraction - provide a foundation for studying the role of semantics in various tasks.', 'mc  #AUTHOR_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of  #AUTHOR_TAG in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cfxxx  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[' #TAUTHOR_TAG.', '']",[' #TAUTHOR_TAG'],0
"['has been well studied  #TAUTHOR_TAG.', '']","['has been well studied  #TAUTHOR_TAG.', '']","['has been well studied  #TAUTHOR_TAG.', 'retrieval techniques can have a large impact on']",[' #TAUTHOR_TAG'],0
['semrep  #TAUTHOR_TAG'],['semrep  #TAUTHOR_TAG'],['semrep  #TAUTHOR_TAG'],"['', 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls )  #AUTHOR_TAG, and the availability of software that leverages this knowledge - - metamap  #AUTHOR_TAG for concept identification and semrep  #TAUTHOR_TAG for relation extraction - - provide a foundation for studying the role of semantics in various tasks.', 'mc  #AUTHOR_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of  #AUTHOR_TAG in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', ' #AUTHOR_TAG.', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
['of  #TAUTHOR_TAG is'],['of  #TAUTHOR_TAG is'],"['not the first to employ a generative approach to directly model content, the seminal work of  #TAUTHOR_TAG is']","['not the first to employ a generative approach to directly model content, the seminal work of  #TAUTHOR_TAG is a noteworthy point of reference and comparison.', 'however, our study differs in several important respects.', 'barzilay and lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques.', 'in contrast, because the discourse structure of medical abstracts is welldefined and training data is relatively easy to obtain, we were able to apply a supervised approach.', 'whereas barzilay and lee evaluated their work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task.', 'nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here']",1
['speech recognition  #TAUTHOR_TAG'],['speech recognition  #TAUTHOR_TAG'],['speech recognition  #TAUTHOR_TAG'],"['interesting aspect of our generative approach is that we model hmm outputs as gaussian vectors ( log probabilities of observing entire sentences based on our language models ), as opposed to sequences of terms, as done in  #AUTHOR_TAG.', 'this technique provides two important advantages.', 'first, gaussian modeling adds an extra degree of freedom during training, by capturing second - order statistics.', 'this is not possible when modeling word sequences, where only the probability of a sentence is actually used in the hmm training.', 'second, using continuous distributions allows us to leverage a variety of tools ( e. g., lda ) that have been shown to be successful in other fields, such as speech recognition  #TAUTHOR_TAG']",0
"['work with a semi - technical text on meteorological phenomena  #TAUTHOR_TAG, meant for primary school students.', 'the text gradually introduces concepts']","['work with a semi - technical text on meteorological phenomena  #TAUTHOR_TAG, meant for primary school students.', 'the text gradually introduces concepts']","['work with a semi - technical text on meteorological phenomena  #TAUTHOR_TAG, meant for primary school students.', 'the text gradually introduces concepts']","['work with a semi - technical text on meteorological phenomena  #TAUTHOR_TAG, meant for primary school students.', 'the text gradually introduces concepts related to precipitation, and explains them.', 'its nature makes it appropriate for the semantic analysis task in an incremental approach.', 'the system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text']",5
"['system  #AUTHOR_TAG.', 'such systems extract information from some types of syntactic units ( clauses in  #TAUTHOR_TAG ; noun phrases in  #AUTHOR_TAG ).', 'lists of semantic relations are designed to capture salient domain information']","['system  #AUTHOR_TAG.', 'such systems extract information from some types of syntactic units ( clauses in  #TAUTHOR_TAG ; noun phrases in  #AUTHOR_TAG ).', 'lists of semantic relations are designed to capture salient domain information']","['to meet the requirements of the domain  #AUTHOR_TAG or the system  #AUTHOR_TAG.', 'such systems extract information from some types of syntactic units ( clauses in  #TAUTHOR_TAG ; noun phrases in  #AUTHOR_TAG ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts  #AUTHOR_TAG.', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain  #AUTHOR_TAG or the system  #AUTHOR_TAG.', 'such systems extract information from some types of syntactic units ( clauses in  #TAUTHOR_TAG ; noun phrases in  #AUTHOR_TAG ).', 'lists of semantic relations are designed to capture salient domain information']",0
"[' #TAUTHOR_TAG, who used a list of arguments surrounding the main verb']","[' #TAUTHOR_TAG, who used a list of arguments surrounding the main verb']","[' #TAUTHOR_TAG, who used a list of arguments surrounding the main verb']","['process a pair p not encountered previously, the system builds a graph centered on the main element ( often the head ) of p.', ""this idea was inspired by  #TAUTHOR_TAG, who used a list of arguments surrounding the main verb together with the verb's subcategorization information and previously processed examples to analyse semantic roles ( case relations )."", 'in recent approaches, syntactic information is translated into features which, together with information from framenet, wordnet or verbnet, will be used with ml tools to make predictions for each example in the test set  #AUTHOR_TAG']",4
"['on semantic relation analysis, the focus is on semantic roles - relations between verbs and their arguments.', 'most approaches rely on verbnet  #TAUTHOR_TAG and framenet  #AUTHOR_TAG to provide associations between verbs and semantic roles,']","['on semantic relation analysis, the focus is on semantic roles - relations between verbs and their arguments.', 'most approaches rely on verbnet  #TAUTHOR_TAG and framenet  #AUTHOR_TAG to provide associations between verbs and semantic roles,']","['current work on semantic relation analysis, the focus is on semantic roles - relations between verbs and their arguments.', 'most approaches rely on verbnet  #TAUTHOR_TAG and framenet  #AUTHOR_TAG to provide associations between verbs and semantic roles,']","['current work on semantic relation analysis, the focus is on semantic roles - relations between verbs and their arguments.', 'most approaches rely on verbnet  #TAUTHOR_TAG and framenet  #AUTHOR_TAG to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions  #AUTHOR_TAG and also  #AUTHOR_TAG']",0
"['', 'the list, a synthesis of a number of relation lists cited in the literature, has been designed to be general, domainindependent  #TAUTHOR_TAG a )']","['in compound clauses, inside a simple clause or in noun phrases.', 'the list, a synthesis of a number of relation lists cited in the literature, has been designed to be general, domainindependent  #TAUTHOR_TAG a )']","['in compound clauses, inside a simple clause or in noun phrases.', 'the list, a synthesis of a number of relation lists cited in the literature, has been designed to be general, domainindependent  #TAUTHOR_TAG a )']","['', 'if such a graph is found, then the system maps previously assigned semantic relations onto the current syntactic graph.', 'we have a list of 47 relations that manifest themselves in compound clauses, inside a simple clause or in noun phrases.', 'the list, a synthesis of a number of relation lists cited in the literature, has been designed to be general, domainindependent  #TAUTHOR_TAG a )']",4
['domain  #TAUTHOR_TAG or'],['domain  #TAUTHOR_TAG or'],"['to meet the requirements of the domain  #TAUTHOR_TAG or the system  #AUTHOR_TAG.', 'such systems extract information from some types of syntactic units ( clauses in  #AUTHOR_TAG ; noun phrases in  #AUTHOR_TAG ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts  #AUTHOR_TAG.', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain  #TAUTHOR_TAG or the system  #AUTHOR_TAG.', 'such systems extract information from some types of syntactic units ( clauses in  #AUTHOR_TAG ; noun phrases in  #AUTHOR_TAG ).', 'lists of semantic relations are designed to capture salient domain information']",0
"['system  #AUTHOR_TAG.', 'such systems extract information from some types of syntactic units ( clauses in  #TAUTHOR_TAG ; noun phrases in  #AUTHOR_TAG ).', 'lists of semantic relations are designed to capture salient domain information']","['system  #AUTHOR_TAG.', 'such systems extract information from some types of syntactic units ( clauses in  #TAUTHOR_TAG ; noun phrases in  #AUTHOR_TAG ).', 'lists of semantic relations are designed to capture salient domain information']","['to meet the requirements of the domain  #AUTHOR_TAG or the system  #AUTHOR_TAG.', 'such systems extract information from some types of syntactic units ( clauses in  #TAUTHOR_TAG ; noun phrases in  #AUTHOR_TAG ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts  #AUTHOR_TAG.', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain  #AUTHOR_TAG or the system  #AUTHOR_TAG.', 'such systems extract information from some types of syntactic units ( clauses in  #TAUTHOR_TAG ; noun phrases in  #AUTHOR_TAG ).', 'lists of semantic relations are designed to capture salient domain information']",0
"[' #TAUTHOR_TAG.', 'the']","['bases by combining components : events, entities and modifiers  #TAUTHOR_TAG.', ""the system's interface facilitates the expert's task of creating""]","[' #TAUTHOR_TAG.', ""the system's interface facilitates the expert's task of creating""]","['the rapid knowledge formation project ( rkf ) a support system was developed for domain experts.', 'it helps them build complex knowledge bases by combining components : events, entities and modifiers  #TAUTHOR_TAG.', ""the system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary""]",0
"['system  #TAUTHOR_TAG.', 'such systems extract information from some types of syntactic units ( clauses in  #AUTHOR_TAG ; noun phrases in  #AUTHOR_TAG ).', 'lists of semantic relations are designed to capture salient domain information']","['system  #TAUTHOR_TAG.', 'such systems extract information from some types of syntactic units ( clauses in  #AUTHOR_TAG ; noun phrases in  #AUTHOR_TAG ).', 'lists of semantic relations are designed to capture salient domain information']","['to meet the requirements of the domain  #AUTHOR_TAG or the system  #TAUTHOR_TAG.', 'such systems extract information from some types of syntactic units ( clauses in  #AUTHOR_TAG ; noun phrases in  #AUTHOR_TAG ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts  #AUTHOR_TAG.', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain  #AUTHOR_TAG or the system  #TAUTHOR_TAG.', 'such systems extract information from some types of syntactic units ( clauses in  #AUTHOR_TAG ; noun phrases in  #AUTHOR_TAG ).', 'lists of semantic relations are designed to capture salient domain information']",0
"['##th century b. c. and the work of panini1.', 'he was a grammarian who analysed sanskrit  #TAUTHOR_TAG.', 'the idea resurfaced forcefully']","['analysing texts, it is essential to see how elements of meaning are interconnected.', 'this is an old idea.', 'the first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century b. c. and the work of panini1.', 'he was a grammarian who analysed sanskrit  #TAUTHOR_TAG.', 'the idea resurfaced forcefully']","['analysing texts, it is essential to see how elements of meaning are interconnected.', 'this is an old idea.', 'the first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century b. c. and the work of panini1.', 'he was a grammarian who analysed sanskrit  #TAUTHOR_TAG.', 'the idea resurfaced forcefully']","['analysing texts, it is essential to see how elements of meaning are interconnected.', 'this is an old idea.', 'the first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century b. c. and the work of panini1.', 'he was a grammarian who analysed sanskrit  #TAUTHOR_TAG.', 'the idea resurfaced forcefully at several points in the more recent history of linguistic research  #AUTHOR_TAG fill - more, 1968 ).', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling  #AUTHOR_TAG']",0
"['current instance, as shown by the systems competing in semantic role labelling competitions  #AUTHOR_TAG and also  #TAUTHOR_TAG']","['current instance, as shown by the systems competing in semantic role labelling competitions  #AUTHOR_TAG and also  #TAUTHOR_TAG']","['are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions  #AUTHOR_TAG and also  #TAUTHOR_TAG']","['current work on semantic relation analysis, the focus is on semantic roles - relations between verbs and their arguments.', 'most approaches rely on verbnet  #AUTHOR_TAG and framenet  #AUTHOR_TAG to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions  #AUTHOR_TAG and also  #TAUTHOR_TAG']",0
['through verb nominalizations  #TAUTHOR_TAG'],['through verb nominalizations  #TAUTHOR_TAG'],['through verb nominalizations  #TAUTHOR_TAG'],"['this work we pursue a well - known and often tacitly assumed line of thinking : connections at the syntactic level reflect connections at the semantic level ( in other words, syntax carries meaning ).', 'anecdotal support for this stance comes from the fact that the grammatical notion of case is the basis for semantic relations  #AUTHOR_TAG.', 'tesniere ( 1959 ), who proposes a grouping of verb arguments into actants and circumstances, gives a set of rules to connect specific types of actants - for example, agent or instrument - to such grammatical elements as subject, direct object, indirect object.', 'this idea was expanded to include nouns and their modifiers through verb nominalizations  #TAUTHOR_TAG']",0
"['current instance, as shown by the systems competing in semantic role labelling competitions  #AUTHOR_TAG and also  #TAUTHOR_TAG']","['current instance, as shown by the systems competing in semantic role labelling competitions  #AUTHOR_TAG and also  #TAUTHOR_TAG']","['are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions  #AUTHOR_TAG and also  #TAUTHOR_TAG']","['current work on semantic relation analysis, the focus is on semantic roles - relations between verbs and their arguments.', 'most approaches rely on verbnet  #AUTHOR_TAG and framenet  #AUTHOR_TAG to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions  #AUTHOR_TAG and also  #TAUTHOR_TAG']",0
"['by a parser with good coverage and detailed syntactic information, dipett  #TAUTHOR_TAG.', 'the parser, written in prolog, implements a classic constituency english grammar from  #AUTHOR_TAG.', 'pairs of syntactic units connected by grammatical']","['by a parser with good coverage and detailed syntactic information, dipett  #TAUTHOR_TAG.', 'the parser, written in prolog, implements a classic constituency english grammar from  #AUTHOR_TAG.', 'pairs of syntactic units connected by grammatical']","['syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information, dipett  #TAUTHOR_TAG.', 'the parser, written in prolog, implements a classic constituency english grammar from  #AUTHOR_TAG.', 'pairs of syntactic units connected by grammatical relations']","['syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information, dipett  #TAUTHOR_TAG.', 'the parser, written in prolog, implements a classic constituency english grammar from  #AUTHOR_TAG.', 'pairs of syntactic units connected by grammatical relations are extracted from the parse trees.', 'a dependency parser would produce a similar output, but dipett also provides verb subcategorization information ( such as, for example, subject - verb - object or subject - verb - objectindirect object ), which we use to select the ( best ) matching syntactic structures']",5
"['system  #AUTHOR_TAG.', 'such systems extract information from some types of syntactic units ( clauses in  #AUTHOR_TAG ; noun phrases in  #TAUTHOR_TAG ).', 'lists of semantic relations are designed to capture salient domain information']","['system  #AUTHOR_TAG.', 'such systems extract information from some types of syntactic units ( clauses in  #AUTHOR_TAG ; noun phrases in  #TAUTHOR_TAG ).', 'lists of semantic relations are designed to capture salient domain information']","['to meet the requirements of the domain  #AUTHOR_TAG or the system  #AUTHOR_TAG.', 'such systems extract information from some types of syntactic units ( clauses in  #AUTHOR_TAG ; noun phrases in  #TAUTHOR_TAG ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts  #AUTHOR_TAG.', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain  #AUTHOR_TAG or the system  #AUTHOR_TAG.', 'such systems extract information from some types of syntactic units ( clauses in  #AUTHOR_TAG ; noun phrases in  #TAUTHOR_TAG ).', 'lists of semantic relations are designed to capture salient domain information']",0
"['several points in the more recent history of linguistic research  #TAUTHOR_TAG.', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling  #AUTHOR_TAG']","['several points in the more recent history of linguistic research  #TAUTHOR_TAG.', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling  #AUTHOR_TAG']","['several points in the more recent history of linguistic research  #TAUTHOR_TAG.', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling  #AUTHOR_TAG']","['analysing texts, it is essential to see how elements of meaning are interconnected.', 'this is an old idea.', 'the first chronicled endeavour to connect text elements and organise connections between them goes back to the 5 th century b. c. and the work of panini 1.', 'he was a grammarian who analysed sanskrit  #AUTHOR_TAG.', 'the idea resurfaced forcefully at several points in the more recent history of linguistic research  #TAUTHOR_TAG.', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling  #AUTHOR_TAG']",0
"['several points in the more recent history of linguistic research  #TAUTHOR_TAG.', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling  #AUTHOR_TAG']","['several points in the more recent history of linguistic research  #TAUTHOR_TAG.', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling  #AUTHOR_TAG']","['several points in the more recent history of linguistic research  #TAUTHOR_TAG.', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling  #AUTHOR_TAG']","['analysing texts, it is essential to see how elements of meaning are interconnected.', 'this is an old idea.', 'the first chronicled endeavour to connect text elements and organise connections between them goes back to the 5 th century b. c. and the work of panini 1.', 'he was a grammarian who analysed sanskrit  #AUTHOR_TAG.', 'the idea resurfaced forcefully at several points in the more recent history of linguistic research  #TAUTHOR_TAG.', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling  #AUTHOR_TAG']",0
"['processes the text.', 'this design idea was adopted from tanka  #TAUTHOR_TAG b ).', 'the only manually encoded']","['processes the text.', 'this design idea was adopted from tanka  #TAUTHOR_TAG b ).', 'the only manually encoded']","['it processes the text.', 'this design idea was adopted from tanka  #TAUTHOR_TAG b ).', 'the only manually encoded']","['system begins operation with a minimum of manually encoded knowledge, and accumulates information as it processes the text.', 'this design idea was adopted from tanka  #TAUTHOR_TAG b ).', 'the only manually encoded knowledge is a dictionary of markers ( subordinators, coordinators, prepositions ).', 'this resource does not affect the syntacticsemantic graph - matching heuristic']",5
"['system  #AUTHOR_TAG.', 'such systems extract information from some types of syntactic units ( clauses in  #TAUTHOR_TAG ; noun phrases in  #AUTHOR_TAG ).', 'lists of semantic relations are designed to capture salient domain information']","['system  #AUTHOR_TAG.', 'such systems extract information from some types of syntactic units ( clauses in  #TAUTHOR_TAG ; noun phrases in  #AUTHOR_TAG ).', 'lists of semantic relations are designed to capture salient domain information']","['to meet the requirements of the domain  #AUTHOR_TAG or the system  #AUTHOR_TAG.', 'such systems extract information from some types of syntactic units ( clauses in  #TAUTHOR_TAG ; noun phrases in  #AUTHOR_TAG ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts  #AUTHOR_TAG.', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain  #AUTHOR_TAG or the system  #AUTHOR_TAG.', 'such systems extract information from some types of syntactic units ( clauses in  #TAUTHOR_TAG ; noun phrases in  #AUTHOR_TAG ).', 'lists of semantic relations are designed to capture salient domain information']",0
"['from processed texts  #TAUTHOR_TAG.', 'in other methods, lexical resources are specifically']","['from processed texts  #TAUTHOR_TAG.', 'in other methods, lexical resources are specifically']","['from processed texts  #TAUTHOR_TAG.', 'in other methods, lexical resources are specifically']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts  #TAUTHOR_TAG.', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain  #AUTHOR_TAG or the system  #AUTHOR_TAG.', 'such systems extract information from some types of syntactic units ( clauses in  #AUTHOR_TAG ; noun phrases in  #AUTHOR_TAG ).', 'lists of semantic relations are designed to capture salient domain information']",0
"['list of semantic relations with which we work is based on extensive literature study  #TAUTHOR_TAG a ).', 'three lists of relations for three syntactic levels - inter - clause, intra - clause ( case ) and nounmod']","['list of semantic relations with which we work is based on extensive literature study  #TAUTHOR_TAG a ).', 'three lists of relations for three syntactic levels - inter - clause, intra - clause ( case ) and nounmodifier']","['list of semantic relations with which we work is based on extensive literature study  #TAUTHOR_TAG a ).', 'three lists of relations for three syntactic levels - inter - clause, intra - clause ( case ) and nounmod']","['list of semantic relations with which we work is based on extensive literature study  #TAUTHOR_TAG a ).', 'three lists of relations for three syntactic levels - inter - clause, intra - clause ( case ) and nounmodifier relations - were next combined based on syntactic and semantic phenomena.', 'the resulting list is the one used in the experiments we present in this paper.', 'the relations are grouped by general similarity into 6 relation classes ( h denotes the head of a base np, m denotes the modifier ).', 'there is no consensus in the literature on a list of semantic relations that would work in all situations.', 'this is, no doubt, because a general list of relations such as the one we use would not be appropriate for the semantic analysis of texts in a specific domain, such as for example medical texts.', 'all the relations in the list we use were necessary, and sufficient, for the analysis of the input text']",5
"['chunk - based verb reordering lattices on arabicenglish  #TAUTHOR_TAG, we tried to adapt the same']","['chunk - based verb reordering lattices on arabicenglish  #TAUTHOR_TAG, we tried to adapt the same']","['chunk - based verb reordering lattices on arabicenglish  #TAUTHOR_TAG, we tried to adapt the same approach to the german - english language pair.', 'it turned out that there is a larger variety of long reordering patterns in this case.', 'nevertheless,']","['reordering between german and english is a complex problem.', 'encouraged by the success of chunk - based verb reordering lattices on arabicenglish  #TAUTHOR_TAG, we tried to adapt the same approach to the german - english language pair.', 'it turned out that there is a larger variety of long reordering patterns in this case.', 'nevertheless, some experiments performed after the official evaluation showed promising results.', 'we plan to pursue this work in several directions : defining a lattice weighting scheme that distinguishes between original word order and reordering paths could help the decoder select the more promising path through the lattice.', 'applying similar reordering rules to the training corpus would reduce the mismatch between the training data and the reordered input sentences.', 'finally, it would be useful to explore the impact of different distortion limits on the decoding of reordering lattices in order to find an optimal trade - off between decoderdriven short - range and lattice - driven long - range reordering']",4
"['of  #TAUTHOR_TAG and  #AUTHOR_TAG, who marginalize over derivations to']","['of  #TAUTHOR_TAG and  #AUTHOR_TAG, who marginalize over derivations to']","['joint source - and - target extension.', 'future research should apply the work of  #TAUTHOR_TAG and  #AUTHOR_TAG, who marginalize over derivations to']","['phrase - based mt suffers from spurious ambiguity : a single translation for a given source sentence can usually be accomplished by many different pscfg derivations.', 'this problem is exacerbated by syntax - augmented mt with its thousands of nonterminals, and made even worse by its joint source - and - target extension.', 'future research should apply the work of  #TAUTHOR_TAG and  #AUTHOR_TAG, who marginalize over derivations to find the most probable translation rather than the most probable derivation, to these multi - nonterminal grammars']",3
"['apply the work of  #AUTHOR_TAG and  #TAUTHOR_TAG, who marginalize over derivations to']","['apply the work of  #AUTHOR_TAG and  #TAUTHOR_TAG, who marginalize over derivations to']","['joint source - and - target extension.', 'future research should apply the work of  #AUTHOR_TAG and  #TAUTHOR_TAG, who marginalize over derivations to']","['phrase - based mt suffers from spurious ambiguity : a single translation for a given source sentence can usually be accomplished by many different pscfg derivations.', 'this problem is exacerbated by syntax - augmented mt with its thousands of nonterminals, and made even worse by its joint source - and - target extension.', 'future research should apply the work of  #AUTHOR_TAG and  #TAUTHOR_TAG, who marginalize over derivations to find the most probable translation rather than the most probable derivation, to these multi - nonterminal grammars']",3
"[' #TAUTHOR_TAG, we examined']","[' #TAUTHOR_TAG, we examined']","['our prior work  #TAUTHOR_TAG, we examined']","['our prior work  #TAUTHOR_TAG, we examined whether techniques used for predicting the helpfulness of product reviews  #AUTHOR_TAG could be tailored to our peer - review domain, where the definition of helpfulness is largely influenced by the educational context of peer review.', 'while previously we used the average of two expert - provided ratings as our gold standard of peer - review helpfulness 1, there are other types of helpfulness rating ( e. g.', 'author perceived helpfulness ) that could be the gold standard, and that could potentially impact the features used to build the helpfulness model.', 'in fact, we observe that peer - review helpfulness seems to differ not only between students and experts ( example 1 ), but also between types of experts ( example 2 )']",2
[' #TAUTHOR_TAG.'],[' #TAUTHOR_TAG.'],['the average - expert model performs can be found in our prior work  #TAUTHOR_TAG.'],"['both algorithms are provided by weka ( http : / / www. cs. waikato. ac. nz / ml / weka / ).', 'provide complementary perspectives.', 'while the former can directly tell us what features are most useful, the latter gives feature ranks which provide more detailed information about differences between features.', 'to compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below.', 'note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings.', '( details of how the average - expert model performs can be found in our prior work  #TAUTHOR_TAG.']",2
"[' #TAUTHOR_TAG in our feature choices, using a fiveword window that includes']","[' #TAUTHOR_TAG in our feature choices, using a fiveword window that includes']","['selecting features for korean, we have to ac - count for relatively free word order  #AUTHOR_TAG.', 'we follow our previous work  #TAUTHOR_TAG in our feature choices, using a fiveword window that includes the target stem and two words on either side for context ( see also  #AUTHOR_TAG.', 'each word is broken down into : stem, aff']","['selecting features for korean, we have to ac - count for relatively free word order  #AUTHOR_TAG.', 'we follow our previous work  #TAUTHOR_TAG in our feature choices, using a fiveword window that includes the target stem and two words on either side for context ( see also  #AUTHOR_TAG.', 'each word is broken down into : stem, affixes, stem pos, and affixes pos.', 'we also have features for the preceding and following noun and verb, thereby approximating relevant selectional properties.', 'although these are relatively shallow features, they provide enough lexical and grammatical context to help select better or worse training data ( section 3 ) and to provide a basis for a preliminary system ( section 4 )']",2
"['on anger detection has been applied, see e. g.  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'from all 4, 832 user turns, 68']","['on anger detection has been applied, see e. g.  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'from all 4, 832 user turns,']","['same annotation scheme as in our previous work on anger detection has been applied, see e. g.  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'from all 4, 832 user turns, 68. 5 % were non - ang']","['same annotation scheme as in our previous work on anger detection has been applied, see e. g.  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'from all 4, 832 user turns, 68. 5 % were non - angry, 14. 3 % slightly angry, 5. 0 % very angry and 12. 2 % contained garbage, i. e. nonspeech events.', 'in total, the number of interaction parameters servings as input variables for the model amounts to 52']",2
"['types in the corpus, and these clusterings have been found useful in many applications  #TAUTHOR_TAG.', 'there are other similar models of distributional clustering of english words which can be similarly effective  #AUTHOR_TAG']","['types in the corpus, and these clusterings have been found useful in many applications  #TAUTHOR_TAG.', 'there are other similar models of distributional clustering of english words which can be similarly effective  #AUTHOR_TAG']","['a hierarchical clustering of the word types in the corpus, and these clusterings have been found useful in many applications  #TAUTHOR_TAG.', 'there are other similar models of distributional clustering of english words which can be similarly effective  #AUTHOR_TAG']","['simple language model that discovers useful embeddings is known as brown clustering  #AUTHOR_TAG.', 'a brown clustering is a class - based bigram model in which ( 1 ) the probability of a document is the product of the probabilities of its bigrams, ( 2 ) the probability of each bigram is the product of the probability of a bigram model over latent classes and the probability of each class generating the actual word types in the bigram, and ( 3 ) each word type has non - zero probability only on a single class.', 'given a one - toone assignment of word types to classes, then, and a corpus of text, it is easy to estimate these probabilities with maximum likelihood by counting the frequencies of the different class bigrams and the frequencies of word tokens of each type in the corpus.', 'the brown clustering algorithm works by starting with an initial assignment of word types to classes ( which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus ), and then iteratively selecting the pair of classes to merge that would lead to the highest post - merge log - likelihood, doing so until all classes have been merged.', 'this process produces a hierarchical clustering of the word types in the corpus, and these clusterings have been found useful in many applications  #TAUTHOR_TAG.', 'there are other similar models of distributional clustering of english words which can be similarly effective  #AUTHOR_TAG']",4
"[' #TAUTHOR_TAG, we also compare the']","[' #TAUTHOR_TAG, we also compare the']","[' #TAUTHOR_TAG, we also compare the performance of our system with a system using features based on the brown clusters of the word types in a document.', 'since, as seen in section 2. 1, brown clusters are hierarchical, we use features corresponding to prefixes of the path']","[' #TAUTHOR_TAG, we also compare the performance of our system with a system using features based on the brown clusters of the word types in a document.', 'since, as seen in section 2. 1, brown clusters are hierarchical, we use features corresponding to prefixes of the path from the root to the leaf for each word type']",5
"['baseline ner system we use.', 'it is inspired by the system described in  #TAUTHOR_TAG.', 'because ner annotations are']","['baseline ner system we use.', 'it is inspired by the system described in  #TAUTHOR_TAG.', 'because ner annotations are']","['this section we describe in detail the baseline ner system we use.', 'it is inspired by the system described in  #TAUTHOR_TAG.', 'because ner annotations are commonly not nested ( for example, in the text "" the us army "", "" us army "" is treated as a single entity, instead of the location "" us "" and the organization "" us army "" ) it is possible to treat ner as a sequence labeling problem, where each token in the sentence receives a label which depends on']","['this section we describe in detail the baseline ner system we use.', 'it is inspired by the system described in  #TAUTHOR_TAG.', 'because ner annotations are commonly not nested ( for example, in the text "" the us army "", "" us army "" is treated as a single entity, instead of the location "" us "" and the organization "" us army "" ) it is possible to treat ner as a sequence labeling problem, where each token in the sentence receives a label which depends on which entity type it belongs to and its position in the entity.', ' #AUTHOR_TAG we use the bilou encoding, where each token can either begin an entity, be inside an entity, be the last token in an entity, be outside an entity, or be the single unique token in an entity']",4
"['syntactic categories  #TAUTHOR_TAG, which successfully utilized such language models to represent']","['syntactic categories  #TAUTHOR_TAG, which successfully utilized such language models to represent']","['a concrete model implementing our scheme.', 'this choice is inspired by recent work on learning syntactic categories  #TAUTHOR_TAG, which successfully utilized such language models to represent word window contexts of target words.', 'however, we note that other richer types of language models,']","['hypothesize that an underlying model that could capture syntagmatic patterns in large word contexts, yet is flexible enough to deal with data sparseness, is desired.', 'it is generally accepted that the semantics of verbs in particular are correlated with their syntagmatic properties  #AUTHOR_TAG.', 'this provides grounds to expect that such model has the potential to excel for verbs.', 'to capture syntagmatic patterns, we choose in this work standard n - gram language models as the basis for a concrete model implementing our scheme.', 'this choice is inspired by recent work on learning syntactic categories  #TAUTHOR_TAG, which successfully utilized such language models to represent word window contexts of target words.', 'however, we note that other richer types of language models, such as class - based  #AUTHOR_TAG or hybrid  #AUTHOR_TAG, can be seamlessly integrated into our scheme']",4
"['##s  #TAUTHOR_TAG ( ramakrishnan a et al.,']","['generative poetry  #AUTHOR_TAG  #AUTHOR_TAG  #AUTHOR_TAG or song lyrics  #TAUTHOR_TAG ( ramakrishnan a et al.,']","['generative poetry  #AUTHOR_TAG  #AUTHOR_TAG  #AUTHOR_TAG or song lyrics  #TAUTHOR_TAG ( ramakrishnan a et al.,']","['majority of nlg focuses on the satisfaction of a communicative goal, with examples such as  #AUTHOR_TAG which produces weather reports from structured data or  #AUTHOR_TAG which generates descriptions of objects from images.', 'our work is more similar to nlg work that concentrates on structural constraints such as generative poetry  #AUTHOR_TAG  #AUTHOR_TAG  #AUTHOR_TAG or song lyrics  #TAUTHOR_TAG ( ramakrishnan a et al., 2009 ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']",1
"[' #TAUTHOR_TAG, which deal with automatic']","[' #TAUTHOR_TAG, which deal with automatic']","[' #TAUTHOR_TAG, which deal with automatic generation of classic fill in the blank questions.', 'our work is naturally complementary to these efforts, as']","['motivation for generation of material for language education exists in work such as  #AUTHOR_TAG and  #TAUTHOR_TAG, which deal with automatic generation of classic fill in the blank questions.', 'our work is naturally complementary to these efforts, as their methods require a corpus of in - vocab text to serve as seed sentences']",4
['generative poetry  #TAUTHOR_TAG  #AUTHOR_TAG  #AUTHOR_TAG or song ly'],"['generative poetry  #TAUTHOR_TAG  #AUTHOR_TAG  #AUTHOR_TAG or song lyrics  #AUTHOR_TAG ( ramakrishnan a et al.,']","['generative poetry  #TAUTHOR_TAG  #AUTHOR_TAG  #AUTHOR_TAG or song lyrics  #AUTHOR_TAG ( ramakrishnan a et al.,']","['majority of nlg focuses on the satisfaction of a communicative goal, with examples such as  #AUTHOR_TAG which produces weather reports from structured data or  #AUTHOR_TAG which generates descriptions of objects from images.', 'our work is more similar to nlg work that concentrates on structural constraints such as generative poetry  #TAUTHOR_TAG  #AUTHOR_TAG  #AUTHOR_TAG or song lyrics  #AUTHOR_TAG ( ramakrishnan a et al., 2009 ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']",1
"['##s  #AUTHOR_TAG ( ramakrishnan  #TAUTHOR_TAG, where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']","['generative poetry  #AUTHOR_TAG  #AUTHOR_TAG  #AUTHOR_TAG or song lyrics  #AUTHOR_TAG ( ramakrishnan  #TAUTHOR_TAG, where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']","['generative poetry  #AUTHOR_TAG  #AUTHOR_TAG  #AUTHOR_TAG or song lyrics  #AUTHOR_TAG ( ramakrishnan  #TAUTHOR_TAG, where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']","['majority of nlg focuses on the satisfaction of a communicative goal, with examples such as  #AUTHOR_TAG which produces weather reports from structured data or  #AUTHOR_TAG which generates descriptions of objects from images.', 'our work is more similar to nlg work that concentrates on structural constraints such as generative poetry  #AUTHOR_TAG  #AUTHOR_TAG  #AUTHOR_TAG or song lyrics  #AUTHOR_TAG ( ramakrishnan  #TAUTHOR_TAG, where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']",1
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['. char - n - grams ( g ) : we start with a character n - gram - based approach  #AUTHOR_TAG, which is most common and followed by many language identification researchers.', 'following the work of  #AUTHOR_TAG, we select character n - grams ( n = 1 to 5 ) and the word as the features in our experiments.', '2. presence in dictionaries ( d ) : we use presence in a dictionary as a features for all available dictionaries in previous experiments.', '3. length of words ( l ) : instead of using the raw length value as a feature, we follow our previous work  #TAUTHOR_TAG and create multiple features for length using a decision tree ( j48 ).', 'we use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.', '4. capitalization ( c ) : we use 3 boolean features to encode capitalization information : whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized']",2
[' #TAUTHOR_TAG and create multiple features for length using'],[' #TAUTHOR_TAG and create multiple features for length using'],[' #TAUTHOR_TAG and create multiple features for length using'],"['. char - n - grams ( g ) : we start with a character n - gram - based approach  #AUTHOR_TAG, which is most common and followed by many language identification researchers.', 'following the work of  #AUTHOR_TAG, we select character n - grams ( n = 1 to 5 ) and the word as the features in our experiments.', '2. presence in dictionaries ( d ) : we use presence in a dictionary as a features for all available dictionaries in previous experiments.', 'raw length value as a feature, we follow our previous work  #TAUTHOR_TAG and create multiple features for length using a decision tree ( j48 ).', 'we use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.', '4. capitalization ( c ) : we use 3 boolean features to encode capitalization information : whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized']",2
"[' #TAUTHOR_TAG b ) was the shape - based metric, according to which the']","[' #TAUTHOR_TAG b ) was the shape - based metric, according to which the']",[' #TAUTHOR_TAG'],"['', ""the only disambiguation metric that we used in our previous work  #TAUTHOR_TAG b ) was the shape - based metric, according to which the ` ` best'' trees are those that are skewed to the right."", 'the explanation for this metric is that text processing is, essentially, a left - to - right process.', 'in many genres, people write texts so that the most important ideas go first, both at the paragraph and at the text levels.', 'i the more text writers add, the more they elaborate on the text that went before : as a consequence, incremental discourse building consists mostly of expansion of the tight branches.', 'according to the shape - based metric, we consider that a discourse tree a is "" better "" than another discourse tree b if a is more skewed to the right than b ( see  #AUTHOR_TAG c ) for a mathematical formulation of the notion of skewedness )']",2
"['statistical methods  #TAUTHOR_TAG to lexical methods  #AUTHOR_TAG.', 'the method we adopted is that of  #AUTHOR_TAG.', 'because']","['statistical methods  #TAUTHOR_TAG to lexical methods  #AUTHOR_TAG.', 'the method we adopted is that of  #AUTHOR_TAG.', 'because']","['statistical methods  #TAUTHOR_TAG to lexical methods  #AUTHOR_TAG.', 'the method we adopted is that of  #AUTHOR_TAG.', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust']","['parallel web pages we collected from various sites are not all of the same quality.', 'some are highly parallel and easy to align while others can be very noisy.', 'aligning english - chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'a number of alignment techniques have been proposed, varying from statistical methods  #TAUTHOR_TAG to lexical methods  #AUTHOR_TAG.', 'the method we adopted is that of  #AUTHOR_TAG.', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two languages.', 'they are commonly found in english and french.', 'in the case of english - chinese alignment, where there are no cognates shared by the two languages, only the html markup in both texts are taken as cognates.', 'because the html structures of parallel pages are normally similar, the markup was found to be helpful for alignment']",0
"['hkust englishchinese corpus  #TAUTHOR_TAG.', 'in this paper, we will describe a method which automatically searches']","['hkust englishchinese corpus  #TAUTHOR_TAG.', 'in this paper, we will describe a method which automatically searches']","['model training.', 'only a few such corpora exist, including the hansard english - french corpus and the hkust englishchinese corpus  #TAUTHOR_TAG.', 'in this paper, we will describe a method which automatically searches']","[', a major obstacle to this approach is the lack of parallel corpora for model training.', 'only a few such corpora exist, including the hansard english - french corpus and the hkust englishchinese corpus  #TAUTHOR_TAG.', 'in this paper, we will describe a method which automatically searches for parallel texts on the web.', ""we will discuss the text mining algorithm we adopted, some issues in translation model training using the generated parallel corpus, and finally the translation model's performance in clir""]",0
"['statistical methods  #AUTHOR_TAG to lexical methods  #TAUTHOR_TAG.', 'the method we adopted is that of  #AUTHOR_TAG.', 'because']","['statistical methods  #AUTHOR_TAG to lexical methods  #TAUTHOR_TAG.', 'the method we adopted is that of  #AUTHOR_TAG.', 'because']","['statistical methods  #AUTHOR_TAG to lexical methods  #TAUTHOR_TAG.', 'the method we adopted is that of  #AUTHOR_TAG.', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust']","['parallel web pages we collected from various sites are not all of the same quality.', 'some are highly parallel and easy to align while others can be very noisy.', 'aligning english - chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'a number of alignment techniques have been proposed, varying from statistical methods  #AUTHOR_TAG to lexical methods  #TAUTHOR_TAG.', 'the method we adopted is that of  #AUTHOR_TAG.', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two languages.', 'they are commonly found in english and french.', 'in the case of english - chinese alignment, where there are no cognates shared by the two languages, only the html markup in both texts are taken as cognates.', 'because the html structures of parallel pages are normally similar, the markup was found to be helpful for alignment']",0
"['html markups, other criteria may also be incorporated.', 'for example, it would be helpful to consider strong correspondence between certain english and chinese words, as in  #TAUTHOR_TAG.', 'we hope to implement such correspondences in our future']","['html markups, other criteria may also be incorporated.', 'for example, it would be helpful to consider strong correspondence between certain english and chinese words, as in  #TAUTHOR_TAG.', 'we hope to implement such correspondences in our future research']","['html markups, other criteria may also be incorporated.', 'for example, it would be helpful to consider strong correspondence between certain english and chinese words, as in  #TAUTHOR_TAG.', 'we hope to implement such correspondences in our future research']","['html markups, other criteria may also be incorporated.', 'for example, it would be helpful to consider strong correspondence between certain english and chinese words, as in  #TAUTHOR_TAG.', 'we hope to implement such correspondences in our future research']",3
"['statistical methods  #TAUTHOR_TAG to lexical methods  #AUTHOR_TAG.', 'the method we adopted is that of  #AUTHOR_TAG.', 'because']","['statistical methods  #TAUTHOR_TAG to lexical methods  #AUTHOR_TAG.', 'the method we adopted is that of  #AUTHOR_TAG.', 'because']","['statistical methods  #TAUTHOR_TAG to lexical methods  #AUTHOR_TAG.', 'the method we adopted is that of  #AUTHOR_TAG.', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust']","['parallel web pages we collected from various sites are not all of the same quality.', 'some are highly parallel and easy to align while others can be very noisy.', 'aligning english - chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'a number of alignment techniques have been proposed, varying from statistical methods  #TAUTHOR_TAG to lexical methods  #AUTHOR_TAG.', 'the method we adopted is that of  #AUTHOR_TAG.', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two languages.', 'they are commonly found in english and french.', 'in the case of english - chinese alignment, where there are no cognates shared by the two languages, only the html markup in both texts are taken as cognates.', 'because the html structures of parallel pages are normally similar, the markup was found to be helpful for alignment']",0
"[""is provided by rosa©'s carmel system ( rosa© 2000 ) ; it uses the spelling correction algorithm devised by  #TAUTHOR_TAG""]","['incorrect acceleration vector by a choice of generated subdialogues.', 'figure 4 shows the architecture of atlas - andes ; any other system built with ape would look similar.', ""robust natural language understanding in atlas - andes is provided by rosa©'s carmel system ( rosa© 2000 ) ; it uses the spelling correction algorithm devised by  #TAUTHOR_TAG""]","[""is provided by rosa©'s carmel system ( rosa© 2000 ) ; it uses the spelling correction algorithm devised by  #TAUTHOR_TAG""]","['first system we have implemented with ape is a prototype atlas - andes system that replaces the hints usually given for an incorrect acceleration vector by a choice of generated subdialogues.', 'figure 4 shows the architecture of atlas - andes ; any other system built with ape would look similar.', ""robust natural language understanding in atlas - andes is provided by rosa©'s carmel system ( rosa© 2000 ) ; it uses the spelling correction algorithm devised by  #TAUTHOR_TAG""]",5
"[""see  #TAUTHOR_TAG for how mimic's dialoguelevel knowledge is used""]","[""see  #TAUTHOR_TAG for how mimic's dialoguelevel knowledge is used""]","[""see  #TAUTHOR_TAG for how mimic's dialoguelevel knowledge is used""]","[""see  #TAUTHOR_TAG for how mimic's dialoguelevel knowledge is used to override default prosodic assignments for concept - to - speech generation""]",0
"['., 1996 ;  #TAUTHOR_TAG ).', 'to instanti']","['at., 1996 ;  #TAUTHOR_TAG ).', 'to instantiate an attribute, mimic adopts']","['., 1996 ;  #TAUTHOR_TAG ).', 'to instanti']","['strategies employed when mimic has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems ( e. g., ( bennacef et at., 1996 ;  #TAUTHOR_TAG ).', 'to instantiate an attribute, mimic adopts the lnfoseek dialogue act to solicit the missing information.', 'in contrast, when mimic has both initiatives, it plays a more active role by presenting the user with additional information comprising valid instantiations of the attribute ( giveoptions ).', 'given an invalid query, mimic notifies the user of the failed query and provides an openended prompt when it only has dialogue initiative.', ""when mimic has both initiatives, however, in addition to no - tifyfailure, it suggests an alternative close to the user's original query and provides a limited prompt."", 'finally, when mimic has neither initiative, it simply adopts no - tifyfailure, allowing the user to determine the next discourse goal']",1
"['of control an agent has in the dialogue interaction  #TAUTHOR_TAG.', 'thus,']","['of control an agent has in the dialogue interaction  #TAUTHOR_TAG.', 'thus,']","['of control an agent has in the dialogue interaction  #TAUTHOR_TAG.', 'thus, a cooperative system may adopt different strategies']","['work has argued that initiative affects the degree of control an agent has in the dialogue interaction  #TAUTHOR_TAG.', 'thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'since task initiative models contribution to domain / problemsolving goals, while dialogue initiative affects the cur - 5an alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results  #AUTHOR_TAG, which we leave for future work.', 'rent discourse goal, we developed alternative strategies for achieving the goals in figure 4 based on initiative distribution, as shown in table 1']",0
"['of control an agent has in the dialogue interaction  #TAUTHOR_TAG.', 'thus,']","['of control an agent has in the dialogue interaction  #TAUTHOR_TAG.', 'thus,']","['of control an agent has in the dialogue interaction  #TAUTHOR_TAG.', 'thus, a cooperative system may adopt different strategies']","['work has argued that initiative affects the degree of control an agent has in the dialogue interaction  #TAUTHOR_TAG.', 'thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'since task initiative models contribution to domain / problemsolving goals, while dialogue initiative affects the cur - 5an alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results  #AUTHOR_TAG, which we leave for future work.', 'rent discourse goal, we developed alternative strategies for achieving the goals in figure 4 based on initiative distribution, as shown in table 1']",0
"['process and results in further detail  #TAUTHOR_TAG.', '']","['process and results in further detail  #TAUTHOR_TAG.', '']","['their results.', 'a companion paper describes the evaluation process and results in further detail  #TAUTHOR_TAG.', 'each experiment involved eight users interacting with mimic and mimic']","[""conducted two experiments to evaluate mimic's automatic adaptation capabilities."", 'we compared mimic with two control systems : mimic - si, a system - initiative version of mimic in which the system retains both initiatives throughout the dialogue, and mimic - mi, a nonadaptive mixed - initiative version of mimic that resembles the behavior of many existing dialogue systems.', 'in this section we summarize these experiments and their results.', 'a companion paper describes the evaluation process and results in further detail  #TAUTHOR_TAG.', 'each experiment involved eight users interacting with mimic and mimic - si or mimic - mi to perform a set of tasks, each requiring the user to obtain specific movie information.', 'user satisfaction was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system.', 'furthermore, a number of performance features, largely based on the paradise dialogue evaluation scheme  #AUTHOR_TAG, were automatically logged, derived, or manually annotated.', 'in addition, we logged the cues automatically detected in each user utterance, as well as the initiative distribution for each turn and the dialogue acts selected to generate each system response']",2
"['based on the ambiguous query and summarize the results  #TAUTHOR_TAG, which we']","['based on the ambiguous query and summarize the results  #TAUTHOR_TAG, which we']","['##an alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results  #TAUTHOR_TAG, which we leave']","['##an alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results  #TAUTHOR_TAG, which we leave for future work']",3
"['.', 'furthermore, a number of performance features, largely based on the paradise dialogue evaluation scheme  #TAUTHOR_TAG, were automatically logged, derived, or manually annotated.', 'in addition, we logged the cues automatically detected in']","['system.', 'furthermore, a number of performance features, largely based on the paradise dialogue evaluation scheme  #TAUTHOR_TAG, were automatically logged, derived, or manually annotated.', 'in addition, we logged the cues automatically detected in']","['', 'furthermore, a number of performance features, largely based on the paradise dialogue evaluation scheme  #TAUTHOR_TAG, were automatically logged, derived, or manually annotated.', 'in addition, we logged the cues automatically detected in each user utterance, as well as the initiative distribution for each turn and the dialogue acts selected to generate each system response']","[""conducted two experiments to evaluate mimic's automatic adaptation capabilities."", 'we compared mimic with two control systems : mimic - si, a system - initiative version of mimic in which the system retains both initiatives throughout the dialogue, and mimic - mi, a nonadaptive mixed - initiative version of mimic that resembles the behavior of many existing dialogue systems.', 'in this section we summarize these experiments and their results.', 'a companion paper describes the evaluation process and results in further detail ( chu -  #AUTHOR_TAG.', 'each experiment involved eight users interacting with mimic and mimic - si or mimic - mi to perform a set of tasks, each requiring the user to obtain specific movie information.', 'user satisfaction was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system.', 'furthermore, a number of performance features, largely based on the paradise dialogue evaluation scheme  #TAUTHOR_TAG, were automatically logged, derived, or manually annotated.', 'in addition, we logged the cues automatically detected in each user utterance, as well as the initiative distribution for each turn and the dialogue acts selected to generate each system response']",5
"['melissa project  #TAUTHOR_TAG.', '']","['melissa project  #TAUTHOR_TAG.', '']","['the melissa project  #TAUTHOR_TAG.', '']","['systems to assist in the development of spoken - langnage systems ( slss ) have focused on building stand - alone, customized applications, such as  #AUTHOR_TAG and  #AUTHOR_TAG.', 'the goal of the javox toolkit is to speech - enable traditional desktop applications - - this is similar to the goals of the melissa project  #TAUTHOR_TAG.', 'it is intended to both speed the development of slss and to localize the speech - specific code within the application.', 'javox allows developers to add speech interfaces to applications at the end of the development process ; slss no longer need to be built from the ground up']",1
"[' #TAUTHOR_TAG lin, 99 ).', 'another interesting approach is']","[' #TAUTHOR_TAG lin, 99 ).', 'another interesting approach is']","['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun - phrase contraints  #AUTHOR_TAG hua chen and chen, 94 ;  #AUTHOR_TAG.', 'it is also possible to focus on non - compositional compounds, a key point in bilingual applications  #TAUTHOR_TAG lin, 99 ).', 'another interesting approach is']","['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun - phrase contraints  #AUTHOR_TAG hua chen and chen, 94 ;  #AUTHOR_TAG.', 'it is also possible to focus on non - compositional compounds, a key point in bilingual applications  #TAUTHOR_TAG lin, 99 ).', 'another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns  #AUTHOR_TAG furuse and iida, 96 ).', 'in this study, we filtered for potential sequences that are likely to be noun phrases, using simple regular expressions over the associated part - of - speech tags.', ""an excerpt of the association probabilities of a unit model trained considering only the np - sequences is given in table 3. applying this filter ( referred to as jrnp in the following ) to the 39, 093 english sequences still surviving after previous filters ~'1 and ~'2 removes 35, 939 of them ( 92 % )."", '4 : completion results of several translation models, spared : theoretical proportion of characters saved ; ok : number of target units accepted by the user ; good : number of target units that matched the expected whether they were proposed or not ; nu : number of sentences for which no target unit was found by the translation model ; u : number of sentences for which at least one helpful unit has been found by the model, but not necessarily proposed']",0
"['., 96 ;  #TAUTHOR_TAG']","['of arbitrary length n - grams ( nagao and mori, 94 ; haruno et al., 96 ; ikehaxa et al., 96 ;  #TAUTHOR_TAG']","['., 96 ;  #TAUTHOR_TAG']","['relevant units in a text has been explored in many areas of natural language processing.', 'our approach relies on distributional and frequency statistics computed on each sequence of words found in a training corpus.', 'for sake of efficiency, we used the suffix array technique to get a compact representation of our training corpus.', 'this method allows the efficient retrieval of arbitrary length n - grams ( nagao and mori, 94 ; haruno et al., 96 ; ikehaxa et al., 96 ;  #TAUTHOR_TAG']",0
"['precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun - phrase contraints  #TAUTHOR_TAG ; hua chen and chen, 94 ;  #AUTHOR_TAG.', 'it is also possible to focus on non - compositional compounds, a key']","['precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun - phrase contraints  #TAUTHOR_TAG ; hua chen and chen, 94 ;  #AUTHOR_TAG.', 'it is also possible to focus on non - compositional compounds, a key']","['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun - phrase contraints  #TAUTHOR_TAG ; hua chen and chen, 94 ;  #AUTHOR_TAG.', 'it is also possible to focus on non - compositional compounds, a key point in bilingual applications  #AUTHOR_TAG lin, 99 ).', 'another interesting approach is']","['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun - phrase contraints  #TAUTHOR_TAG ; hua chen and chen, 94 ;  #AUTHOR_TAG.', 'it is also possible to focus on non - compositional compounds, a key point in bilingual applications  #AUTHOR_TAG lin, 99 ).', 'another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns  #AUTHOR_TAG furuse and iida, 96 ).', 'in this study, we filtered for potential sequences that are likely to be noun phrases, using simple regular expressions over the associated part - of - speech tags.', ""an excerpt of the association probabilities of a unit model trained considering only the np - sequences is given in table 3. applying this filter ( referred to as jrnp in the following ) to the 39, 093 english sequences still surviving after previous filters ~'1 and ~'2 removes 35, 939 of them ( 92 % )."", '4 : completion results of several translation models, spared : theoretical proportion of characters saved ; ok : number of target units accepted by the user ; good : number of target units that matched the expected whether they were proposed or not ; nu : number of sentences for which no target unit was found by the translation model ; u : number of sentences for which at least one helpful unit has been found by the model, but not necessarily proposed']",5
['sri commandtalk system  #TAUTHOR_TAG ; stent et'],"['sri commandtalk system  #TAUTHOR_TAG ; stent et a., 1999 ).', '']",['is based on that of the sri commandtalk system  #TAUTHOR_TAG ; stent et'],"['speech and language processing architecture is based on that of the sri commandtalk system  #TAUTHOR_TAG ; stent et a., 1999 ).', 'the system comprises a suite of about 20 agents, connected together using the spd open agent architecture ( oaa ;  #AUTHOR_TAG ).', '']",5
"['', 'cornmandtalk  #TAUTHOR_TAG']","['interbot project  #AUTHOR_TAG.', 'a number of other systems have addressed part of the task.', 'cornmandtalk  #TAUTHOR_TAG']","['##rs interbot project  #AUTHOR_TAG.', 'a number of other systems have addressed part of the task.', 'cornmandtalk  #TAUTHOR_TAG']","['basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system.', 'as evidence of the importance of this task in the nlp community note that the early, influential system shrdlu  #AUTHOR_TAG was intended to address just this type of problem.', 'more recent work on spoken language interfaces to semi - antonomous robots include srrs flakey robot  #AUTHOR_TAG and ncarars interbot project  #AUTHOR_TAG.', 'a number of other systems have addressed part of the task.', 'cornmandtalk  #TAUTHOR_TAG, circuit fix - it shop  #AUTHOR_TAG and trains - 96  #AUTHOR_TAG are spoken language systems but they interface to simulation or help facilities rather than semi - autonomous agents.', ""jack's moose lodge  #AUTHOR_TAG takes text rather than speech as natural language input and the avatars being controlled are not semi - autonomous."", 'other researchers have considered particular aspects of the problem such as accounting for various aspects of actions  #AUTHOR_TAG.', 'in most of this and other related work the treatment is some variant of the following.', 'if there is a speech interface, the input speech signal is converted into text.', ""text either from the recognizer or directly input by the user is then converted into some kind of logical formula, which abstractly represents the user's intended command ; this formula is then fed into a command interpreter, which executes the command""]",0
"['( merialdo  #TAUTHOR_TAG.', 'i in the']","['this with a first - order hmm part - ofspeech tagger ( merialdo  #TAUTHOR_TAG.', 'i in the']","['to disambiguate them.', 'we do this with a first - order hmm part - ofspeech tagger ( merialdo  #TAUTHOR_TAG.', 'i in the rest']","['.', 'an aligner.', 'after identification of word and sentence boundaries the text is processed into a bi - text by an alignment program.', 'this alignment is done on the basis of both length ( gale and church [ 7 ] ) and a notion of cognateness ( simard [ 161 ).', '2. transducers.', 'in order to compare numerical expressions, which often diverge in format between given pairs of languages, normalisation toward a common format is required.', 'this is done with transducers ( kaplan and kay, [ 10 ] ).', '3. part - of - speech tagger.', 'misleading similarities in graphical form can sometime induce translation mistakes ( deceptive cognates ).', '~ these forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.', 'we do this with a first - order hmm part - ofspeech tagger ( merialdo  #TAUTHOR_TAG.', 'i in the rest of the paper, we will use deceptive cognate very iosely often to refer to normative usage of word in general']",5
['on the basis of both length ( gale and church  #TAUTHOR_TAG and a notion of cognateness ( simard'],['on the basis of both length ( gale and church  #TAUTHOR_TAG and a notion of cognateness ( simard'],"['', 'an aligner.', 'after identification of word and sentence boundaries the text is processed into a bi - text by an alignment program.', 'this alignment is done on the basis of both length ( gale and church  #TAUTHOR_TAG and a notion of cognateness ( simard']","['.', 'an aligner.', 'after identification of word and sentence boundaries the text is processed into a bi - text by an alignment program.', 'this alignment is done on the basis of both length ( gale and church  #TAUTHOR_TAG and a notion of cognateness ( simard [ 16 ] ).', '2. transducers.', 'in order to compare numerical expressions, which often diverge in format between given pairs of languages, normalisation toward a common format is required.', 'this is done with transducers ( kaplan and kay, [ 10 ] ).', '3. part - of - speech tagger.', 'misleading similarities in graphical form can sometime induce translation mistakes ( deceptive cognates ).', '~ these forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.', 'we do this with a first - order hmm part - ofspeech tagger ( merialdo [ 13 ] ).', 'i in the rest of the paper, we will use deceptive cognate very iosely often to refer to normative usage of word in general']",5
"['##t  #TAUTHOR_TAG,']","['with textract  #TAUTHOR_TAG,']","['##t  #TAUTHOR_TAG,']","['##¢ before indexing the text, we process it with textract  #TAUTHOR_TAG, which performs lemmatization, and discovers proper names and technical terms.', 'we added a new module ( resporator ) which annotates text segments with qa - tokens using pattern matching.', 'thus the text "" for 5 centuries "" matches the durations pattern "" for : cardinal _ timeperiod "", where : car - dinal is the label for cardinal numbers, and _ timeperiod marks a time expression']",5
"['well as the full forms of unknown words.', 'we are using a lexicon of approx.', '100000 word stems of german  #TAUTHOR_TAG']","['well as the full forms of unknown words.', 'we are using a lexicon of approx.', '100000 word stems of german  #TAUTHOR_TAG']","['well as the full forms of unknown words.', 'we are using a lexicon of approx.', '100000 word stems of german  #TAUTHOR_TAG']","['##ana : morphological analysis provided by sines yields the word stems of nouns, verbs and adjectives, as well as the full forms of unknown words.', 'we are using a lexicon of approx.', '100000 word stems of german  #TAUTHOR_TAG']",5
"['processing  #TAUTHOR_TAG.', 'the fundamental design criterion of']","['processing  #TAUTHOR_TAG.', 'the fundamental design criterion of sines is to provide a set of basic, powerful, robust, and efficient stp components and 4almost all tools we examined build']","['for real - world german text processing  #TAUTHOR_TAG.', 'the fundamental design criterion of']","['preprocessing of text documents is carried out by re - using smes, an information extraction core system for real - world german text processing  #TAUTHOR_TAG.', 'the fundamental design criterion of sines is to provide a set of basic, powerful, robust, and efficient stp components and 4almost all tools we examined build a single multicategorizer except for svm - light, which builds multiple binary classifiers.', 'generic linguistic knowledge sources that can easily be customized to deal with different tasks in a flexible manner, sines includes a text tokenizer, a lexical processor and a chunk parser.', 'the chunk parser itself is subdivided into three components.', 'in the first step, phrasal fragments like general nominal expressions and verb groups are recognized.', 'next, the dependency - based structure of the fragments of each sentence is computed using a set of specific sentence patterns.', 'third, the grammatical functions are determined for each dependency - based structure on the basis of a large subcategorization lexicon.', 'the present application benefits from the high modularity of the usage of the components.', 'thus, it is possible to run only a subset of the components and to tailor their output.', 'the experiments described in section 4 make use of this feature']",5
"[', which is described in detail in  #TAUTHOR_TAG.', 'empir']","[', which is described in detail in  #TAUTHOR_TAG.', 'empire']","[', which is described in detail in  #TAUTHOR_TAG.', 'empir']","['', 'our implementation of the np - based qa system uses the empire noun phrase finder, which is described in detail in  #TAUTHOR_TAG.', 'empire identifies base nps - - non - recursive noun phrases - - using a very simple algorithm that matches part - of - speech tag sequences based on a learned noun phrase grammar.', 'the approach is able to achieve 94 % precision and recall for base nps derived from the penn treebank wall street journal  #AUTHOR_TAG.', 'in the experiments below, the np filter follows the application of the document retrieval and text summarization components.', 'pronoun answer hypotheses are discarded, and the nps are assembled into 50 - byte chunks']",5
"['of realistic settings  #TAUTHOR_TAG, we again propose the use of vector space methods from ir, which can be easily extended to']","['of realistic settings  #TAUTHOR_TAG, we again propose the use of vector space methods from ir, which can be easily extended to']","['in a variety of realistic settings  #TAUTHOR_TAG, we again propose the use of vector space methods from ir, which can be easily extended to the summarization task  #AUTHOR_TAG']","['next hypothesize that query - dependent text summarization algorithms will improve the performance of the qa system by focusing the system on the most relevant portions of the retrieved documents.', 'the goal for query - dependent summarization algorithms is to provide a short summary of a document with respect to a specific query.', 'although a number of methods for query - dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings  #TAUTHOR_TAG, we again propose the use of vector space methods from ir, which can be easily extended to the summarization task  #AUTHOR_TAG']",1
"['in  #TAUTHOR_TAG.', '']","['in  #TAUTHOR_TAG.', '']","['for the maximum entropy approach in  #TAUTHOR_TAG.', '']","['aim of this paper is to give a detailed account of the techniques used in tnt.', 'additionally, we present results of the tagger on the negra corpus  #AUTHOR_TAG and the penn treebank  #AUTHOR_TAG.', 'the penn treebank results reported here for the markov model approach are at least equivalent to those reported for the maximum entropy approach in  #TAUTHOR_TAG.', 'for a comparison to other taggers, the reader is referred to  #AUTHOR_TAG']",1
"['in  #TAUTHOR_TAG, the maximum entropy framework seems to be the only other approach yielding comparable']","['in  #TAUTHOR_TAG, the maximum entropy framework seems to be the only other approach yielding comparable']","['in  #TAUTHOR_TAG, the maximum entropy framework seems to be the only other approach yielding comparable results to the one presented here.', 'it is']","['', 'the architecture remains applicable to a large variety of languages.', 'according to current tagger comparisons ( van  #AUTHOR_TAG, and according to a comparsion of the results presented here with those in  #TAUTHOR_TAG, the maximum entropy framework seems to be the only other approach yielding comparable results to the one presented here.', 'it is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both']",1
"['##98 (....., ) ) improved a. ccura ( : y.', ""this confirms that although kozima's approach  #TAUTHOR_TAG is computationally expensive,""]","['dot density measure ( r98 ( m, ( lot ) ) yield similar results.', 'our spread activation based semantic measure ( r98 (....., ) ) improved a. ccura ( : y.', ""this confirms that although kozima's approach  #TAUTHOR_TAG is computationally expensive,""]","['##98 (....., ) ) improved a. ccura ( : y.', ""this confirms that although kozima's approach  #TAUTHOR_TAG is computationally expensive,""]","['similarity measures were examined.', 'the cosine coefficient ( r98 ( s, co, ) ) and dot density measure ( r98 ( m, ( lot ) ) yield similar results.', 'our spread activation based semantic measure ( r98 (....., ) ) improved a. ccura ( : y.', ""this confirms that although kozima's approach  #TAUTHOR_TAG is computationally expensive, it does produce more precise segmentation""]",1
"[""##98 (,,,, a ) uses a variant of kozima's semantic similarity measure  #TAUTHOR_TAG to compute block similarity."", 'word similarity is a function of word co - occurrence statistics in the given document.', 'words that belong to the']","[""##98 (,,,, a ) uses a variant of kozima's semantic similarity measure  #TAUTHOR_TAG to compute block similarity."", 'word similarity is a function of word co - occurrence statistics in the given document.', 'words that belong to the']","[""##98 (,,,, a ) uses a variant of kozima's semantic similarity measure  #TAUTHOR_TAG to compute block similarity."", 'word similarity is a function of word co - occurrence statistics in the given document.', 'words that belong to the same sentence are considered to be related.', 'given the co - occurrence frequencies']","[""##98 (,,,, a ) uses a variant of kozima's semantic similarity measure  #TAUTHOR_TAG to compute block similarity."", 'word similarity is a function of word co - occurrence statistics in the given document.', 'words that belong to the same sentence are considered to be related.', 'given the co - occurrence frequencies f ( wi, wj ), the transition probability matrix t is computed by equation 10.', 'equation 11 defines our spread activation scheme, s denotes the word similarity matrix, x is the number of activation steps and norm ( y ) converts a matrix y into a transition matrix, x = 5 was used in the experiment']",2
"['algorithm is at variance with the findings of  #TAUTHOR_TAG, who report large speed - ups']","['unification algorithm is at variance with the findings of  #TAUTHOR_TAG, who report large speed - ups']","['is at variance with the findings of  #TAUTHOR_TAG, who report large speed - ups']","['related work,  #AUTHOR_TAG describes an ap - proach to packing in which alternative feature structures are represented as packed, distributed disjunctions of feature structure fragments.', 'although the approach may have potential, the shifting of complex accounting into the unification algorithm is at variance with the findings of  #TAUTHOR_TAG, who report large speed - ups from the elimination of disjunction processing during unification.', 'unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison']",1
"['in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'we']","['in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'we']","['in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'we leave this']","['this paper we have provided an original mathematical argument in favour of this thesis.', 'our results hold for bilexical context - free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism ( see section 1 ).', 'we perceive that these results can be extended to other language models that properly embed bilexical context - free grammars, as for instance the more general history - based models used in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'we leave this for future work']",3
"['in  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'we']","['in  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'we']","['in  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'we leave this']","['this paper we have provided an original mathematical argument in favour of this thesis.', 'our results hold for bilexical context - free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism ( see section 1 ).', 'we perceive that these results can be extended to other language models that properly embed bilexical context - free grammars, as for instance the more general history - based models used in  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'we leave this for future work']",3
"['proposed by  #TAUTHOR_TAG.', 'windowdiff returns 0 in']","['proposed by  #TAUTHOR_TAG.', 'windowdiff returns 0 in']","['proposed by  #TAUTHOR_TAG.', 'windowdiff returns 0 in']","['is not the best measure to assess segmentation quality, therefore we also conducted experiments using the windowdiff measure as proposed by  #TAUTHOR_TAG.', 'windowdiff returns 0 in case of a perfect segmentation ; 1 is the worst possible score.', 'however, it only takes into account segment boundaries and disregards segment types.', 'in section 5. 2, we mentioned that loopy bp is not guaranteed to converge in a finite number of iterations.', 'since we optimize pseudolikelihood for parameter estimation, we are not affected by this limitation in the training phase.', 'however, we use loopy bp with a trp schedule during testing, so we must expect to encounter non - convergence for some examples.', 'theoretical results on this topic are discussed by  #AUTHOR_TAG.', 'we give here an empirical observation of convergence behaviour of loopy bp in our setting ; the maximum number of iterations of the trp schedule was restricted to 1, 000.', 'table 4 shows the percentage of examples converging within this limit and the average number of iterations required by the converging examples, broken down by the different corpora.', 'from these results, we conclude that there is a connection between the quality of the annotation and the convergence behaviour of loopy bp.', ""in practice, even though loopy bp didn't converge for some examples, the solutions after 1, 000 iterations where satisfactory""]",5
"['y occurs in an upward monotone context  #TAUTHOR_TAG, in practice genuine contradictions between']","['y occurs in an upward monotone context  #TAUTHOR_TAG, in practice genuine contradictions between y - values sharing a meronym relationship are']","['y occurs in an upward monotone context  #TAUTHOR_TAG, in practice genuine contradictions between']","['##onyms : for some relations, there is no contradiction when y 1 and y 2 share a meronym, i. e. "" part of "" relation.', 'for example, in the set born in ( mozart, • ) there is no contradiction between the y values "" salzburg "" and "" austria "", but "" salzburg "" conflicts with "" vienna "".', 'although this is only true in cases where y occurs in an upward monotone context  #TAUTHOR_TAG, in practice genuine contradictions between y - values sharing a meronym relationship are extremely rare.', 'we therefore simply assigned contradictions between meronyms a probability close to zero.', 'we used the tipster gazetteer 4 and wordnet to identify meronyms, both of which have high precision but low coverage']",4
['sentence with the collins parser  #TAUTHOR_TAG'],['sentence with the collins parser  #TAUTHOR_TAG'],"['twice the average rate.', 'we do not use any other lexical φ - features that reference x, for fear that they would enable the learner to explain the rationales without changing θ as desired ( see the end of section 5. 3 ). 14', 'we parse each sentence with the collins parser  #TAUTHOR_TAG.', 'then the document has one big parse tree, whose']","['train our model, we use l - bfgs to locally maximize the log of the objective function ( 1 ) : 15 these are the function words with count ≥ 40 in a random sample of 100 documents, and which were associated with the o - i tag transition at more than twice the average rate.', 'we do not use any other lexical φ - features that reference x, for fear that they would enable the learner to explain the rationales without changing θ as desired ( see the end of section 5. 3 ). 14', 'we parse each sentence with the collins parser  #TAUTHOR_TAG.', 'then the document has one big parse tree, whose root is doc, with each sentence being a child of doc']",5
"['of  #TAUTHOR_TAG, 9 which consists of 1000 positive and 1000 negative movie reviews, tokenized']","['of  #TAUTHOR_TAG, 9 which consists of 1000 positive and 1000 negative movie reviews, tokenized']","[', we introduced the movie review polarity dataset enriched with annotator rationales. 8', 'it is based on the dataset of  #TAUTHOR_TAG, 9 which consists of 1000 positive and 1000 negative movie reviews, tokenized']","[', we introduced the movie review polarity dataset enriched with annotator rationales. 8', 'it is based on the dataset of  #TAUTHOR_TAG, 9 which consists of 1000 positive and 1000 negative movie reviews, tokenized and divided into 10 folds ( f0 - - f9 ).', 'all our experiments use f9 as their final blind test set']",2
[' #TAUTHOR_TAG and use them to'],[' #TAUTHOR_TAG and use them to'],"['us infer the true θ.', 'we collect substring rationales for a sentiment classification task  #TAUTHOR_TAG and use them to']","['human annotator can provide hints to a machine learner by highlighting contextual "" rationales "" for each of his or her annotations  #AUTHOR_TAG.', 'how can one exploit this side information to better learn the desired parameters θ?', 'we present a generative model of how a given annotator, knowing the true θ, stochastically chooses rationales.', 'thus, observing the rationales helps us infer the true θ.', 'we collect substring rationales for a sentiment classification task  #TAUTHOR_TAG and use them to obtain significant accuracy improvements for each annotator.', 'our new generative approach exploits the rationales more effectively than our previous "" masking svm "" approach.', 'it is also more principled, and could be adapted to help learn other kinds of probabilistic classifiers for quite different tasks']",5
"['on this dataset  #TAUTHOR_TAG.', 'specifically,']","['on this dataset  #TAUTHOR_TAG.', 'specifically,']","['on this dataset  #TAUTHOR_TAG.', 'specifically,']","['f ( • ) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and z θ ( x ) def = y u ( x, y ) is a normalizer.', 'we use the same set of binary features as in previous work on this dataset  #TAUTHOR_TAG.', 'specifically, let v = { v 1,..., v 17744 } be the set of word types with count ≥ 4 in the full 2000 - document corpus.', 'define f h ( x, y ) to be y if v h appears at least once in x, and 0 otherwise.', 'thus θ ∈ r 17744, and positive weights in θ favor class label y = + 1 and equally discourage y = −1, while negative weights do the opposite.', 'this standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales.', 'future work should consider more complex features and how they are signaled by rationales, as discussed in section 3. 2']",5
"['( 1 ).', 'we also test an mi model inspired by  #AUTHOR_TAG : misim n ; v = log x simn _ ; n prv ; n _ n _ _ sims n', 'we gather similar words using  #TAUTHOR_TAG a ), mining similar verbs from a comparable - sized parse']","['( 1 ).', 'we also test an mi model inspired by  #AUTHOR_TAG : misim n ; v = log x simn _ ; n prv ; n _ n _ _ sims n', ""we gather similar words using  #TAUTHOR_TAG a ), mining similar verbs from a comparable - sized parsed corpus, and collecting similar nouns from a broader 10 gb corpus of english text. 4 we also use  #AUTHOR_TAG's approach to obtaining web - counts""]","['( 1 ).', 'we also test an mi model inspired by  #AUTHOR_TAG : misim n ; v = log x simn _ ; n prv ; n _ n _ _ sims n', ""we gather similar words using  #TAUTHOR_TAG a ), mining similar verbs from a comparable - sized parsed corpus, and collecting similar nouns from a broader 10 gb corpus of english text. 4 we also use  #AUTHOR_TAG's approach to obtaining web - counts""]","['first evaluate d s p on disambiguating positives from pseudo - negatives, comparing to recently - proposed systems that also require no manually - compiled resources like wordnet.', 'we convert da - gan et al. ( 1999 ) s similarity - smoothed probability to mi by replacing the empirical pr n | v in equa - tion ( 2 ) with the smoothed prsim from equation ( 1 ).', 'we also test an mi model inspired by  #AUTHOR_TAG : misim n ; v = log x simn _ ; n prv ; n _ n _ _ sims n', ""we gather similar words using  #TAUTHOR_TAG a ), mining similar verbs from a comparable - sized parsed corpus, and collecting similar nouns from a broader 10 gb corpus of english text. 4 we also use  #AUTHOR_TAG's approach to obtaining web - counts""]",5
[' #TAUTHOR_TAG. 2this data provides counts for pairs'],[' #TAUTHOR_TAG. 2this data provides counts for pairs'],[' #TAUTHOR_TAG. 2this data provides counts for pairs'],"['also made use of the person - name / instance pairs automatically extracted by  #TAUTHOR_TAG. 2this data provides counts for pairs such as "" edwin moses, hurdler "" and "" william farley, industrialist.', 'we have features for all concepts and therefore learn their association with each verb']",5
"[""9recall that even the  #TAUTHOR_TAG system, built on the world's largest corpus,""]","[""9recall that even the  #TAUTHOR_TAG system, built on the world's largest corpus,""]","[""9recall that even the  #TAUTHOR_TAG system, built on the world's largest corpus, achieves only""]","[', we evaluate dsp on a common application of selectional preferences : choosing the correct antecedent for pronouns in text  #AUTHOR_TAG.', ""we study the cases where a 9recall that even the  #TAUTHOR_TAG system, built on the world's largest corpus, achieves only 34 % recall ( table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed, but see footnote 5 )."", '']",1
"['pr ( n | braise ), pr ( n | ration ), and pr ( n | garnish ).', ' #TAUTHOR_TAG a )']","['pr ( n | braise ), pr ( n | ration ), and pr ( n | garnish ).', "" #TAUTHOR_TAG a )'s similar word list for eat misses these""]","['pr ( n | braise ), pr ( n | ration ), and pr ( n | garnish ).', ' #TAUTHOR_TAG a )']","['is interesting to inspect the feature weights returned by our system.', 'in particular, the weights on the verb co - occurrence features ( section 3. 3. 1 )', 'provide a high - quality, argument - specific similarityranking of other verb contexts.', 'the dsp parameters for eat, for example, place high weight on features like pr ( n | braise ), pr ( n | ration ), and pr ( n | garnish ).', "" #TAUTHOR_TAG a )'s similar word list for eat misses these but includes sleep ( ranked 6 ) and sit ( ranked 14 ), because these have similar subjects to eat."", 'discriminative, context - specific training seems to yield a better set of similar predicates, e. g. the highest - ranked contexts for dsp cooc on the verb join, 3 lead 1. 42, rejoin 1. 39, form 1. 34, belong to 1. 31, found 1. 31, quit 1. 29, guide 1. 19, induct 1. 19, launch ( subj ) 1. 18, work at 1. 14 give a better sims ( join ) for equation ( 1 ) than the top similarities returned by  #AUTHOR_TAG a other features are also weighted intuitively.', 'note that case is a strong indicator for some arguments, for example the weight on being lower - case is high for become ( 0. 972 ) and eat ( 0. 505 ), but highly negative for accuse ( - 0. 675 ) and embroil ( - 0. 573 ) which often take names of people and organizations']",0
"[' #TAUTHOR_TAG approach will be undefined if the pair is unobserved on the web.', 'as a reasonable default for these cases, we assign them a negative decision']","[' #TAUTHOR_TAG approach will be undefined if the pair is unobserved on the web.', 'as a reasonable default for these cases, we assign them a negative decision']","['', 'also, the  #TAUTHOR_TAG approach will be undefined if the pair is unobserved on the web.', 'as a reasonable default for these cases, we assign them a negative decision']","['available from the ldc as ldc2006t13.', 'this collection was generated from approximately 1 trillion tokens of online text.', 'unfortunately, tokens appearing less than 200 times have been mapped to the unk symbol, and only n - grams appearing more than 40 times are included.', 'unlike results from search engines, however, experiments with this corpus are replicable.', 'not be able to provide a score for each example.', 'the similarity - smoothed examples will be undefined if sims ( w ) is empty.', 'also, the  #TAUTHOR_TAG approach will be undefined if the pair is unobserved on the web.', 'as a reasonable default for these cases, we assign them a negative decision']",5
"['usual evaluation task for sp models  #TAUTHOR_TAG.', 'this']","['usual evaluation task for sp models  #TAUTHOR_TAG.', 'this']","['pseudodisambiguation, the usual evaluation task for sp models  #TAUTHOR_TAG.', 'this data consists of triples (']","['training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for sp models  #TAUTHOR_TAG.', 'this data consists of triples ( v, n, n ′ ) where v, n is a predicateargument pair observed in the corpus and v, n ′ has not been observed.', '']",1
"['application of interest has been shown previously by  #TAUTHOR_TAG.', 'they optimize a few metaparameters separately']","['application of interest has been shown previously by  #TAUTHOR_TAG.', 'they optimize a few metaparameters separately']","['advantage of tuning similarity to the application of interest has been shown previously by  #TAUTHOR_TAG.', 'they optimize a few metaparameters separately']","['advantage of tuning similarity to the application of interest has been shown previously by  #TAUTHOR_TAG.', 'they optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation.', 'our approach, on the other hand, discriminatively sets millions of individual similarity values.', ' #AUTHOR_TAG, our similarity values are asymmetric']",1
"[', n ).', 'normalizing by pr ( v ) ( yielding mi ) allows us to use a constant threshold across all verbs.', 'mi was also recently used for inference - rule sps by  #TAUTHOR_TAG']","['( v, n ).', 'normalizing by pr ( v ) ( yielding mi ) allows us to use a constant threshold across all verbs.', 'mi was also recently used for inference - rule sps by  #TAUTHOR_TAG']","[', n ).', 'normalizing by pr ( v ) ( yielding mi ) allows us to use a constant threshold across all verbs.', 'mi was also recently used for inference - rule sps by  #TAUTHOR_TAG']","['that is, every feature is an intersection of the occurrence of a particular predicate, v, and some feature of the argument f ( n ).', 'for example, a feature for a verb - object pair might be, "" the verb is eat and the object is lower - case. ""', 'in this representation, features for one predicate will be completely independent from those for every other predicate.', ""thus rather than a single training procedure, we can actually partition the examples by predicate, and train a for a fixed verb, mi is proportional to  #AUTHOR_TAG's conditional probability scores for pseudodisambiguation of ( v, n, n ′ ) triples : pr ( v | n ) = pr ( v, n ) / pr ( n ), which was shown to be a better measure of association than co - occurrence frequency f ( v, n )."", 'normalizing by pr ( v ) ( yielding mi ) allows us to use a constant threshold across all verbs.', 'mi was also recently used for inference - rule sps by  #TAUTHOR_TAG']",0
"['usual evaluation task for sp models  #TAUTHOR_TAG.', 'this']","['usual evaluation task for sp models  #TAUTHOR_TAG.', 'this']","['pseudodisambiguation, the usual evaluation task for sp models  #TAUTHOR_TAG.', 'this data consists of triples (']","['training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for sp models  #TAUTHOR_TAG.', 'this data consists of triples ( v, n, n ′ ) where v, n is a predicateargument pair observed in the corpus and v, n ′ has not been observed.', '']",1
"['also be inferred from clustering  #TAUTHOR_TAG.', ' #AUTHOR_TAG compare a number of wordnet - based approaches,']","['also be inferred from clustering  #TAUTHOR_TAG.', ' #AUTHOR_TAG compare a number of wordnet - based approaches,']","['also be inferred from clustering  #TAUTHOR_TAG.', ' #AUTHOR_TAG compare a number of wordnet - based approaches, including  #AUTHOR_TAG']","['approaches to sps generalize from observed predicate - argument pairs to semantically similar ones by modeling the semantic class of the argument, following  #AUTHOR_TAG.', 'for example, we might have a class mexican food and learn that the entire class is suitable for eating.', 'usually, the classes are from wordnet  #AUTHOR_TAG, although they can also be inferred from clustering  #TAUTHOR_TAG.', ' #AUTHOR_TAG compare a number of wordnet - based approaches, including  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, and found that the more sophisticated class - based approaches do not always outperform simple frequency - based models']",0
"['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times  #TAUTHOR_TAG.', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not']","['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times  #TAUTHOR_TAG.', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not']","['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times  #TAUTHOR_TAG.', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not']","['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times  #TAUTHOR_TAG.', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'we wish to use our model on arguments of any frequency, including those that never occurred in the training corpus ( and therefore have empty cooccurrence features ( section 3. 3. 1 ) ).', 'we proceed as follows : first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise.', 'next, we omit verb co - occurrence features for nouns that occur less than 10 times, and instead fire a low - count feature.', 'when we move to a new corpus, previouslyunseen nouns are treated like these low - count training nouns']",1
"['of paraphrases and inference rules  #TAUTHOR_TAG.', 'inferences']","['of paraphrases and inference rules  #TAUTHOR_TAG.', 'inferences']","['##al preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules  #TAUTHOR_TAG.', 'inferences']","['##al preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules  #TAUTHOR_TAG.', 'inferences such as "" [ x wins y ] ⇒ [ x plays y ] "" are only valid for certain argu - ments x and y.', 'we follow  #AUTHOR_TAG in using automatically - extracted semantic classes to help characterize plausible arguments']",0
"[""and  #TAUTHOR_TAG a )'s information - theoretic metric""]","[""and  #TAUTHOR_TAG a )'s information - theoretic metric""]","[""and  #TAUTHOR_TAG a )'s information - theoretic metric work best."", 'similarity - smoothed models are simple to compute, potentially']","['sim ( v ′, v ) returns a real - valued similarity between two verbs v ′ and v ( normalized over all pair similarities in the sum ).', 'in contrast,  #AUTHOR_TAG generalizes by substituting similar arguments, while  #AUTHOR_TAG use the cross - product of similar pairs.', 'one key issue is how to define the set of similar words, sims ( w ).', "" #AUTHOR_TAG compared a number of techniques for creating similar - word sets and found that both the jaccard coefficient and  #TAUTHOR_TAG a )'s information - theoretic metric work best."", 'similarity - smoothed models are simple to compute, potentially adaptable to new domains, and require no manually - compiled resources such as wordnet']",0
"['the 3 gb aquaint corpus  #AUTHOR_TAG using minipar  #TAUTHOR_TAG b ), and collected verb']","['parsed the 3 gb aquaint corpus  #AUTHOR_TAG using minipar  #TAUTHOR_TAG b ), and collected']","['parsed the 3 gb aquaint corpus  #AUTHOR_TAG using minipar  #TAUTHOR_TAG b ), and collected verb']","['parsed the 3 gb aquaint corpus  #AUTHOR_TAG using minipar  #TAUTHOR_TAG b ), and collected verb - object and verb - subject frequencies, building an empirical mi model from this data.', 'verbs and nouns were converted to their ( possibly multi - token ) root, and string case was preserved.', 'passive subjects ( the car was bought ) were converted to objects ( bought car ).', 'we set the mi - threshold, τ, to be 0, and the negative - to - positive ratio, k, to be 2']",5
"['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times  #TAUTHOR_TAG.', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not']","['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times  #TAUTHOR_TAG.', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not']","['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times  #TAUTHOR_TAG.', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not']","['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times  #TAUTHOR_TAG.', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'we wish to use our model on arguments of any frequency, including those that never occurred in the training corpus ( and therefore have empty cooccurrence features ( section 3. 3. 1 ) ).', 'we proceed as follows : first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise.', 'next, we omit verb co - occurrence features for nouns that occur less than 10 times, and instead fire a low - count feature.', 'when we move to a new corpus, previouslyunseen nouns are treated like these low - count training nouns']",1
"['.', 'we measure this association using pointwise mutual information ( mi )  #TAUTHOR_TAG.', 'the mi between a verb predicate, v, and']","['then extract the predicate - argument pairs that have a statistical association in this data.', 'we measure this association using pointwise mutual information ( mi )  #TAUTHOR_TAG.', 'the mi between a verb predicate, v, and']","['then extract the predicate - argument pairs that have a statistical association in this data.', 'we measure this association using pointwise mutual information ( mi )  #TAUTHOR_TAG.', 'the mi between a verb predicate, v, and']","['learn a discriminative model of selectional preference, we create positive and negative training examples automatically from raw text.', 'to create the positives, we automatically parse a large corpus, and then extract the predicate - argument pairs that have a statistical association in this data.', 'we measure this association using pointwise mutual information ( mi )  #TAUTHOR_TAG.', 'the mi between a verb predicate, v, and its object argument, n, is']",5
"[""train a 1for a fixed verb, mi is proportional to  #TAUTHOR_TAG's conditional probability scores""]","[""train a 1for a fixed verb, mi is proportional to  #TAUTHOR_TAG's conditional probability scores""]","[""train a 1for a fixed verb, mi is proportional to  #TAUTHOR_TAG's conditional probability scores""]","['that is, every feature is an intersection of the occurrence of a particular predicate, v, and some feature of the argument f ( n ).', 'for example, a feature for a verb - object pair might be, "" the verb is eat and the object is lower - case. ""', 'in this representation, features for one predicate will be completely independent from those for every other predicate.', ""thus rather than a single training procedure, we can actually partition the examples by predicate, and train a 1for a fixed verb, mi is proportional to  #TAUTHOR_TAG's conditional probability scores for pseudodisambiguation of ( v, n, n [UNK] ) triples : pr ( v | n ) = pr ( v, n ) / pr ( n ), which was shown to be a better measure of association than co - occurrence frequency f ( v, n )."", 'normalizing by pr ( v ) ( yielding mi ) allows us to use a constant threshold across all verbs.', 'mi was also recently used for inference - rule sps by  #AUTHOR_TAG']",4
['introduced by  #TAUTHOR_TAG'],['introduced by  #TAUTHOR_TAG'],"['is given the following set of six domainindependent rules, which are similar to the upward monotone rules introduced by  #TAUTHOR_TAG']","['is given the following set of six domainindependent rules, which are similar to the upward monotone rules introduced by  #TAUTHOR_TAG']",1
"[' #TAUTHOR_TAG, which convert']","[' #TAUTHOR_TAG, which convert']","[' #TAUTHOR_TAG, which convert the inputs']","['entailment systems are given two textual fragments, text t and hypothesis h, and attempt to decide if the meaning of h can be inferred from the meaning of t  #AUTHOR_TAG.', ""while many approaches have addressed this problem, our work is most closely related to that of  #TAUTHOR_TAG, which convert the inputs into logical forms and then attempt to ` prove'h from t plus a set of axioms."", 'for instance,  #AUTHOR_TAG represents t, h, and a set of rewrite rules in a description logic framework, and determines entailment by solving an integer linear program derived from that representation']",1
"['studies presented by  #TAUTHOR_TAG and  #AUTHOR_TAG differed in the number of states that they used.', 'evaluated against the reduced tag']","['studies presented by  #TAUTHOR_TAG and  #AUTHOR_TAG differed in the number of states that they used.', 'evaluated against the reduced tag']","['studies presented by  #TAUTHOR_TAG and  #AUTHOR_TAG differed in the number of states that they used.', 'evaluated against the reduced tag set of 17 tags developed']","['studies presented by  #TAUTHOR_TAG and  #AUTHOR_TAG differed in the number of states that they used.', 'evaluated against the reduced tag set of 17 tags developed by  #AUTHOR_TAG, while  #AUTHOR_TAG evaluated against the full penn treebank tag set.', 'we ran all our estimators in both conditions here ( thanks to noah smith for supplying us with his tag set )']",1
"['of our evaluation measures, confirming the results reported in  #TAUTHOR_TAG.', 'this is perhaps not too']","['of our evaluation measures, confirming the results reported in  #TAUTHOR_TAG.', 'this is perhaps not too surprising,']","['of our evaluation measures, confirming the results reported in  #TAUTHOR_TAG.', 'this is perhaps not too']","['might be expected, our evaluation measures disagree somewhat, but the following broad tendancies seem clear.', 'on small data sets all of the bayesian estimators strongly outperform em ( and, to a lesser extent, vb ) with respect to all of our evaluation measures, confirming the results reported in  #TAUTHOR_TAG.', 'this is perhaps not too surprising, as the bayesian prior plays a comparatively stronger role with a smaller training corpus ( which makes the likelihood term smaller ) and the approximation used by variational bayes is likely to be less accurate on smaller data sets']",1
['presented in  #AUTHOR_TAG and  #TAUTHOR_TAG'],['presented in  #AUTHOR_TAG and  #TAUTHOR_TAG'],['presented in  #AUTHOR_TAG and  #TAUTHOR_TAG'],['resulting training procedure is analogous to the one presented in  #AUTHOR_TAG and  #TAUTHOR_TAG'],1
"['( m > = n )  #TAUTHOR_TAG.', ' #AUTHOR_TAG a ) give a shortest path dependency kernel for relation']","['( m > = n )  #TAUTHOR_TAG.', ' #AUTHOR_TAG a ) give a shortest path dependency kernel for relation extraction.', 'their kernel, however, does not find similarity between two sentences but between the shortest']","['( mn2 ) space, where m and n are the number of nodes of the two trees ( m > = n )  #TAUTHOR_TAG.', ' #AUTHOR_TAG a ) give a shortest path dependency kernel for relation extraction.', 'their kernel, however, does not find similarity between two sentences but between the shortest dependency paths connecting the two entities of interests in the sentences.', 'this kernel uses general dependency graphs but if the graph is a tree then the shortest path is the only path between the entities.', 'their']","['few kernels based on dependency trees have also been proposed.', ' #AUTHOR_TAG proposed a tree kernel over shallow parse tree representations of sentences.', 'this tree kernel was slightly generalized by  #AUTHOR_TAG to compute similarity between two dependency trees.', 'in addition to the words, this kernel also incorporates word classes into the kernel.', 'the kernel is based on counting matching subsequences of children of matching nodes.', 'but as was also noted in  #AUTHOR_TAG a ), this kernel is opaque i. e. it is not obvious what the implicit features are and the authors do not describe it either.', 'in contrast, our dependency - based word subsequence kernel, which also computes similarity between two dependency trees, is very transparent with the implicit features being simply the dependency paths.', 'their kernel is also very time consuming and in their more general sparse setting it requires o ( mn3 ) time and o ( mn2 ) space, where m and n are the number of nodes of the two trees ( m > = n )  #TAUTHOR_TAG.', ' #AUTHOR_TAG a ) give a shortest path dependency kernel for relation extraction.', 'their kernel, however, does not find similarity between two sentences but between the shortest dependency paths connecting the two entities of interests in the sentences.', 'this kernel uses general dependency graphs but if the graph is a tree then the shortest path is the only path between the entities.', 'their kernel also uses word classes in addition to the words themselves']",3
"['behavior.', 'secondly, as  #TAUTHOR_TAG show, marginalizing out the different segmentations during decoding leads to improved performance.', 'we']","['behavior.', 'secondly, as  #TAUTHOR_TAG show, marginalizing out the different segmentations during decoding leads to improved performance.', 'we']","['behavior.', 'secondly, as  #TAUTHOR_TAG show, marginalizing out the different segmentations during decoding leads to improved performance.', 'we plan to build']","['', 'firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior.', 'secondly, as  #TAUTHOR_TAG show, marginalizing out the different segmentations during decoding leads to improved performance.', 'we plan to build our own decoder ( based on itg ) where different ideas can be tested including tractable ways for achieving a marginalization effect.', 'apart from a new decoder, it will be worthwhile adapting the prior probability in our model to allow for consistent estimation.', 'finally, it would be interesting to study properties of the penalized deleted estimation used in this paper']",3
"['good study comparing document categorization algorithms can be found in  #AUTHOR_TAG.', 'more recently,  #AUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG']","['good study comparing document categorization algorithms can be found in  #AUTHOR_TAG.', 'more recently,  #AUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG']","['good study comparing document categorization algorithms can be found in  #AUTHOR_TAG.', 'more recently,  #AUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG']","['good study comparing document categorization algorithms can be found in  #AUTHOR_TAG.', 'more recently,  #AUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG']",0
"['to overfitting while attaining higher likelihood of the training data than the heuristic estimator.', 'based on this advise ( moore and  #TAUTHOR_TAG exclude the latent segmentation variables and opt']","['to overfitting while attaining higher likelihood of the training data than the heuristic estimator.', 'based on this advise ( moore and  #TAUTHOR_TAG exclude the latent segmentation variables and opt']","['most similar efforts to ours, mainly ( de  #AUTHOR_TAG, conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator.', 'based on this advise ( moore and  #TAUTHOR_TAG exclude the latent segmentation variables and opt']","['most similar efforts to ours, mainly ( de  #AUTHOR_TAG, conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator.', 'based on this advise ( moore and  #TAUTHOR_TAG exclude the latent segmentation variables and opt for a heuristic training procedure.', 'in this work we also start out from a generative model with latent segmentation variables.', 'however, we find out that concentrating the learning effort on smoothing is crucial for good performance.', 'for this, we devise itg - based priors over segmentations and employ a penalized version of deleted estimation working with em at its core.', 'the fact that our results ( at least ) match the heuristic estimates on a reasonably sized data set ( 947k parallel sentence pairs ) is rather encouraging']",1
['- see  #TAUTHOR_TAG on'],['of word classes ; more flexible regions - - see  #TAUTHOR_TAG on'],['- see  #TAUTHOR_TAG on learning a state transition diagram for acoustic regions in phone recognition'],"['future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and string - transduction tasks.', 'we would like to use features that look at wide context on the input side, which is inexpensive  #AUTHOR_TAG.', 'latent variables we wish to consider are an increased number of word classes ; more flexible regions - - see  #TAUTHOR_TAG on learning a state transition diagram for acoustic regions in phone recognition - - and phonological features and syllable boundaries.', 'indeed, our local log - linear features over several aligned latent strings closely resemble the soft constraints used by phonologists  #AUTHOR_TAG.', 'finally, rather than define a fixed set of feature templates as in fig. 2, we would like to refine empirically useful features during training, resulting in language - specific backoff patterns and adaptively sized n - gram windows.', 'many of these enhancements will increase the computational burden, and we are interested in strategies to mitigate this, including approximation methods']",0
"['use features that look at wide context on the input side, which is inexpensive  #TAUTHOR_TAG.', 'latent variables']","['use features that look at wide context on the input side, which is inexpensive  #TAUTHOR_TAG.', 'latent variables']","['use features that look at wide context on the input side, which is inexpensive  #TAUTHOR_TAG.', 'latent variables']","['future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and string - transduction tasks.', 'we would like to use features that look at wide context on the input side, which is inexpensive  #TAUTHOR_TAG.', 'latent variables we wish to consider are an increased number of word classes ; more flexible regions - see  #AUTHOR_TAG on learning a state transition diagram for acoustic regions in phone recognition - and phonological features and syllable boundaries.', 'indeed, our local log - linear features over several aligned latent strings closely resemble the soft constraints used by phonologists  #AUTHOR_TAG.', 'finally, rather than define a fixed set of feature templates as in fig. 2, we would like to refine empirically useful features during training, resulting in language - specific backoff patterns and adaptively sized n - gram windows.', 'many of these enhancements will increase the computational burden, and we are interested in strategies to mitigate this, including approximation methods']",3
"['', ' #TAUTHOR_TAG for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples']","['explore more advanced methods to improve the similarity estimates.', ' #TAUTHOR_TAG for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples']","['explore more advanced methods to improve the similarity estimates.', ' #TAUTHOR_TAG for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples']","['', 'on the full training set the best method performed 2. 33 % better than the fully supervised model, which is a 10. 91 % error reduction.', 'using only 5 % of the training data the best semi - supervised model still achieved 60. 29 %, compared to 40. 49 % by the supervised model, which is an error reduction of 33. 27 %.', 'these results demonstrate that the latent words learned by the lwlm help for this complex information extraction task.', 'furthermore we have shown that the latent words are simple to incorporate in an existing classifier by adding additional features.', 'we would like to perform experiments on employing this model in other information extraction tasks, such as word sense disambiguation or named entity recognition.', 'the current model uses the context in a very straightforward way, i. e. the two words left and right of the current word, but in the future we would like to explore more advanced methods to improve the similarity estimates.', ' #TAUTHOR_TAG for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples']",0
"['. g.,  #TAUTHOR_TAG, to ranking models']","['( e. g.,  #TAUTHOR_TAG, to ranking models']","['. g.,  #TAUTHOR_TAG, to ranking models']","['this paper, we extend two classes of model adaptation methods ( i. e., model interpolation and error - driven learning ), which have been well studied in statistical language modeling for speech and natural language applications ( e. g.,  #TAUTHOR_TAG, to ranking models for web search applications']",0
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['example  #TAUTHOR_TAG,']","['already mentioned in the literature, see for example  #TAUTHOR_TAG, knowledge about implicit predicates could be potentially useful for a variety of nlp tasks such as language generation, information extraction, question answering or machine translation.', 'many applications of semantic relations in nlp are connected to paraphrasing or query expansion, see for example  #AUTHOR_TAG.', '']",0
"['numbers on this test set  #TAUTHOR_TAG.', 'in']","['numbers on this test set  #TAUTHOR_TAG.', 'in']","['on this test set  #TAUTHOR_TAG.', 'in']","['', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set  #TAUTHOR_TAG.', '']",1
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],1
"['products of latent variable grammars  #TAUTHOR_TAG, despite being']","['products of latent variable grammars  #TAUTHOR_TAG, despite being']","['products of latent variable grammars  #TAUTHOR_TAG, despite being a single generative pcfg.', 'our most accurate product model achieves']",[' #TAUTHOR_TAG'],1
"['the purpose of disentanglement  #TAUTHOR_TAG.', 'in our corpus we found 175 instances']","['the purpose of disentanglement  #TAUTHOR_TAG.', 'in our corpus we found 175 instances']","['the purpose of disentanglement  #TAUTHOR_TAG.', 'in our corpus we found 175 instances']","[""possibly critical feature is the'mention of names '."", ""in multi - party discussion people usually mention each other's name for the purpose of disentanglement  #TAUTHOR_TAG."", ""in our corpus we found 175 instances where a participant mentions other participant's name."", ""in addition to these,'subject of the email ','topic - shift cue words'can also be beneficial for a model."", 'as a next step for this research, we will investigate how to exploit these features in our methods.', 'we are also interested in the near future to transfer our approach to other similar domains by hierarchical bayesian multi - task learning and other domain adaptation methods.', 'we plan to work on both synchronous ( e. g., chats, meetings ) and asynchronous ( e. g., blogs ) domains']",0
"['to produce useful cluster prototypes.', 'we found that the oldest system  #TAUTHOR_TAG yielded']","['to produce useful cluster prototypes.', 'we found that the oldest system  #TAUTHOR_TAG yielded']","['to produce useful cluster prototypes.', 'we found that the oldest system  #TAUTHOR_TAG yielded the best prototypes, and that using these prototypes gave']","[', we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'we found that the oldest system  #TAUTHOR_TAG yielded the best prototypes, and that using these prototypes gave state - of - the - art performance on wsj, as well as improvements on nearly all of the non - english corpora.', 'these promising results suggest a new direction for future research : improving pos induction by developing methods targeted towards extracting better prototypes, rather than focusing on improving clustering of the entire data set']",0
"['of the several properties a relation can possess.', 'others include selectional preferences, transitivity  #TAUTHOR_TAG, mutual exclusion, symmetry, etc..', 'these properties are very useful in increasing our']","['of the several properties a relation can possess.', 'others include selectional preferences, transitivity  #TAUTHOR_TAG, mutual exclusion, symmetry, etc..', 'these properties are very useful in increasing our']","['of the several properties a relation can possess.', 'others include selectional preferences, transitivity  #TAUTHOR_TAG, mutual exclusion, symmetry, etc..', 'these properties are very useful in increasing our']","['run our techniques on a large set of relations to output a first repository of typed functional relations.', 'we release this list for further use by the research community. 2', 'future work : functionality is one of the several properties a relation can possess.', 'others include selectional preferences, transitivity  #TAUTHOR_TAG, mutual exclusion, symmetry, etc..', 'these properties are very useful in increasing our understanding about these open ie relation strings.', 'we believe that the general principles developed in this work, for example, connecting the open ie knowledge with an existing knowledge resource, will come in very handy in identifying these other properties']",0
"['combining phrase tables proposed in  #TAUTHOR_TAG.', 'the first phrase table is']","['combining phrase tables proposed in  #TAUTHOR_TAG.', 'the first phrase table is']","['combining phrase tables proposed in  #TAUTHOR_TAG.', 'the first phrase table is extracted from word alignments for the balanced concatenation with repetitions, which are then truncated so that they are kept for only one copy of the indonesian - english bi - text.', 'the second table is built from the simple concatenation.', 'the two tables']","['phrase table combination.', 'finally, we experiment with a method for combining phrase tables proposed in  #TAUTHOR_TAG.', 'the first phrase table is extracted from word alignments for the balanced concatenation with repetitions, which are then truncated so that they are kept for only one copy of the indonesian - english bi - text.', 'the second table is built from the simple concatenation.', '']",5
[' #TAUTHOR_TAG experimented with various techniques'],[' #TAUTHOR_TAG experimented with various techniques'],[' #TAUTHOR_TAG experimented with various techniques'],"['third relevant line of research is on reusing bitexts between related languages without or with very little adaptation, which works well for very closely related languages.', 'for example, our previous work  #TAUTHOR_TAG experimented with various techniques for combining a small bi - text for a resource - poor language ( indonesian or spanish, pretending that spanish is resource - poor ) with a much larger bi - text for a related resource - rich language ( malay or portuguese ) ; the target language of all bi - texts was english.', 'however, our previous work did not attempt language adaptation, except for very simple transliteration for portuguese - spanish that ignored context entirely ; since it could not substitute one word for a completely different word, it did not help much for malay - indonesian, which use unified spelling.', 'still, once we have language - adapted the large bi - text, it makes sense to try to combine it further with the small bi - text ; thus, below we will directly compare and combine these two approaches']",1
"[' #TAUTHOR_TAG, which is roughly equivalent to our core module.', 'our approach outperforms both by 17 % with similar estimated productivity']","[' #TAUTHOR_TAG, which is roughly equivalent to our core module.', 'our approach outperforms both by 17 % with similar estimated productivity']","[' #TAUTHOR_TAG, which is roughly equivalent to our core module.', 'our approach outperforms both by 17 % with similar estimated productivity']","['mainly compute precision for this task, as the recall of paraphrase fragments is difficult to define.', ""however, we do include a measure we call productivity to indicate the algorithm's completeness."", 'it is defined as the ratio between the number of resulting fragment pairs and the number of sentence pairs used as input.', 'tab. 2 shows the evaluation results.', 'we reach our best precision by using the vp - fragment heuristics, which is still more productive than the lcs method.', 'the grammatical filter gives us a higher precision compared to the purely alignment - based approaches.', 'enhancing the system with coreference resolution raises the score even further.', 'we cannot directly compare this performance to other systems, as all other approaches have different data sources.', 'however, precision is usually manually evaluated, so the figures are at least indicative for a comparison with previous work : one state - of - theart system introduced by  #AUTHOR_TAG extracts paraphrase fragments from bilingual parallel corpora and reaches a precision of 0. 67.', 'we found the same number using our previous approach  #TAUTHOR_TAG, which is roughly equivalent to our core module.', 'our approach outperforms both by 17 % with similar estimated productivity']",2
"['on mining script information  #TAUTHOR_TAG.', 'in this earlier']","['on mining script information  #TAUTHOR_TAG.', 'in this earlier work,']","['on mining script information  #TAUTHOR_TAG.', 'in this earlier work, we focused on event structures and']","['take some core ideas from our previous work on mining script information  #TAUTHOR_TAG.', 'in this earlier work, we focused on event structures and their possible realizations in natural language.', 'the corpus used in those experiments were short crowd - sourced descriptions of everyday tasks written in bullet point style.', 'we aligned them with a hand - crafted similarity measure that was specifically designed for this text type.', 'in this current work, we target the general task of extracting paraphrases for events rather than the much more specific scriptrelated task.', 'the current approach uses a domainindependent similarity measure instead of a specific hand - crafted similarity score and is thus applicable to standard texts']",2
"['fragments using a generative model of noisy translations.', 'our own work  #TAUTHOR_TAG extends the first idea to paraphrase fragment']","['fragments using a generative model of noisy translations.', 'our own work  #TAUTHOR_TAG extends the first idea to paraphrase fragment']","['the sub - sentential level is mainly based on phrase pair extraction techniques from the mt literature.', ' #AUTHOR_TAG extract subsentential translation pairs from comparable corpora using the log - likelihood - ratio of word translation probability.', ' #AUTHOR_TAG extract fragments using a generative model of noisy translations.', 'our own work  #TAUTHOR_TAG extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora.', 'our current approach also uses']","['', 'in both cases, each pattern or rule contains one or several slots, which are restricted to certain type of words, e. g., named entities ( ne ) or content words.', 'they are quite successful in ne - centered tasks, like information extraction, but their level of generalization or coverage is insufficient for applications like recognizing textual entailment  #AUTHOR_TAG.', 'the research on general paraphrase fragment extraction at the sub - sentential level is mainly based on phrase pair extraction techniques from the mt literature.', ' #AUTHOR_TAG extract subsentential translation pairs from comparable corpora using the log - likelihood - ratio of word translation probability.', ' #AUTHOR_TAG extract fragments using a generative model of noisy translations.', 'our own work  #TAUTHOR_TAG extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora.', 'our current approach also uses word - word alignment, however, we use syntactic dependency trees to compute grammatical fragments.', 'our use of dependency trees is inspired by the constituent - tree - based experiments of callison -  #AUTHOR_TAG']",2
[' #TAUTHOR_TAG used a chunk'],[' #TAUTHOR_TAG used a chunker3'],[' #TAUTHOR_TAG used a chunker3'],"['with the candidate fragment elements, we previously  #TAUTHOR_TAG used a chunker3 to finalize the output fragments, in order to follow the linguistic definition of a ( para - ) phrase.', 'we extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments ( sec.', '5. 3 ).', 'finally, we filter out trivial fragment pairs, such as identical or the original sentence pairs']",2
"['to  #TAUTHOR_TAG a ), our summar']","['to  #TAUTHOR_TAG a ), our summarization system is, which consists of']","['to  #TAUTHOR_TAG a ), our summarization system is, which consists of']","['to  #TAUTHOR_TAG a ), our summarization system is, which consists of three key components : an initial sentence pre - selection module to select some important sentence candidates ; the above compression model to generate n - best compressions for each sentence ; and then an ilp summarization method to select the best summary sentences from the multiple compressed sentences']",1
[' #TAUTHOR_TAG where'],[' #TAUTHOR_TAG where'],['to the problem is more compatible with the empirical evidence we presented in our prior work  #TAUTHOR_TAG where'],"['approach to the problem is more compatible with the empirical evidence we presented in our prior work  #TAUTHOR_TAG where we analyzed the output of chinese to english machine translation and found that there is no correlation between sentence length and mt quality.', 'rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple english sentences.', 'this prior work was carried over a dataset containing a single reference translation for each chinese sentence.', 'in the work presented in this paper, we strengthen our findings by examining multiple reference translations for each chinese sentence.', 'we define heavy sentences based on agreement of translator choices and reader preferences']",1
"['on linguistic rules  #TAUTHOR_TAG.', 'subsequently, tect']","['on linguistic rules  #TAUTHOR_TAG.', 'subsequently, tectogrammatical functors']","['the tectogrammatical one.', 'these automatic transformations are based on linguistic rules  #TAUTHOR_TAG.', 'subsequently, tect']","['the tectogrammatical parsing of czech, the analytical tree structure is converted into the tectogrammatical one.', 'these automatic transformations are based on linguistic rules  #TAUTHOR_TAG.', 'subsequently, tectogrammatical functors are assigned by the c4. 5 classifier ( 2abokrtsk9 et al., 2002 )']",5
"['ii  #TAUTHOR_TAG.', 'in the second step, we used a module for automatic analytical functor assignment ( 2abokrtskyt et al., 2002 )']","['ii  #TAUTHOR_TAG.', 'in the second step, we used a module for automatic analytical functor assignment ( 2abokrtskyt et al., 2002 )']","['ii  #TAUTHOR_TAG.', 'in the second step, we used a module for automatic analytical functor assignment ( 2abokrtskyt et al., 2002 )']","['analytical parsing of czech runs in two steps : the statistical dependency parser, which creates the structure of a dependency tree, and a classifier assigning analytical functors.', 'we carried out two parallel experiments with two parsers available for czech, parser i  #AUTHOR_TAG and parser ii  #TAUTHOR_TAG.', 'in the second step, we used a module for automatic analytical functor assignment ( 2abokrtskyt et al., 2002 )']",5
"['- 4, see  #TAUTHOR_TAG on the training part of the english']","['domain of financial news, we created a probabilistic czechenglish dictionary by running giza + + training ( translation models 1 - 4, see  #TAUTHOR_TAG on the training part of the english - czech wsj parallel corpus extended by the parallel corpus of entry / translation pairs from the manual dictionary.', '']","['a specific domain, which is in our case the domain of financial news, we created a probabilistic czechenglish dictionary by running giza + + training ( translation models 1 - 4, see  #TAUTHOR_TAG on the training part of the english - czech wsj parallel corpus extended by the parallel corpus of entry / translation pairs from the manual dictionary.', '']","['make the dictionary more sensitive to a specific domain, which is in our case the domain of financial news, we created a probabilistic czechenglish dictionary by running giza + + training ( translation models 1 - 4, see  #TAUTHOR_TAG on the training part of the english - czech wsj parallel corpus extended by the parallel corpus of entry / translation pairs from the manual dictionary.', 'as a result, the entry / translation pairs seen in the parallel corpus of wsj become more probable.', 'for entry / translation pairs not seen in the parallel text, the probability distribution among translations is uniform.', 'the translation is "" giza + + se - lected "" if its probability is higher than a threshold, which is in our case set to 0. 10']",5
"['score  #TAUTHOR_TAG.', '']","['use the bleu score  #TAUTHOR_TAG.', '']","['- tion 7.', 'for the evaluation of the results we use the bleu score  #TAUTHOR_TAG.', 'section 8 compares translations']","[', we summarize resources available for the experiments ( section 2 ).', 'section 3 describes the automatic procedures used for the preparation of both training and testing data, including morphological tagging, and analytical and tectogrammatical parsing of czech input.', 'in section 4 we describe the process of the filtering of dictionaries used in the transfer procedure ( for its characterization, see section 5 ).', 'the generation process consisting mainly of word reordering and lexical insertions is explained in section 6, an example illustrating the generation steps is presented in sec - tion 7.', 'for the evaluation of the results we use the bleu score  #TAUTHOR_TAG.', 'section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'we also compare the results with the output generated by the statistical translation system giza + + / isi rewrite decoder ( al -  #AUTHOR_TAG, trained on the same parallel corpus']",5
"['giza + + / isi rewrite decoder  #TAUTHOR_TAG, trained on the same parallel corpus']","['giza + + / isi rewrite decoder  #TAUTHOR_TAG, trained on the same parallel corpus']","['by the statistical translation system giza + + / isi rewrite decoder  #TAUTHOR_TAG, trained on the same parallel corpus']","[', we summarize resources available for the experiments ( section 2 ).', 'section 3 describes the automatic procedures used for the preparation of both training and testing data, including morphological tagging, and analytical and tectogrammatical parsing of czech input.', 'in section 4 we describe the process of the filtering of dictionaries used in the transfer procedure ( for its characterization, see section 5 ).', 'the generation process consisting mainly of word reordering and lexical insertions is explained in section 6, an example illustrating the generation steps is presented in sec - tion 7.', 'for the evaluation of the results we use the bleu score  #AUTHOR_TAG.', 'section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'we also compare the results with the output generated by the statistical translation system giza + + / isi rewrite decoder  #TAUTHOR_TAG, trained on the same parallel corpus']",1
"['metric  #TAUTHOR_TAG, using the']","['metric  #TAUTHOR_TAG, using the']","['metric  #TAUTHOR_TAG, using the same evaluation method and reference retranslations that were used']","[""evaluated our translations with ibm's bleu evaluation metric  #TAUTHOR_TAG, using the same evaluation method and reference retranslations that were used for evaluation at hlt workshop 2002 at clsp ( haji 6 et al., 2002 )."", 'we used four reference retranslations of 490 sentences selected from the wsj sections 22, 23, and 24, which were themselves used as the fifth reference.', 'the evaluation method used is to hold out each reference in turn and evaluate it against the remaining four, averaging the five bleu scores']",5
"['- 4 translation model  #TAUTHOR_TAG.', 'a description of the system can be found in  #AUTHOR_TAG.', 'table 5 presents an']","['in producing a more focused lexicon probability distribution.', 'we performed translation experiments with an implementation of the ibm - 4 translation model  #TAUTHOR_TAG.', 'a description of the system can be found in  #AUTHOR_TAG.', 'table 5 presents an']","['in producing a more focused lexicon probability distribution.', 'we performed translation experiments with an implementation of the ibm - 4 translation model  #TAUTHOR_TAG.', 'a description of the system can be found in  #AUTHOR_TAG.', 'table 5 presents an assessment of translation quality for both the language pairs english - catalan and english - spanish.', 'we see that there is a significant decrease in error rate']","['compared the two statistical lexica obtained from the baseline system and from the maximum entropy training on the transformed corpus.', 'for the baseline lexicon, we observed an average of 5. 82 catalan translation candidates per english word and 6. 16 spanish translation candidates.', 'these numbers are significantly reduced in the lexicon which was trained on the transformed corpus using maximum entropy : there, we have an average of 4. 20 for catalan and 4. 46 for spanish.', 'especially for ( nominative ) english pronouns ( which have many verbs as translation candidates in the baseline lexicon ), the number of translation candidates was substantially scaled down by a factor around 4. this shows that our method was successful in producing a more focused lexicon probability distribution.', 'we performed translation experiments with an implementation of the ibm - 4 translation model  #TAUTHOR_TAG.', 'a description of the system can be found in  #AUTHOR_TAG.', 'table 5 presents an assessment of translation quality for both the language pairs english - catalan and english - spanish.', 'we see that there is a significant decrease in error rate for the translation into catalan.', 'this change is consistent across both error rates, the wer and 100 - bleu.', 'for translations from english into spanish, the improvement is less substantial.', 'a reason for this might be that the spanish vocabulary contains more entries and the ratio between fullforms and baseforms is higher : 1. 57 for spanish versus 1. 53 for catalan4.', 'this makes it more difficult for the system to choose the correct inflection when generating a spanish sentence.', 'we assume that the extension of our approach to other word classes than verbs will yield a quality gain for translations into spanish']",5
['maximum entropy approach  #TAUTHOR_TAG presents a powerful'],['maximum entropy approach  #TAUTHOR_TAG presents a powerful'],"['maximum entropy approach  #TAUTHOR_TAG presents a powerful framework for the combination of several knowledge sources.', 'this principle recommends to']","['maximum entropy approach  #TAUTHOR_TAG presents a powerful framework for the combination of several knowledge sources.', 'this principle recommends to choose the distribution which preserves as much uncertainty as possible in terms of maximizing the entropy.', 'the distribution is required to satisfy constraints, which represent facts known from the data.', 'these constraints are expressed on the basis of feature functions hu, ( s, t )']",5
['example  #TAUTHOR_TAG'],['example  #TAUTHOR_TAG'],['example  #TAUTHOR_TAG'],"['input string can be preprocessed before being passed to the search algorithm.', 'if necessary, the inverse of these transformations will be applied to the generated output string.', 'in the work presented here, we restrict ourselves to transforming only one language of the two : the source, which has the less inflected morphology.', 'for descriptions of smt systems see for example  #TAUTHOR_TAG']",0
"['feature function hm.', 'for an introduction to maximum entropy modeling and training procedures, the reader is referred to the corresponding literature, for instance  #TAUTHOR_TAG or  #AUTHOR_TAG']","['feature function hm.', 'for an introduction to maximum entropy modeling and training procedures, the reader is referred to the corresponding literature, for instance  #TAUTHOR_TAG or  #AUTHOR_TAG']","['each feature function hm.', 'for an introduction to maximum entropy modeling and training procedures, the reader is referred to the corresponding literature, for instance  #TAUTHOR_TAG or  #AUTHOR_TAG']","['a = { am } is the set of model parameters with one weight a, for each feature function hm.', 'for an introduction to maximum entropy modeling and training procedures, the reader is referred to the corresponding literature, for instance  #TAUTHOR_TAG or  #AUTHOR_TAG']",0
['example  #TAUTHOR_TAG'],['example  #TAUTHOR_TAG'],['example  #TAUTHOR_TAG'],"['input string can be preprocessed before being passed to the search algorithm.', 'if necessary, the inverse of these transformations will be applied to the generated output string.', 'in the work presented here, we restrict ourselves to transforming only one language of the two : the source, which has the less inflected morphology.', 'for descriptions of smt systems see for example  #TAUTHOR_TAG']",0
"['to using a global model like crfs, our previous work in  #TAUTHOR_TAG c ) reported the best results over the']","['to using a global model like crfs, our previous work in  #TAUTHOR_TAG c ) reported the best results over the']","['to using a global model like crfs, our previous work in  #TAUTHOR_TAG c ) reported the best results over the evaluated corpora of bakeoff - 2 until now7.', 'though those results are slightly better than the results here, we still see that the results of character - level dependency parsing']","['to using a global model like crfs, our previous work in  #TAUTHOR_TAG c ) reported the best results over the evaluated corpora of bakeoff - 2 until now7.', 'though those results are slightly better than the results here, we still see that the results of character - level dependency parsing approach ( scheme e ) are comparable to those state - of - the - art ones on each evaluated corpus']",1
"['the representation in  #TAUTHOR_TAG is even more compact than ours for grammars that are not self - embedding.', 'however, in this paper we use our representation as an intermediate result in approximating an unrestricted']","['the representation in  #TAUTHOR_TAG is even more compact than ours for grammars that are not self - embedding.', 'however, in this paper we use our representation as an intermediate result in approximating an unrestricted context - free grammar, with the final objective of obtaining a single minimal deterministic automaton.', ""for this purpose, mohri and pereira's representation offers little advantage""]","['the representation in  #TAUTHOR_TAG is even more compact than ours for grammars that are not self - embedding.', 'however, in this paper we use our representation as an intermediate result in approximating an unrestricted context - free grammar, with the final objective of obtaining a single minimal deterministic automaton.', ""for this purpose, mohri and pereira's representation offers little advantage""]","['the representation in  #TAUTHOR_TAG is even more compact than ours for grammars that are not self - embedding.', 'however, in this paper we use our representation as an intermediate result in approximating an unrestricted context - free grammar, with the final objective of obtaining a single minimal deterministic automaton.', ""for this purpose, mohri and pereira's representation offers little advantage""]",1
"[' #TAUTHOR_TAG.', 'since the latest publication in this']","[' #TAUTHOR_TAG.', 'since the latest publication in this']","[' #TAUTHOR_TAG.', 'since the latest publication in this area is more explicit in its presentation, we will base our treatment on this, instead of going to the historical roots of the method']","['restricting the height of the stack of a pushdown automaton, one obstructs recognition of a set of strings in the context - free language, and therefore a subset approximation results.', 'this idea was proposed by krauwer and des  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, and was rediscovered by  #AUTHOR_TAG and recently by  #TAUTHOR_TAG.', 'since the latest publication in this area is more explicit in its presentation, we will base our treatment on this, instead of going to the historical roots of the method']",0
"['##hrase the method of  #TAUTHOR_TAG as follows : first, we construct']","['rephrase the method of  #TAUTHOR_TAG as follows : first, we construct']","['rephrase the method of  #TAUTHOR_TAG as follows : first, we construct the approximating finite automaton according to the unparameterized rtn method above.', 'then an additional mechanism is introduced that ensures for each rule a - - ~ x1 •.. xm separately that the list']","['rephrase the method of  #TAUTHOR_TAG as follows : first, we construct the approximating finite automaton according to the unparameterized rtn method above.', 'then an additional mechanism is introduced that ensures for each rule a - - ~ x1 •.. xm separately that the list of visits to the states qo,.. • • qm satisfies some reasonable criteria : a visit to qi, with 0 < i < m, should be followed by one to qi + l or q0.', 'the latter option amounts to a nested incarnation of the rule.', 'there is a complementary condition for what should precede a visit to qi, with 0 < i < m']",5
"['method can be generalized, inspired by  #TAUTHOR_TAG, who derive n - gram probabilities from stochastic']","['method can be generalized, inspired by  #TAUTHOR_TAG, who derive n - gram probabilities from stochastic context - free grammars.', 'by ignoring the probabilities, each n = 1, 2, 3.... gives rise to a superset approximation that can be described as follows : the']","['method can be generalized, inspired by  #TAUTHOR_TAG, who derive n - gram probabilities from stochastic context - free grammars.', 'by ignoring the probabilities, each n = 1, 2, 3.... gives rise to a superset approximation that can be described as follows : the set of strings derivable from a nonterminal a is approximated by the set of strings']","['method can be generalized, inspired by  #TAUTHOR_TAG, who derive n - gram probabilities from stochastic context - free grammars.', 'by ignoring the probabilities, each n = 1, 2, 3.... gives rise to a superset approximation that can be described as follows : the set of strings derivable from a nonterminal a is approximated by the set of strings al... an such that • for each substring v = ai + l... ai + n ( 0 < i < n - - n ) we have a - - + * wvy, for some w and y']",0
[' #TAUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata'],[' #TAUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata'],[' #TAUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata'],[' #TAUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata'],0
"['not given here.', 'a very similar formulation, for another grammar transformation, is given in  #TAUTHOR_TAG']","['not given here.', 'a very similar formulation, for another grammar transformation, is given in  #TAUTHOR_TAG']","['##2, assuming the value right. the full formulation of the integrated grammar transformation and construction of the finite automaton is rather long and is therefore not given here.', 'a very similar formulation, for another grammar transformation, is given in  #TAUTHOR_TAG']","['10 ( b ) is no longer possible, since no nonterminal in the transformed grammar would contain 1 in its superscript. because of the demonstrated increase of the counter f, this transformation is guaranteed to remove self - embedding from the grammar.', 'however, it is not as selective as the transformation we saw before, in the sense that it may also block subderivations that are not of the form a - - * * ~ afl.', 'consider for example the subderivation from figure10, but replacing the lower occurrence of s by any other nonterminal c that is mutually recursive with s, a, and b. such a subderivation s - - - * * b c c d a would also be blocked by choosing d = 0.', 'in general, increasing d allows more of such derivations that are not of the form a ~ "" o ~ afl but also allows more derivations that are of that form. the reason for considering this transformation rather than any other that eliminates self - embedding is purely pragmatic : of the many variants we have tried that yield nontrivial subset approximations, this transformation has the lowest complexity in terms of the sizes of intermediate structures and of the resulting finite automata. in the actual implementation, we have integrated the grammar transformation and the construction of the finite automaton, which avoids reanalysis of the grammar to determine the partition of mutually recursive nonterminals after transformation.', 'this integration makes use, for example, of the fact that for fixed ni and fixed f, the set of nonterminals of the form a, f, with a c ni, is ( potentially ) mutually right - recursive. a set of such nonterminals can therefore be treated as the corresponding case from figure2, assuming the value right. the full formulation of the integrated grammar transformation and construction of the finite automaton is rather long and is therefore not given here.', 'a very similar formulation, for another grammar transformation, is given in  #TAUTHOR_TAG']",1
"['sets are those described by  #AUTHOR_TAG, mc  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG']","['sets are those described by  #AUTHOR_TAG, mc  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG']","['a formal model of universal computation ( post 1943 ).', 'conventionally, these rules are specified by an expert linguist, conversant with the sound and spelling systems of the language of concern.', 'typical letter - to - sound rule sets are those described by  #AUTHOR_TAG, mc  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG']","['states that the letter substring b with left context a and right context c receives the pronunciation ( i. e., phoneme substring ) d. such rules can also be straightforwardly cast in the if... then form commonly featured in high - level programming languages and employed in expert, knowledge - based systems technology.', 'they also constitute a formal model of universal computation ( post 1943 ).', 'conventionally, these rules are specified by an expert linguist, conversant with the sound and spelling systems of the language of concern.', 'typical letter - to - sound rule sets are those described by  #AUTHOR_TAG, mc  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG']",0
"['.', 'for instance,  #TAUTHOR_TAG recently wrote : ` ` to our knowledge, learning algorithms, although promising, have not ( yet ) reached']","['rules.', 'however, this possibility is not usually given much credence.', 'for instance,  #TAUTHOR_TAG recently wrote : ` ` to our knowledge, learning algorithms, although promising, have not ( yet ) reached']","['is also conceivable that data - driven techniques can actually outperform traditional rules.', 'however, this possibility is not usually given much credence.', 'for instance,  #TAUTHOR_TAG recently wrote : ` ` to our knowledge, learning algorithms, although promising, have not ( yet ) reached']","['is also conceivable that data - driven techniques can actually outperform traditional rules.', 'however, this possibility is not usually given much credence.', ""for instance,  #TAUTHOR_TAG recently wrote : ` ` to our knowledge, learning algorithms, although promising, have not ( yet ) reached the level of rule sets developed by humans'' ( p. 520 )."", '520 ).', ' #AUTHOR_TAG takes this further, stating "" such training - based strategies are often assumed to exhibit much more intelligence than they do in practice, as revealed by their poor transcription scores "" ( p.', '115, note 14 )']",0
"['of  #TAUTHOR_TAG, which considers computer - based pronunciation by analogy but does not mention the possible']","['of  #TAUTHOR_TAG, which considers computer - based pronunciation by analogy but does not mention the possible']","['ago by dedina and  #AUTHOR_TAG, 1991 ).', 'see also the work of  #TAUTHOR_TAG, which considers computer - based pronunciation by analogy but does not mention the possible application to text - to - speech synthesis.', 'as detailed by  #AUTHOR_TAG and  #AUTHOR_TAG, pba shares many similarities with the artificial intelligence paradigms']","['##unciation by analogy ( pba ) is a data - driven technique for the automatic phonemization of text, originally proposed as a model of reading, e. g., by  #AUTHOR_TAG and  #AUTHOR_TAG.', 'it was first proposed for tts applications over a decade ago by dedina and  #AUTHOR_TAG, 1991 ).', 'see also the work of  #TAUTHOR_TAG, which considers computer - based pronunciation by analogy but does not mention the possible application to text - to - speech synthesis.', 'as detailed by  #AUTHOR_TAG and  #AUTHOR_TAG, pba shares many similarities with the artificial intelligence paradigms variously called case - based, memory - based, or instance - based reasoning as applied to letter - to - phoneme conversion ( stanfill and waltz 1986 ; lehnert 1987 ; stanfill 1987  #AUTHOR_TAG golding 1991 ; golding and rosenbloom 1991 ; van den bosch and daelemans 1993 )']",0
"['.,  #TAUTHOR_TAG ;']","[',  #TAUTHOR_TAG ;']","['.,  #TAUTHOR_TAG ; van halteren, zavrel,']","[', the above characterization is very wide ranging.', 'consequently, fusion has been applied to a wide variety of pattern recognition and decision theoretic problems - - using a plethora of theories, techniques, and tools - - including some applications in computational linguistics ( e. g.,  #TAUTHOR_TAG ; van halteren, zavrel, and daelemans 1998 ) and speech technology ( e. g., bowles and damper 1989 ; romary and pierre11989 ).', 'according to  #AUTHOR_TAG, 290 ), "" while the reasons [ that ] combining models works so well are not rigorously understood, there is ample evidence that improvements over single models are typical....', 'a strong case can be made for combining models across algorithm families as a means of providing uncorrelated output estimates. ""', 'our purpose in this paper is to study and exploit such fusion by model ( or strategy ) combination as a way of achieving performance gains in pba']",0
[';  #TAUTHOR_TAG'],"['hirakawa 1994 ;  #TAUTHOR_TAG 1995 ; wu and xia 1994 ).', 'most of these algorithms can be summarized as follows']",['hirakawa 1994 ;  #TAUTHOR_TAG'],"['researchers have proposed greedy algorithms for estimating nonprobabilistic word - to - word translation models, also known as translation lexicons ( e. g., catizone, russell, and warwick 1989 ; gale and church 1991 ; fung 1995 ; kumano and hirakawa 1994 ;  #TAUTHOR_TAG 1995 ; wu and xia 1994 ).', 'most of these algorithms can be summarized as follows']",0
"[' #TAUTHOR_TAG b ).', 'these models involve conditional probabilities, but']","[' #TAUTHOR_TAG b ).', 'these models involve conditional probabilities, but']","[' #TAUTHOR_TAG b ).', 'these models involve conditional probabilities, but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution.', 'i shall review these models using the notation in']","['probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by  #TAUTHOR_TAG b ).', 'these models involve conditional probabilities, but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution.', 'i shall review these models using the notation in table 1']",0
"['models proposed by  #TAUTHOR_TAG b ), this model is symmetric,']","['models proposed by  #TAUTHOR_TAG b ), this model is symmetric,']","['', 'unlike the models proposed by  #TAUTHOR_TAG b ), this model is symmetric,']","['probability distribution trans (. 1, ~ ) is a word - to - word translation model.', 'unlike the models proposed by  #TAUTHOR_TAG b ), this model is symmetric, because both word bags are generated together from a joint probability distribution.', ""brown and his colleagues'models, reviewed in section 4. 3, generate one half of the bitext given the other hall so they are represented by conditional probability distributions."", 'a sequenceto - sequence translation model can be obtained from a word - to - word translation model by combining equation 11 with order information as in equation 8']",1
"['bitext space is another kind of collocation.', ""if each word's translation is treated as a sense tag  #TAUTHOR_TAG, then ` ` translational""]","['bitext space is another kind of collocation.', ""if each word's translation is treated as a sense tag  #TAUTHOR_TAG, then ` ` translational'' collocations have the unique property that the collocate and the word sense are one and the""]","['bitext space is another kind of collocation.', ""if each word's translation is treated as a sense tag  #TAUTHOR_TAG, then ` ` translational""]","['bitext space is another kind of collocation.', ""if each word's translation is treated as a sense tag  #TAUTHOR_TAG, then ` ` translational'' collocations have the unique property that the collocate and the word sense are one and the same!"", 'method b exploits this property under the hypothesis that "" one sense per collocation "" holds for translational collocations.', 'this hypothesis implies that if u and v are possible mutual translations, and a token u co - occurs with a token v in the bitext, then with very high probability the pair ( u, v ) was generated from the same concept and should be linked.', 'to test this hypothesis, i ran one iteration of method a on 300, 000 aligned sentence pairs from the canadian hansards bitext.', 'i then plotted the links ( u, v ) ratio ~ for several values of cooc ( u, v ) in figure 2. the curves show that the ratio links ( u, v ) cooc ( u, v ) tends to be either very high or very low.', 'this bimodality is not an artifact of the competitive linking process, because in the first iteration, linking decisions are based only on the initial similarity metric']",5
"['informal experiments described elsewhere  #TAUTHOR_TAG, i found that the g2 statistic suggested by  #AUTHOR_TAG slightly']","['informal experiments described elsewhere  #TAUTHOR_TAG, i found that the g2 statistic suggested by  #AUTHOR_TAG slightly']","['informal experiments described elsewhere  #TAUTHOR_TAG, i found that the g2 statistic suggested by  #AUTHOR_TAG slightly']","['informal experiments described elsewhere  #TAUTHOR_TAG, i found that the g2 statistic suggested by  #AUTHOR_TAG slightly outperforms 02.', 'let the cells of the contingency table be named as follows']",2
"[',  #TAUTHOR_TAG, a¢ certain machine - assisted translation tools ( e']","[',  #TAUTHOR_TAG, a¢ certain machine - assisted translation tools ( e. g., macklovitch']","[',  #TAUTHOR_TAG, a¢ certain machine - assisted translation tools ( e']","['##¢ cross - language information retrieval ( e. g., mccarley 1999 ), a¢ multilingual document filtering ( e. g., oard 1997 ), a¢ computer - assisted language learning ( e. g.,  #TAUTHOR_TAG, a¢ certain machine - assisted translation tools ( e. g., macklovitch 1994 ; melamed 1996a ), a¢ concordancing for bilingual lexicography ( e. g., catizone, russell, and warwick 1989 ; gale and church 1991 )']",0
"[""this situation,  #TAUTHOR_TAG b, 293 ) recommend ` ` evaluating the expectations using only a single, probable alignment.''"", 'the single most probable assignment ama ~ is the maximum a posteriori ( map ) assignment']","[""this situation,  #TAUTHOR_TAG b, 293 ) recommend ` ` evaluating the expectations using only a single, probable alignment.''"", 'the single most probable assignment ama ~ is the maximum a posteriori ( map ) assignment']","[""this situation,  #TAUTHOR_TAG b, 293 ) recommend ` ` evaluating the expectations using only a single, probable alignment.''"", 'the single most probable assignment ama ~ is the maximum a posteriori ( map ) assignment']","[""this situation,  #TAUTHOR_TAG b, 293 ) recommend ` ` evaluating the expectations using only a single, probable alignment.''"", 'the single most probable assignment ama ~ is the maximum a posteriori ( map ) assignment']",4
"['not ( cfxxx  #TAUTHOR_TAG.', 'brown et']","['not ( cfxxx  #TAUTHOR_TAG.', 'brown et al. 1993 ).', 'when the auxiliary parameters are conditioned on different link classes, their optimization is']","['entries in an on - line bilingual dictionary separately from those that do not ( cfxxx  #TAUTHOR_TAG.', 'brown et']","['method b, the estimation of the auxiliary parameters a + and a - depends only on the overall distribution of co - occurrence counts and link frequencies.', 'all word pairs that co - occur the same number of times and are linked the same number of times are assigned the same score.', 'more accurate models can be induced by taking into account various features of the linked tokens.', 'for example, frequent words are translated less consistently than rare words ( catizone, russell, and warwick 1989 ).', 'to account for these differences, we can estimate separate values of a + and a - for different ranges of cooc ( u, v ).', 'similarly, the auxiliary parameters can be conditioned on the linked parts of speech.', 'a kind of word order correlation bias can be effected by conditioning the auxiliary parameters on the relative positions of linked word tokens in their respective texts.', 'just as easily, we can model link types that coincide with entries in an on - line bilingual dictionary separately from those that do not ( cfxxx  #TAUTHOR_TAG.', 'brown et al. 1993 ).', 'when the auxiliary parameters are conditioned on different link classes, their optimization is carried out separately for each class : b ( links ( u, v ) [ cooc ( u, v ), a + ) scorec ( u, vlz = class ( u, v ) ) = log b ( links ( u, v ) [ cooc ( u, v ), a z ) "" ( 37 ) section 6. 1. 1 describes the link classes used in the experiments below']",5
"['to other models  #TAUTHOR_TAG b ).', 'objective and more accurate tests can be']","['to other models  #TAUTHOR_TAG b ).', 'objective and more accurate tests can be']","['to other models  #TAUTHOR_TAG b ).', 'objective and more accurate tests can be carried out using a "" gold standard. ""', 'i hired bilingual annotators to link roughly 16']","[""now, translation models have been evaluated either subjectively ( e. g. white and o'connell 1993 ) or using relative metrics, such as perplexity with respect to other models  #TAUTHOR_TAG b )."", 'objective and more accurate tests can be carried out using a "" gold standard. ""', 'i hired bilingual annotators to link roughly 16, 000 corresponding words between on - line versions of the bible in french and english.', 'this bitext was selected to facilitate widespread use and standardization ( see melamed [ 1998c ] for details ).', 'the entire bible bitext comprised 29, 614 verse pairs, of which 250 verse pairs were hand - linked using a specially developed annotation tool.', 'the annotation style guide ( melamed 1998b ) was based on the intuitions of the annotators, so it was not biased towards any particular translation model.', 'the annotation was replicated five times by seven different annotators']",1
"['informal experiments described elsewhere ( melamed 1995 ), i found that the g2 statistic suggested by  #TAUTHOR_TAG slightly outperforms 02.', 'let the cells of the contingency']","['informal experiments described elsewhere ( melamed 1995 ), i found that the g2 statistic suggested by  #TAUTHOR_TAG slightly outperforms 02.', 'let the cells of the contingency']","['informal experiments described elsewhere ( melamed 1995 ), i found that the g2 statistic suggested by  #TAUTHOR_TAG slightly outperforms 02.', 'let the cells of the contingency table be named as follows']","['informal experiments described elsewhere ( melamed 1995 ), i found that the g2 statistic suggested by  #TAUTHOR_TAG slightly outperforms 02.', 'let the cells of the contingency table be named as follows']",0
"['. 1990 ; shieber 1994 ;  #TAUTHOR_TAG, lexical conceptual structures (']","['al. 1990 ; shieber 1994 ;  #TAUTHOR_TAG, lexical conceptual structures ( dorr 1992 ) and wordnet synsets ( fellbaum 1998 ; vossen 1998 ).', 'of course, for a representation to be used, a method must exist for estimating its distribution in data']","['. 1990 ; shieber 1994 ;  #TAUTHOR_TAG, lexical conceptual structures (']","['above equation holds regardless of how we represent concepts.', 'there are many plausible representations, such as pairs of trees from synchronous tree adjoining grammars ( abeille et al. 1990 ; shieber 1994 ;  #TAUTHOR_TAG, lexical conceptual structures ( dorr 1992 ) and wordnet synsets ( fellbaum 1998 ; vossen 1998 ).', 'of course, for a representation to be used, a method must exist for estimating its distribution in data']",0
"['.,  #TAUTHOR_TAG, a¢ multilingual']","[',  #TAUTHOR_TAG, a¢ multilingual']","['.,  #TAUTHOR_TAG, a¢ multil']","['##¢ cross - language information retrieval ( e. g.,  #TAUTHOR_TAG, a¢ multilingual document filtering ( e. g., oard 1997 ), a¢ computer - assisted language learning ( e. g., nerbonne et al. 1997 ), a¢ certain machine - assisted translation tools ( e. g., macklovitch 1994 ; melamed 1996a ), a¢ concordancing for bilingual lexicography ( e. g., catizone, russell, and warwick 1989 ; gale and church 1991 )']",0
"[""' s book  #TAUTHOR_TAG."", 'the common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints (']","[""' s book  #TAUTHOR_TAG."", 'the common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints ( such as unambiguous reference ) or']","[""' s book  #TAUTHOR_TAG."", 'the common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints (']","[""other such cases are described in danlos's book  #TAUTHOR_TAG."", 'the common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints ( such as unambiguous reference ) or performing linguistic optimizations ( such as using pronouns instead of longer referring expressions whenever possible ) in cases where the constraints or optimizations depend on decisions made in multiple modules.', 'this is largely due to the fact that pipelined systems cannot perform general search over a decision space that includes decisions made in more than one module']",0
['systems  #TAUTHOR_TAG'],"['pipelines have many engineering advantages, and in practice the sort of problems pointed out by danlos and other pipeline critics do not seem to be a major problem in current applied nlg systems  #TAUTHOR_TAG']",['and other pipeline critics do not seem to be a major problem in current applied nlg systems  #TAUTHOR_TAG'],"['these arguments, most applied nlg systems use a pipelined architecture ; indeed, a pipeline was used in every one of the systems surveyed by  #AUTHOR_TAG and  #AUTHOR_TAG.', 'this may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by danlos and other pipeline critics do not seem to be a major problem in current applied nlg systems  #TAUTHOR_TAG']",0
"['of the systems surveyed by  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'this may be']","['of the systems surveyed by  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'this may be']","['of the systems surveyed by  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'this may be']","['these arguments, most applied nlg systems use a pipelined architecture ; indeed, a pipeline was used in every one of the systems surveyed by  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'this may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by danlos and other pipeline critics do not seem to be a major problem in current applied nlg systems ( mittal et al. 1998 )']",0
"['upper model  #TAUTHOR_TAG.', 'although the idea of a two - level representation accommodating language - neutral and language - specific requirements is not new ( see']","['upper model  #TAUTHOR_TAG.', 'although the idea of a two - level representation accommodating language - neutral and language - specific requirements is not new ( see']","[', the upper model  #TAUTHOR_TAG.', 'although the idea of a two - level representation accommodating language - neutral and language - specific requirements is not new ( see']","['first i found chapters 4 through 8 slightly overwhelming, as they introduce several levels of representation, each with its own terminology and acronyms.', 'however, at a second, more - careful, reading, everything falls into place.', 'the resulting approach has at its center a lexicon that partly implements current theories of lexical semantics such as  #AUTHOR_TAG and  #AUTHOR_TAG.', 'the lexicon is used to mediate and map between a language - independent domain model and a language - dependent ontology widely used in nlg, the upper model  #TAUTHOR_TAG.', 'although the idea of a two - level representation accommodating language - neutral and language - specific requirements is not new ( see for example nirenburg and levin [ 1992 ], dorr and voss [ 1993 ], and di eugenio [ 1998 ] ), stede is among the few who make effective use of those two levels in a complex system']",0
"[""in  #TAUTHOR_TAG.''"", 'these']","[""in  #TAUTHOR_TAG.''"", 'these']","[""in  #TAUTHOR_TAG.''"", 'these works inspired koskenniemi']","[""after the publication of the sound pattern of english ( chomsky and halle 1968 ), kornai points out, ` `  #AUTHOR_TAG demonstrated that the context - sensitive machinery of spe... [ could ] be replaced by a much simpler one, based on finite - state transducers ( fsts ) ; the same conclusion was reached independently by kaplan and kay, whose work remained an underground classic until it was finally published in  #TAUTHOR_TAG.''"", ""these works inspired koskenniemi's two - level system, and the xerox rule compiler ( dalrymple et al. 1987 )."", 'both are now dominant tools in the fields of computational phonology and morphology, as exemplified by tateno et al. ( chapter 6 ), "" the japanese lexical transducer based on stem - suffix style forms "" and kim and jang ( chapter 7 ), "" acquiring rules for reducing morphological ambiguity from pos tagged corpus in korean. ""', 'the latter includes an algorithm for automatically inferring regular grammar rules for morphological relations directly from part - of - speech tagged corpora']",0
"['. g.,  #TAUTHOR_TAG b ) is of course an empirical question']","['therefore holds out the promise of simplifying grammars, by factoring all complex phenomena into simple surface - level constraints that partially mask one another. 1 whether this is always possible under an appropriate definition of "" simple constraints "" ( e. g.,  #TAUTHOR_TAG b ) is of course an empirical question']","['. g.,  #TAUTHOR_TAG b ) is of course an empirical question']","['therefore holds out the promise of simplifying grammars, by factoring all complex phenomena into simple surface - level constraints that partially mask one another. 1 whether this is always possible under an appropriate definition of "" simple constraints "" ( e. g.,  #TAUTHOR_TAG b ) is of course an empirical question']",0
"['cases rankings may work well enough.', ' #TAUTHOR_TAG report excellent part - of - speech tagging results using a handcrafted approach that is close to ot. 3 more speculatively, imagine an ot grammar']","['cases rankings may work well enough.', ' #TAUTHOR_TAG report excellent part - of - speech tagging results using a handcrafted approach that is close to ot. 3 more speculatively, imagine an ot grammar']","[', weights are an annoyance when writing grammars by hand.', 'in some cases rankings may work well enough.', ' #TAUTHOR_TAG report excellent part - of - speech tagging results using a handcrafted approach that is close to ot. 3 more speculatively, imagine an ot grammar']","[', weights are an annoyance when writing grammars by hand.', 'in some cases rankings may work well enough.', ' #TAUTHOR_TAG report excellent part - of - speech tagging results using a handcrafted approach that is close to ot. 3 more speculatively, imagine an ot grammar for stylistic revision of parsed sentences.', ""the tension between preserving the original author's text ( faithfulness to the underlying form ) and making it readable in various ways ( well - formedness ) is right up ot's alley."", 'the same applies to document layout : i have often wished i could write ot - style tex macros ~ third, even in statistical corpus - based nlp, estimating a full gibbs distribution is not always feasible.', 'even if strict ranking is not quite accurate, sparse data or the complexity of parameter estimation may make it easier to learn a good ot grammar than a good arbitrary gibbs model.', 'a well - known example is  #AUTHOR_TAG work on word sense disambiguation using decision lists ( a kind of ot grammar ).', 'although decision lists are not very powerful because of their simple output space, they have the characteristic ot property that each generalization partially masks lower - ranked generalizations']",0
"[""from ~ ', based on criteria of well - formedness ( transition probabilities ) and faithfulness to the input ( emission probabilities )."", 'but typical ot grammars offer much richer finite - state models of left context  #TAUTHOR_TAG a ) than provided by']","[""from ~ ', based on criteria of well - formedness ( transition probabilities ) and faithfulness to the input ( emission probabilities )."", 'but typical ot grammars offer much richer finite - state models of left context  #TAUTHOR_TAG a ) than provided by']","[""from ~ ', based on criteria of well - formedness ( transition probabilities ) and faithfulness to the input ( emission probabilities )."", 'but typical ot grammars offer much richer finite - state models of left context  #TAUTHOR_TAG a ) than provided by the traditional hmm finite - state topologies.', 'now, among methods that use a gibbs distribution to choose among linguistic forms, ot generation is special in that the distribution ranks the features strictly, rather than weighting them in a gentler way that allows tradeoffs.', 'when']","['example, consider the relevance to hidden markov models ( hmms ), another restricted class of gibbs distributions used in speech recognition or part - of - speech tagging.', ""just like ot grammars, hmm viterbi decoders are functions that pick the optimal output from ~ ', based on criteria of well - formedness ( transition probabilities ) and faithfulness to the input ( emission probabilities )."", 'but typical ot grammars offer much richer finite - state models of left context  #TAUTHOR_TAG a ) than provided by the traditional hmm finite - state topologies.', 'now, among methods that use a gibbs distribution to choose among linguistic forms, ot generation is special in that the distribution ranks the features strictly, rather than weighting them in a gentler way that allows tradeoffs.', 'when is this appropriate?', 'it seems to me that there are three possible uses']",0
"['effectively discarded as well ( smadja 1993 ).', ' #TAUTHOR_TAG use mutual information to identify collocations, a method they claim is reasonably effective for words with a frequency of not less than five']","['effectively discarded as well ( smadja 1993 ).', ' #TAUTHOR_TAG use mutual information to identify collocations, a method they claim is reasonably effective for words with a frequency of not less than five']","['', ""in smadja's collocation algorithm xtract, the lowest - frequency words are effectively discarded as well ( smadja 1993 )."", ' #TAUTHOR_TAG use mutual information to identify collocations, a method they claim is reasonably effective for words with a frequency of not less than five']","['original question concerned the extent to which recall and precision are influenced by the size of the window.', 'it turns out, however, that a preliminary question needs to be answered first, namely, how to gauge the significance of the large effect of the lowest - frequency words on recall, precision, and the number of words extracted as potentially relevant terms.', 'it is common practice in information retrieval to discard the lowest - frequency words a priori as nonsignificant ( rijsbergen 1979 ).', ""in smadja's collocation algorithm xtract, the lowest - frequency words are effectively discarded as well ( smadja 1993 )."", ' #TAUTHOR_TAG use mutual information to identify collocations, a method they claim is reasonably effective for words with a frequency of not less than five']",0
['with a technique commonly used in corpus linguistics based on the mutual information ( mi ) measure  #TAUTHOR_TAG'],['with a technique commonly used in corpus linguistics based on the mutual information ( mi ) measure  #TAUTHOR_TAG'],['with a technique commonly used in corpus linguistics based on the mutual information ( mi ) measure  #TAUTHOR_TAG'],"['', 'and 79 / 170 = 46. 7 %,', ""respectively, using fisher's exact test."", ""note that this technique is optimal for the extraction of the lowest - frequency words, leading to identical performance for g 2 and fisher's exact test for these words."", ""for the higherfrequency words, fisher's exact test leads to a slightly better recall with the same precision scores ( 0. 31 for both tests )."", ""while we have observed reasonable results with both g2 and fisher's exact test, we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( mi ) measure  #TAUTHOR_TAG""]",0
"['not frequently used ) in the real corpus.', 'for example,  #TAUTHOR_TAG proves that chinese numerals']","['not frequently used ) in the real corpus.', 'for example,  #TAUTHOR_TAG proves that chinese numerals']","['not frequently used ) in the real corpus.', 'for example,  #TAUTHOR_TAG proves that chinese numerals']",[' #TAUTHOR_TAG'],0
"['purposes  #TAUTHOR_TAG.', 'this']","['purposes  #TAUTHOR_TAG.', 'this']","['purposes  #TAUTHOR_TAG.', 'this parser']","['particular analysis - level style markers can be calculated only when this specific computational tool is utilized.', 'however, the scbd is a general - purpose tool and was not designed for providing stylistic information exclusively.', 'thus, any nlp tool ( e. g., part - of - speech taggers, parsers, etc. ) can provide similar measures.', 'the appropriate analysis - level style markers have to be defined according to the methodology used by the tool in order to analyze the text.', 'for example, some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes  #TAUTHOR_TAG.', 'this parser produces trees to represent the structure of the sentences that compose the text.', 'however, it is set to "" skip "" or surrender attempts to parse clauses after reaching a time - out threshold.', 'when the parser skips, it notes that in the parse tree.', 'the measures proposed by  #AUTHOR_TAG as indicators of clausal complexity are the average parse tree depth and the number of parser skips per sentence, which in essence are analysis - level style markers']",0
"['that group centroid.', 'discriminant analysis has been employed by researchers in automatic text genre detection  #TAUTHOR_TAG b ; karlgren and cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables']","['that group centroid.', 'discriminant analysis has been employed by researchers in automatic text genre detection  #TAUTHOR_TAG b ; karlgren and cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables']","['that group centroid.', 'discriminant analysis has been employed by researchers in automatic text genre detection  #TAUTHOR_TAG b ; karlgren and cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables']","['cx is the covariance matrix of x.', 'using this classification method we can also derive the probability that a case belongs to a particular group ( i. e., posterior probabilities ), which is roughly proportional to the mahalanobis distance from that group centroid.', 'discriminant analysis has been employed by researchers in automatic text genre detection  #TAUTHOR_TAG b ; karlgren and cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables']",0
['of  #TAUTHOR_TAG on word sense'],"['of  #TAUTHOR_TAG on word sense disambiguation.', 'he classified the senses of a']","['algorithm we implemented is inspired by the work of  #TAUTHOR_TAG on word sense disambiguation.', 'he classified the senses of a word on the basis of other words that the given word co - occurs with.', '']","['algorithm we implemented is inspired by the work of  #TAUTHOR_TAG on word sense disambiguation.', 'he classified the senses of a word on the basis of other words that the given word co - occurs with.', '']",4
"['##net.', 'the changes made were inspired by those described in  #TAUTHOR_TAG, page 75 ).', 'to lemma']","['to maximize the matching between our corpus and wordnet.', 'the changes made were inspired by those described in  #TAUTHOR_TAG, page 75 ).', 'to']","['##net.', 'the changes made were inspired by those described in  #TAUTHOR_TAG, page 75 ).', 'to lemma']","['automatic annotation of nouns and verbs in the corpus has been done by matching them with the wordnet database files.', 'before doing the annotation, though, some preprocessing of the data was required to maximize the matching between our corpus and wordnet.', 'the changes made were inspired by those described in  #TAUTHOR_TAG, page 75 ).', 'to lemmatize the words we used morpha, a lemmatizer developed by john a. carroll and freely available at the address : http : / / www. informatics. susx. ac. uk. / research / nlp / carroll / morph. html.', 'upon simple observation, it showed a better performance than the frequently used porter stemmer for this task']",4
"[' #TAUTHOR_TAG a ), while being more than an']","[' #TAUTHOR_TAG a ), while being more than an']","[' #TAUTHOR_TAG a ), while being more than an order']","['section 2 we describe our general framework of the generic beam - search algorithm and the generalized perceptron.', 'then in the subsequent sections we describe each task in turn, based on conference papers including  #AUTHOR_TAG, 2008a, 2008b, presented in our single coherent framework.', 'we give an updated set of results, plus a number of additional experiments which probe further into the advantages and disadvantages of our framework.', 'for the segmentation task, we also compare our beam - search framework with alternative decoding algorithms including an exact dynamic - programming method, showing that the beam - search method is significantly faster with comparable accuracy.', 'for the joint segmentation and pos - tagging task, we present a novel solution using the framework in this article, and show that it gives comparable accuracies to our previous work  #TAUTHOR_TAG a ), while being more than an order of magnitude faster']",1
"[' #TAUTHOR_TAG, we assessed the importance of']","[' #TAUTHOR_TAG, we assessed the importance of']","[' #TAUTHOR_TAG, we assessed the importance of various implicit argument feature groups by conducting feature ablation tests.', 'in each test, the discriminative model was retrained']","[' #TAUTHOR_TAG, we assessed the importance of various implicit argument feature groups by conducting feature ablation tests.', 'in each test, the discriminative model was retrained and reevaluated without a particular group of features.', 'we summarize the findings of this study in this section']",2
"['reported  #TAUTHOR_TAG, in which fixed partitions were used for training, development, and testing']","['reported  #TAUTHOR_TAG, in which fixed partitions were used for training, development, and testing']","['reported  #TAUTHOR_TAG, in which fixed partitions were used for training, development, and testing']","['order to remove any confounding factors caused by specific documents, we first randomized the annotated predicate instances.', 'following this, we split the predicate instances evenly into ten folds and used each fold as testing data for a model trained on the instances outside the fold.', 'this evaluation set - up is an improvement versus the one we previously reported  #TAUTHOR_TAG, in which fixed partitions were used for training, development, and testing']",2
"['resulting decoder.', 'ccgbank  #TAUTHOR_TAG is used to train the model.', 'for']","['resulting decoder.', 'ccgbank  #TAUTHOR_TAG is used to train the model.', 'for']","['of the resulting decoder.', 'ccgbank  #TAUTHOR_TAG is used to train the model.', 'for each training sentence,']","['now give an overview of the training algorithm, which is crucial to both the speed and accuracy of the resulting decoder.', 'ccgbank  #TAUTHOR_TAG is used to train the model.', 'for each training sentence, the corresponding ccgbank derivation together with all its sub - derivations are treated as gold - standard hypotheses.', 'all other hypotheses that can be constructed from the same bag of words are non - gold hypotheses.', 'from the generation perspective this assumption is too strong, because sentences can have multiple orderings ( with multiple derivations ) that are both gram - matical and fluent.', 'nevertheless, it is the most feasible choice given the training data available']",5
"['way as for the parsing problem.', 'in our previous papers  #TAUTHOR_TAG ; zhang, blackwood, and clark 2012 ), we applied a set of beams to']","['way as for the parsing problem.', 'in our previous papers  #TAUTHOR_TAG ; zhang, blackwood, and clark 2012 ), we applied a set of beams to']","['we are tackling.', 'second, although we use the term chart, the structure for accepted hypotheses is not a dynamic programming chart in the same way as for the parsing problem.', 'in our previous papers  #TAUTHOR_TAG ; zhang, blackwood, and clark 2012 ), we applied a set of beams to']","['', 'when a hypothesis from the agenda is expanded, it is combined with all accepted hypotheses in all possible ways to produce new hypotheses.', 'the data structure for accepted hypotheses is similar to that used for best - first parsing ( caraballo and charniak 1998 ), and we adopt the term chart for this structure.', 'however, note there are important differences to the parsing problem.', 'first, the parsing problem has a fixed word order and is considerably simpler than the word ordering problem we are tackling.', 'second, although we use the term chart, the structure for accepted hypotheses is not a dynamic programming chart in the same way as for the parsing problem.', 'in our previous papers  #TAUTHOR_TAG ; zhang, blackwood, and clark 2012 ), we applied a set of beams to this structure, which makes it similar to the data structure used for phrase - based mt decoding ( koehn 2010 ).', 'however, we will show later that this structure is unnecessary when the model has more discriminative power, and a conceptually simpler single beam can be used.', 'we will also investigate the possibility of applying dynamic - programming - style pruning to the chart']",1
"['by mert optimization  #TAUTHOR_TAG towards post - edits under the ter target metric.', 'note that the goal of our experi - ments is not to']","['by mert optimization  #TAUTHOR_TAG towards post - edits under the ter target metric.', 'note that the goal of our experi - ments is not to']","['w * to define the utility model.', 'this was done by mert optimization  #TAUTHOR_TAG towards post - edits under the ter target metric.', 'note that the goal of our experi - ments is not to']","['experiment is to confirm our theoretical analysis by showing convergence in regret for learning from weak and strong feedback.', 'we select feedback of varying grade by directly inspecting the optimal w *, thus this feedback is idealized.', 'however, the experiment also has a realistic background since we show that α - informative feedback corresponds to improvements under standard evaluation metrics such as lowercased and tokenized ter, and that learning from weak and strong feedback leads to convergence in ter on test data.', 'for this experiment, the post - edit data from the lig corpus were randomly split into 3 subsets : pe - train ( 6, 881 sentences ), pe - dev, and pe - test ( 2, 000 sentences each ).', 'pe - train was used for our online learning experiments.', ""pe - test was held out for testing the algorithms'progress on unseen data."", 'pe - dev was used to obtain w * to define the utility model.', 'this was done by mert optimization  #TAUTHOR_TAG towards post - edits under the ter target metric.', 'note that the goal of our experi - ments is not to improve smt performance over any algorithm that has access to full information to compute w *.', 'rather, we want to show that learning from weak feedback leads to convergence in regret with respect to the optimal model, albeit at a slower rate than learning from strong feedback.', 'the feedback data in this experiment were generated by searching the n - best list for translations that are α - informative at α ∈ { 0. 1, 0. 5, 1. 0 } ( with possible non - zero slack ).', 'this is achieved by scanning the n - best list output for every input x t and returning the firsty t = y t that satisfies equation ( 2 ). 5 this setting can be thought of as an idealized scenario where a user picks translations from the n - best list that are considered improvements under the optimal w *']",5
['test set  #TAUTHOR_TAG'],['test set  #TAUTHOR_TAG'],"['set  #TAUTHOR_TAG.', 'the difference to']","['for online - to - batch conversion.', 'in practice, perceptron - type algorithms are often applied in a batch learning scenario, i. e., the algorithm is applied for k epochs to a training sample of size t and then used for prediction on an unseen test set  #TAUTHOR_TAG.', '']",1
"['decoder  #TAUTHOR_TAG with dense features.', 'we replicated a similar moses system using the same monolingual and parallel data : a 5 - gram language model was estimated with the kenlm toolkit  #AUTHOR_TAG on news. en', 'data ( 48']","['system  #AUTHOR_TAG, based on the moses phrase - based decoder  #TAUTHOR_TAG with dense features.', 'we replicated a similar moses system using the same monolingual and parallel data : a 5 - gram language model was estimated with the kenlm toolkit  #AUTHOR_TAG on news. en', 'data ( 48. 65m', 'sentences, 1. 13b tokens ), pre - processed with the tools from the cdec toolkit  #AUTHOR_TAG.', 'perceptron cycling theorem  #AUTHOR_TAG should suffice to show a similar bound.', 'parallel data ( europarl + news - comm, 1. 64m sentences ) were similarly pre - processed and aligned with fast align  #AUTHOR_TAG.', 'in all experiments, training is started with the moses default weights.', 'the size of the n - best list, where used, was set to 1, 000.', 'irrespective of the use of re - scaling in perceptron training, a constant']","[' #AUTHOR_TAG, based on the moses phrase - based decoder  #TAUTHOR_TAG with dense features.', 'we replicated a similar moses system using the same monolingual and parallel data : a 5 - gram language model was estimated with the kenlm toolkit  #AUTHOR_TAG on news. en', 'data ( 48']","['used the lig corpus 3 which consists of 10, 881 tuples of french - english post - edits  #AUTHOR_TAG.', 'the corpus is a subset of the newscommentary dataset provided at wmt 4 and contains input french sentences, mt outputs, postedited outputs and english references.', 'to prepare smt outputs for post - editing, the creators of the corpus used their own wmt10 system  #AUTHOR_TAG, based on the moses phrase - based decoder  #TAUTHOR_TAG with dense features.', 'we replicated a similar moses system using the same monolingual and parallel data : a 5 - gram language model was estimated with the kenlm toolkit  #AUTHOR_TAG on news. en', 'data ( 48. 65m', 'sentences, 1. 13b tokens ), pre - processed with the tools from the cdec toolkit  #AUTHOR_TAG.', 'perceptron cycling theorem  #AUTHOR_TAG should suffice to show a similar bound.', 'parallel data ( europarl + news - comm, 1. 64m sentences ) were similarly pre - processed and aligned with fast align  #AUTHOR_TAG.', 'in all experiments, training is started with the moses default weights.', 'the size of the n - best list, where used, was set to 1, 000.', 'irrespective of the use of re - scaling in perceptron training, a constant learning rate of −5 was used for learning from simulated feedback, and 10 −4 for learning from surrogate translations']",5
"['contrast, a single statistical model allows one to maintain a single table  #TAUTHOR_TAG']","['contrast, a single statistical model allows one to maintain a single table  #TAUTHOR_TAG']","['contrast, a single statistical model allows one to maintain a single table  #TAUTHOR_TAG']","['contrast, a single statistical model allows one to maintain a single table  #TAUTHOR_TAG']",0
['##n uses standard chart generation techniques  #TAUTHOR_TAG in its base generator to efficiently produce generation candidates'],['##n uses standard chart generation techniques  #TAUTHOR_TAG in its base generator to efficiently produce generation candidates'],['##n uses standard chart generation techniques  #TAUTHOR_TAG in its base generator to efficiently produce generation candidates'],['##n uses standard chart generation techniques  #TAUTHOR_TAG in its base generator to efficiently produce generation candidates'],0
"[""of jordan's work on the coconut domain  #TAUTHOR_TAG""]","[""of jordan's work on the coconut domain  #TAUTHOR_TAG""]","[""that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen, unless we know in advance all of the properties that can be attributed to a given object, as in the case of jordan's work on the coconut domain  #TAUTHOR_TAG""]","[""that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen, unless we know in advance all of the properties that can be attributed to a given object, as in the case of jordan's work on the coconut domain  #TAUTHOR_TAG""]",0
['of  #AUTHOR_TAG )  #TAUTHOR_TAG a )'],['of  #AUTHOR_TAG )  #TAUTHOR_TAG a )'],['of  #AUTHOR_TAG )  #TAUTHOR_TAG a )'],['has been argued that generating such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of  #AUTHOR_TAG )  #TAUTHOR_TAG a )'],0
"['wordnet, annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2some descriptions of int modifiers can be found in  #TAUTHOR_TAG b )']","['wordnet, annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2some descriptions of int modifiers can be found in  #TAUTHOR_TAG b )']","['wordnet, annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2some descriptions of int modifiers can be found in  #TAUTHOR_TAG b )']","['wordnet, annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2some descriptions of int modifiers can be found in  #TAUTHOR_TAG b )']",0
"[""and expressing the speaker's emotional attitude towards the referent  #TAUTHOR_TAG""]","["", noun phrases ( np ) can also serve communicative goals such as providing new information about the referent and expressing the speaker's emotional attitude towards the referent  #TAUTHOR_TAG""]","[""to a referring function, noun phrases ( np ) can also serve communicative goals such as providing new information about the referent and expressing the speaker's emotional attitude towards the referent  #TAUTHOR_TAG""]","[""addition to a referring function, noun phrases ( np ) can also serve communicative goals such as providing new information about the referent and expressing the speaker's emotional attitude towards the referent  #TAUTHOR_TAG""]",0
"['adjective - noun combinations discussed in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'table 2 shows the five most likely interpretations']","['adjective - noun combinations discussed in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'table 2 shows the five most likely interpretations']","['eight adjective - noun combinations discussed in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'table 2 shows the five most likely interpretations']","['what follows we explain the properties of the model by applying it to a small number of adjective - noun combinations taken from the lexical semantics literature.', 'table 1 gives the interpretations of eight adjective - noun combinations discussed in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'table 2 shows the five most likely interpretations for these combinations as derived by the model discussed in the previous sections ( v is the most likely interpretation, v 2 is the second most likely interpretation, etc. )']",5
"['context, viz., the noun or noun class they modify ( see  #TAUTHOR_TAG and the references therein )']","['context, viz., the noun or noun class they modify ( see  #TAUTHOR_TAG and the references therein )']","['context, viz., the noun or noun class they modify ( see  #TAUTHOR_TAG and the references therein )']","['recent work in lexical semantics has been concerned with accounting for regular polysemy, i. e., the regular and predictable sense alternations certain classes of words are subject to.', 'adjectives, more than other categories, are a striking example of regular polysemy since they are able to take on different meanings depending on their context, viz., the noun or noun class they modify ( see  #TAUTHOR_TAG and the references therein )']",0
"['.', 'we chose the adjectives as follows : we first compiled a list of all the polysemous adjectives mentioned in the lexical semantics literature  #TAUTHOR_TAG.', 'from these']","['chose nine adjectives according to a set of minimal criteria and paired each adjective with 10 nouns randomly selected from the bnc.', 'we chose the adjectives as follows : we first compiled a list of all the polysemous adjectives mentioned in the lexical semantics literature  #TAUTHOR_TAG.', 'from these']","['chose nine adjectives according to a set of minimal criteria and paired each adjective with 10 nouns randomly selected from the bnc.', 'we chose the adjectives as follows : we first compiled a list of all the polysemous adjectives mentioned in the lexical semantics literature  #TAUTHOR_TAG.', 'from these we randomly sampled nine adjectives ( difficult, easy, fast, good, hard, right, safe, slow, wrong ).', 'these adjectives had to be unambiguous with']","['chose nine adjectives according to a set of minimal criteria and paired each adjective with 10 nouns randomly selected from the bnc.', 'we chose the adjectives as follows : we first compiled a list of all the polysemous adjectives mentioned in the lexical semantics literature  #TAUTHOR_TAG.', 'from these we randomly sampled nine adjectives ( difficult, easy, fast, good, hard, right, safe, slow, wrong ).', 'these adjectives had to be unambiguous with respect to their part - of - speech : each adjective was unambiguously tagged as "" adjective "" 98. 6 % of the time, measured as the number of different part - of - speech tags assigned to the word in the bnc.', 'we identified adjective - noun pairs using gsearch  #AUTHOR_TAG, a chart parser which detects syntactic patterns in a tagged corpus by exploiting a userspecified context free grammar and a syntactic query.', 'gsearch was run on a lemmatized version of the bnc so as to compile a comprehensive corpus count of all nouns occurring in a modifier - head relationship with each of the nine adjectives.', 'from the syntactic analysis provided by we used the model outlined in section 2 to derive meanings for the 90 adjective - noun combinations.', 'we employed no threshold on the frequencies f ( a, v ) and f ( rel, v, n ).', 'in order to obtain the frequency f ( a, v ) the adjective was mapped to its corresponding adverb.', 'in particular, good was mapped to good and well, fast to fast, easy to easily, hard to hard, right to rightly and right, safe to safely and safe, slow to slowly and slow and wrong to wrongly and wrong.', 'the adverbial function of the adjective difficult is expressed only periphrastically ( i. e., in a difficult manner, with difficulty ).', 'as a result, the frequency f ( difficult, v ) was estimated only on the basis of infinitival constructions ( see ( 17 ) ).', 'we estimated the probability p ( a, n, v, rel ) for each adjective - noun pair by varying both the terms v and rel']",5
"['eat.', ' #TAUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns']","['eat.', ' #TAUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns']","['eat.', ' #TAUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify.', 'pustejovsky treats nouns as having a qualia structure as part of their lexical entries, which among other things, specifies possible events associated with the entity.', 'for example, the telic ( purpose ) role of the qualia structure']","['', ' #TAUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify.', 'pustejovsky treats nouns as having a qualia structure as part of their lexical entries, which among other things, specifies possible events associated with the entity.', 'for example, the telic ( purpose ) role of the qualia structure for problem has a value equivalent to solve.', 'when the adjective easy is combined with problem, it predicates over the telic role of problem and consequently the adjective - noun combination receives the interpretation a problem that is easy to solve.', ' #AUTHOR_TAG does not give an exhaustive list of the telic roles a given noun may have.', 'furthermore, in cases where more than one interpretations are provided ( see  #AUTHOR_TAG ), no information is given with respect to the likelihood of these interpretations.', 'outof context, the number of interpretations for fast scientist is virtually unlimited, yet some interpretations are more likely than others : fast scientist is more likely to be a scientist who performs experiments quickly or who publishes quickly than a scientist who draws or drinks quickly']",0
"[' #TAUTHOR_TAG where multiple naive bayesian classifiers, each based on co - - occurrence features from varying sized windows of context, is shown to perform']","[' #TAUTHOR_TAG where multiple naive bayesian classifiers, each based on co - - occurrence features from varying sized windows of context, is shown to perform']","['have presented an ensemble approach to word sense disambiguation  #TAUTHOR_TAG where multiple naive bayesian classifiers, each based on co - - occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line']","['have presented an ensemble approach to word sense disambiguation  #TAUTHOR_TAG where multiple naive bayesian classifiers, each based on co - - occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG for a discussion'],0
['definitions of predicates may be found in  #TAUTHOR_TAG'],['definitions of predicates may be found in  #TAUTHOR_TAG'],['definitions of predicates may be found in  #TAUTHOR_TAG'],['definitions of predicates may be found in  #TAUTHOR_TAG'],0
['generalisation do not show an over - generalisation compared with those given in  #TAUTHOR_TAG'],['generalisation do not show an over - generalisation compared with those given in  #TAUTHOR_TAG'],['generalisation do not show an over - generalisation compared with those given in  #TAUTHOR_TAG'],"['appeared to solve the problem, and the results presented later for the average degree of generalisation do not show an over - generalisation compared with those given in  #TAUTHOR_TAG']",1
['task we used to compare different generalisation techniques is similar to that used by  #TAUTHOR_TAG and  #AUTHOR_TAG'],['task we used to compare different generalisation techniques is similar to that used by  #TAUTHOR_TAG and  #AUTHOR_TAG'],['task we used to compare different generalisation techniques is similar to that used by  #TAUTHOR_TAG and  #AUTHOR_TAG'],['task we used to compare different generalisation techniques is similar to that used by  #TAUTHOR_TAG and  #AUTHOR_TAG'],1
"['tasks, low counts are important  #TAUTHOR_TAG']","['tasks, low counts are important  #TAUTHOR_TAG']","[', arbitrary, and there is evidence to suggest that, for some tasks, low counts are important  #TAUTHOR_TAG']","['problem with this approach is that any threshold is, to some extent, arbitrary, and there is evidence to suggest that, for some tasks, low counts are important  #TAUTHOR_TAG']",4
"[',  #TAUTHOR_TAG claims that the log - likelihood']","[',  #TAUTHOR_TAG claims that the log - likelihood chisquared statistic ( g2 ) is more appropriate for corpus - based nlp']","[',  #TAUTHOR_TAG claims that the log - likelihood']","[',  #TAUTHOR_TAG claims that the log - likelihood chisquared statistic ( g2 ) is more appropriate for corpus - based nlp']",4
['task we used to compare different generalisation techniques is similar to that used by  #AUTHOR_TAG and  #TAUTHOR_TAG'],['task we used to compare different generalisation techniques is similar to that used by  #AUTHOR_TAG and  #TAUTHOR_TAG'],['task we used to compare different generalisation techniques is similar to that used by  #AUTHOR_TAG and  #TAUTHOR_TAG'],['task we used to compare different generalisation techniques is similar to that used by  #AUTHOR_TAG and  #TAUTHOR_TAG'],1
"['as g2, throwing doubt on the claim by  #TAUTHOR_TAG that the g2 statistic is better suited for use in corpus - based nlp']","['as g2, throwing doubt on the claim by  #TAUTHOR_TAG that the g2 statistic is better suited for use in corpus - based nlp']","['as g2, throwing doubt on the claim by  #TAUTHOR_TAG that the g2 statistic is better suited for use in corpus - based nlp']","['x2 statistic is performing at least as well as g2, throwing doubt on the claim by  #TAUTHOR_TAG that the g2 statistic is better suited for use in corpus - based nlp']",1
"[""' s minimalist program  #TAUTHOR_TAG""]","[""' s minimalist program  #TAUTHOR_TAG""]","[""' s minimalist program  #TAUTHOR_TAG""]","['theory of verbal argument structure can be imple - mented in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program  #TAUTHOR_TAG""]",1
[':  #TAUTHOR_TAG propose that argument structure'],['of theoretical linguists :  #TAUTHOR_TAG propose that argument structure'],['of theoretical linguists :  #TAUTHOR_TAG propose that argument structure'],"['observations and this line of reasoning has not escaped the attention of theoretical linguists :  #TAUTHOR_TAG propose that argument structure is, in fact, encoded syntactically.', 'they describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.', 'this present framework builds on the work of hale and keyser, but in addition to advancing a more refined theory of verbal argument structure, i also describe a computational implementation']",0
"['functional head, voice  #TAUTHOR_TAG, whose specifi']","['functional head, voice  #TAUTHOR_TAG, whose specifier is']","['light verb v do licenses an atelic non - inchoative event, and is compatible with verbal roots expressing activity.', 'it projects a functional head, voice  #TAUTHOR_TAG, whose specifi']","['light verb v do licenses an atelic non - inchoative event, and is compatible with verbal roots expressing activity.', 'it projects a functional head, voice  #TAUTHOR_TAG, whose specifier is the external argument.', 'lexical entries in the system are minimally specified, each consisting of a phonetic form, a list of relevant features, and semantics in the form of a λ expression']",0
"[""' s minimalist program  #TAUTHOR_TAG""]","[""' s minimalist program  #TAUTHOR_TAG""]","[""' s minimalist program  #TAUTHOR_TAG""]","['theory of verbal argument structure can be imple - mented in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program  #TAUTHOR_TAG""]",1
"['this paper, i present a computational implementation of distributed morphology  #TAUTHOR_TAG, a non - lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation.', 'this']","['this paper, i present a computational implementation of distributed morphology  #TAUTHOR_TAG, a non - lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation.', 'this']","['this paper, i present a computational implementation of distributed morphology  #TAUTHOR_TAG, a non - lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation.', 'this framework leads to finer - grained semantics capable of better capturing linguistic generalizations']","['this paper, i present a computational implementation of distributed morphology  #TAUTHOR_TAG, a non - lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation.', 'this framework leads to finer - grained semantics capable of better capturing linguistic generalizations']",5
['propbank  #TAUTHOR_TAG'],['propbank  #TAUTHOR_TAG'],['propbank  #TAUTHOR_TAG'],"['common lexical semantic representation in the computational linguistics literature is a frame - based model where syntactic arguments are associated with various semantic roles ( essentially frame slots ).', 'verbs are viewed as simple predicates over their arguments.', ""this approach has its roots in fillmore's  #AUTHOR_TAG, and serves as the foundation for two current large - scale semantic annotation projects : framenet  #AUTHOR_TAG and propbank  #TAUTHOR_TAG""]",0
"[' #TAUTHOR_TAG, i present evidence from mandarin chinese that this analysis is on the right track.', 'the']","[' #TAUTHOR_TAG, i present evidence from mandarin chinese that this analysis is on the right track.', 'the']","[' #TAUTHOR_TAG, i present evidence from mandarin chinese that this analysis is on the right track.', 'the rest of this paper, however, will be concerned with the computational implementation of my theoretical framework']","[' #TAUTHOR_TAG, i present evidence from mandarin chinese that this analysis is on the right track.', 'the rest of this paper, however, will be concerned with the computational implementation of my theoretical framework']",2
"['also of semantic content.', 'due to advances in statistical syntactic parsing techniques  #TAUTHOR_TAG, attention has recently shifted']","['also of semantic content.', 'due to advances in statistical syntactic parsing techniques  #TAUTHOR_TAG, attention has recently shifted']","['also of semantic content.', 'due to advances in statistical syntactic parsing techniques  #TAUTHOR_TAG, attention has recently shifted']","['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'due to advances in statistical syntactic parsing techniques  #TAUTHOR_TAG, attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences']",0
"['as causality and inchoativity  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'consider the following']","['as causality and inchoativity  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'consider the following example']","['as causality and inchoativity  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'consider the following example']",0
"['of the argument alternations described by  #AUTHOR_TAG using a  #TAUTHOR_TAG style analysis.', 'i believe that with a suitable lexicon ( either hand cra']","['of the argument alternations described by  #AUTHOR_TAG using a  #TAUTHOR_TAG style analysis.', 'i believe that with a suitable lexicon ( either hand']","['of the argument alternations described by  #AUTHOR_TAG using a  #TAUTHOR_TAG style analysis.', 'i believe that with a suitable lexicon ( either hand cra']","['', 'although the effectiveness and coverage of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'with a minimal set of features and a small number of lexical entries,  #AUTHOR_TAG has successfully modeled many of the argument alternations described by  #AUTHOR_TAG using a  #TAUTHOR_TAG style analysis.', 'i believe that with a suitable lexicon ( either hand crafted or automatically induced ), my framework can be elaborated into a system whose performance is comparable to that of current statistical parsers, but with the added advantage of simultaneously providing a richer lexical semantic representation of the input sentence than flat predicate argument structures based on semantic roles']",0
"['temporally bounded ( telic ) ; achievements are punctual, whereas accomplishments extend over a period of time.', ' #TAUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration, which is often a question of granularity']","['temporally bounded ( telic ) ; achievements are punctual, whereas accomplishments extend over a period of time.', ' #TAUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration, which is often a question of granularity']","['are temporally bounded ( telic ) ; achievements are punctual, whereas accomplishments extend over a period of time.', ' #TAUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration, which is often a question of granularity']","[""know run believe walk accomplishments achievements paint a picture recognize make a chair find under vendler's classification, activities and states both depict situations that are inherently temporally unbounded ( atelic ) ; states denote static situations, whereas activities denote on - going dynamic situations."", 'accomplishments and achievements both express a change of state, and hence are temporally bounded ( telic ) ; achievements are punctual, whereas accomplishments extend over a period of time.', ' #TAUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration, which is often a question of granularity']",0
"['of the argument alternations described by  #TAUTHOR_TAG using a  #AUTHOR_TAG style analysis.', 'i believe that with a suitable lexicon ( either hand cra']","['of the argument alternations described by  #TAUTHOR_TAG using a  #AUTHOR_TAG style analysis.', 'i believe that with a suitable lexicon ( either hand']","['of the argument alternations described by  #TAUTHOR_TAG using a  #AUTHOR_TAG style analysis.', 'i believe that with a suitable lexicon ( either hand cra']","['', 'although the effectiveness and coverage of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'with a minimal set of features and a small number of lexical entries,  #AUTHOR_TAG has successfully modeled many of the argument alternations described by  #TAUTHOR_TAG using a  #AUTHOR_TAG style analysis.', 'i believe that with a suitable lexicon ( either hand crafted or automatically induced ), my framework can be elaborated into a system whose performance is comparable to that of current statistical parsers, but with the added advantage of simultaneously providing a richer lexical semantic representation of the input sentence than flat predicate argument structures based on semantic roles']",0
"[' #TAUTHOR_TAG a ), among many others.', 'note that such an']","[' #TAUTHOR_TAG a ), among many others.', 'note that such an']","[' #TAUTHOR_TAG a ), among many others.', 'note that such an approach is no longer lexicalist : each lexical item does not fully encode its associated syntactic and semantic structures.', 'rather, meanings are composed from component morphemes']","['that the only difference between flat. adj and flatten. v is the suffix - en, it must be the source of inchoativity and contribute the change of state reading that distinguishes the verb from the adjective.', 'here, we have evidence that derivational affixes affect the semantic representation of lexical items, that is, fragments of event structure are directly associated with derivational morphemes.', 'we have the following situation : in this case, the complete event structure of a word can be compositionally derived from its component morphemes.', ""this framework, where the ` ` semantic load'' is spread more evenly throughout the lexicon to lexical categories not typically thought to bear semantic content, is essentially the model advocated by  #TAUTHOR_TAG a ), among many others."", 'note that such an approach is no longer lexicalist : each lexical item does not fully encode its associated syntactic and semantic structures.', 'rather, meanings are composed from component morphemes']",0
"[""' s minimalist program  #TAUTHOR_TAG""]","[""' s minimalist program  #TAUTHOR_TAG""]","[""' s minimalist program  #TAUTHOR_TAG""]","['theory of verbal argument structure can be imple - mented in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program  #TAUTHOR_TAG""]",1
['by  #TAUTHOR_TAG and decompose lexical verbs'],['by  #TAUTHOR_TAG and decompose lexical verbs'],['by  #TAUTHOR_TAG and decompose lexical verbs'],"['the non - lexicalist tradition, these primitives are argued to occupy functional projections in the syntactic structure, as so - called light verbs.', 'here, i adopt the model proposed by  #TAUTHOR_TAG and decompose lexical verbs into verbalizing heads and verbal roots.', 'verbalizing heads introduce relevant eventive interpretations in the syntax, and correspond to ( assumed ) universal primitives of the human cognitive system.', 'on the other hand, verbal roots represent abstract ( categoryless ) concepts and basically correspond to open - class items drawn from encyclopedic knowledge.', 'i assume an inventory of three verbalizing heads, each corresponding to an aforementioned primitive']",5
['two current large - scale semantic annotation projects : framenet  #TAUTHOR_TAG'],['two current large - scale semantic annotation projects : framenet  #TAUTHOR_TAG'],['two current large - scale semantic annotation projects : framenet  #TAUTHOR_TAG'],"['common lexical semantic representation in the computational linguistics literature is a frame - based model where syntactic arguments are associated with various semantic roles ( essentially frame slots ).', 'verbs are viewed as simple predicates over their arguments.', ""this approach has its roots in fillmore's  #AUTHOR_TAG, and serves as the foundation for two current large - scale semantic annotation projects : framenet  #TAUTHOR_TAG and propbank  #AUTHOR_TAG""]",0
"['as causality and inchoativity  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'consider the following']","['as causality and inchoativity  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'consider the following example']","['as causality and inchoativity  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'consider the following example']",0
"['also of semantic content.', 'due to advances in statistical syntactic parsing techniques  #TAUTHOR_TAG, attention has recently shifted']","['also of semantic content.', 'due to advances in statistical syntactic parsing techniques  #TAUTHOR_TAG, attention has recently shifted']","['also of semantic content.', 'due to advances in statistical syntactic parsing techniques  #TAUTHOR_TAG, attention has recently shifted']","['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'due to advances in statistical syntactic parsing techniques  #TAUTHOR_TAG, attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences']",0
"['as causality and inchoativity  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'consider the following']","['as causality and inchoativity  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'consider the following example']","['as causality and inchoativity  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'consider the following example']",0
['has developed an agenda - driven chart parser for the feature - driven formalism described'],['has developed an agenda - driven chart parser for the feature - driven formalism described'],['has developed an agenda - driven chart parser for the feature - driven formalism described'],"['has developed an agenda - driven chart parser for the feature - driven formalism described above ; please refer to his paper for a description of the parsing algorithm.', 'i have adapted it for my needs and developed grammar fragments that reflect my non - lexicalist semantic framework.', 'as an example, a simplified derivation of the sentence "" the tire flattened. "" is shown in figure 1']",2
"['argument realization patterns  #TAUTHOR_TAG, possibly arranged in an inheritance hierarchy.', 'the argument structure and syntax -']","['argument realization patterns  #TAUTHOR_TAG, possibly arranged in an inheritance hierarchy.', 'the argument structure and syntax - tosemantics mapping would']","['argument realization patterns  #TAUTHOR_TAG, possibly arranged in an inheritance hierarchy.', 'the argument structure and syntax -']","['typical solution to the redundancy problem is to group verbs according to their argument realization patterns  #TAUTHOR_TAG, possibly arranged in an inheritance hierarchy.', 'the argument structure and syntax - tosemantics mapping would then only need to be specified once for each verb class.', 'in addition, lexical rules could be formulated to derive certain alternations from more basic forms']",1
"['as causality and inchoativity  #AUTHOR_TAG b ; rappaport  #TAUTHOR_TAG.', 'consider the following']","['as causality and inchoativity  #AUTHOR_TAG b ; rappaport  #TAUTHOR_TAG.', 'consider the following example']","['as causality and inchoativity  #AUTHOR_TAG b ; rappaport  #TAUTHOR_TAG.', 'consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity  #AUTHOR_TAG b ; rappaport  #TAUTHOR_TAG.', 'consider the following example']",0
"['result, the state of the floor being clean.', ""a more recent approach, advocated by  #AUTHOR_TAG, describes a basic set of event templates corresponding to vendler's event classes  #TAUTHOR_TAG : ( 3 ) a""]","['result, the state of the floor being clean.', ""a more recent approach, advocated by  #AUTHOR_TAG, describes a basic set of event templates corresponding to vendler's event classes  #TAUTHOR_TAG : ( 3 ) a.""]","['result, the state of the floor being clean.', ""a more recent approach, advocated by  #AUTHOR_TAG, describes a basic set of event templates corresponding to vendler's event classes  #TAUTHOR_TAG : ( 3 ) a. [ x act < manner > ] (""]","['##ty breaks the event described by ( 2 ) into two subevents, the activity of sweeping the floor and its result, the state of the floor being clean.', ""a more recent approach, advocated by  #AUTHOR_TAG, describes a basic set of event templates corresponding to vendler's event classes  #TAUTHOR_TAG : ( 3 ) a. [ x act < manner > ] ( activity ) b. [ x < state > ] ( state ) c. [ become [ x < state > ] ] ( achievement ) d. [ x cause [ become [ x < state > ] ] ] ( accomplishment""]",0
"['surface structure format via language generation tools.', ' #TAUTHOR_TAG b ) and  #AUTHOR_TAG a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']","['surface structure format via language generation tools.', ' #TAUTHOR_TAG b ) and  #AUTHOR_TAG a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']","['the surface structure format via language generation tools.', ' #TAUTHOR_TAG b ) and  #AUTHOR_TAG a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']","['second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'this method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'the first syntactic transformation method is presented by  #AUTHOR_TAG a ).', ' #AUTHOR_TAG b ) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'in other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', ' #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', ' #TAUTHOR_TAG b ) and  #AUTHOR_TAG a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']",0
"['synonyms.', 'the first lexical substitution method was proposed by  #TAUTHOR_TAG.', 'later works,']","['synonyms.', 'the first lexical substitution method was proposed by  #TAUTHOR_TAG.', 'later works,']","['synonyms.', 'the first lexical substitution method was proposed by  #TAUTHOR_TAG.', 'later works,']","['simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'the first lexical substitution method was proposed by  #TAUTHOR_TAG.', 'later works, such as  #AUTHOR_TAG a ),  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', ' #AUTHOR_TAG attempt to use context by prioritizing the alternatives using an ngram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', ' #AUTHOR_TAG and  #AUTHOR_TAG b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']",0
"['sentences.', ' #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG a ) all belong to the syntactic']","['sentences.', ' #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG a ) all belong to the syntactic']","['of the text by adjusting the structural properties of intermediate representations of sentences.', 'in other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', ' #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms']","['second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'this method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'the first syntactic transformation method is presented by  #AUTHOR_TAG a ).', ' #AUTHOR_TAG b ) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'in other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', ' #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', ' #AUTHOR_TAG b ) and  #AUTHOR_TAG a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']",0
"[' #TAUTHOR_TAG a ),  #AUTHOR_TAG,  #AUTHOR_TAG']","[' #TAUTHOR_TAG a ),  #AUTHOR_TAG,  #AUTHOR_TAG']","[' #TAUTHOR_TAG a ),  #AUTHOR_TAG,  #AUTHOR_TAG']","['simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'the first lexical substitution method was proposed by  #AUTHOR_TAG.', 'later works, such as  #TAUTHOR_TAG a ),  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', ' #AUTHOR_TAG attempt to use context by prioritizing the alternatives using an n - gram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', ' #AUTHOR_TAG and  #AUTHOR_TAG b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']",0
"[' #TAUTHOR_TAG b ), further made use of']","[' #TAUTHOR_TAG b ), further made use of part - ofspeech taggers and electronic dictionaries,']","[' #TAUTHOR_TAG b ), further made use of']","['simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'the first lexical substitution method was proposed by  #AUTHOR_TAG.', 'later works, such as  #AUTHOR_TAG a ),  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', ' #AUTHOR_TAG attempt to use context by prioritizing the alternatives using an ngram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', ' #AUTHOR_TAG and  #AUTHOR_TAG b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']",0
"[',  #TAUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG a ) all belong to the syntactic']","[',  #TAUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG a ) all belong to the syntactic']","[',  #TAUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms']","['second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'this method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'the first syntactic transformation method is presented by  #AUTHOR_TAG a ).', ' #AUTHOR_TAG b ) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'in other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', ' #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', ' #AUTHOR_TAG b ) and  #AUTHOR_TAG a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']",0
"[' #AUTHOR_TAG a ),  #TAUTHOR_TAG,  #AUTHOR_TAG']","[' #AUTHOR_TAG a ),  #TAUTHOR_TAG,  #AUTHOR_TAG']","[' #AUTHOR_TAG a ),  #TAUTHOR_TAG,  #AUTHOR_TAG']","['simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'the first lexical substitution method was proposed by  #AUTHOR_TAG.', 'later works, such as  #AUTHOR_TAG a ),  #TAUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', ' #AUTHOR_TAG attempt to use context by prioritizing the alternatives using an ngram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', ' #AUTHOR_TAG and  #AUTHOR_TAG b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']",0
"['such as lexical disambiguation  #TAUTHOR_TAG, and contains english n - grams and']","['such as lexical disambiguation  #TAUTHOR_TAG, and contains english n - grams and']","['many tasks such as lexical disambiguation  #TAUTHOR_TAG, and contains english n - grams and']","['google n - gram data was collected by google research for statistical language modelling, and has been used for many tasks such as lexical disambiguation  #TAUTHOR_TAG, and contains english n - grams and their observed frequency counts, for counts of at least 40.', 'the striking feature of the n - gram corpus is the large number of n - grams and the size of the counts, since the counts were extracted from over 1 trillion word tokens of english text on publicly accessible web pages collected in january 2006.', 'for example, the 5 - gram phrase the part that you were has a count of 103.', 'the compressed data is around 24 gb on disk']",0
"['cover medium, by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer  #TAUTHOR_TAG.', 'the covert communication is such that the very act of communication is to be kept secret from outside observers.', 'a related']","['cover medium, by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer  #TAUTHOR_TAG.', 'the covert communication is such that the very act of communication is to be kept secret from outside observers.', 'a related']","['##ganography is concerned with hiding information in some cover medium, by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer  #TAUTHOR_TAG.', 'the covert communication is such that the very act of communication is to be kept secret from outside observers.', 'a related area is watermarking, in which modifications are made to a cover medium in']","['##ganography is concerned with hiding information in some cover medium, by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer  #TAUTHOR_TAG.', 'the covert communication is such that the very act of communication is to be kept secret from outside observers.', 'a related area is watermarking, in which modifications are made to a cover medium in order to identify it, for example for the purposes of copyright.', 'here the changes may be known to an observer, and the task is to make the changes in such a way that the watermark cannot easily be removed']",0
"['sentences.', ' #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms']","['sentences.', ' #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms']","['of the text by adjusting the structural properties of intermediate representations of sentences.', 'in other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', ' #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms']","['second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'this method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'the first syntactic transformation method is presented by  #AUTHOR_TAG a ).', ' #AUTHOR_TAG b ) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'in other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', ' #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', ' #AUTHOR_TAG b ) and  #AUTHOR_TAG a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']",0
"['.', ' #TAUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition, with the idea that some presuppositional information can be removed without changing']","['available from ontological semantic resources.', ' #TAUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition, with the idea that some presuppositional information can be removed without changing']","['available from ontological semantic resources.', ' #TAUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition, with the idea that some presuppositional information can be removed without changing the meaning of a sentence']","['semantic transformation method is the most sophisticated approach for linguistic steganography, and perhaps impractical given the current state - ofthe - art for nlp technology.', 'it requires some sophisticated tools and knowledge to model natural language semantics.', ' #AUTHOR_TAG used semantic transformations and embed information in textmeaning representation ( tmr ) trees of the text by either pruning, grafting or substituting the tree structure with information available from ontological semantic resources.', ' #TAUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition, with the idea that some presuppositional information can be removed without changing the meaning of a sentence']",0
"['at the end of last year with the end of this year.', ' #TAUTHOR_TAG also note that the applicability of paraphrases is strongly']","['at the end of last year with the end of this year.', ' #TAUTHOR_TAG also note that the applicability of paraphrases is strongly']","['the end of this year in the sentence the chart compares the gold price at the end of last year with the end of this year.', ' #TAUTHOR_TAG also note that the applicability of paraphrases is strongly']","['1 gives summary statistics of the paraphrase dictionary and its coverage on section 00 of the penn treebank.', 'the length of the extracted n - gram phrases ranges from unigrams to five - grams.', 'the coverage figure gives the percentage of sentences which have at least one phrase in the dictionary.', 'the coverage is important for us because it determines the payload capacity of the embedding method described in section 5. original phrase paraphrases the end of this year later this year the end of the year year end a number of people some of my colleagues differences the european peoples party the ppe group dictionary is a mapping from phrases to sets of possible paraphrases.', 'each paraphrase also has a probability, based on a statistical machine translation model, but we do not use that feature here.', 'the examples show that, while some of the paraphrases are of a high quality, some are not.', 'for example, differences is unlikely to be a suitable paraphrase for a number of people in any context.', 'moreover, there are some phrase, paraphrase pairs which are only suitable in particular contexts.', 'for example, year end is an unsuitable paraphrase for the end of this year in the sentence the chart compares the gold price at the end of last year with the end of this year.', ' #TAUTHOR_TAG also note that the applicability of paraphrases is strongly influenced by context.', 'section 4 describes our method for determining if a paraphrase is suitable in a given context']",0
"['for us by chris callison - burch, using the technique described in  #TAUTHOR_TAG, which exploits a parallel corpus and methods developed for statistical machine translation']","['for us by chris callison - burch, using the technique described in  #TAUTHOR_TAG, which exploits a parallel corpus and methods developed for statistical machine translation']","['phrases that occur in section 00.', 'the paraphrase dictionary that we use was generated for us by chris callison - burch, using the technique described in  #TAUTHOR_TAG, which exploits a parallel corpus and methods developed for statistical machine translation']","['cover text used for our experiments consists of newspaper sentences from section 00 of the penn treebank  #AUTHOR_TAG.', 'hence we require possible paraphrases for phrases that occur in section 00.', 'the paraphrase dictionary that we use was generated for us by chris callison - burch, using the technique described in  #TAUTHOR_TAG, which exploits a parallel corpus and methods developed for statistical machine translation']",5
"['sentences.', ' #TAUTHOR_TAG,']","['sentences.', ' #TAUTHOR_TAG,']","['of the text by adjusting the structural properties of intermediate representations of sentences.', 'in other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', ' #TAUTHOR_TAG,']","['second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'this method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'the first syntactic transformation method is presented by  #AUTHOR_TAG a ).', ' #AUTHOR_TAG b ) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'in other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', ' #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', ' #AUTHOR_TAG b ) and  #AUTHOR_TAG a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']",0
"['2 describes some of the previous transformations used in linguistic steganography.', 'note that we are concerned with transformations which are linguistic in nature, rather than dealing with superficial properties of the text, e. g. the amount of white space between words  #TAUTHOR_TAG.', 'our proposed method is based on']","['2 describes some of the previous transformations used in linguistic steganography.', 'note that we are concerned with transformations which are linguistic in nature, rather than dealing with superficial properties of the text, e. g. the amount of white space between words  #TAUTHOR_TAG.', 'our proposed method is based on']","['2 describes some of the previous transformations used in linguistic steganography.', 'note that we are concerned with transformations which are linguistic in nature, rather than dealing with superficial properties of the text, e. g. the amount of white space between words  #TAUTHOR_TAG.', 'our proposed method is based on']","['2 describes some of the previous transformations used in linguistic steganography.', 'note that we are concerned with transformations which are linguistic in nature, rather than dealing with superficial properties of the text, e. g. the amount of white space between words  #TAUTHOR_TAG.', 'our proposed method is based on the automatically acquired paraphrase dictionary described in callison -  #AUTHOR_TAG, in which the application of paraphrases from the dictionary encodes secret bits.', 'one advantage of the dictionary is that it has wide coverage, being automatically extracted ; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts.', 'since we require any changes to be imperceptible to a human observer, it is crucial to our system that any uses of paraphrasing are grammatical and retain the meaning of the original cover text']",1
"['described in  #TAUTHOR_TAG, in']","['described in  #TAUTHOR_TAG, in']","['described in  #TAUTHOR_TAG, in']","['2 describes some of the previous transformations used in linguistic steganography.', 'note that we are concerned with transformations which are linguistic in nature, rather than dealing with superficial properties of the text, e. g. the amount of white space between words  #AUTHOR_TAG.', 'our proposed method is based on the automatically acquired paraphrase dictionary described in  #TAUTHOR_TAG, in which the application of paraphrases from the dictionary encodes secret bits.', 'one advantage of the dictionary is that it has wide coverage, being automatically extracted ; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts.', 'since we require any changes to be imperceptible to a human observer, it is crucial to our system that any uses of paraphrasing are grammatical and retain the meaning of the original cover text']",5
"['hide information, is small compared with other media  #TAUTHOR_TAG.', 'the likely']","['hide information, is small compared with other media  #TAUTHOR_TAG.', 'the likely']","['hide information, is small compared with other media  #TAUTHOR_TAG.', 'the likely reason is that it is easier to']","['is a large literature on image steganography and watermarking, in which images are modified to encode a hidden message or watermark.', 'image stegosystems exploit the redundancy in an image representation together with limitations of the human visual system.', 'for example, a standard image stegosystem uses the least - significant - bit ( lsb ) substitution technique.', 'since the difference between 11111111 and 11111110 in the value for red / green / blue intensity is likely to be undetectable by the human eye, the lsb can be used to hide information other than colour, without being perceptable by a human observer. 1', 'key question for any steganography system is the choice of cover medium.', 'given the ubiquitous nature of natural languages and electronic text, text is an obvious medium to consider.', 'however, the literature on linguistic steganography, in which linguistic properties of a text are modified to hide information, is small compared with other media  #TAUTHOR_TAG.', 'the likely reason is that it is easier to make changes to images and other nonlinguistic media which are undetectable by an observer.', 'language has the property that even small local changes to a text, e. g.', 'replacing a word by a word with similar meaning, may result in text which is anomalous at the document level, or anomalous with respect to the state of the world.', 'hence finding linguistic transformations which can be applied reliably and often is a challenging problem for linguistic steganography']",0
"['basic google ngram method.', 'we use the  #TAUTHOR_TAG cc']","['basic google ngram method.', 'we use the  #TAUTHOR_TAG ccg parser to analyse']","['order to improve the grammaticality checking, we use a parser as an addition to the basic google ngram method.', 'we use the  #TAUTHOR_TAG ccg parser to analyse']","['order to improve the grammaticality checking, we use a parser as an addition to the basic google ngram method.', 'we use the  #TAUTHOR_TAG ccg parser to analyse the sentence before and after paraphrasing.', 'combinatory categorial grammar ( ccg ) is a lexicalised grammar formalism, in which ccg lexical categories - typically expressing subcategorisation information - are assigned to each word in a sentence.', 'the grammatical check works by checking if the words in the sentence outside of the phrase and paraphrase receive the same lexical categories before and after paraphrasing.', 'if there is any change in lexical category assignment to these words then the paraphrase is judged ungrammatical.', 'hence the grammar check is at the word, rather than derivation, level ; however, ccg lexical categories contain a large amount of syntactic information which this method is able to exploit']",5
"['sentences.', ' #AUTHOR_TAG,  #TAUTHOR_TAG,']","['sentences.', ' #AUTHOR_TAG,  #TAUTHOR_TAG,']","['of the text by adjusting the structural properties of intermediate representations of sentences.', 'in other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', ' #AUTHOR_TAG,  #TAUTHOR_TAG,']","['second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'this method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'the first syntactic transformation method is presented by  #AUTHOR_TAG a ).', ' #AUTHOR_TAG b ) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'in other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', ' #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', ' #AUTHOR_TAG b ) and  #AUTHOR_TAG a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']",0
"[' #TAUTHOR_TAG, we applied']","[' #TAUTHOR_TAG, we applied']","['our previous work  #TAUTHOR_TAG, we applied']","['our previous work  #TAUTHOR_TAG, we applied our approach to tokenized arabic and our da - msa transfer component used feature transfer rules only.', 'we did not use a language model to pick the best path ; instead we kept the ambiguity in the lattice and passed it to our smt system.', 'in contrast, in this paper, we run elissa on untokenized arabic, we use feature, lemma, and surface form transfer rules, and we pick the best path of the generated msa lattice through a language model.', ""certain aspects of our approach are similar to  #AUTHOR_TAG's, in that we use morphological analysis for da to help da - english mt ; but unlike them, we use a rule - based approach to model da morphology""]",1
"['using giza + +  #TAUTHOR_TAG.', 'phrase translations of up to 10 words']","['using giza + +  #TAUTHOR_TAG.', 'phrase translations of up to 10 words']","['- aligned using giza + +  #TAUTHOR_TAG.', 'phrase translations of up to 10 words']","['use the open - source moses toolkit  #AUTHOR_TAG to build a phrase - based smt system trained on mostly msa data ( 64m words on the arabic side ) obtained from several ldc corpora including some limited da data.', 'our system uses a standard phrase - based architecture.', 'the parallel corpus is word - aligned using giza + +  #TAUTHOR_TAG.', 'phrase translations of up to 10 words are extracted in the moses phrase table.', 'the language model for our system is trained on the english side of the bitext augmented with english gigaword  #AUTHOR_TAG.', 'we use a 5 - gram language model with modified kneser - ney smoothing.', 'feature weights are tuned to maximize bleu on the nist mteval 2006 test set using minimum error rate training  #AUTHOR_TAG.', 'this is only done on the baseline systems.', 'the english data is tokenized using simple punctuation - based rules.', 'the arabic side is segmented according to the arabic treebank ( atb ) tokenization scheme  #AUTHOR_TAG using the mada + tokan morphological analyzer and tokenizer v3. 1  #AUTHOR_TAG.', 'the arabic text is also alif / ya normalized.', 'mada - produced arabic lemmas are used for word alignment']",5
['in  #TAUTHOR_TAG'],['in  #TAUTHOR_TAG'],"['msa paraphrases can improve the english translation.', 'this is a similar conclusion to our previous work in  #TAUTHOR_TAG']","['the last system group, phrase + word - based selection, phrase - based selection is used to select phrases and add them on top of the best performers of the previous two groups.', 'phrase - based trans - lation is also added to word - based translation.', 'results show that selecting and translating phrases improve the three best performers of word - based selection.', 'the best performer, shown in the last raw, suggests using phrase - based selection and restricted word - based selection.', 'the restriction is to include oov words and selected low frequency words that have at least one dialectal analysis or appear in our dialectal dictionaries.', 'comparing the best performer to the oov selection mode system shows that translating low frequency in - vocabulary dialectal words and phrases to their msa paraphrases can improve the english translation.', 'this is a similar conclusion to our previous work in  #TAUTHOR_TAG']",1
['##es toolkit  #TAUTHOR_TAG to build a phrase - based smt system trained on mostly msa'],['use the open - source moses toolkit  #TAUTHOR_TAG to build a phrase - based smt system trained on mostly msa'],['use the open - source moses toolkit  #TAUTHOR_TAG to build a phrase - based smt system trained on mostly msa data ( 64m words on the arabic side )'],"['use the open - source moses toolkit  #TAUTHOR_TAG to build a phrase - based smt system trained on mostly msa data ( 64m words on the arabic side ) obtained from several ldc corpora including some limited da data.', 'our system uses a standard phrase - based architecture.', 'the parallel corpus is word - aligned using giza + +  #AUTHOR_TAG.', 'phrase translations of up to 10 words are extracted in the moses phrase table.', 'the language model for our system is trained on the english side of the bitext augmented with english gigaword  #AUTHOR_TAG.', 'we use a 5 - gram language model with modified kneser - ney smoothing.', 'feature weights are tuned to maximize bleu on the nist mteval 2006 test set using minimum error rate training  #AUTHOR_TAG.', 'this is only done on the baseline systems.', 'the english data is tokenized using simple punctuation - based rules.', 'the arabic side is segmented according to the arabic treebank ( atb ) tokenization scheme  #AUTHOR_TAG using the mada + tokan morphological analyzer and tokenizer v3. 1  #AUTHOR_TAG.', 'the arabic text is also alif / ya normalized.', 'mada - produced arabic lemmas are used for word alignment']",5
"[' #TAUTHOR_TAG shriberg,']","[' #TAUTHOR_TAG shriberg,']","[' #TAUTHOR_TAG shriberg,']","[""implicit assumptions of psychological and computational theories that ignore disfluencies must be either that people aren't disfluent, or that disfluencies make processing more difficult, and so theories of fluent speech processing should be developed before the research agenda turns to disfluent speech processing."", 'the first assumption is clearly false ; disfluency rates in spontaneous speech are estimated by  #AUTHOR_TAG and by  #AUTHOR_TAG to be about 6 disfluencies per 100 words, not including silent pauses.', 'the rate is lower for speech to machines  #AUTHOR_TAG, due in part to utterance length ; that is, disfluency rates are higher in longer utterances, where planning is more difficult, and utterances addressed to machines tend to be shorter than those addressed to people, often because dialogue interfaces are designed to take on more initiative.', 'the average speaker may believe, quite rightly, that machines are imperfect speech processors, and plan their utterances to machines more carefully.', 'the good news is that speakers can adapt to machines ; the bad news is that they do so by recruiting limited cognitive resources that could otherwise be focused on the task itself.', 'as for the second assumption, if the goal is to eventually process unrestricted, natural human speech, then committing to an early and exclusive focus on processing fluent utterances is risky.', 'in humans, speech production and speech processing are done incrementally, using contextual information from the earliest moments of processing ( see, e. g., tanenhaus et al. 1995 ).', 'this sort of processing requires quite a different architecture and different mechanisms for ambiguity resolution than one that begins processing only at the end of a complete and well - formed utterance.', 'few approaches to parsing have tried to handle disfluent utterances ( notable exceptions are  #TAUTHOR_TAG shriberg, bear, &  #AUTHOR_TAG']",0
"['described in  #TAUTHOR_TAG for our experiments, using the naive']","['described in  #TAUTHOR_TAG for our experiments, using the naive']","['described in  #TAUTHOR_TAG for our experiments, using the naive bayes algorithm as our classifier.', 'knowledge sources used include partsof - speech, surrounding words, and local collocations.', 'this approach achieves state - of -']","['to our previous work  #AUTHOR_TAG b ), we used the supervised wsd approach described in  #TAUTHOR_TAG for our experiments, using the naive bayes algorithm as our classifier.', 'knowledge sources used include partsof - speech, surrounding words, and local collocations.', 'this approach achieves state - of - the - art accuracy.', 'all accuracies reported in our experiments are micro - averages over all test examples']",5
"['', ' #AUTHOR_TAG also chunked the sentences before doing 10 - fold cross validation, but used an uneven split.', 'we chose to follow  #TAUTHOR_TAG and split the sentences evenly to facilitate further comparison']","['our partitions of the ctb corpus are different.', ' #AUTHOR_TAG also chunked the sentences before doing 10 - fold cross validation, but used an uneven split.', 'we chose to follow  #TAUTHOR_TAG and split the sentences evenly to facilitate further comparison']","['', ' #AUTHOR_TAG also chunked the sentences before doing 10 - fold cross validation, but used an uneven split.', 'we chose to follow  #TAUTHOR_TAG and split the sentences evenly to facilitate further comparison']","['overall tagging accuracy of our joint model was comparable to but less than the joint model of  #AUTHOR_TAG.', 'despite the higher accuracy improvement from the baseline, the joint system did not give higher overall accuracy.', 'one likely reason is that  #AUTHOR_TAG included knowledge about special characters and semantic knowledge from web corpora ( which may explain the higher baseline accuracy ), while our system is completely data - driven.', 'however, the comparison is indirect because our partitions of the ctb corpus are different.', ' #AUTHOR_TAG also chunked the sentences before doing 10 - fold cross validation, but used an uneven split.', 'we chose to follow  #TAUTHOR_TAG and split the sentences evenly to facilitate further comparison']",5
[' #TAUTHOR_TAG and'],[' #TAUTHOR_TAG and'],[' #TAUTHOR_TAG and'],"['built a two - stage baseline system, using the perceptron segmentation model from our previous work  #TAUTHOR_TAG and the perceptron pos tagging model from  #AUTHOR_TAG.', 'we use baseline system to refer to the system which performs segmentation first, followed by pos tagging ( using the single - best segmentation ) ; baseline segmentor to refer to the segmentor from  #AUTHOR_TAG which performs segmentation only ; and baseline postagger to refer to the collins tagger which performs pos tagging only ( given segmentation ).', 'the features used by the baseline segmentor are shown in table 1.', 'the features used by the pos tagger, some of which are different to those from  #AUTHOR_TAG and are specific to chinese, are shown in table 2']",2
"['by  #TAUTHOR_TAG.', 'however, their model is strictly customized']","['by  #TAUTHOR_TAG.', 'however, their model is strictly customized']","['by  #TAUTHOR_TAG.', 'however, their model is strictly customized']","['present an extension of the hierarchical dirichlet process ( hdp ) model which is able to represent each observable object ( i. e., event mention ) by a finite number of feature types l.', 'our hdp extension is also inspired from the bayesian model proposed by  #TAUTHOR_TAG.', 'however, their model is strictly customized for entity coreference resolution, and therefore, extending it to include additional features for each observable object is a challenging task  #AUTHOR_TAG']",4
['of them are inspired by previous work  #TAUTHOR_TAG : 1'],['of them are inspired by previous work  #TAUTHOR_TAG : 1'],['of them are inspired by previous work  #TAUTHOR_TAG : 1'],"['approach has the merit of easily combining different features to predict the probability of each class. we incorporate into the me based model the following informative context - based features to train cbsm and cbtm.', 'these features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work  #TAUTHOR_TAG : 1']",4
['of them are inspired by previous work  #TAUTHOR_TAG : 1'],['of them are inspired by previous work  #TAUTHOR_TAG : 1'],['of them are inspired by previous work  #TAUTHOR_TAG : 1'],"['approach has the merit of easily combining different features to predict the probability of each class.', 'we incorporate into the me based model the following informative context - based features to train cbsm and cbtm.', 'these features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work  #TAUTHOR_TAG : 1']",4
"['recently, an alignment selection approach was proposed in  #TAUTHOR_TAG, which computes confidence scores for']","['recently, an alignment selection approach was proposed in  #TAUTHOR_TAG, which computes confidence scores for']","['recently, an alignment selection approach was proposed in  #TAUTHOR_TAG, which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand - picked threshold.', 'the alignments used in that work were generated from different aligners ( hmm, block model, and maximum entropy model ).', 'in this work, we use soft voting with weighted confidence scores, where the weights']","['recently, an alignment selection approach was proposed in  #TAUTHOR_TAG, which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand - picked threshold.', 'the alignments used in that work were generated from different aligners ( hmm, block model, and maximum entropy model ).', 'in this work, we use soft voting with weighted confidence scores, where the weights can be tuned with a specific objective function.', 'there is no need for a pre - determined threshold as used in  #AUTHOR_TAG.', 'also, we utilize various knowledge sources to enrich the alignments instead of using different aligners.', 'our strategy is to diversify and then combine in order to catch any complementary information captured in the word alignments for low - resource languages']",1
"['of chunk type definition.', 'the second is more complicated.', 'inspired by  #TAUTHOR_TAG,']","['of chunk type definition.', 'the second is more complicated.', 'inspired by  #TAUTHOR_TAG,']","[', pp, of current chunk.', 'the column chunk 1 illustrates this kind of chunk type definition.', 'the second is more complicated.', 'inspired by  #TAUTHOR_TAG,']","['introduce two types of chunks.', 'the first is simply the phrase type, such as np, pp, of current chunk.', 'the column chunk 1 illustrates this kind of chunk type definition.', 'the second is more complicated.', ""inspired by  #TAUTHOR_TAG, we split one phrase type into several subsymbols, which contain category information of current constituent's parent."", '']",4
"[' #TAUTHOR_TAG.', ' #AUTHOR_TAG introduced a second word to trigger']","[' #TAUTHOR_TAG.', ' #AUTHOR_TAG introduced a second word to trigger']","['feature of head word trigger which we apply to the log - linear model is motivated by the trigger - based approach  #TAUTHOR_TAG.', ' #AUTHOR_TAG introduced a second word to trigger the target word without considering any linguistic information.', 'furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved.', ' #AUTHOR_TAG built a maximum entropy model which combines rich context information']","['feature of head word trigger which we apply to the log - linear model is motivated by the trigger - based approach  #TAUTHOR_TAG.', ' #AUTHOR_TAG introduced a second word to trigger the target word without considering any linguistic information.', 'furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved.', ' #AUTHOR_TAG built a maximum entropy model which combines rich context information for selecting translation rules during decoding.', 'however, as the size of the corpus increases, the maximum entropy model will become larger.', 'similarly, in  #AUTHOR_TAG, context language model is proposed for better rule selection.', 'taking the dependency edge as condition, our approach is very different from previous approaches of exploring context information']",4
['references in the form of hyperlinks  #TAUTHOR_TAG'],['references in the form of hyperlinks  #TAUTHOR_TAG'],['sentiment - analysis work in different domains has considered inter - document similarity  #AUTHOR_TAG or explicit inter - document references in the form of hyperlinks  #TAUTHOR_TAG'],['sentiment - analysis work in different domains has considered inter - document similarity  #AUTHOR_TAG or explicit inter - document references in the form of hyperlinks  #TAUTHOR_TAG'],0
['texts )  #TAUTHOR_TAG'],['texts )  #TAUTHOR_TAG'],"['( the "" unlabeled "" texts )  #TAUTHOR_TAG.', 'an exception is  #AUTHOR_TAG, who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']",[' #TAUTHOR_TAG'],0
"['to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be']","['to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
['texts )  #TAUTHOR_TAG'],['texts )  #TAUTHOR_TAG'],"['( the "" unlabeled "" texts )  #TAUTHOR_TAG.', 'an exception is  #AUTHOR_TAG, who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']",[' #TAUTHOR_TAG'],0
"['early papers on graph - based semisupervised learning include  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG.', ' #AUTHOR_TAG maintains a survey of this']","['early papers on graph - based semisupervised learning include  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG.', ' #AUTHOR_TAG maintains a survey of this area']","['early papers on graph - based semisupervised learning include  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG.', ' #AUTHOR_TAG maintains a survey of this area']","['early papers on graph - based semisupervised learning include  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG.', ' #AUTHOR_TAG maintains a survey of this area']",0
"['to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our']","['to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"['sophisticated approaches have been proposed  #TAUTHOR_TAG, including an extension']","['sophisticated approaches have been proposed  #TAUTHOR_TAG, including an extension']","['sophisticated approaches have been proposed  #TAUTHOR_TAG, including an extension']","['sophisticated approaches have been proposed  #TAUTHOR_TAG, including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments  #AUTHOR_TAG.', 'also relevant is work on the gen - eral problems of dialog - act tagging  #AUTHOR_TAG, citation analysis  #AUTHOR_TAG, and computational rhetorical analysis  #AUTHOR_TAG']",0
['computational rhetorical analysis  #TAUTHOR_TAG'],['computational rhetorical analysis  #TAUTHOR_TAG'],['computational rhetorical analysis  #TAUTHOR_TAG'],"['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed  #AUTHOR_TAG, including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments  #AUTHOR_TAG.', 'also relevant is work on the general problems of dialog - act tagging  #AUTHOR_TAG, citation analysis  #AUTHOR_TAG, and computational rhetorical analysis  #TAUTHOR_TAG']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit  #TAUTHOR_TAG.', '']",0
"['( see  #AUTHOR_TAG but cfxxx  #TAUTHOR_TAG.', 'agreement evidence can be a powerful aid in our classification task : for ex - ample, we can easily categor']","['two speakers, such as explicit assertions ( i second that! ) or quotation of messages in emails or postings ( see  #AUTHOR_TAG but cfxxx  #TAUTHOR_TAG.', 'agreement evidence can be a powerful aid in our classification task : for ex - ample, we can easily categorize a complicated ( or overly terse ) document if we find within it indica - tions of agreement with a clearly positive text']","['( see  #AUTHOR_TAG but cfxxx  #TAUTHOR_TAG.', 'agreement evidence can be a powerful aid in our classification task : for ex - ample, we can easily categor']","['sentiment - polarity classifiers proposed in the recent literature categorize each document in - dependently. a few others incorporate various measures of inter - document similarity between the texts to be labeled  #AUTHOR_TAG.', 'many interesting opinion - oriented docu - ments, however, can be linked through certain re - lationships that occur in the context of evaluative discussions.', 'for example, we may find textual4 evidence of a high likelihood of agreement be - tween two speakers, such as explicit assertions ( i second that! ) or quotation of messages in emails or postings ( see  #AUTHOR_TAG but cfxxx  #TAUTHOR_TAG.', 'agreement evidence can be a powerful aid in our classification task : for ex - ample, we can easily categorize a complicated ( or overly terse ) document if we find within it indica - tions of agreement with a clearly positive text']",0
['texts )  #TAUTHOR_TAG'],['texts )  #TAUTHOR_TAG'],"['( the "" unlabeled "" texts )  #TAUTHOR_TAG.', 'an exception is  #AUTHOR_TAG, who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']",[' #TAUTHOR_TAG'],0
"['as the positive class and using plain unigrams as features. 5', 'fol - lowing standard practice in sentiment analysis  #TAUTHOR_TAG, the input to svmlight']","['as the positive class and using plain unigrams as features. 5', 'fol - lowing standard practice in sentiment analysis  #TAUTHOR_TAG, the input to svmlight con - sisted of normalized presence - of - feature ( rather than frequency - of - feature ) vectors.', 'the ind value for']","['as the positive class and using plain unigrams as features. 5', 'fol - lowing standard practice in sentiment analysis  #TAUTHOR_TAG, the input to svmlight con -']","['our experiments, we employed the well - known classifier svmlight to obtain individual - document classification scores, treating y as the positive class and using plain unigrams as features. 5', 'fol - lowing standard practice in sentiment analysis  #TAUTHOR_TAG, the input to svmlight con - sisted of normalized presence - of - feature ( rather than frequency - of - feature ) vectors.', 'the ind value for each speech segment s was based on the signed distanceds fromthevectorrepresentingstothe trained svm decision plane : ds 234s d s 234s ds 234s def ds = + 234s 2 ind s ; y where 34s is the standard deviation of d s over all speech segments s in the debate in question, and def ind s ; n = ind s ; y']",5
['and topic - based text categorization to politically oriented text  #TAUTHOR_TAG'],['and topic - based text categorization to politically oriented text  #TAUTHOR_TAG'],['have applied the nlp technologies of near - duplicate detection and topic - based text categorization to politically oriented text  #TAUTHOR_TAG'],['have applied the nlp technologies of near - duplicate detection and topic - based text categorization to politically oriented text  #TAUTHOR_TAG'],0
"['to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our']","['to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"[', namely, automatically distinguishing between positive and negative documents  #TAUTHOR_TAG']","[', namely, automatically distinguishing between positive and negative documents  #TAUTHOR_TAG']","['an active bibliography ).', ""in particular, since we treat each individual speech within a debate as a single ` ` document'', we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents  #TAUTHOR_TAG""]","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG ; see  #AUTHOR_TAG for an active bibliography ).', ""in particular, since we treat each individual speech within a debate as a single ` ` document'', we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents  #TAUTHOR_TAG""]",0
['sentiment - analysis work in different domains has considered inter - document similarity  #TAUTHOR_TAG or explicit'],['sentiment - analysis work in different domains has considered inter - document similarity  #TAUTHOR_TAG or explicit'],['sentiment - analysis work in different domains has considered inter - document similarity  #TAUTHOR_TAG or explicit inter - document references in the form of hyper - links  #AUTHOR_TAG'],['sentiment - analysis work in different domains has considered inter - document similarity  #TAUTHOR_TAG or explicit inter - document references in the form of hyper - links  #AUTHOR_TAG'],0
"['to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our']","['to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
['future work  #TAUTHOR_TAG'],['future work  #TAUTHOR_TAG'],['future work  #TAUTHOR_TAG'],"['##light is available at svmlight. joachims. org.', 'default parameters were used, although experimentation with different parameter settings is an important direction for future work  #TAUTHOR_TAG']",3
"['citation analysis  #TAUTHOR_TAG,']","['citation analysis  #TAUTHOR_TAG,']","['citation analysis  #TAUTHOR_TAG,']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed  #AUTHOR_TAG, including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments  #AUTHOR_TAG.', 'also relevant is work on the general problems of dialog - act tagging  #AUTHOR_TAG, citation analysis  #TAUTHOR_TAG, and computational rhetorical analysis  #AUTHOR_TAG']",0
['labeled  #TAUTHOR_TAG'],['labeled  #TAUTHOR_TAG'],"['be labeled  #TAUTHOR_TAG.', 'many interesting opinion - oriented documents, however, can be linked through certain relationships']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled  #TAUTHOR_TAG.', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'for example, we may find textual 4 evidence of a high likelihood of agreement be - 4 because we are most interested in techniques applicable across domains, we restrict consideration to nlp aspects of the problem, ignoring external problem - specific information.', 'for example, although most votes in our corpus were almost completely along party lines ( and despite the fact that sameparty information is easily incorporated via the methods we propose ), we did not use party - affiliation data.', ""indeed, in other settings ( e. g., a movie - discussion listserv ) one may not be able to determine the participants'political leanings, and such information may not lead to significantly improved results even if it were available""]",0
['labeled  #TAUTHOR_TAG'],['labeled  #TAUTHOR_TAG'],"['be labeled  #TAUTHOR_TAG.', 'many interesting opinion - oriented documents, however, can be linked through certain relationships']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled  #TAUTHOR_TAG.', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'for example, we may find textual 4 evidence of a high likelihood of agreement be - 4 because we are most interested in techniques applicable across domains, we restrict consideration to nlp aspects of the problem, ignoring external problem - specific information.', 'for example, although most votes in our corpus were almost completely along party lines ( and despite the fact that sameparty information is easily incorporated via the methods we propose ), we did not use party - affiliation data.', ""indeed, in other settings ( e. g., a movie - discussion listserv ) one may not be able to determine the participants'political leanings, and such information may not lead to significantly improved results even if it were available""]",0
['labeled  #TAUTHOR_TAG'],['labeled  #TAUTHOR_TAG'],"['be labeled  #TAUTHOR_TAG.', 'many interesting opinion - oriented documents, however, can be linked through certain relationships']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled  #TAUTHOR_TAG.', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'for example, we may find textual 4 evidence of a high likelihood of agreement be - 4 because we are most interested in techniques applicable across domains, we restrict consideration to nlp aspects of the problem, ignoring external problem - specific information.', 'for example, although most votes in our corpus were almost completely along party lines ( and despite the fact that sameparty information is easily incorporated via the methods we propose ), we did not use party - affiliation data.', ""indeed, in other settings ( e. g., a movie - discussion listserv ) one may not be able to determine the participants'political leanings, and such information may not lead to significantly improved results even if it were available""]",0
"['to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be']","['to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"[', in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments  #TAUTHOR_TAG.', 'also relevant is']","[', in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments  #TAUTHOR_TAG.', 'also relevant is']","[', in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments  #TAUTHOR_TAG.', 'also relevant is work on the general problems of dialog - act tagging  #AUTHOR_TAG']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed  #AUTHOR_TAG, including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments  #TAUTHOR_TAG.', 'also relevant is work on the general problems of dialog - act tagging  #AUTHOR_TAG, citation analysis  #AUTHOR_TAG, and computational rhetorical analysis  #AUTHOR_TAG']",0
"['nlp literature  #TAUTHOR_TAG,']","['nlp literature  #TAUTHOR_TAG,']","['has been previously observed and exploited in the nlp literature  #TAUTHOR_TAG, the above optimization function, unlike']","['has been previously observed and exploited in the nlp literature  #TAUTHOR_TAG, the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision']",1
"['to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be']","['to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"[""'  #TAUTHOR_TAG."", 'additionally, much media']","[""'' #TAUTHOR_TAG."", 'additionally, much media']","[""'  #TAUTHOR_TAG."", 'additionally, much media attention has been focused recently on the potential impact that internet sites may have']","[""the united states, for example, governmental bodies are providing and soliciting political documents via the internet, with lofty goals in mind : electronic rulemaking ( erulemaking ) initiatives involving the ` ` electronic collection, distribution, synthesis, and analysis of public commentary in the regulatory rulemaking process'', may ` ` [ alter ] the citizen - government relationship''  #TAUTHOR_TAG."", 'additionally, much media attention has been focused recently on the potential impact that internet sites may have on politics 2, or at least on political journalism 3.', 'regardless of whether one views such claims as clear - sighted prophecy or mere hype, it is obviously important to help people understand and analyze politically oriented text, given the importance of enabling informed participation in the political process']",0
"[', namely, automatically distinguishing between positive and negative documents  #TAUTHOR_TAG']","[', namely, automatically distinguishing between positive and negative documents  #TAUTHOR_TAG']","['a debate as a single "" document "", we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents  #TAUTHOR_TAG']","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG ; see  #AUTHOR_TAG for an active bibliography ).', 'in particular, since we treat each individual speech within a debate as a single "" document "", we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents  #TAUTHOR_TAG']",0
"['early papers on graph - based semisupervised learning include  #AUTHOR_TAG,  #AUTHOR_TAG, kondor and  #AUTHOR_TAG, and  #AUTHOR_TAG.', ' #TAUTHOR_TAG maintains a survey of this']","['early papers on graph - based semisupervised learning include  #AUTHOR_TAG,  #AUTHOR_TAG, kondor and  #AUTHOR_TAG, and  #AUTHOR_TAG.', ' #TAUTHOR_TAG maintains a survey of this']","['early papers on graph - based semisupervised learning include  #AUTHOR_TAG,  #AUTHOR_TAG, kondor and  #AUTHOR_TAG, and  #AUTHOR_TAG.', ' #TAUTHOR_TAG maintains a survey of this area']","['early papers on graph - based semisupervised learning include  #AUTHOR_TAG,  #AUTHOR_TAG, kondor and  #AUTHOR_TAG, and  #AUTHOR_TAG.', ' #TAUTHOR_TAG maintains a survey of this area']",0
['sentiment - analysis work in different domains has considered inter - document similarity  #TAUTHOR_TAG or explicit'],['sentiment - analysis work in different domains has considered inter - document similarity  #TAUTHOR_TAG or explicit'],['sentiment - analysis work in different domains has considered inter - document similarity  #TAUTHOR_TAG or explicit inter - document references in the form of hyperlinks  #AUTHOR_TAG'],['sentiment - analysis work in different domains has considered inter - document similarity  #TAUTHOR_TAG or explicit inter - document references in the form of hyperlinks  #AUTHOR_TAG'],0
"['includes  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG ; see  #AUTHOR_TAG']","['includes  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG ; see  #AUTHOR_TAG']","['to the computational treatment of subjective or opinion - oriented language ( early work includes  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG ; see  #AUTHOR_TAG']","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG ; see  #AUTHOR_TAG for an active bibliography ).', 'in particular, since we treat each individual speech within a debate as a single "" document "", we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents  #AUTHOR_TAG']",0
"['nlp literature  #TAUTHOR_TAG,']","['nlp literature  #TAUTHOR_TAG,']","['has been previously observed and exploited in the nlp literature  #TAUTHOR_TAG, the above optimization function, unlike']","['has been previously observed and exploited in the nlp literature  #TAUTHOR_TAG, the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision']",1
"[', directly inspired by  #TAUTHOR_TAG, integrates both perspectives, optimizing']","[', directly inspired by  #TAUTHOR_TAG, integrates both perspectives, optimizing']","['speech segments belonging to the same debate.', 'our classification framework, directly inspired by  #TAUTHOR_TAG, integrates both perspectives, optimizing']","['support / oppose classification problem can be approached through the use of standard classifiers such as support vector machines ( svms ), which consider each text unit in isolation.', 'as discussed in section 1, however, the conversational nature of our data implies the existence of various relationships that can be exploited to improve cumulative classification accuracy for speech segments belonging to the same debate.', 'our classification framework, directly inspired by  #TAUTHOR_TAG, integrates both perspectives, optimizing its labeling of speech segments based on both individual speech - segment classification scores and preferences for groups of speech segments to receive the same label.', 'in this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships']",5
['and topic - based text categorization to politically oriented text  #TAUTHOR_TAG'],['and topic - based text categorization to politically oriented text  #TAUTHOR_TAG'],['have applied the nlp technologies of near - duplicate detection and topic - based text categorization to politically oriented text  #TAUTHOR_TAG'],['have applied the nlp technologies of near - duplicate detection and topic - based text categorization to politically oriented text  #TAUTHOR_TAG'],0
"['includes  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG ; see  #AUTHOR_TAG']","['includes  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG ; see  #AUTHOR_TAG']","['to the computational treatment of subjective or opinion - oriented language ( early work includes  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG ; see  #AUTHOR_TAG']","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG ; see  #AUTHOR_TAG for an active bibliography ).', 'in particular, since we treat each individual speech within a debate as a single "" document "", we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents  #AUTHOR_TAG']",0
"['early papers on graph - based semisupervised learning include  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', ' #AUTHOR_TAG maintains a survey of this']","['early papers on graph - based semisupervised learning include  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', ' #AUTHOR_TAG maintains a survey of this area']","['early papers on graph - based semisupervised learning include  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', ' #AUTHOR_TAG maintains a survey of this area']","['early papers on graph - based semisupervised learning include  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', ' #AUTHOR_TAG maintains a survey of this area']",0
['the unlabeled items  #TAUTHOR_TAG consider sequential relations between different types of emails ('],"['the unlabeled items  #TAUTHOR_TAG consider sequential relations between different types of emails ( e. g., between requests']",['the unlabeled items  #TAUTHOR_TAG consider sequential relations between different types of emails ('],"['currently do not have an efficient means to encode disagreement information as hard constraints ; we plan to investigate incorporating such information in future work.', 'relationships between the unlabeled items  #TAUTHOR_TAG consider sequential relations between different types of emails ( e. g., between requests and satisfactions thereof ) to classify messages, and thus also explicitly exploit the structure of conversations']",0
"['that ( u. s. ) bills often reach several hundred pages in length  #TAUTHOR_TAG.', 'moreover, political opinions are exsional bills and related data was launched in january 1995, when mosaic was not quite two years old and altavista did not yet exist']","['that ( u. s. ) bills often reach several hundred pages in length  #TAUTHOR_TAG.', 'moreover, political opinions are exsional bills and related data was launched in january 1995, when mosaic was not quite two years old and altavista did not yet exist']","['of legislative language and the fact that ( u. s. ) bills often reach several hundred pages in length  #TAUTHOR_TAG.', 'moreover, political opinions are exsional bills and related data was launched in january 1995, when mosaic was not quite two years old and altavista did not yet exist']","[""##ative and persuasive documents, such as a politician's speech regarding a bill or a blogger's commentary on a legislative proposal, form a particularly interesting type of politically oriented text."", 'people are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion, given the dense nature of legislative language and the fact that ( u. s. ) bills often reach several hundred pages in length  #TAUTHOR_TAG.', 'moreover, political opinions are exsional bills and related data was launched in january 1995, when mosaic was not quite two years old and altavista did not yet exist']",0
"['nlp literature  #TAUTHOR_TAG,']","['nlp literature  #TAUTHOR_TAG,']","['has been previously observed and exploited in the nlp literature  #TAUTHOR_TAG, the above optimization function, unlike']","['has been previously observed and exploited in the nlp literature  #TAUTHOR_TAG, the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision']",1
"[', namely, automatically distinguishing between positive and negative documents  #TAUTHOR_TAG']","[', namely, automatically distinguishing between positive and negative documents  #TAUTHOR_TAG']","['an active bibliography ).', ""in particular, since we treat each individual speech within a debate as a single ` ` document'', we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents  #TAUTHOR_TAG""]","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG ; see  #AUTHOR_TAG for an active bibliography ).', ""in particular, since we treat each individual speech within a debate as a single ` ` document'', we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents  #TAUTHOR_TAG""]",0
"['early papers on graph - based semisupervised learning include  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', ' #AUTHOR_TAG maintains a survey of this']","['early papers on graph - based semisupervised learning include  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', ' #AUTHOR_TAG maintains a survey of this area']","['early papers on graph - based semisupervised learning include  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', ' #AUTHOR_TAG maintains a survey of this area']","['early papers on graph - based semisupervised learning include  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', ' #AUTHOR_TAG maintains a survey of this area']",0
"[', namely, automatically distinguishing between positive and negative documents  #TAUTHOR_TAG']","[', namely, automatically distinguishing between positive and negative documents  #TAUTHOR_TAG']","['an active bibliography ).', ""in particular, since we treat each individual speech within a debate as a single ` ` document'', we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents  #TAUTHOR_TAG""]","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG ; see  #AUTHOR_TAG for an active bibliography ).', ""in particular, since we treat each individual speech within a debate as a single ` ` document'', we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents  #TAUTHOR_TAG""]",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit  #TAUTHOR_TAG.', '']",0
"['includes  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG ; see  #AUTHOR_TAG']","['includes  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG ; see  #AUTHOR_TAG']","['to the computational treatment of subjective or opinion - oriented language ( early work includes  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG ; see  #AUTHOR_TAG']","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG ; see  #AUTHOR_TAG for an active bibliography ).', 'in particular, since we treat each individual speech within a debate as a single "" document "", we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents  #AUTHOR_TAG']",0
"['to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be']","['to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed  #TAUTHOR_TAG.', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"['early papers on graph - based semisupervised learning include  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG.', ' #AUTHOR_TAG maintains a survey of this']","['early papers on graph - based semisupervised learning include  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG.', ' #AUTHOR_TAG maintains a survey of this area']","['early papers on graph - based semisupervised learning include  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG.', ' #AUTHOR_TAG maintains a survey of this area']","['early papers on graph - based semisupervised learning include  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG.', ' #AUTHOR_TAG maintains a survey of this area']",0
['computational rhetorical analysis  #TAUTHOR_TAG'],['computational rhetorical analysis  #TAUTHOR_TAG'],['computational rhetorical analysis  #TAUTHOR_TAG'],"['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed  #AUTHOR_TAG, including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments  #AUTHOR_TAG.', 'also relevant is work on the general problems of dialog - act tagging  #AUTHOR_TAG, citation analysis  #AUTHOR_TAG, and computational rhetorical analysis  #TAUTHOR_TAG']",0
"['', 'an exception is  #TAUTHOR_TAG, who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']","['', 'an exception is  #TAUTHOR_TAG, who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']","['( the "" unlabeled "" texts )  #AUTHOR_TAG.', 'an exception is  #TAUTHOR_TAG, who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']","['', 'an exception is  #TAUTHOR_TAG, who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']",0
['future work  #TAUTHOR_TAG'],['future work  #TAUTHOR_TAG'],['future work  #TAUTHOR_TAG'],"['##light is available at svmlight. joachims. org.', 'default parameters were used, although experimentation with different parameter settings is an important direction for future work  #TAUTHOR_TAG']",3
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit  #TAUTHOR_TAG.', '']",0
"['( see  #TAUTHOR_TAG but cfxxx  #AUTHOR_TAG ).', 'agreement evidence can be a powerful aid in our classification task : for example, we can easily categor']","['high likelihood of agreement between two speakers, such as explicit assertions ( i second that! ) or quotation of messages in emails or postings ( see  #TAUTHOR_TAG but cfxxx  #AUTHOR_TAG ).', 'agreement evidence can be a powerful aid in our classification task : for example, we can easily categorize a complicated ( or overly terse ) document if we find within it indications of agreement with a clearly positive text']","['( see  #TAUTHOR_TAG but cfxxx  #AUTHOR_TAG ).', 'agreement evidence can be a powerful aid in our classification task : for example, we can easily categor']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled  #AUTHOR_TAG.', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'for example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions ( i second that! ) or quotation of messages in emails or postings ( see  #TAUTHOR_TAG but cfxxx  #AUTHOR_TAG ).', 'agreement evidence can be a powerful aid in our classification task : for example, we can easily categorize a complicated ( or overly terse ) document if we find within it indications of agreement with a clearly positive text']",0
['sentiment - analysis work in different domains has considered inter - document similarity  #TAUTHOR_TAG or explicit'],['sentiment - analysis work in different domains has considered inter - document similarity  #TAUTHOR_TAG or explicit'],['sentiment - analysis work in different domains has considered inter - document similarity  #TAUTHOR_TAG or explicit inter - document references in the form of hyperlinks  #AUTHOR_TAG'],['sentiment - analysis work in different domains has considered inter - document similarity  #TAUTHOR_TAG or explicit inter - document references in the form of hyperlinks  #AUTHOR_TAG'],0
"['includes  #AUTHOR_TAG,  #TAUTHOR_TAG']","['includes  #AUTHOR_TAG,  #TAUTHOR_TAG']","['to the computational treatment of subjective or opinion - oriented language ( early work includes  #AUTHOR_TAG,  #TAUTHOR_TAG']","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG ; see  #AUTHOR_TAG for an active bibliography ).', 'in particular, since we treat each individual speech within a debate as a single "" document "", we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents  #AUTHOR_TAG']",0
['version of bleu as in  #TAUTHOR_TAG'],"['systematically shorter than the reference, but these two cases can not be distinguished by single - sentence objective functions.', 'our plan is to implement a windowed or moving - average version of bleu as in  #TAUTHOR_TAG']","['systematically shorter than the reference, but these two cases can not be distinguished by single - sentence objective functions.', 'our plan is to implement a windowed or moving - average version of bleu as in  #TAUTHOR_TAG']","['have thus far implemented two objective functions which operate on individual sentences without regard for choices made on other sentences.', 'when the final evaluation metric incorporates global statistics, however, an objective function which takes them into account is desirable.', 'for example, when using bleu, it makes a big difference whether individual sentences are both longer and shorter than the reference or systematically shorter than the reference, but these two cases can not be distinguished by single - sentence objective functions.', 'our plan is to implement a windowed or moving - average version of bleu as in  #TAUTHOR_TAG']",3
"['g.  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'however, the similarity between subjective words, which have multiple senses against other words']","[' #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'however, the similarity between subjective words, which have multiple senses against other words']","['g.  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'however, the similarity between subjective words, which have multiple senses against other words']","['flexible architecture we have presented enables interesting future research : ( i ) a straightforward improvement is the use of lexical similarity to reduce data sparseness, e. g.  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'however, the similarity between subjective words, which have multiple senses against other words may negatively impact the system accuracy.', 'therefore, the use of the syntactic / semantic kernels, i. e.  #AUTHOR_TAG a ;  #AUTHOR_TAG b ), to syntactically contextualize word similarities may improve the reranker accuracy.', '']",0
['described in  #TAUTHOR_TAG where significant'],['described in  #TAUTHOR_TAG where significant'],"['described in  #TAUTHOR_TAG where significant improvements could be achieved by increasing the number of source languages.', 'so this casts some doubt on these.', 'however, as english was not considered as a source language there,']","['right column in table 1 shows the scores if ( using the product - of - ranks algorithm ) four source languages are taken into account in parallel.', 'as can be seen, with an average score of 51. 8 the improvement over the english only variant ( 50. 6 ) is minimal.', 'this contrasts with the findings described in  #TAUTHOR_TAG where significant improvements could be achieved by increasing the number of source languages.', 'so this casts some doubt on these.', 'however, as english was not considered as a source language there, the performance levels were mostly between 10 and 20, leaving much room for improvement.', 'this is not the case here, where we try to improve on a score of around 50 for english.', 'remember that this is a somewhat conservative score as we count correct but alternative translations, as errors.', 'as this is already a performance much closer to the optimum, making further performance gains is more difficult.', 'therefore, perhaps we should take it as a success that the product - of - ranks algorithm could achieve a minimal performance gain despite the fact that the influence of the non - english languages was probably mostly detrimental']",1
"['', 'as suggested in  #TAUTHOR_TAG this']","['algorithm.', 'as suggested in  #TAUTHOR_TAG this']","['', 'as suggested in  #TAUTHOR_TAG this']","['far, we always computed translations to single source words.', 'however, if we assume, for example, that we already have word equations for four languages, and all we want is to compute the translations into a fifth language, then we can simply extend our approach to what we call the product - of - ranks algorithm.', 'as suggested in  #TAUTHOR_TAG this can be done by looking up the ranks of each of the four given words ( i. e. the words occurring in a particular word equation ) within the association vector of a translation candidate, and by multiplying these ranks.', '']",4
"[' #TAUTHOR_TAG dealt only with an english corpus, the current work shows that this methodology is applicable to a wide range of languages and corpora.', 'we were able to shed some']","[' #TAUTHOR_TAG dealt only with an english corpus, the current work shows that this methodology is applicable to a wide range of languages and corpora.', 'we were able to shed some']","[' #TAUTHOR_TAG dealt only with an english corpus, the current work shows that this methodology is applicable to a wide range of languages and corpora.', 'we were able to shed some light on criteria influencing performance,']","[' #TAUTHOR_TAG dealt only with an english corpus, the current work shows that this methodology is applicable to a wide range of languages and corpora.', 'we were able to shed some light on criteria influencing performance, such as the selection of text type and the direction of a language pair.', 'for example, it is more promising to look at occurrences of english words in a german corpus rather than the other way around.', 'because of the special status of english it is also advisable to use it as a pivot wherever possible']",1
"['to learning paraphrases from a corpus that is described in  #TAUTHOR_TAG.', 'an important component of our future effort will be to evaluate']","['to learning paraphrases from a corpus that is described in  #TAUTHOR_TAG.', 'an important component of our future effort will be to evaluate']","['automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in  #TAUTHOR_TAG.', 'an important component of our future effort will be to evaluate']","['current work has focussed on high - level mapping rules which can be used both for generation from databases and knowledge representations and also for generation from text.', 'in future work, we will focus on mapping text ( in monologue form ) to dialogue.', 'for this we need to combine the highlevel rules with rules for paraphrasing the text in the monologue with text for the dialogue acts that express the same information in dialogue form.', 'for automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in  #TAUTHOR_TAG.', 'an important component of our future effort will be to evaluate whether automatically generating dialogues from naturally - occurring monologues, following the approach described here, results in dialogues that are fluent and coherent and preserve the information from the input monologue']",3
['of  #TAUTHOR_TAG'],['of  #TAUTHOR_TAG'],"['of  #TAUTHOR_TAG.', 'this would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry']","['', 'disappointingly, we did not succeed in establishing a clear improvement for more than one of the six corpora.', 'although we have not been successful in figure 6 : learning curve for super grec figure 7 : learning curve for epi proving our initial hypothesis we argue that our results calls for further study due to several concerns raised by the results remaining unanswered.', 'it may be that our notion of distance to lexical resource entries is too naive.', 'a possible future direction would be to compare the query string to retrieved results using a method similar to that of  #TAUTHOR_TAG.', 'this would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry']",3
"['approach described in  #TAUTHOR_TAG.', 'the model']","['article classifier is a discriminative model that draws on the state - of - the - art approach described in  #TAUTHOR_TAG.', 'the model']","['article classifier is a discriminative model that draws on the state - of - the - art approach described in  #TAUTHOR_TAG.', 'the model']","['article classifier is a discriminative model that draws on the state - of - the - art approach described in  #TAUTHOR_TAG.', 'the model makes use of the averaged perceptron ( ap ) algorithm  #AUTHOR_TAG and is trained on the training data of the shared task with rich features.', 'the article module uses the pos and chunker output to generate some of its features and candidates ( likely contexts for missing articles )']",5
"['other machine - learning methods on error correction tasks  #TAUTHOR_TAG.', 'thus, the classifiers trained on the learner data']","['other machine - learning methods on error correction tasks  #TAUTHOR_TAG.', 'thus, the classifiers trained on the learner data']","['other machine - learning methods on error correction tasks  #TAUTHOR_TAG.', 'thus, the classifiers trained on the learner data']","['choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine - learning methods on error correction tasks  #TAUTHOR_TAG.', 'thus, the classifiers trained on the learner data make use of a discriminative model.', 'because the google corpus does not contain complete sentences but only n - gram counts of length up to five, training a discriminative model is not desirable, and we thus use nb ( details in  #AUTHOR_TAG )']",4
['disagreement  #TAUTHOR_TAG and'],['of agreement / disagreement  #TAUTHOR_TAG and'],['disagreement  #TAUTHOR_TAG and'],"['line of research that is correlated with ours is recognition of agreement / disagreement  #TAUTHOR_TAG and classification of stances  #AUTHOR_TAG in online forums.', 'for future work, we can utilize textual features ( contextual, dependency, discourse markers ), relevant multiword expressions and topic modeling  #AUTHOR_TAG, and thread structure  #AUTHOR_TAG to improve the agree / disagree classification accuracy']",1
['on agreement and disagreement  #TAUTHOR_TAG'],['on agreement and disagreement  #TAUTHOR_TAG'],"['', 'in addition, we consider several types of lexical features ( lexf ) inspired by previous work on agreement and disagreement  #TAUTHOR_TAG']","['', 'the mi approach discovers the words that are highly associated with agree / disagree categories and these words turn to be useful features for classification.', 'in addition, we consider several types of lexical features ( lexf ) inspired by previous work on agreement and disagreement  #TAUTHOR_TAG']",4
['disagreement  #TAUTHOR_TAG and'],['of agreement / disagreement  #TAUTHOR_TAG and'],['disagreement  #TAUTHOR_TAG and'],"['line of research that is correlated with ours is recognition of agreement / disagreement  #TAUTHOR_TAG and classification of stances  #AUTHOR_TAG in online forums.', 'for future work, we can utilize textual features ( contextual, dependency, discourse markers ), relevant multiword expressions and topic modeling  #AUTHOR_TAG, and thread structure  #AUTHOR_TAG to improve the agree / disagree classification accuracy']",1
['on agreement and disagreement  #TAUTHOR_TAG'],['on agreement and disagreement  #TAUTHOR_TAG'],"['', 'in addition, we consider several types of lexical features ( lexf ) inspired by previous work on agreement and disagreement  #TAUTHOR_TAG']","['', 'the mi approach discovers the words that are highly associated with agree / disagree categories and these words turn to be useful features for classification.', 'in addition, we consider several types of lexical features ( lexf ) inspired by previous work on agreement and disagreement  #TAUTHOR_TAG']",4
