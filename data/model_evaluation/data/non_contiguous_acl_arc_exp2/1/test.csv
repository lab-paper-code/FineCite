unique_id,citing_id,citing_title,cited_title,cited_authors,section_title,cited_abstract,citation_context,cite_context_paragraph,citation_class_label,dynamic_contexts_combined
CC388,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,finding terminology translations from nonparallel corpora,"['Pascale Fung', 'Kathleen McKeown']",introduction,"We present a statistical word feature, the Word Relation Matrix, which can be used to find translated pairs of words and terms from non-parallel corpora, across language groups. Online dictionary entries are used as seed words to generate Word Relation Matrices for the unknown words according to correlation measures. Word Relation Matrices are then mapped across the corpora to find translation pairs. Translation accuracies are around 30% when only the top candidate is counted. Nevertheless, top 20 candidate output give a 50.9% average increase in accuracy on human translator performance.","#AUTHOR_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora .","['All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Röscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', '#AUTHOR_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora .', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.', 'The respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.', 'More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:']",0,"['There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', '#AUTHOR_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora .', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.']"
CC389,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,the selfextending phrasal lexicon,"['Uri Zernik', 'Michael Dyer']",introduction,,"â¢ Learnability ( #AUTHOR_TAG ) â¢ Text generation ( Hovy 1988 ; Milosavljevic , Tulloch , and Dale 1996 ) â¢ Speech generation ( Rayner and Carter 1997 ) â¢ Localization ( Sch Â¨ aler 1996 )","['â\x80¢ Learnability ( #AUTHOR_TAG ) â\x80¢ Text generation ( Hovy 1988 ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation ( Rayner and Carter 1997 ) â\x80¢ Localization ( Sch Â¨ aler 1996 )']",0,"['â\x80¢ Learnability ( #AUTHOR_TAG ) â\x80¢ Text generation ( Hovy 1988 ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation ( Rayner and Carter 1997 ) â\x80¢ Localization ( Sch Â¨ aler 1996 )']"
CC390,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,a languageneutral sparsedata algorithm for extracting translation patterns,"['Kevin McTait', 'Arturo Trujillo']",introduction,,"Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , #AUTHOR_TAG , Carl ( 1999 ) , and Brown ( 2000 ) , inter alia .","['Block distinguishes chunks from ""patterns,"" as we do: His chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', 'Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', 'Watanabe (1993) combines lexical and dependency mappings to form his generalizations.', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , #AUTHOR_TAG , Carl ( 1999 ) , and Brown ( 2000 ) , inter alia .']",0,"['Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', 'Watanabe (1993) combines lexical and dependency mappings to form his generalizations.', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , #AUTHOR_TAG , Carl ( 1999 ) , and Brown ( 2000 ) , inter alia .']"
CC391,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,generating language with a phrasal lexicon,['Edward Hovy'],introduction,"In this paper, we ask: How should language be represented in a generator program? In particular, how do the concepts the generator must express, the grammar it is to use, and the words and phrases with which it must express them, relate? The answer presented here is that all linguistic knowledge -- all language -- should be contained in the lexicon. The argument is the following: A generator performs three types of task to produce text (deciding what material to include; ordering the parts within paragraphs and sentences; and expressing the parts as appropriate phrases and parts of speech). It gets the information it requires to do these tasks from three sources: from the grammar, from partially frozen phrases (including multi-predicate phrasal patterns), and from certain words. In a functionally organized system, there is no reason why an a priori distinction should be made between the contents of the lexicon and the contents of the grammar. From the generator's perspective, the difference between these sources is not important. Rules of grammar, multi-predicate phrases, and phrasal and verb predicate patterns can all be viewed as phrases, frozen to a greater or lesser degree, and should all be part of the lexicon. Some such ""phrases"" can be quite complex, prescribing a series of actions and tests to perform the three tasks: these can be thought of as specialist procedures. Others can be very simple: templates. This paper also describes the elements that constitute the lexicon of a phrasal generator program and the way the elements are used.","â¢ Learnability ( Zernik and Dyer 1987 ) â¢ Text generation ( #AUTHOR_TAG ; Milosavljevic , Tulloch , and Dale 1996 ) â¢ Speech generation ( Rayner and Carter 1997 ) â¢ Localization ( Sch Â¨ aler 1996 )","['â\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation ( #AUTHOR_TAG ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation ( Rayner and Carter 1997 ) â\x80¢ Localization ( Sch Â¨ aler 1996 )']",0,"['â\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation ( #AUTHOR_TAG ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation ( Rayner and Carter 1997 ) â\x80¢ Localization ( Sch Â¨ aler 1996 )']"
CC392,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,toward memorybased translation,"['Satoshi Sato', 'Makoto Nagao']",introduction,,"Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #AUTHOR_TAG ; Veale and Way 1997 ; Carl 1999 ) .7","['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #AUTHOR_TAG ; Veale and Way 1997 ; Carl 1999 ) .7']",0,"['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #AUTHOR_TAG ; Veale and Way 1997 ; Carl 1999 ) .7']"
CC393,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,what’s been forgotten in translation memory in envisioning machine translation in the information future,"['Elliott Macklovitch', 'Graham Russell']",introduction,,"From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a search-and-replace engine , albeit a rather sophisticated one ( #AUTHOR_TAG ) .","[""In quite a short space of time, translation memory (TM) systems have become a very useful tool in the translator's armory."", 'TM systems store a set of source, target translation pairs in their databases.', 'If a new input string cannot be found exactly in the translation database, a search is conducted for close (or ""fuzzy"") matches of the input string, and these are retrieved together with their translations for the translator to manipulate into the final, output translation.', 'From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a search-and-replace engine , albeit a rather sophisticated one ( #AUTHOR_TAG ) .']",0,"[""In quite a short space of time, translation memory (TM) systems have become a very useful tool in the translator's armory."", 'TM systems store a set of source, target translation pairs in their databases.', 'From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a search-and-replace engine , albeit a rather sophisticated one ( #AUTHOR_TAG ) .']"
CC394,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,aligning clauses in parallel texts,"['Sotiris Boutsis', 'Stelios Piperidis']",introduction,"This paper describes a method for the automatic alignment of parallel texts at clause level. The method features statistical techniques coupled with shallow linguistic processing. It presupposes a parallel bilingual corpus and identifies alignments between the clauses of the source and target language sides of the corpus. Parallel texts are first statistically aligned at sentence level and then tagged with their part-of-speech categories. Regular grammars functioning on tags, recognize clauses on both sides of the parallel text. A probabilistic model is applied next, operating on the basis of word occurrence and co-occurrence probabilities and character lengths. Depending on sentence size, possible alignments arc fed into a dynamic progranuning framework or a simulated annealing system in order to find or approxim~te the best alignment. 1he method has been tested on a Small Eng~ lish-Greek corpus consisting of texts relevant to software systems and has produced promising results in terms of correctly identified clause alignments.",#AUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities .,"['All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Röscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', '#AUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities .', 'The respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.', 'More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:']",0,['#AUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities .']
CC395,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,gaijin a bootstrapping templatedriven approach to examplebased machine translation,"['Tony Veale', 'Andy Way']",introduction,,"â¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction ( Juola 1998 ) â¢ grammar optimization ( Juola 1994 ) â¢ insights into universal grammar ( Juola 1998 ) â¢ machine translation ( Juola 1994 , 1997 ; #AUTHOR_TAG ; Gough , Way , and Hearne 2002 )","['â\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; #AUTHOR_TAG ; Gough , Way , and Hearne 2002 )']",0,"['â\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; #AUTHOR_TAG ; Gough , Way , and Hearne 2002 )']"
CC396,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,the necessity of syntax markers two experiments with artificial languages,['Thomas Green'],introduction,,"â¢ language learning ( #AUTHOR_TAG ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction ( Juola 1998 ) â¢ grammar optimization ( Juola 1994 ) â¢ insights into universal grammar ( Juola 1998 ) â¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )","['â\x80¢ language learning ( #AUTHOR_TAG ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']",0,"['â\x80¢ language learning ( #AUTHOR_TAG ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']"
CC397,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,integrating translations from multiple sources with the pangloss mark iii machine translation system,"['Robert Frederking', 'Sergei Nirenburg', 'David Farwell', 'Steven Helmreich', 'Eduard Hovy', 'Kevin Knight', 'Stephen Beale', 'Constantin Domashnev', 'Donna Attardo', 'Dean Grannes', 'Ralf Brown']",conclusion,"Since MT systems, whatever translation method they employ, do not reach an optimum output on free text; each method handles some problems better than others. The PANGLOSS Mark III system is an MT environment that uses the best results from a variety of independent MT systems or engines working simultaneously within a single framework on the same text. This paper describes the method used to combine the outputs of the engines into a single text.","Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by Frederking and Nirenburg ( 1994 ) , #AUTHOR_TAG , and Hogan and Frederking ( 1998 ) .","['Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by Frederking and Nirenburg ( 1994 ) , #AUTHOR_TAG , and Hogan and Frederking ( 1998 ) .']",1,"['Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by Frederking and Nirenburg ( 1994 ) , #AUTHOR_TAG , and Hogan and Frederking ( 1998 ) .']"
CC398,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,subsentential exploitation of translation memories,"['Michel Simard', 'Philippe Langlais']",introduction,Laboratoire de recherche appliquee en linguistique informatique (RALI) Departement d&apos;Informatique et recherche operationnelle Universite de Montrea,"More recently , #AUTHOR_TAG have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch Â¨ aler ( 2002 ) and Sch Â¨ aler , Way , and Carl ( 2003 , pages 108 -- 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment .","['More recently , #AUTHOR_TAG have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch Â¨ aler ( 2002 ) and Sch Â¨ aler , Way , and Carl ( 2003 , pages 108 -- 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment .', 'This, they suggest, may result in a paradigm shift from TM to EBMT via the phrasal lexicon: Translators are on the whole wary of MT technology, but once subsentential alignment is enabled, translators will become aware of the benefits to be gained from _source, target_ phrasal segments, and from there they suggest that �it is a reasonably short step to enabling an automated solution via the recombination element of EBMT systems such as those described in [Carl and Way 2003].�']",0,"['More recently , #AUTHOR_TAG have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch Â¨ aler ( 2002 ) and Sch Â¨ aler , Way , and Carl ( 2003 , pages 108 -- 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment .']"
CC399,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,further experiments in bilingual text alignment,['Harold Somers'],introduction,"We describe and experimentally evaluate an alternative algorithm for aligning and extracting vocabulary from parallel texts using recency vectors and a similarity measure based on Levenshtein distance. The work is largely inspired by Fung and McKeown 's DK-vec, though we use a simpler algorithm. The technique is tested on two sets of parallel corpora involving English, French, German, Dutch, Spanish, and Japanese. We attempt to evaluate the importance of parameters such as frequency of words chosen as candidates, the effect of different language pairings, and differences between the two corpora.",#AUTHOR_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance .,"['All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Röscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', '#AUTHOR_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance .', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.', 'The respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.', 'More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:']",0,"['There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Roscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', '#AUTHOR_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance .', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.']"
CC400,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,a psycholinguistic approach to corpusbased machine translation,['Patrick Juola'],introduction,,"#AUTHOR_TAG , 1997 ) assumes that words ending in - ed are verbs .","['#AUTHOR_TAG , 1997 ) assumes that words ending in - ed are verbs .', 'However, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category.', 'Instead, we take advantage of the fact that the initial phrasal chunks correspond to rule right-hand sides.', 'That is, for a rule in the Penn Treebank VP −→ VBG, NP, PP, we are certain (if the annotators have done their job correctly) that the first word in each of the strings corresponding to this right-hand side is a VBG, that is, a present participle.', 'Given this information, in such cases we tag such words with the <LEX> tag.', 'Taking expanding the board to 14 members −→ augmente le conseilà 14 membres as an example, we extract the chunks in ( 24 We ignore here the trivially true lexical chunk ""<QUANT> 14 : 14.""']",1,"['#AUTHOR_TAG , 1997 ) assumes that words ending in - ed are verbs .', 'Instead, we take advantage of the fact that the initial phrasal chunks correspond to rule right-hand sides.']"
CC401,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,the weighted majority algorithm,"['Nick Littlestone', 'Manfred Warmuth']",introduction,"AbstractWe study the construction of prediction algorithms in a situation in which a learner faces a sequence of trials, with a prediction to be made in each, and the goal of the learner is to make few mistakes. We are interested in the case where the learner has reason to believe that one of some pool of known algorithms will perform well, but the learner does not know which one. A simple and effective method, based on weighted voting, is introduced for constructing a compound algorithm in such a circumstance. We call this method the Weighted Majority Algorithm. We show that this algorithm is robust in the presence of errors in the data. We discuss various versions of the Weighted Majority Algorithm and prove mistake bounds for them that are closely related to the mistake bounds of the best algorithms of the pool. For example, given a sequence of trials, if there is an algorithm in the pool A that makes at most m mistakes then the Weighted Majority Algorithm will make at most c(log |A| + m) mistakes on that sequence, where c is fixed constant","We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm ( #AUTHOR_TAG ) .","['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks, as is usual in most EBMT systems (cf.', 'Sato and Nagao 1990;Veale and Way 1997;Carl 1999). 7', 's an example, consider the translation into French of the house collapsed.', 'Assume the conditional probabilities in ( 33  These mistranslations are all caused by boundary friction.', 'Each of the translations in ( 37) and ( 38) would be output with an associated weight and ranked by the system.', 'We would like to incorporate into our model a procedure whereby translation chunks extracted from the phrasal and marker lexicons are more highly regarded than those constructed by inserting words from the word-level lexicon into generalized marker chunks.', 'That is, we want to allocate a larger portion of the probability space to the phrasal and marker lexicons than to the generalized or wordlevel lexicons.', 'We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm ( #AUTHOR_TAG ) .']",3,"['We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm ( #AUTHOR_TAG ) .']"
CC402,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,gaijin a bootstrapping templatedriven approach to examplebased machine translation,"['Tony Veale', 'Andy Way']",introduction,,"Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; #AUTHOR_TAG ; Carl 1999 ) .7","['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; #AUTHOR_TAG ; Carl 1999 ) .7']",0,"['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; #AUTHOR_TAG ; Carl 1999 ) .7']"
CC403,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,the world wide web as a resource for examplebased machine translation tasks,['Gregory Grefenstette'],experiments,"The WWW is two orders of magnitude larger than the largest corpora. Although noisy, web text presents language as it is used, and statistics derived from the Web can have practical uses in many NLP applications. For this reason, the WWW should be seen and studied as any other computationally available linguistic resource. In this article, we illustrate this by showing that an Example-Based approach to lexical choice for machine translation can use the Web as an adequate and free resource.","However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #AUTHOR_TAG .","['The problem of boundary friction is clearly visible here: We have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural NP.', 'However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #AUTHOR_TAG .', 'Grefenstette shows that the Web can be used as a filter on translation quality simply by searching for competing translation candidates and selecting the one that is found most often.', 'Rather than search for competing candidates, we select the ""best"" translation and have its morphological variants searched for on-line.', ""In the example above, namely, the personal computers, we search for les ordinateurs personnels versus the wrong alternatives le/la/l'ordinateurs personnels."", 'Interestingly, using Lycos, and setting the search language to French, the correct form les ordinateurs personnels is uniquely preferred over the other alternatives, as it is found 2,454 times, whereas the others are not found at all.', 'In this case, this translation overrides the highest-ranked translation (50) and is output as the final translation.', 'In fact, in checking the translations obtained for NPs using system combination ABC, we noted that 251 NPs out of the test set of 500 could be improved.', 'Of these 251, 207 (82.5%) were improved post hoc via the Web, with no improvement for the remaining 43 cases.', 'We consider this to be quite a significant result.']",5,"['However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #AUTHOR_TAG .', 'Grefenstette shows that the Web can be used as a filter on translation quality simply by searching for competing translation candidates and selecting the one that is found most often.', 'Rather than search for competing candidates, we select the ""best"" translation and have its morphological variants searched for on-line.', 'Of these 251, 207 (82.5%) were improved post hoc via the Web, with no improvement for the remaining 43 cases.']"
CC404,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,a psycholinguistic approach to corpusbased machine translation,['Patrick Juola'],introduction,,"#AUTHOR_TAG , 1997 ) conducts some small experiments using his METLA system to show the viability of this approach for English â > French and English â > Urdu .","['Given that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks.', '#AUTHOR_TAG , 1997 ) conducts some small experiments using his METLA system to show the viability of this approach for English â\x88\x92 > French and English â\x88\x92 > Urdu .', 'For the English −→ French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data.', 'For English −→ Urdu, Juola (1997, page 213) notes that ""the system learned the original training corpus . . .', 'perfectly and could reproduce it without errors""; that is, it scored 100% accuracy when tested against the training corpus.', 'On novel test sentences, he gives results of 72% correct translation.', 'In their Gaijin system, Veale and Way (1997) give a result of 63% accurate translations obtained for English −→ German on a test set of 791 sentences from CorelDRAW manuals.']",0,"['#AUTHOR_TAG , 1997 ) conducts some small experiments using his METLA system to show the viability of this approach for English â\x88\x92 > French and English â\x88\x92 > Urdu .', 'For English -- Urdu, Juola (1997, page 213) notes that ""the system learned the original training corpus . . .', 'On novel test sentences, he gives results of 72% correct translation.', 'In their Gaijin system, Veale and Way (1997) give a result of 63% accurate translations obtained for English -- German on a test set of 791 sentences from CorelDRAW manuals.']"
CC405,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,a psycholinguistic approach to corpusbased machine translation,['Patrick Juola'],introduction,,"â¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction ( Juola 1998 ) â¢ grammar optimization ( #AUTHOR_TAG ) â¢ insights into universal grammar ( Juola 1998 ) â¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )","['â\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( #AUTHOR_TAG ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']",0,"['â\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( #AUTHOR_TAG ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']"
CC406,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,examplebased incremental synchronous interpretation,['Hans-Ulrich Block'],introduction,"This article describes a new approach to example based incremental translation for automatic interpretation systems developed in Verbmobil. The translation module is completely learned from a bilingual corpus. The training phase combines statistical word alignment with precomputation of translation ""chunks"" and contextual clustering of syntactic equivalence classes (word classes). The system gives incremental output for every piece of input being it words or sequences of words. It thus tries to mimic the behaviour of a human synchronous interpreter. If a larger context leads to the need for reformulation the system utters a correction marker like I mean, and restarts the output from the starting position of the reformulation. The system is currently effective for German = English. German = Chinese and German a Japanese are under construction. In the Verbmobil evaluation, this approach reached 79% of approximately correct translations on speech recognition output.","In a final processing stage , we generalize over the marker lexicon following a process found in #AUTHOR_TAG .","['In a final processing stage , we generalize over the marker lexicon following a process found in #AUTHOR_TAG .', ""In Block's approach, word alignments are assigned probabilities by means of a statistical word alignment tool."", 'In a subsequent stage, chunk pairs are extracted, which are then generalized to produce a set of translation templates for each source, target segment.']",5,"['In a final processing stage , we generalize over the marker lexicon following a process found in #AUTHOR_TAG .', ""In Block's approach, word alignments are assigned probabilities by means of a statistical word alignment tool.""]"
CC407,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,examplebased incremental synchronous interpretation,['Hans-Ulrich Block'],introduction,"This article describes a new approach to example based incremental translation for automatic interpretation systems developed in Verbmobil. The translation module is completely learned from a bilingual corpus. The training phase combines statistical word alignment with precomputation of translation ""chunks"" and contextual clustering of syntactic equivalence classes (word classes). The system gives incremental output for every piece of input being it words or sequences of words. It thus tries to mimic the behaviour of a human synchronous interpreter. If a larger context leads to the need for reformulation the system utters a correction marker like I mean, and restarts the output from the starting position of the reformulation. The system is currently effective for German = English. German = Chinese and German a Japanese are under construction. In the Verbmobil evaluation, this approach reached 79% of approximately correct translations on speech recognition output.","Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #AUTHOR_TAG to permit a limited form of insertion in the translation process .","['• language learning (Green 1979;Mori and Moeser 1983;Morgan, Meier, and Newport 1989) • monolingual grammar induction (Juola 1998) • grammar optimization (Juola 1994) • insights into universal grammar (Juola 1998) • machine translation (Juola 1994(Juola , 1997Veale and Way 1997;Gough, Way, and Hearne 2002) With respect to translation, a potential problem in using the marker hypothesis is that some languages do not have marker words such as articles, for instance.', ""Green's (1979) work showed that artificial languages, both with and without specific marker words, may be learned more accurately and quickly if such psycholinguistic cues exist."", 'The research of Mori and Moeser (1983) showed a similar effect due to case marking on pseudowords in such artificial languages, and Morgan, Meier, and Newport (1989) demonstrated that languages that do not permit pronouns as substitutes for phrases also provide evidence in favor of the marker hypothesis.', 'Juola\'s (1994Juola\'s ( , 1998 work on grammar optimization and induction shows that context-free grammars can be converted to ""marker-normal form.""', 'However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word.', 'Nevertheless, Juola (1998, page 23) observes that ""a slightly more general mapping, where two adjacent terminal symbols can be merged into a single lexical item (for example, a word and its case-marking), can capture this sort of result quite handily.""', 'Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated.', 'Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #AUTHOR_TAG to permit a limited form of insertion in the translation process .', 'As a byproduct of the chosen methodology, we also derive a standard ""word-level"" translation lexicon.', 'These various resources render the set of original translation pairs far more useful in deriving translations of previously unseen input.']",5,"['* language learning (Green 1979;Mori and Moeser 1983;Morgan, Meier, and Newport 1989) * monolingual grammar induction (Juola 1998) * grammar optimization (Juola 1994) * insights into universal grammar (Juola 1998) * machine translation (Juola 1994(Juola , 1997Veale and Way 1997;Gough, Way, and Hearne 2002) With respect to translation, a potential problem in using the marker hypothesis is that some languages do not have marker words such as articles, for instance.', ""Green's (1979) work showed that artificial languages, both with and without specific marker words, may be learned more accurately and quickly if such psycholinguistic cues exist."", 'Juola\'s (1994Juola\'s ( , 1998 work on grammar optimization and induction shows that context-free grammars can be converted to ""marker-normal form.""', 'However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word.', 'Nevertheless, Juola (1998, page 23) observes that ""a slightly more general mapping, where two adjacent terminal symbols can be merged into a single lexical item (for example, a word and its case-marking), can capture this sort of result quite handily.""', 'Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated.', 'Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #AUTHOR_TAG to permit a limited form of insertion in the translation process .', 'As a byproduct of the chosen methodology, we also derive a standard ""word-level"" translation lexicon.', 'These various resources render the set of original translation pairs far more useful in deriving translations of previously unseen input.']"
CC408,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,gaijin a bootstrapping templatedriven approach to examplebased machine translation,"['Tony Veale', 'Andy Way']",introduction,,"In their Gaijin system , #AUTHOR_TAG give a result of 63 % accurate translations obtained for English â > German on a test set of 791 sentences from CorelDRAW manuals .","['Given that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks.', 'Juola (1994Juola ( , 1997 conducts some small experiments using his METLA system to show the viability of this approach for English −→ French and English −→ Urdu.', 'For the English −→ French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data.', 'For English −→ Urdu, Juola (1997, page 213) notes that ""the system learned the original training corpus . . .', 'perfectly and could reproduce it without errors""; that is, it scored 100% accuracy when tested against the training corpus.', 'On novel test sentences, he gives results of 72% correct translation.', 'In their Gaijin system , #AUTHOR_TAG give a result of 63 % accurate translations obtained for English â\x88\x92 > German on a test set of 791 sentences from CorelDRAW manuals .']",1,"['In their Gaijin system , #AUTHOR_TAG give a result of 63 % accurate translations obtained for English â\x88\x92 > German on a test set of 791 sentences from CorelDRAW manuals .']"
CC409,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,the role of syntax markers and semantic referents in learning an artificial language,"['Kazuo Mori', 'Shannon Moeser']",introduction,,"â¢ language learning ( Green 1979 ; #AUTHOR_TAG ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction ( Juola 1998 ) â¢ grammar optimization ( Juola 1994 ) â¢ insights into universal grammar ( Juola 1998 ) â¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )","['â\x80¢ language learning ( Green 1979 ; #AUTHOR_TAG ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']",0,"['â\x80¢ language learning ( Green 1979 ; #AUTHOR_TAG ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']"
CC410,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,on psycholinguistic grammars,['Patrick Juola'],introduction,"It has long been known that language acquisition is only possible if information is available above and beyond the mere presence of a set of strings in the language. One commonly postulated source of such information is a (possibly innate) constraint on the syntactic forms that a grammar can take. This paper develops and presents a set of formalisms based on the Marker Hypothesis that natural languages are ""marked"" for complex syntactic structure at surface form. It further compares the expressivity and restrictedness of these formalisms and shows that, first, not all constraints are actually restrictive, and second, that the Marker Hypothesis, and its implicit function/content word distinction, provide strong restrictions on the form of allowable grammars. These restrictions may in turn provide evidence about its actual psychological reality and salience. In particular, the class of strongly marked languages can be demonstrated not to admit all finite languages and thus not be subject to the hangman's noose of Gold's learnability proofs, and it is conjectured that these languages may provide a computable method of inferring human-like languages.","â¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction ( #AUTHOR_TAG ) â¢ grammar optimization ( Juola 1994 ) â¢ insights into universal grammar ( Juola 1998 ) â¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )","['â\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( #AUTHOR_TAG ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']",0,"['â\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( #AUTHOR_TAG ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']"
CC411,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,corpusbased acquisition of transfer functions using psycholinguistic principles,['Patrick Juola'],introduction,,"For English â\x88\x92 > Urdu , #AUTHOR_TAG , page 213 ) notes that ""the system learned the original training corpus ... perfectly and could reproduce it without errors  ; that is , it scored 100 % accuracy when tested against the training corpus .","['Given that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of _source, target_ chunks.', 'Juola (1994, 1997) conducts some small experiments using his METLA system to show the viability of this approach for English __ French and English __ Urdu.', 'For the English __ French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data.', 'For English â\x88\x92 > Urdu , #AUTHOR_TAG , page 213 ) notes that ""the system learned the original training corpus ... perfectly and could reproduce it without errors  ; that is , it scored 100 % accuracy when tested against the training corpus .', 'On novel test sentences, he gives results of 72% correct translation.', 'In their Gaijin system, Veale and Way (1997) give a result of 63% accurate translations obtained for English __ German on a test set of 791 sentences from CorelDRAW manuals.']",0,"['For English â\\x88\\x92 > Urdu , #AUTHOR_TAG , page 213 ) notes that ""the system learned the original training corpus ... perfectly and could reproduce it without errors  ; that is , it scored 100 % accuracy when tested against the training corpus .']"
CC412,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,inducing translation templates for examplebased machine translation,['Michael Carl'],introduction,"This paper describes an example-based machine translation (EBMT) system which relays on various knowledge resources. Morphologic analyses abstract the surface forms of the languages to be translated. A shallow syntactic rule formalism is used to percolate features in derivation trees. Translation examples serve the decomposition of the text to be translated and determine the transfer of lexical values into the target language. Translation templates determine the word order of the target language and the type of phrases (e.g. noun phrase, prepositional phase, ...) to be generated in the target language. An induction mechanism generalizes translation templates from translation examples. The paper outlines the basic idea underlying the EBMT system and investigates the possibilities and limits of the translation template induction process.","Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , #AUTHOR_TAG , and Brown ( 2000 ) , inter alia .","['Block distinguishes chunks from ""patterns,"" as we do: His chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', 'Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', 'Watanabe (1993) combines lexical and dependency mappings to form his generalizations.', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , #AUTHOR_TAG , and Brown ( 2000 ) , inter alia .']",0,"['Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', 'Watanabe (1993) combines lexical and dependency mappings to form his generalizations.', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , #AUTHOR_TAG , and Brown ( 2000 ) , inter alia .']"
CC413,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,examplebased incremental synchronous interpretation,['Hans-Ulrich Block'],introduction,"This article describes a new approach to example based incremental translation for automatic interpretation systems developed in Verbmobil. The translation module is completely learned from a bilingual corpus. The training phase combines statistical word alignment with precomputation of translation ""chunks"" and contextual clustering of syntactic equivalence classes (word classes). The system gives incremental output for every piece of input being it words or sequences of words. It thus tries to mimic the behaviour of a human synchronous interpreter. If a larger context leads to the need for reformulation the system utters a correction marker like I mean, and restarts the output from the starting position of the reformulation. The system is currently effective for German = English. German = Chinese and German a Japanese are under construction. In the Verbmobil evaluation, this approach reached 79% of approximately correct translations on speech recognition output.","This is then generalized , following a methodology based on #AUTHOR_TAG , to generate the `` generalized marker lexicon . ''","['In this section, we describe how the memory of our EBMT system is seeded with a set of translations obtained from Web-based MT systems.', 'From this initial resource, we subsequently derive a number of different databases that together allow many new input sentences to be translated that it would not be possible to translate in other systems.', 'First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon.', ""This is then generalized , following a methodology based on #AUTHOR_TAG , to generate the `` generalized marker lexicon . ''"", 'Finally, as a result of the We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www.up.univ-mrs.fr/˜veronis/biblios/ptp.htm.', 'methodology chosen, we automatically derive a fourth resource, namely, a ""word-level lexicon.""']",5,"['From this initial resource, we subsequently derive a number of different databases that together allow many new input sentences to be translated that it would not be possible to translate in other systems.', 'First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon.', ""This is then generalized , following a methodology based on #AUTHOR_TAG , to generate the `` generalized marker lexicon . ''"", 'Finally, as a result of the We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www.up.univ-mrs.fr/~veronis/biblios/ptp.htm.', 'methodology chosen, we automatically derive a fourth resource, namely, a ""word-level lexicon.""']"
CC414,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,hybrid language processing in the spoken language translator,"['Manny Rayner', 'David Carter']",introduction,"We present an overview of the Spoken Language Translator (SLT) system's hybrid language-processing architecture, focusing on the way in which rule-based and statistical methods are combined to achieve robust and efficient performance within a linguistically motivated framework. In general, we argue that rules are desirable in order to encode domain-independent linguistic constraints and achieve high-quality grammatical output, while corpus-derived statistics are needed if systems are to be efficient and robust; further, that hybrid architectures are superior from the point of view of portability to architectures which only make use of one type of information. We address the topics of ""multi-engine"" strategies for robust translation; robust bottom-up parsing using pruning and grammar specialization; rational development of linguistic rule-sets using balanced domain corpora; and efficient supervised training by interactive disambiguation. All work described is fully implemented in the current version of the SLT-2 system.","â¢ Learnability ( Zernik and Dyer 1987 ) â¢ Text generation ( Hovy 1988 ; Milosavljevic , Tulloch , and Dale 1996 ) â¢ Speech generation ( #AUTHOR_TAG ) â¢ Localization ( Sch Â¨ aler 1996 )","['â\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation ( Hovy 1988 ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation ( #AUTHOR_TAG ) â\x80¢ Localization ( Sch Â¨ aler 1996 )']",0,"['â\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation ( Hovy 1988 ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation ( #AUTHOR_TAG ) â\x80¢ Localization ( Sch Â¨ aler 1996 )']"
CC415,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,the necessity of syntax markers two experiments with artificial languages,['Thomas Green'],introduction,,"Each set of translations is stored separately , and for each set the `` marker hypothesis '' ( #AUTHOR_TAG ) is used to segment the phrasal lexicon into a `` marker lexicon . ''","[""Each set of translations is stored separately , and for each set the `` marker hypothesis '' ( #AUTHOR_TAG ) is used to segment the phrasal lexicon into a `` marker lexicon . ''"", 'The marker hypothesis is a universal psycholinguistic constraint which states that natural languages are ""marked"" for complex syntactic structure at surface form by a closed set of specific lexemes and morphemes.', 'That is, a basic phrase-level segmentation of an input sentence can be achieved by exploiting a closed list of known marker words to signal the start and end of each segment.']",5,"[""Each set of translations is stored separately , and for each set the `` marker hypothesis '' ( #AUTHOR_TAG ) is used to segment the phrasal lexicon into a `` marker lexicon . ''"", 'The marker hypothesis is a universal psycholinguistic constraint which states that natural languages are ""marked"" for complex syntactic structure at surface form by a closed set of specific lexemes and morphemes.']"
CC416,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,examplebased incremental synchronous interpretation,['Hans-Ulrich Block'],introduction,"This article describes a new approach to example based incremental translation for automatic interpretation systems developed in Verbmobil. The translation module is completely learned from a bilingual corpus. The training phase combines statistical word alignment with precomputation of translation ""chunks"" and contextual clustering of syntactic equivalence classes (word classes). The system gives incremental output for every piece of input being it words or sequences of words. It thus tries to mimic the behaviour of a human synchronous interpreter. If a larger context leads to the need for reformulation the system utters a correction marker like I mean, and restarts the output from the starting position of the reformulation. The system is currently effective for German = English. German = Chinese and German a Japanese are under construction. In the Verbmobil evaluation, this approach reached 79% of approximately correct translations on speech recognition output.","That is, where #AUTHOR_TAG substitutes variables for various words in his templates, we replace certain lexical items with their marker tag.","['That is, where #AUTHOR_TAG substitutes variables for various words in his templates, we replace certain lexical items with their marker tag.', 'Given that examples such as ��<DET> a : un� are likely to exist in the word-level lexicon, they may be inserted at the point indicated by the marker tag to form the correct translation un bon homme.', 'We thus cluster on marker words to improve the coverage of our system (see Section 5 for results that show exactly how clustering on marker words helps); others (notably Brown [2000, 2003]) use clustering techniques to determine equivalence classes of individual words that can occur in the same context, and in so doing derive translation templates from individual translation examples.']",1,"['That is, where #AUTHOR_TAG substitutes variables for various words in his templates, we replace certain lexical items with their marker tag.', 'Given that examples such as <DET> a : un are likely to exist in the word-level lexicon, they may be inserted at the point indicated by the marker tag to form the correct translation un bon homme.', 'We thus cluster on marker words to improve the coverage of our system (see Section 5 for results that show exactly how clustering on marker words helps); others (notably Brown [2000, 2003]) use clustering techniques to determine equivalence classes of individual words that can occur in the same context, and in so doing derive translation templates from individual translation examples.']"
CC417,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,on psycholinguistic grammars,['Patrick Juola'],introduction,"It has long been known that language acquisition is only possible if information is available above and beyond the mere presence of a set of strings in the language. One commonly postulated source of such information is a (possibly innate) constraint on the syntactic forms that a grammar can take. This paper develops and presents a set of formalisms based on the Marker Hypothesis that natural languages are ""marked"" for complex syntactic structure at surface form. It further compares the expressivity and restrictedness of these formalisms and shows that, first, not all constraints are actually restrictive, and second, that the Marker Hypothesis, and its implicit function/content word distinction, provide strong restrictions on the form of allowable grammars. These restrictions may in turn provide evidence about its actual psychological reality and salience. In particular, the class of strongly marked languages can be demonstrated not to admit all finite languages and thus not be subject to the hangman's noose of Gold's learnability proofs, and it is conjectured that these languages may provide a computable method of inferring human-like languages.","Nevertheless , #AUTHOR_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . ''","['• language learning (Green 1979;Mori and Moeser 1983;Morgan, Meier, and Newport 1989) • monolingual grammar induction (Juola 1998) • grammar optimization (Juola 1994) • insights into universal grammar (Juola 1998) • machine translation (Juola 1994(Juola , 1997Veale and Way 1997;Gough, Way, and Hearne 2002) With respect to translation, a potential problem in using the marker hypothesis is that some languages do not have marker words such as articles, for instance.', ""Green's (1979) work showed that artificial languages, both with and without specific marker words, may be learned more accurately and quickly if such psycholinguistic cues exist."", 'The research of Mori and Moeser (1983) showed a similar effect due to case marking on pseudowords in such artificial languages, and Morgan, Meier, and Newport (1989) demonstrated that languages that do not permit pronouns as substitutes for phrases also provide evidence in favor of the marker hypothesis.', 'Juola\'s (1994Juola\'s ( , 1998 work on grammar optimization and induction shows that context-free grammars can be converted to ""marker-normal form.""', 'However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word.', ""Nevertheless , #AUTHOR_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . ''"", 'Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated.', 'Following construction of the marker lexicon, the source, target chunks are generalized further using a methodology based on Block (2000) to permit a limited form of insertion in the translation process.', 'As a byproduct of the chosen methodology, we also derive a standard ""word-level"" translation lexicon.', 'These various resources render the set of original translation pairs far more useful in deriving translations of previously unseen input.']",0,"[""Green's (1979) work showed that artificial languages, both with and without specific marker words, may be learned more accurately and quickly if such psycholinguistic cues exist."", ""Nevertheless , #AUTHOR_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . ''""]"
CC418,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,automated generalization of translation examples,['Ralf Brown'],introduction,"Previous work has shown that adding generalization of the examples in the corpus of an example-based machine translation (EBMT) system can reduce the required amount of pretranslated example text by as much as an order of magnitude for Spanish-English and FrenchEnglish EBMT. Using word clustering to automatically generalize the example corpus can provide the majority of this improvement for French-English with no manual intervention; the prior work required a large bilingual dictionary tagged with parts of speech and the manual creation of grammar rules. By seeding the clustering with a small amount of manuallycreated information, even better performance can be achieved. This paper describes a method whereby bilingual word clustering can be performed using standard monolingual document clustering techniques, and its effectiveness at reducing the size of the example corpus required.  1 Introduction  Example-Based Machine Translation (EBMT) relies on a collection of textual units (usuall..","Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , Carl ( 1999 ) , and #AUTHOR_TAG , inter alia .","['Block distinguishes chunks from ""patterns,"" as we do: His chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', 'Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', 'Watanabe (1993) combines lexical and dependency mappings to form his generalizations.', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , Carl ( 1999 ) , and #AUTHOR_TAG , inter alia .']",0,"['Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', 'Watanabe (1993) combines lexical and dependency mappings to form his generalizations.', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , Carl ( 1999 ) , and #AUTHOR_TAG , inter alia .']"
CC419,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,the phrasal lexicon,['Joseph Becker'],introduction,"Theoretical linguists have in recent years concentrated their attention on the productive aspect of language, wherein utterances are formed combinatorically from units the size of words or smaller. This paper will focus on the contrary aspect of language, wherein utterances are formed by repetition, modification, and concatenation of previously-known phrases consisting of more than one word. I suspect that we speak mostly by stitching together swatches of text that we have heard before; productive processes have the secondary role of adapting the old phrases to the new situation. The advantage of this point of view is that it has the potential to account for the observed linguistic behavior of native speakers, rather than discounting their actual behavior as irrelevant to their language. In particular, this point of view allows us to concede that most utterances are produced in stereotyped social situations, where the communicative and ritualistic functions of language demand not novelty, but rather an appropriate combination of formulas, cliches, idioms, allusions, slogans, and so forth. Language must have originated in such constrained social contexts, and they are still the predominant arena for language production. Therefore an understanding of the use of phrases is basic to the understanding of language as a whole.You are currently reading a much-abridged version of a paper that will be published elsewhere later.","More specifically , the notion of the phrasal lexicon ( used first by #AUTHOR_TAG ) has been used successfully in a number of areas :","['All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Röscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.', 'The respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.', 'More specifically , the notion of the phrasal lexicon ( used first by #AUTHOR_TAG ) has been used successfully in a number of areas :']",0,"['More specifically , the notion of the phrasal lexicon ( used first by #AUTHOR_TAG ) has been used successfully in a number of areas :']"
CC420,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,a framework of a mechanical translation between japanese and english by analogy principle,['Makoto Nagao'],introduction,"Problems inherent in current machine translation systems have been reviewed and have been shown to be inherently inconsistent. The present paper defines a model based on a series of human language processing and in particular the use of analogical thinking. Machine translation systems developed so far have a kind of inherent contradiction in themselves. The more detailed a system has become by the additional improvements, the clearer the limitation and the boundary will be for the translation ability. To break through this difficulty we have to think about the mechanism of human translation, and have to build a model based on the fundamental function of language processing in the human brain. The following is an attempt to do this based on the ability of analogy finding in human beings.","All EBMT systems , from the initial proposal by #AUTHOR_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .","['All EBMT systems , from the initial proposal by #AUTHOR_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Röscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.', 'The respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.', 'More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:']",0,"['All EBMT systems , from the initial proposal by #AUTHOR_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Roscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.', 'The respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.', 'More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:']"
CC421,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,a method for extracting translation patterns from translation examples,['Hideo Watanabe'],introduction,,#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .,"['Block distinguishes chunks from ""patterns,"" as we do: His chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', 'Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', '#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .', 'Other similar approaches include those of Cicekli and Güvenir (1996), McTait and Trujillo (1999), Carl (1999), and Brown (2000, inter alia.']",0,"['Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', '#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .']"
CC422,J04-3001,Sample Selection for Statistical Parsing,elements of information theory,"['Thomas M Cover', 'Joy A Thomas']",,"Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index.","Further details about the properties of entropy can be found in textbooks on information theory ( e.g. , #AUTHOR_TAG ) .","['where V is a random variable that can take any possible outcome in set V, and p(v) = Pr(V = v) is the density function.', 'Further details about the properties of entropy can be found in textbooks on information theory ( e.g. , #AUTHOR_TAG ) .', 'Determining the parse tree for a sentence from a set of possible parses can be viewed as assigning a value to a random variable.', 'Thus, a direct application of the entropy definition to the probability distribution of the parses for sentence w in G computes its tree entropy, TE(w, G), the expected number of bits needed to encode the distribution of possible parses for w.', 'However, we may not wish to compare two sentences with different numbers of parses by their entropy directly.', 'If the parse probability distributions for both sentences are uniform, the sentence with more parses will have a higher entropy.', 'Because longer sentences typically have more parses, using entropy directly would result in a bias toward selecting long sentences.', 'To normalize for the number of parses, the uncertainty-based evaluation function, f unc , is defined as a measurement of similarity between the actual probability distribution of the parses and a hypothetical uniform distribution for that set of parses.', 'In particular, we divide the tree entropy by the log of the number of parses: 10']",0,"['Further details about the properties of entropy can be found in textbooks on information theory ( e.g. , #AUTHOR_TAG ) .']"
CC423,J04-3001,Sample Selection for Statistical Parsing,applying cotraining methods to statistical parsing,['Anoop Sarkar'],related work,"We propose a novel Co-Training method for statistical parsing. The algorithm takes as input a small corpus (9695 sentences) annotated with parse trees, a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text. The algorithm iteratively labels the entire data set with parse trees. Using empirical results based on parsing the Wall Street Journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly out-performs training only on the labeled data.","The work of #AUTHOR_TAG and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing .","['Aside from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing.', 'For example, Henderson and Brill (2002) consider the case in which acquiring additional human-annotated training data is not possible.', 'They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers.', 'This approach assumes that there are enough existing labeled data to train the individual parsers.', 'Another technique for making better use of unlabeled data is cotraining (Blum and Mitchell 1998), in which two sufficiently different learners help each other learn by labeling training data for one another.', 'The work of #AUTHOR_TAG and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing .', 'Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training.', 'Similar approaches are being explored for parsing Hwa et al. 2003).']",0,"['Aside from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing.', 'They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers.', 'This approach assumes that there are enough existing labeled data to train the individual parsers.', 'The work of #AUTHOR_TAG and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing .', 'Similar approaches are being explored for parsing Hwa et al. 2003).']"
CC424,J04-3001,Sample Selection for Statistical Parsing,heterogeneous uncertainty sampling for supervised learning,"['David D Lewis', 'Jason Catlett']",related work,"Uncertainty sampling methods iteratively request class labels for training instances whose classes are uncertain despite the previous labeled instances. These methods can greatly reduce the number of instances that an expert need label. One problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instances. We test the use of one classifier (a highly efficient probabilistic one) to select examples for training another (the C4.5 rule induction program). Despite being chosen by this heterogeneous approach, the uncertainty samples yielded classifiers with lower error rates than random samples ten times larger.","Some examples include text categorization ( #AUTHOR_TAG ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) .","['Sample selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization ( #AUTHOR_TAG ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) .']",0,"['Sample selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization ( #AUTHOR_TAG ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) .']"
CC425,J04-3001,Sample Selection for Statistical Parsing,combining labeled and unlabeled data with cotraining,"['Avrim Blum', 'Tom Mitchell']",related work,,"Another technique for making better use of unlabeled data is cotraining ( #AUTHOR_TAG ) , in which two sufficiently different learners help each other learn by labeling training data for one another .","['Aside from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing.', 'For example, Henderson and Brill (2002) consider the case in which acquiring additional human-annotated training data is not possible.', 'They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers.', 'This approach assumes that there are enough existing labeled data to train the individual parsers.', 'Another technique for making better use of unlabeled data is cotraining ( #AUTHOR_TAG ) , in which two sufficiently different learners help each other learn by labeling training data for one another .', 'The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing.', 'Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training.', 'Similar approaches are being explored for parsing Hwa et al. 2003).']",0,"['Another technique for making better use of unlabeled data is cotraining ( #AUTHOR_TAG ) , in which two sufficiently different learners help each other learn by labeling training data for one another .']"
CC426,J04-3001,Sample Selection for Statistical Parsing,stochastic lexicalized contextfree grammar,"['Yves Schabes', 'Richard Waters']",,"Stochastic lexicalized context-free grammar (SLCFG) is an attractive compromise between the parsing efficiency of stochastic context-free grammar (SCFG) and the lexical sensitivity of stochastic lexicalized tree-adjoining grammar (SLTAG) . SLCFG is a restricted form of SLTAG that can only generate context-free languages and can be parsed in cubic time. However, SLCFG retains the lexical sensitivity of SLTAG and is therefore a much better basis for capturing distributional information about words than SCFG.","Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #AUTHOR_TAG ; Hwa 1998 ) , and Collins 's Model 2 parser ( 1997 ) .","[""In applying sample selection to training a PP-attachment model, we have observed that all effective evaluation functions make use of the model's current hypothesis in estimating the training utility of the candidates."", 'Although knowledge about the problem space seems to help sharpening the learning curve initially, overall, it is not a good predictor.', 'In this section, we investigate whether these observations hold true for training statistical parsing models as well.', ""Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #AUTHOR_TAG ; Hwa 1998 ) , and Collins 's Model 2 parser ( 1997 ) ."", 'Although both models are lexicalized, statistical parsers, their learning algorithms are different.', 'The Collins Parser is a fully supervised, history-based learner that models the parameters of the parser by taking statistics directly from the training data.', ""In contrast, PLTIG's expectation-maximization-based induction algorithm is partially supervised; the model's parameters are estimated indirectly from the training data.""]",5,"[""Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #AUTHOR_TAG ; Hwa 1998 ) , and Collins 's Model 2 parser ( 1997 ) ."", 'Although both models are lexicalized, statistical parsers, their learning algorithms are different.']"
CC427,J04-3001,Sample Selection for Statistical Parsing,prepositional phrase attachment through a backedoff model,"['Michael Collins', 'James Brooks']",,"Recent work has considered corpus-based or statistical approaches to the problem of prepositional phrase attachment ambiguity. Typically, ambiguous verb phrases of the form v np1 p np2 are resolved through a model which considers values of the four head words (v, n1, p and n2). This paper shows that the problem is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable. Results on Wall Street Journal data of 84.5% accuracy are obtained using this method. A surprising result is the importance of low-count events -- ignoring events which occur less than 5 times in training data reduces performance to 81.6%.","Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( #AUTHOR_TAG ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .","['One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( #AUTHOR_TAG ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .', 'Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model.', 'We use the Collins-Brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier.']",0,"['One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( #AUTHOR_TAG ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .', 'Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model.']"
CC428,J04-3001,Sample Selection for Statistical Parsing,scaling to very very large corpora for natural language disambiguation,"['Michele Banko', 'Eric Brill']",related work,"The amount of readily available online  text has reached hundreds of  billions of words and continues to  grow. Yet for most core natural  language tasks, algorithms continue  to be optimized, tested and compared  after training on corpora consisting  of only one million words or less. In  this paper, we evaluate the  performance of different learning  methods on a prototypical natural  language disambiguation task,  confusion set disambiguation, when  trained on orders of magnitude more  labeled data than has previously been  used. We are fortunate that for this  particular application, correctly  labeled training data is free. Since  this will often not be the case, we  examine methods for effectively  exploiting very large corpora when  labeled data comes at a cost","Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( #AUTHOR_TAG ) , and word sense disambiguation ( Fujii et al. 1998 ) .","['Sample selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( #AUTHOR_TAG ) , and word sense disambiguation ( Fujii et al. 1998 ) .']",0,"['Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( #AUTHOR_TAG ) , and word sense disambiguation ( Fujii et al. 1998 ) .']"
CC429,J04-3001,Sample Selection for Statistical Parsing,the estimation of stochastic contextfree grammars using the insideoutside algorithm computer speech and language,"['Karim A Lari', 'Steve J Young']",,,"Using the bottom-up , dynamic programming technique ( see the appendix for details ) of computing inside probabilities ( #AUTHOR_TAG ) , we can efficiently compute the probability of the sentence , P ( w | G ) .","['Using the bottom-up , dynamic programming technique ( see the appendix for details ) of computing inside probabilities ( #AUTHOR_TAG ) , we can efficiently compute the probability of the sentence , P ( w | G ) .', 'Similarly, the algorithm can be modified to compute the quantity']",5,"['Using the bottom-up , dynamic programming technique ( see the appendix for details ) of computing inside probabilities ( #AUTHOR_TAG ) , we can efficiently compute the probability of the sentence , P ( w | G ) .']"
CC430,J04-3001,Sample Selection for Statistical Parsing,insideoutside reestimation from partially bracketed corpora,"['Fernando C N Pereira', 'Yves Schabes']",,"The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modelling of hierarchical structure than the original one. In particular, over 90% of the constituents in the most likely analyses of a test set are compatible with test set constituents for a grammar trained on a corpus of 700 hand-parsed part-of-speech strings for ATIS sentences.",Our algorithm is similar to the approach taken by #AUTHOR_TAG for inducing PCFG parsers .,"['In the first experiment, we use an induction algorithm (Hwa 2001a) based on the expectation-maximization (EM) principle that induces parsers for PLTIGs.', ""The algorithm performs heuristic search through an iterative reestimation procedure to find local optima: sets of values for the grammar parameters that maximizes the grammar's likelihood of generating the training data."", 'In principle, the algorithm supports unsupervised learning; however, because the search space has too many local optima, the algorithm tends to converge on a model that is unsuitable for parsing.', 'Here, we consider a partially supervised variant in which we assume that the learner is given the phrasal boundaries of the training sentences but not the label of the constituent units.', 'For example, the sentence Several fund managers expect a rough market this morning before prices stabilize.', 'would be labeled as ""((Several fund managers) (expect ((a rough market) (this morning)) (before (prices stabilize))).)""', 'Our algorithm is similar to the approach taken by #AUTHOR_TAG for inducing PCFG parsers .']",1,"['In the first experiment, we use an induction algorithm (Hwa 2001a) based on the expectation-maximization (EM) principle that induces parsers for PLTIGs.', ""The algorithm performs heuristic search through an iterative reestimation procedure to find local optima: sets of values for the grammar parameters that maximizes the grammar's likelihood of generating the training data."", 'In principle, the algorithm supports unsupervised learning; however, because the search space has too many local optima, the algorithm tends to converge on a model that is unsuitable for parsing.', 'Our algorithm is similar to the approach taken by #AUTHOR_TAG for inducing PCFG parsers .']"
CC431,J04-3001,Sample Selection for Statistical Parsing,insideoutside reestimation from partially bracketed corpora,"['Fernando C N Pereira', 'Yves Schabes']",introduction,"The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modelling of hierarchical structure than the original one. In particular, over 90% of the constituents in the most likely analyses of a test set are compatible with test set constituents for a grammar trained on a corpus of 700 hand-parsed part-of-speech strings for ATIS sentences.","For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data ( #AUTHOR_TAG ) .","['Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data.', 'For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data ( #AUTHOR_TAG ) .', 'Current state-of-the-art statistical parsers (Collins 1999;Charniak 2000) are all trained on large annotated corpora such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993).', 'However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest.', 'For example, one might need lexical-semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora.', 'Because supervised training demands significant human involvement (e.g., annotating the syntactic structure of each sentence by hand), creating a new corpus is a labor-intensive and time-consuming endeavor.', ""The goal of this work is to minimize a system's reliance on annotated training data.""]",0,"['For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data ( #AUTHOR_TAG ) .']"
CC432,J04-3001,Sample Selection for Statistical Parsing,rule writing or annotation costefficient resource usage for base noun phrase chunking,"['Grace Ngai', 'David Yarowsky']",related work,"This paper presents a comprehensive empirical comparison between two approaches for developing a base noun phrase chunker: human rule writing and active learning using interactive real-time human annotation. Several novel variations on active learning are investigated, and underlying cost models for cross-modal machine learning comparison are presented and explored. Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment.","Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( #AUTHOR_TAG ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) .","['Sample selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( #AUTHOR_TAG ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) .']",0,"['Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( #AUTHOR_TAG ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) .']"
CC433,J04-3001,Sample Selection for Statistical Parsing,an empirical evaluation of probabilistic lexicalized tree insertion grammars,['Rebecca Hwa'],,"We present an empirical study of the applicability of Probabilistic Lexicalized Tree Insertion Grammars (PLTIG), a lexicalized counterpart to Probabilistic Context-Free Grammars (PCFG), to problems in stochastic natural-language processing. Comparing the performance of PLTIGs, with non-hierarchical N-gram models and PCFGs, we show that PLTIG combines the best aspects of both, with language modeling capability comparable to N-gram models and PCFGs, we show that PLTIG combines the best aspects of both, with language modeling capability comparable to N-grams, and improved parsing performance over its nonlexicalized counterpart. Furthermore, training of PLTIGs displays faster convergence than PCFGs.","Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( Schabes and Waters 1993 ; #AUTHOR_TAG ) , and Collins 's Model 2 parser ( 1997 ) .","[""In applying sample selection to training a PP-attachment model, we have observed that all effective evaluation functions make use of the model's current hypothesis in estimating the training utility of the candidates."", 'Although knowledge about the problem space seems to help sharpening the learning curve initially, overall, it is not a good predictor.', 'In this section, we investigate whether these observations hold true for training statistical parsing models as well.', ""Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( Schabes and Waters 1993 ; #AUTHOR_TAG ) , and Collins 's Model 2 parser ( 1997 ) ."", 'Although both models are lexicalized, statistical parsers, their learning algorithms are different.', 'The Collins Parser is a fully supervised, history-based learner that models the parameters of the parser by taking statistics directly from the training data.', ""In contrast, PLTIG's expectation-maximization-based induction algorithm is partially supervised; the model's parameters are estimated indirectly from the training data.""]",5,"[""Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( Schabes and Waters 1993 ; #AUTHOR_TAG ) , and Collins 's Model 2 parser ( 1997 ) .""]"
CC434,J04-3001,Sample Selection for Statistical Parsing,limitations of cotraining for natural language learning from large datasets,"['David Pierce', 'Claire Cardie']",related work,"Co-Training is a weakly supervised learning paradigm in which the redundancy of the learning task is captured by training two classifiers using separate views of the same data. This enables bootstrapping from a small set of labeled training data via a large set of unlabeled data. This study examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels. Using base noun phrase bracketing as a case study, we find that co-training reduces by 36% the difference in error between co-trained classifiers and fully supervised classifiers trained on a labeled version of all available data. However, degradation in the quality of the bootstrapped data arises as an obstacle to further improvement. To address this, we propose a moderately supervised variant of cotraining in which a human corrects the mistakes made during automatic labeling. Our analysis suggests that corrected co-training and similar moderately supervised methods may help cotraining scale to large natural language learning tasks.","#AUTHOR_TAG have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for large-scale training .","['Aside from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing.', 'For example, Henderson and Brill (2002) consider the case in which acquiring additional human-annotated training data is not possible.', 'They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers.', 'This approach assumes that there are enough existing labeled data to train the individual parsers.', 'Another technique for making better use of unlabeled data is cotraining (Blum and Mitchell 1998), in which two sufficiently different learners help each other learn by labeling training data for one another.', 'The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing.', '#AUTHOR_TAG have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for large-scale training .', 'Similar approaches are being explored for parsing Hwa et al. 2003).']",0,"['For example, Henderson and Brill (2002) consider the case in which acquiring additional human-annotated training data is not possible.', '#AUTHOR_TAG have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for large-scale training .']"
CC435,J04-3001,Sample Selection for Statistical Parsing,headdriven statistical models for natural language parsing,['Michael Collins'],introduction,"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.","Current state-of-the-art statistical parsers ( #AUTHOR_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) .","['Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data.', 'For example, while it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data (Pereira and Schabes 1992).', 'Current state-of-the-art statistical parsers ( #AUTHOR_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) .', 'However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest.', 'For example, one might need lexical-semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora.', 'Because supervised training demands significant human involvement (e.g., annotating the syntactic structure of each sentence by hand), creating a new corpus is a labor-intensive and time-consuming endeavor.', ""The goal of this work is to minimize a system's reliance on annotated training data.""]",0,"['Current state-of-the-art statistical parsers ( #AUTHOR_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) .']"
CC436,J04-3001,Sample Selection for Statistical Parsing,a rule based approach to pp attachment disambiguation,"['Eric Brill', 'Philip S Resnik']",,,"Some well-known approaches include rule-based models ( #AUTHOR_TAG ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .","['One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( #AUTHOR_TAG ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .', 'Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model.', 'We use the Collins-Brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier.']",0,"['One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( #AUTHOR_TAG ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .', 'Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model.']"
CC437,J04-3001,Sample Selection for Statistical Parsing,learning probabilistic lexicalized grammars for natural language processing,['Rebecca Hwa'],,"A good representation of language is essential to building natural language processing (NLP) applications. In recent years, the growing availability of machine-readable text corpora has popularized the use of corpus-trained probabilistic grammars to represent languages in NLP systems. Although automatically inducing grammars from large corpora is an appealing idea, it faces several challenges. First, the trained grammar must capture the complexity and ambiguities inherent in human languages. Second, to be useful in practical applications, the grammar must be computationally tractable. Third, although there exists an abundance of raw text, the induction of high-quality grammars depends on manually annotated training data, which are scarce; therefore, the learning algorithm must be able to generalize well. Finally, there are inherent trade-offs in attempting to meet all three challenges; a meta-level challenge is to find a good compromise between the competing factors.  To address these challenges, this thesis presents a partially supervised induction algorithm based on the Expectation-Maximization principle for the Probabilistic Lexicalized Tree Insertion Grammar (PLTIG) formalism. Using the lexical properties of the PLTIG formalism in the learning algorithm, we show that it is possible to automatically induce a grammar for a natural language that adequately resolves ambiguities and manages domain complexity at a tractable computational cost. By augmenting the basic learning algorithm with training techniques such as grammar adaptation and sample-selection, we show that the induction's dependency on annotated training data can be significantly reduced. Our empirical studies indicate that even with a 36% reduction in annotated training data, the learning algorithm can nonetheless induce grammars without degrading their quality.","In the first experiment , we use an induction algorithm ( #AUTHOR_TAGa ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs .","['In the first experiment , we use an induction algorithm ( #AUTHOR_TAGa ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs .', ""The algorithm performs heuristic search through an iterative reestimation procedure to find local optima: sets of values for the grammar parameters that maximizes the grammar's likelihood of generating the training data."", 'In principle, the algorithm supports unsupervised learning; however, because the search space has too many local optima, the algorithm tends to converge on a model that is unsuitable for parsing.', 'Here, we consider a partially supervised variant in which we assume that the learner is given the phrasal boundaries of the training sentences but not the label of the constituent units.', 'For example, the sentence Several fund managers expect a rough market this morning before prices stabilize.', 'would be labeled as ""((Several fund managers) (expect ((a rough market) (this morning)) (before (prices stabilize))).)""', 'Our algorithm is similar to the approach taken by Pereira and Schabes (1992) for inducing PCFG parsers.']",5,"['In the first experiment , we use an induction algorithm ( #AUTHOR_TAGa ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs .', ""The algorithm performs heuristic search through an iterative reestimation procedure to find local optima: sets of values for the grammar parameters that maximizes the grammar's likelihood of generating the training data."", 'In principle, the algorithm supports unsupervised learning; however, because the search space has too many local optima, the algorithm tends to converge on a model that is unsuitable for parsing.', 'Here, we consider a partially supervised variant in which we assume that the learner is given the phrasal boundaries of the training sentences but not the label of the constituent units.']"
CC438,J04-3001,Sample Selection for Statistical Parsing,heterogeneous uncertainty sampling for supervised learning,"['David D Lewis', 'Jason Catlett']",,"Uncertainty sampling methods iteratively request class labels for training instances whose classes are uncertain despite the previous labeled instances. These methods can greatly reduce the number of instances that an expert need label. One problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instances. We test the use of one classifier (a highly efficient probabilistic one) to select examples for training another (the C4.5 rule induction program). Despite being chosen by this heterogeneous approach, the uncertainty samples yielded classifiers with lower error rates than random samples ten times larger.","That is , if the current hypothesis is unable to label a candidate or is uncertain about it , then the candidate might be a good training example ( #AUTHOR_TAG ) .","['Performance of the hypothesis: Testing the candidates on the current working hypothesis shows the type of input data on which the hypothesis may perform weakly.', 'That is , if the current hypothesis is unable to label a candidate or is uncertain about it , then the candidate might be a good training example ( #AUTHOR_TAG ) .', 'The underlying assumption is that an uncertain output is likely to be wrong.']",0,"['Performance of the hypothesis: Testing the candidates on the current working hypothesis shows the type of input data on which the hypothesis may perform weakly.', 'That is , if the current hypothesis is unable to label a candidate or is uncertain about it , then the candidate might be a good training example ( #AUTHOR_TAG ) .', 'The underlying assumption is that an uncertain output is likely to be wrong.']"
CC439,J04-3001,Sample Selection for Statistical Parsing,corrected cotraining for statistical parsers,"['Rebecca Hwa', 'Miles Osborne', 'Anoop Sarkar', 'Mark Steedman']",related work,,"Similar approaches are being explored for parsing ( Steedman , #AUTHOR_TAG ; Hwa et al. 2003 ) .","['Aside from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing.', 'For example, Henderson and Brill (2002) consider the case in which acquiring additional human-annotated training data is not possible.', 'They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers.', 'This approach assumes that there are enough existing labeled data to train the individual parsers.', 'Another technique for making better use of unlabeled data is cotraining (Blum and Mitchell 1998), in which two sufficiently different learners help each other learn by labeling training data for one another.', 'The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing.', 'Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training.', 'Similar approaches are being explored for parsing ( Steedman , #AUTHOR_TAG ; Hwa et al. 2003 ) .']",0,"['The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing.', 'Similar approaches are being explored for parsing ( Steedman , #AUTHOR_TAG ; Hwa et al. 2003 ) .']"
CC440,J04-3001,Sample Selection for Statistical Parsing,natural language parsing as statistical pattern recognition,['David Magerman'],,"Traditional natural language parsers are based on rewrite rule systems developed in an arduous, time-consuming manner by grammarians. A majority of the grammarian's efforts are devoted to the disambiguation process, first hypothesizing rules which dictate constituent categories and relationships among words in ambiguous sentences, and then seeking exceptions and corrections to these rules.  In this work, I propose an automatic method for acquiring a statistical parser from a set of parsed sentences which takes advantage of some initial linguistic input, but avoids the pitfalls of the iterative and seemingly endless grammar development process. Based on distributionally-derived and linguistically-based features of language, this parser acquires a set of statistical decision trees which assign a probability distribution on the space of parse trees given the input sentence. These decision trees take advantage of significant amount of contextual information, potentially including all of the lexical information in the sentence, to produce highly accurate statistical models of the disambiguation process. By basing the disambiguation criteria selection on entropy reduction rather than human intuition, this parser development method is able to consider more sentences than a human grammarian can when making individual disambiguation rules.  In experiments between a parser, acquired using this statistical framework, and a grammarian's rule-based parser, developed over a ten-year period, both using the same training material and test sentences, the decision tree parser significantly outperformed the grammar-based parser on the accuracy measure which the grammarian was trying to maximize, achieving an accuracy of 78% compared to the grammar-based parser's 69%.",The head words can be automatically extracted using a heuristic table lookup in the manner described by #AUTHOR_TAG .,"['The Collins-Brooks PP-attachment classification algorithm.', 'preposition, and the prepositional noun phrase, respectively, and a specifies the attachment classification.', 'For example, (wrote a book in three days, attach-verb) would be annotated as (wrote, book, in, days, verb).', 'The head words can be automatically extracted using a heuristic table lookup in the manner described by #AUTHOR_TAG .', 'For this learning problem, the supervision is the one-bit information of whether p should attach to v or to n.', 'In order to learn the attachment preferences of prepositional phrases, the system builds attachment statistics for each the characteristic tuple of all training examples.', 'A characteristic tuple is some subset of the four head words in the example, with the condition that one of the elements must be the preposition.', 'Each training example forms eight characteristic tuples:']",5,['The head words can be automatically extracted using a heuristic table lookup in the manner described by #AUTHOR_TAG .']
CC441,J04-3001,Sample Selection for Statistical Parsing,statistical models for unsupervised prepositional phrase attachment,['Adwait Ratnaparkhi'],,"We present several unsupervised statistical models for the prepositional phrase attachment task that approach the accuracy of the best supervised methods for this task. Our unsupervised approach uses a heuristic based on attachment proximity and trains from raw text that is annotated with only part-of-speech tags and morphological base forms, as opposed to attachment information. It is therefore less resource-intensive and more portable than previous corpus-based algorithms proposed for this task. We present results for prepositional phrase attachment in both English and Spanish.Comment: uses colacl.st","Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( #AUTHOR_TAG ) .","['One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( #AUTHOR_TAG ) .', 'Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model.', 'We use the Collins-Brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier.']",0,"['One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( #AUTHOR_TAG ) .', 'Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model.']"
CC442,J04-3001,Sample Selection for Statistical Parsing,the estimation of stochastic contextfree grammars using the insideoutside algorithm computer speech and language,"['Karim A Lari', 'Steve J Young']",,,We follow the notation convention of #AUTHOR_TAG .,"['For illustrative purposes, we describe the computation process using a PCFG expressed in Chomsky normal form. 14', 'The basic idea is to compose the tree entropy of the entire sentence from the tree entropy of the subtrees.', 'The process is similar to that for computing the probability of the entire sentence from the probabilities of substrings (called Inside Probabilities).', 'We follow the notation convention of #AUTHOR_TAG .']",5,"['For illustrative purposes, we describe the computation process using a PCFG expressed in Chomsky normal form. 14', 'The basic idea is to compose the tree entropy of the entire sentence from the tree entropy of the subtrees.', 'The process is similar to that for computing the probability of the entire sentence from the probabilities of substrings (called Inside Probabilities).', 'We follow the notation convention of #AUTHOR_TAG .']"
CC443,J04-3001,Sample Selection for Statistical Parsing,selective sampling using the query by committee algorithm,"['Yoav Freund', 'H Sebastian Seung', 'Eli Shamir', 'Naftali Tishby']",,"We analyze the ""query by committee"" algorithm, a method for filtering informative queries from a random stream of inputs. We show that if the two-member committee algorithm achieves information gain with positive lower bound, then the prediction error decreases exponentially with the number of queries. We show that, in particular, this exponential decrease holds for query learning of perceptrons.","The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #AUTHOR_TAG ) .","['Unlike traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set.', 'There are two types of selection algorithms: committee-based and single learner.', 'A committee-based selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem).', 'The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #AUTHOR_TAG ) .', 'For computationally intensive problems, such as parsing, keeping multiple learners may be impractical.']",0,"['Unlike traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set.', 'There are two types of selection algorithms: committee-based and single learner.', 'A committee-based selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem).', 'The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #AUTHOR_TAG ) .', 'For computationally intensive problems, such as parsing, keeping multiple learners may be impractical.']"
CC444,J05-3003,Gaussian coordinates and the large scale universe,duden—das stilworterbuch duden—the style dictionary,['editor Dudenredaktion'],,,"She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden ( #AUTHOR_TAG ) .","['Carroll and Rooth (1998) achieve an F-score of 77% against the OALD when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each.', 'Their system recognizes 15 frames, and these do not contain details of subcategorizedfor prepositions.', 'Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3. Sarkar and Zeman (2000) evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88%.', 'However, their evaluation does not examine the extracted subcategorization frames but rather the argument-adjunct distinctions posited by their system.', 'The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German.', 'She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden ( #AUTHOR_TAG ) .', 'We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3.']",0,"['Carroll and Rooth (1998) achieve an F-score of 77% against the OALD when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each.', 'The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German.', 'She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden ( #AUTHOR_TAG ) .', 'We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3.']"
CC445,J05-3003,Gaussian coordinates and the large scale universe,automatic extraction of subcategorization from corpora,"['Edward Briscoe', 'John Carroll']",,"We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount1.","#AUTHOR_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX .","['Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As noted above, it is well documented (Roland and Jurafsky 1998) that subcategorization frames (and their frequencies) vary across domains.', 'We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ.', 'For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon.', 'It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases.', 'This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs.', 'Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples.', 'As a generalization, Briscoe (2001) notes that lexicons such as COMLEX tend to demonstrate high precision but low recall.', '#AUTHOR_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX .', 'Precision was quite high (95%), but recall was low (84%).', 'This has an effect on both the precision and recall scores of our system against COMLEX.', 'In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26.', 'We randomly selected 80 false negatives (fn) and 80 false positives (fp) across a range of active frame types containing prepositional and particle detail taken from Penn-III and manually examined them in order to classify them as ""correct"" or ""incorrect.""', 'Of the 80 fps, 33 were manually judged to be legitimate subcategorization frames.', 'For example, as Table 26 shows, there are a number of correct transitive verbs ([subj,obj]) in our automatically induced lexicon which are not included in COMLEX.', 'This examination was also useful in highlighting to us the frame types on which the lexical extraction procedure was performing poorly, in our case, those containing XCOMPs and those containing OBJ2S.', 'Out of 80 fns, 14 were judged to be incorrect when manually examined.', 'These can be broken down as follows: one intransitive frame, three ditransitive frames, three frames containing a COMP, and seven frames containing an oblique were found to be invalid.']",1,"['Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As noted above, it is well documented (Roland and Jurafsky 1998) that subcategorization frames (and their frequencies) vary across domains.', 'For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon.', 'This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs.', '#AUTHOR_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX .', 'Of the 80 fps, 33 were manually judged to be legitimate subcategorization frames.']"
CC446,J05-3003,Gaussian coordinates and the large scale universe,natural language parsing as statistical pattern recognition,['David Magerman'],related work,"Traditional natural language parsers are based on rewrite rule systems developed in an arduous, time-consuming manner by grammarians. A majority of the grammarian's efforts are devoted to the disambiguation process, first hypothesizing rules which dictate constituent categories and relationships among words in ambiguous sentences, and then seeking exceptions and corrections to these rules.  In this work, I propose an automatic method for acquiring a statistical parser from a set of parsed sentences which takes advantage of some initial linguistic input, but avoids the pitfalls of the iterative and seemingly endless grammar development process. Based on distributionally-derived and linguistically-based features of language, this parser acquires a set of statistical decision trees which assign a probability distribution on the space of parse trees given the input sentence. These decision trees take advantage of significant amount of contextual information, potentially including all of the lexical information in the sentence, to produce highly accurate statistical models of the disambiguation process. By basing the disambiguation criteria selection on entropy reduction rather than human intuition, this parser development method is able to consider more sentences than a human grammarian can when making individual disambiguation rules.  In experiments between a parser, acquired using this statistical framework, and a grammarian's rule-based parser, developed over a ten-year period, both using the same training material and test sentences, the decision tree parser significantly outperformed the grammar-based parser on the accuracy measure which the grammarian was trying to maximize, achieving an accuracy of 78% compared to the grammar-based parser's 69%.","The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #AUTHOR_TAG and Collins ( 1997 ) .","['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing.', ""The extraction procedure utilizes a head percolation table as introduced by Magerman (1995) in combination with a variation of Collins's (1997) approach to the differentiation between complement and adjunct."", 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #AUTHOR_TAG and Collins ( 1997 ) .', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.', 'The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman (1994) and Collins (1997).', 'Each node is subsequently assigned the relevant category based on its constituent type and surface configuration.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Unlike our approach, those of Xia (1999) and Hockenmaier, Bierner, and Baldridge (2004) include a substantial initial correction and clean-up of the Penn-II trees.', 'Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of ""inverse schemata.""']",0,"['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #AUTHOR_TAG and Collins ( 1997 ) .', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.']"
CC447,J05-3003,Gaussian coordinates and the large scale universe,tree adjoining grammars in,['Aravind Joshi'],introduction,"The VERBMOBIL project is developing a translation system that can assist a face-to-face dialogue between two non-native english speakers. Instead of having continiously speak english, the dialogue partners have the option to switch to their respective mother tongues (currently german or japanese) in cases where they can't find the required word, phrase or sentence. In such situations, the users activate VERBMOBIL to translate their utterances into english.  A very important requirement for such a system is realtime processing. Realtime processing is essentially necessary, if such a system is to be smoothly integrated into an ongoing communication. This can be achieved by the use of anytime processing, which always provides a result. The quality of the result however, depends on the computation time given to the system. Early interruptions can only produce shallow results. Aiming at such a processing mode, methods for fast but preliminary translation must be integrated into the system assisted by others that refine these results. In this case we suggest structural translation with Synchronous Tree Adjoining Grammars (S-TAGs), which can serve as a fast and shallow realisation of all steps necessary during translation, i.e. analysis, transfer and generation, in a system capable of running anytime methods. This mode is especially adequate for standardized speech acts and simple sentences. Furthermore, it provides a result for early interruptions of the translation process. By building an explicit linguistic structure, methods for refining the result can rearrange the structure in order to increase the quality of the translation given extended execution time. This paper describes the formalism of S-TAGs and the parsing algorithm implemented in VERBMOBIL. Furthermore the language covered by the german grammar is described. Finally we list examples together with the execution time required for their processing","In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ #AUTHOR_TAG ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ #AUTHOR_TAG ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']",0,"['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ #AUTHOR_TAG ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']"
CC448,J05-3003,Gaussian coordinates and the large scale universe,lexical functional grammar volume 34 of syntax and semantics,['Mary Dalrymple'],,,"While many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) , #AUTHOR_TAG questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language-specific constituent structure level .","['While many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) , #AUTHOR_TAG questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language-specific constituent structure level .', 'LFG argues that subcategorization requirements are best stated at the f-structure level, in functional rather than phrasal terms.', 'This is because of the assumption that abstract grammatical functions are primitive concepts as opposed to derivatives of phrase structural position.', 'In LFG, the subcategorization requirements of a particular predicate are expressed by its semantic form: FOCUS (↑ SUBJ)(↑ OBL on ) in Figure 1.']",0,"['While many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) , #AUTHOR_TAG questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language-specific constituent structure level .', 'This is because of the assumption that abstract grammatical functions are primitive concepts as opposed to derivatives of phrase structural position.']"
CC449,J05-3003,Gaussian coordinates and the large scale universe,automatic acquisition of a large subcategorisation dictionary from corpora,['Christopher Manning'],related work,,#AUTHOR_TAG attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur .,"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', 'Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", '#AUTHOR_TAG attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur .', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']",0,"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', '#AUTHOR_TAG attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur .', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.']"
CC450,J05-3003,Gaussian coordinates and the large scale universe,automatic extraction of subcategorization from corpora,"['Edward Briscoe', 'John Carroll']",experiments,"We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount1.","#AUTHOR_TAG , by comparison , employ 163 distinct predefined frames .","['Applying an absolute threshold of five occurrences, we still generate 162 frame types  from Penn-II and 221 from Penn-III.', '#AUTHOR_TAG , by comparison , employ 163 distinct predefined frames .']",0,"['Applying an absolute threshold of five occurrences, we still generate 162 frame types  from Penn-II and 221 from Penn-III.', '#AUTHOR_TAG , by comparison , employ 163 distinct predefined frames .']"
CC451,J05-3003,Gaussian coordinates and the large scale universe,extracting tree adjoining grammars from bracketed corpora,['Fei Xia'],related work,"ExtractingTreeAdjoiningGrammarsfromBracketedCorp oraFeiXiaDepartmentofComputerandInformationScienceUniversityofPennsylvania3401WalnutStreet,Suite400APhiladelphiaPA19104,USAfxia@linc.cis.upenn.eduAbstractInthispap er,werep ortourorkonextractinglexi-calizedtreeadjoininggrammars(LTAGs)frompar-tiallybracketedcorp ora.Thealgorithm rstfullybracketsthecorp ora,thenextractselementarytrees(etrees),and nally ltersoutinvalidusinglinguisticknowledge.Weshowthatthesetofex-tractedetreesmaynotb ecompleteenoughtocoverthewholelanguage,butthiswillnothaveabigim-pactonparsing.1Intro ductionLexicalizedTreeAdjoiningGrammar(LAG)isatree-rewritingformalism.Itismoreexpressivethanacontext-freegrammar(CFG),1andthereforeb et-terformalismforrepresentingvariousphenomenainnaturallanguages.Inthelastdecade,ithasb eenappliedtovariousNLPtaskssuchasparsing(Srini-vas,1997),machinetranslation(Palmeretal.,1998),informationretrieval(ChandrasekarandSrinivas,1997),generation(StoneandDoran,1997;McCoyetal.,1992),andsummarizationapplications(Bald-winetal.,1997).Awide-coverageLTGforapar-ticularnaturallanguageoftencontainsthousandsoftreesandtakesyearstobuild.Therehasb eenworkonextractingCFGs(Shi-raietal.,1995;Charniak,1996;Krotovandoth-ers,1998)andlexicalizedtreegrammars(Neumann,1998;Srinivas,1997)frombracketedcorp ora.Inthispap er,weprop oseanewmetho dforlearningLTAGsfromsuchcorp ora.22PennTreebankandLAG2.1PennTreebankInthispap er,weusetheEnglishPennTreebankasourbracketedcorpus,whichincludesab out1millionTheauthorwishestothankChung-hyeHan,AravindJoshi,MarthaPalmer,CarlosProlo,Ano opSarkar,andthreeanonymousreviewersformanyhelpfulcomments.1LTAGismoreexpressivethanCFformalismb othinweakandstronggenerativecapacity,e.g.itcanhandlecrossdep endencyelegantly.2Arelatedworkis(Srinivas,1997),butitsgoalnottolearnanewLTAGbuttoextracttheusefulinformation,suchasdep endencyandfrequencyoftrees,foranexistingLTAG.","Unlike our approach , those of #AUTHOR_TAG and Hockenmaier , Bierner , and Baldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees .","['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing.', ""The extraction procedure utilizes a head percolation table as introduced by Magerman (1995) in combination with a variation of Collins's (1997) approach to the differentiation between complement and adjunct."", 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.', 'The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman (1994) and Collins (1997).', 'Each node is subsequently assigned the relevant category based on its constituent type and surface configuration.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Unlike our approach , those of #AUTHOR_TAG and Hockenmaier , Bierner , and Baldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees .', 'Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of ""inverse schemata.""']",1,"['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing.', 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Unlike our approach , those of #AUTHOR_TAG and Hockenmaier , Bierner , and Baldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees .', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.']"
CC452,J05-3003,Gaussian coordinates and the large scale universe,automated extraction of tags from the penn treebank,"['John Chen', 'K Vijay-Shanker']",introduction,"The accuracy of statistical parsing models can be improved with the use of lexical information. Statistical parsing using Lexicalized tree adjoining grammar (LTAG), a kind of lexicalized grammar, has remained relatively unexplored. We believe that is largely in part due to the absence of large corpora accurately bracketed in terms of a perspicuous yet broad coverage LTAG. Our work attempts to alleviate this difficulty. We extract different LTAGs from the Penn Treebank. We show that certain strategies yield an improved extracted LTAG in terms of compactness, broad coverage, and supertagging accuracy. Furthermore, we perform a preliminary investigation in smoothing these grammars by means of an external linguistic resource, namely, the tree families of an XTAG grammar, a hand built grammar of English.","Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( #AUTHOR_TAG ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .","['Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( #AUTHOR_TAG ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).', 'However, our approach also generalizes to CFG category-based approaches.', 'In LFG, subcategorization requirements are enforced through semantic forms specifying which grammatical functions are required by a particular predicate.', 'Our approach is based on earlier work on LFG semantic form extraction (van Genabith, Sadler, and Way 1999) and recent progress in automatically annotating the Penn-II and Penn-III Treebanks with LFG f-structures (Cahill et al. 2002;Cahill, McCarthy, et al. 2004).', 'Our technique requires a treebank annotated with LFG functional schemata.', 'In the early approach of van Genabith, Sadler, and Way (1999), this was provided by manually annotating the rules extracted from the publicly available subset of the AP Treebank to automatically produce corresponding f-structures.', 'If the f-structures are of high quality, reliable LFG semantic forms can be generated quite simply by recursively reading off the subcategorizable grammatical functions for each local PRED value at each level of embedding in the f-structures.', 'The work reported in van Genabith, Sadler, and Way (1999) was small scale (100 trees) and proof of concept and required considerable manual annotation work.', 'It did not associate frames with probabilities, discriminate between frames for active and passive constructions, properly reflect the effects of long-distance dependencies (LDDs), or include CFG category information.', 'In this article we show how the extraction process can be scaled to the complete Wall Street Journal (WSJ) section of the Penn-II Treebank, with about one million words in 50,000 sentences, based on the automatic LFG f-structure annotation algorithm described in Cahill et al. (2002) and Cahill, McCarthy, et al. (2004).', 'More recently we have extended the extraction approach to the larger, domain-diverse Penn-III Treebank.', 'Aside from the parsed WSJ section, this version of the treebank contains parses for a subsection of the Brown corpus (almost 385,000 words in 24,000 trees) taken from a variety of text genres. 1 In addition to extracting grammatical-function-For the remainder of this work, when we refer to the Penn-II Treebank, we mean the parse-annotated WSJ, and when we refer to the Penn-III Treebank, we mean the parse-annotated WSJ and Brown corpus combined.']",0,"['Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( #AUTHOR_TAG ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'Our approach is based on earlier work on LFG semantic form extraction (van Genabith, Sadler, and Way 1999) and recent progress in automatically annotating the Penn-II and Penn-III Treebanks with LFG f-structures (Cahill et al. 2002;Cahill, McCarthy, et al. 2004).', 'Our technique requires a treebank annotated with LFG functional schemata.', 'In the early approach of van Genabith, Sadler, and Way (1999), this was provided by manually annotating the rules extracted from the publicly available subset of the AP Treebank to automatically produce corresponding f-structures.', 'The work reported in van Genabith, Sadler, and Way (1999) was small scale (100 trees) and proof of concept and required considerable manual annotation work.', 'In this article we show how the extraction process can be scaled to the complete Wall Street Journal (WSJ) section of the Penn-II Treebank, with about one million words in 50,000 sentences, based on the automatic LFG f-structure annotation algorithm described in Cahill et al. (2002) and Cahill, McCarthy, et al. (2004).', 'More recently we have extended the extraction approach to the larger, domain-diverse Penn-III Treebank.', 'Aside from the parsed WSJ section, this version of the treebank contains parses for a subsection of the Brown corpus (almost 385,000 words in 24,000 trees) taken from a variety of text genres. 1 In addition to extracting grammatical-function-For the remainder of this work, when we refer to the Penn-II Treebank, we mean the parse-annotated WSJ, and when we refer to the Penn-III Treebank, we mean the parse-annotated WSJ and Brown corpus combined.']"
CC453,J05-3003,Gaussian coordinates and the large scale universe,automatic extraction of subcategorization from corpora,"['Edward Briscoe', 'John Carroll']",related work,"We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount1.","#AUTHOR_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection .","['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', 'Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', '#AUTHOR_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection .', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']",0,"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', '#AUTHOR_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection .', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.']"
CC454,J05-3003,Gaussian coordinates and the large scale universe,the tiger treebank,"['Sabine Brants', 'Stefanie Dipper', 'Silvia Hansen', 'Wolfgang Lezius', 'George Smith']",,"Proceedings of the 16th Nordic Conference   of Computational Linguistics NODALIDA-2007.  Editors: Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek and Mare Koit.  University of Tartu, Tartu, 2007.  ISBN 978-9985-4-0513-0 (online)  ISBN 978-9985-4-0514-7 (CD-ROM)  pp. 81-88","We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank ( #AUTHOR_TAG ) and Penn Chinese Treebank (Xue, Chiou, and Palmer 2002), extracting wide-coverage, probabilistic LFG grammar approximations and lexical resources for German (Cahill et al. 2003) and Chinese (Burke, Lam, et al. 2004).","['We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank ( #AUTHOR_TAG ) and Penn Chinese Treebank (Xue, Chiou, and Palmer 2002), extracting wide-coverage, probabilistic LFG grammar approximations and lexical resources for German (Cahill et al. 2003) and Chinese (Burke, Lam, et al. 2004).', 'The lexical resources, however, have not yet been evaluated. This, and much else, has to await further research.']",5,"['We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank ( #AUTHOR_TAG ) and Penn Chinese Treebank (Xue, Chiou, and Palmer 2002), extracting wide-coverage, probabilistic LFG grammar approximations and lexical resources for German (Cahill et al. 2003) and Chinese (Burke, Lam, et al. 2004).', 'The lexical resources, however, have not yet been evaluated. This, and much else, has to await further research.']"
CC455,J05-3003,Gaussian coordinates and the large scale universe,lexical functional grammar a formal system for grammatical representation,"['Ronald Kaplan', 'Joan Bresnan']",,In learning their native language children develop a remarkable set of capabilities They acquire knowledge and skills that enable them to pro duce and comprehend an inde nite number of novel utterances and to make quite subtle judgments about certain of their properties The ma jor goal of psycholinguistic research is to devise an explanatory account of the mental operations that underlie these linguistic abilities In pursuing this goal we have adopted what we call the Competence Hypothesis as a methodological principle We assume that an explana tory model of human language performance will incorporate a theoreti cally justi ed representation of the native speaker s linguistic knowledge a grammar as a component separate both from the computational mech anisms that operate on it a processor and from other nongrammatical processing parameters that might in uence the processor s behavior To a certain extent the various components that we postulate can be studied independently guided where appropriate by the well established methods and evaluation standards of linguistics computer science and experimen tal psychology However the requirement that the various components ultimately must t together in a consistent and coherent model imposes even stronger constraints on their structure and operation,The subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure ( #AUTHOR_TAG ) : An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs .,['The subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure ( #AUTHOR_TAG ) : An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs .'],0,['The subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure ( #AUTHOR_TAG ) : An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs .']
CC456,J05-3003,Gaussian coordinates and the large scale universe,lexical functional grammar volume 34 of syntax and semantics,['Mary Dalrymple'],,,Lexical functional grammar ( Kaplan and Bresnan 1982 ; Bresnan 2001 ; #AUTHOR_TAG ) is a member of the family of constraint-based grammars .,"['Lexical functional grammar ( Kaplan and Bresnan 1982 ; Bresnan 2001 ; #AUTHOR_TAG ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', 'C-structure takes the form of phrase structure trees and is defined in terms of CFG rules and lexical entries.', 'F-structure is produced from functional annotations on the nodes of the c-structure and implemented in terms of recursive feature structures (attribute-value matrices).', 'This is exemplified by the analysis of the string The inquiry soon focused on the judge (wsj 0267 72) using the grammar in Figure 1, which results in the annotated c-structure and f-structure in Figure 2.']",0,['Lexical functional grammar ( Kaplan and Bresnan 1982 ; Bresnan 2001 ; #AUTHOR_TAG ) is a member of the family of constraint-based grammars .']
CC457,J05-3003,Gaussian coordinates and the large scale universe,lexicalfunctional syntax,['Joan Bresnan'],,,Lexical functional grammar ( Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .,"['Lexical functional grammar ( Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', 'C-structure takes the form of phrase structure trees and is defined in terms of CFG rules and lexical entries.', 'F-structure is produced from functional annotations on the nodes of the c-structure and implemented in terms of recursive feature structures (attribute-value matrices).', 'This is exemplified by the analysis of the string The inquiry soon focused on the judge (wsj 0267 72) using the grammar in Figure 1, which results in the annotated c-structure and f-structure in Figure 2.']",0,['Lexical functional grammar ( Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .']
CC458,J05-3003,Gaussian coordinates and the large scale universe,lexical functional grammar volume 34 of syntax and semantics,['Mary Dalrymple'],,,"According to #AUTHOR_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ .","['The value of the PRED attribute in an f-structure is a semantic form Π gf 1 , gf 2 , . . .', ', gf n , where Π is a lemma and gf a grammatical function.', 'The semantic form provides an argument list gf 1 ,gf 2 , . . .', ',gf n specifying the governable grammatical functions (or arguments) required by the predicate to form a grammatical construction.', 'In Figure 1 the verb FOCUS requires a subject and an oblique object introduced by the preposition on: FOCUS (↑ SUBJ)(↑ OBL on ) .', 'The argument list can be empty, as in the PRED value for judge in Figure 1.', 'According to #AUTHOR_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ .', 'OBJ θ and OBL θ represent families of grammatical functions indexed by their semantic role, represented by the theta subscript.', 'This list of grammatical functions is divided into governable (subcategorizable) grammatical functions (arguments) and nongovernable (nonsubcategorizable) grammatical functions (modifiers/adjuncts), as summarized in Table 1.']",0,"['The semantic form provides an argument list gf 1 ,gf 2 , . . .', ',gf n specifying the governable grammatical functions (or arguments) required by the predicate to form a grammatical construction.', 'According to #AUTHOR_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ .', 'OBJ th and OBL th represent families of grammatical functions indexed by their semantic role, represented by the theta subscript.', 'This list of grammatical functions is divided into governable (subcategorizable) grammatical functions (arguments) and nongovernable (nonsubcategorizable) grammatical functions (modifiers/adjuncts), as summarized in Table 1.']"
CC459,J05-3003,Gaussian coordinates and the large scale universe,automatic extraction of subcategorization frames for czech,"['Anoop Sarkar', 'Daniel Zeman']",,"We present some novel machine learning techniques for the identification of subcategorization information for verbs in Czech. We compare three different statistical techniques applied to this problem. We show how the learning algorithm can be used to discover previously unknown subcategorization frames from the Czech Prague Dependency Treebank. The algorithm can then be used to label dependents of a verb in the Czech treebank as either arguments or adjuncts. Using our techniques, we are able to achieve 88 % precision on unseen parsed text.",#AUTHOR_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % .,"['Carroll and Rooth (1998) achieve an F-score of 77% against the OALD when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each.', 'Their system recognizes 15 frames, and these do not contain details of subcategorized- for prepositions.', 'Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3.', '#AUTHOR_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % .', 'However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument�adjunct distinctions posited by their sys- tem.', 'The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German.', 'She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (Dudenredaktion 2001).', 'We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3.']",1,"['Their system recognizes 15 frames, and these do not contain details of subcategorized- for prepositions.', 'Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3.', '#AUTHOR_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % .', 'However, their evaluation does not examine the extracted subcatego- rization frames but rather the argumentadjunct distinctions posited by their sys- tem.', 'The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German.']"
CC460,J05-3003,Gaussian coordinates and the large scale universe,the automatic acquisition of frequencies of verb subcategorization frames from tagged corpora,"['Akira Ushioda', 'David Evans', 'Ted Gibson', 'Alex Waibel']",method,"We describe a mechanism for automatically acquiring verb subcategorization frames and their frequencies in a large corpus. A tagged corpus is first partially parsed to identify noun phrases and then a finear grammar is used to estimate the appropriate subcategorization frame for each verb token in the corpus. In an experiment involving the identification of six fixed subcategorization frames, our current system showed more than 80% accuracy. In addition, a new statistical approach substantially improves the accuracy of the frequency estimation.","Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause ( #AUTHOR_TAG ) .","['The syntactic functions COMP and XCOMP refer to clausal complements with different predicate control patterns as described in Section 2. However, as it stands, neither of these functions betrays anything about the syntactic nature of the constructs in question.', 'Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause ( #AUTHOR_TAG ) .', 'With only a slight modification, our system, along with the details provided by the automatically generated f-structures, allows us to extract frames with an equivalent level of detail.', 'For example, to identify a that-clause, we use']",0,"['Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause ( #AUTHOR_TAG ) .']"
CC461,J05-3003,Gaussian coordinates and the large scale universe,statistical decision tree models for parsing,['David Magerman'],related work,"Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to text-processing in general. In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result. This work is based on the following premises: (1) grammars are too complex and detailed to develop manually for most interesting domains; (2) parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and (3) existing {$n$}-gram modeling techniques are inadequate for parsing models. In experiments comparing SPATTER with IBM's computer manuals parser, SPATTER significantly outperforms the grammar-based parser. Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATTER achieves 86 % precision, 86 % recall, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91 % precision, 90 % recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length.Comment: uses aclap.sty, psfig.tex (v1.9), postscript figure",The extraction procedure utilizes a head percolation table as introduced by #AUTHOR_TAG in combination with a variation of Collins 's ( 1997 ) approach to the differentiation between complement and adjunct .,"['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing.', ""The extraction procedure utilizes a head percolation table as introduced by #AUTHOR_TAG in combination with a variation of Collins 's ( 1997 ) approach to the differentiation between complement and adjunct ."", 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.', 'The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman (1994) and Collins (1997).', 'Each node is subsequently assigned the relevant category based on its constituent type and surface configuration.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Unlike our approach, those of Xia (1999) and Hockenmaier, Bierner, and Baldridge (2004) include a substantial initial correction and clean-up of the Penn-II trees.', 'Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of ""inverse schemata.""']",0,"['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing.', ""The extraction procedure utilizes a head percolation table as introduced by #AUTHOR_TAG in combination with a variation of Collins 's ( 1997 ) approach to the differentiation between complement and adjunct ."", 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.']"
CC462,J05-3003,Gaussian coordinates and the large scale universe,from grammar to lexicon unsupervised learning of lexical syntax,['Michael Brent'],related work,"Imagine a language that is completely unfamiliar; the only means of studying it are an ordinary grammar book and a very large corpus of text. No dictionary is available. How can easily recognized, surface grammatical facts be used to extract from a corpus as much syntactic information as possible about individual words? This paper describes an approach based on two principles. First, rely on local morpho-syntactic cues to structure rather than trying to parse entire sentences. Second, treat these cues as probabilistic rather than absolute indicators of syntactic structure. Apply inferential statistics to the data collected using the cues, rather than drawing a categorical conclusion from a single occurrence of a cue. The effectiveness of this approach for inferring the syntactic frames of verbs is supported by experiments on an English corpus using a program called Lerner. Lerner starts out with no knowledge of content words---it bootstraps from determiners, auxiliaries, modals, prepositions, pronouns, complementizers, coordinating conjunctions, and punctuation.","The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #AUTHOR_TAG .","['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', 'Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #AUTHOR_TAG .', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']",0,"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #AUTHOR_TAG .', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.']"
CC463,J05-3003,Gaussian coordinates and the large scale universe,lexical functional grammar a formal system for grammatical representation,"['Ronald Kaplan', 'Joan Bresnan']",introduction,In learning their native language children develop a remarkable set of capabilities They acquire knowledge and skills that enable them to pro duce and comprehend an inde nite number of novel utterances and to make quite subtle judgments about certain of their properties The ma jor goal of psycholinguistic research is to devise an explanatory account of the mental operations that underlie these linguistic abilities In pursuing this goal we have adopted what we call the Competence Hypothesis as a methodological principle We assume that an explana tory model of human language performance will incorporate a theoreti cally justi ed representation of the native speaker s linguistic knowledge a grammar as a component separate both from the computational mech anisms that operate on it a processor and from other nongrammatical processing parameters that might in uence the processor s behavior To a certain extent the various components that we postulate can be studied independently guided where appropriate by the well established methods and evaluation standards of linguistics computer science and experimen tal psychology However the requirement that the various components ultimately must t together in a consistent and coherent model imposes even stronger constraints on their structure and operation,"We applied lexical-redundancy rules ( #AUTHOR_TAG ) to automatically convert the active COMLEX frames to their passive counterparts: For example, subjects are demoted to optional by oblique agents, and direct objects become subjects.","['We applied lexical-redundancy rules ( #AUTHOR_TAG ) to automatically convert the active COMLEX frames to their passive counterparts: For example, subjects are demoted to optional by oblique agents, and direct objects become subjects.', 'The resulting precision was very high (from 72.3% to 80.2%), and there was the expected drop in recall when prepositional details were included (from 54.7% to 29.3%).']",5,"['We applied lexical-redundancy rules ( #AUTHOR_TAG ) to automatically convert the active COMLEX frames to their passive counterparts: For example, subjects are demoted to optional by oblique agents, and direct objects become subjects.']"
CC464,J05-3003,Gaussian coordinates and the large scale universe,a comparison of evaluation metrics for a broad coverage parser,"['Richard Crouch', 'Ron Kaplan', 'Tracy King', 'Stefan Riezler']",method,,Both use the evaluation software and triple encoding presented in #AUTHOR_TAG .,"['In order to ensure the quality of the semantic forms extracted by our method, we must first ensure the quality of the f-structure annotations.', 'The results of two different evaluations of the automatically generated f-structures are presented in Table 2.', 'Both use the evaluation software and triple encoding presented in #AUTHOR_TAG .', 'The first of these is against the DCU 105, a gold-standard set of 105 hand-coded f-structures']",5,"['The results of two different evaluations of the automatically generated f-structures are presented in Table 2.', 'Both use the evaluation software and triple encoding presented in #AUTHOR_TAG .', 'The first of these is against the DCU 105, a gold-standard set of 105 hand-coded f-structures']"
CC465,J05-3003,Gaussian coordinates and the large scale universe,the parc 700 dependency bank,"['Tracy Holloway King', 'Richard Crouch', 'Stefan Riezler', 'Mary Dalrymple', 'Ronald Kaplan']",method,"An automatic method for annotating the Penn-II Treebank (Marcus et al., 1994) with high-level Lexical Functional Grammar (Kaplan and Bresnan, 1982; Bresnan, 2001; Dalrymple, 2001) f-structure representations is described in (Cahill et al., 2002; Cahill et al., 2004a; Cahill et al., 2004b; O'Donovan et al., 2004). The annotation algorithm and the automatically-generated f-structures are the basis for the automatic acquisition of wide-coverage and robust probabilistic approximations of LFG grammars (Cahill et al., 2002; Cahill et al., 2004a) and for the induction of LFG semantic forms (O'Donovan et al., 2004). The quality of the annotation algorithm and the f-structures it generates is, therefore, extremely important. To date, annotation quality has been measured in terms of precision and recall against the DCU 105. The annotation algorithm currently achieves an f-score of 96.57% for complete f-structures and 94.3% for preds-only f-structures. There are a number of problems with evaluating against a gold standard of this size, most notably that of overfitting. There is a risk of assuming that the gold standard is a complete and balanced representation of the linguistic phenomena in a language and basing design decisions on this. It is, therefore, preferable to evaluate against a more extensive, external standard. Although the DCU 105 is publicly available, 1 a larger well-established external standard can provide a more widely-recognised benchmark against which the quality of the f-structure annotation algorithm can be evaluated. For these reasons, we present an evaluation of the f-structure annotation algorithm of (Cahill et al., 2002; Cahill et al., 2004a; Cahill et al., 2004b; O'Donovan et al., 2004) against the PARC 700 Dependency Bank (King et al., 2003). Evaluation against an external gold standard is a non-trivial task as linguistic analyses may differ systematically between the gold standard and the output to be evaluated as regards feature geometry and nomenclature. We present conversion software to automatically account for many (but not all) of the systematic differences. Currently, we achieve an f-score of 87.31% for the f-structures generated from the original Penn-II trees and an f-score of 81.79% for f-structures from parse trees produced by Charniak's (2000) parser in our pipeline parsing architecture against the PARC 700","More recently , Burke , Cahill , et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank ( #AUTHOR_TAG ) , a set of 700 randomly selected sentences from Section 23 which have been parsed , converted to dependency format , and manually corrected and extended by human validators .","['from Section 23 of the Penn Treebank as described in Cahill, McCarthy, et al. (2004).', 'For the full set of annotations they achieve precision of over 96.5% and recall of over 96.6%.', 'There is, however, a risk of overfitting when evaluation is limited to a gold standard of this size.', 'More recently , Burke , Cahill , et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank ( #AUTHOR_TAG ) , a set of 700 randomly selected sentences from Section 23 which have been parsed , converted to dependency format , and manually corrected and extended by human validators .', 'They report precision of over 88.5% and recall of over 86% (Table 2).', 'The PARC 700 Dependency Bank differs substantially from both the DCU 105 f-structure bank and the automatically generated f-structures in regard to the style of linguistic analysis, feature nomenclature, and feature geometry.', 'Some, but not all, of these differences are captured by automatic conversion software.', 'A detailed discussion of the issues inherent in this process and a full analysis of results is presented in Burke, Cahill, et al. (2004a).', 'Results broken down by grammatical function for the DCU 105 evaluation are presented in Table 3. OBL (prepositional phrase) arguments are traditionally difficult to annotate reliably.', 'The results show, however, that with respect to obliques, the annotation algorithm, while slightly conservative (recall of 82%), is very accurate: 96% of the time it annotates an oblique, the annotation is correct.']",0,"['More recently , Burke , Cahill , et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank ( #AUTHOR_TAG ) , a set of 700 randomly selected sentences from Section 23 which have been parsed , converted to dependency format , and manually corrected and extended by human validators .', 'The PARC 700 Dependency Bank differs substantially from both the DCU 105 f-structure bank and the automatically generated f-structures in regard to the style of linguistic analysis, feature nomenclature, and feature geometry.']"
CC466,J05-3003,Gaussian coordinates and the large scale universe,natural language parsing as statistical pattern recognition,['David Magerman'],related work,"Traditional natural language parsers are based on rewrite rule systems developed in an arduous, time-consuming manner by grammarians. A majority of the grammarian's efforts are devoted to the disambiguation process, first hypothesizing rules which dictate constituent categories and relationships among words in ambiguous sentences, and then seeking exceptions and corrections to these rules.  In this work, I propose an automatic method for acquiring a statistical parser from a set of parsed sentences which takes advantage of some initial linguistic input, but avoids the pitfalls of the iterative and seemingly endless grammar development process. Based on distributionally-derived and linguistically-based features of language, this parser acquires a set of statistical decision trees which assign a probability distribution on the space of parse trees given the input sentence. These decision trees take advantage of significant amount of contextual information, potentially including all of the lexical information in the sentence, to produce highly accurate statistical models of the disambiguation process. By basing the disambiguation criteria selection on entropy reduction rather than human intuition, this parser development method is able to consider more sentences than a human grammarian can when making individual disambiguation rules.  In experiments between a parser, acquired using this statistical framework, and a grammarian's rule-based parser, developed over a ten-year period, both using the same training material and test sentences, the decision tree parser significantly outperformed the grammar-based parser on the accuracy measure which the grammarian was trying to maximize, achieving an accuracy of 78% compared to the grammar-based parser's 69%.","Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #AUTHOR_TAG and Collins ( 1997 ) ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.","['Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii (2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #AUTHOR_TAG and Collins ( 1997 ) ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of �inverse schemata.�']",5,"['Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #AUTHOR_TAG and Collins ( 1997 ) ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.']"
CC467,J05-3003,Gaussian coordinates and the large scale universe,on the order of words,"['Anthony Ades', 'Mark Steedman']",introduction,"Visual referring expression recognition is a challenging task that requires natural language understanding in the context of an image. We critically examine RefCOCOg, a standard benchmark for this task, using a human study and show that 83.7% of test instances do not require reasoning on linguistic structure, i.e., words are enough to identify the target object, the word order doesn't matter. To measure the true progress of existing models, we split the test set into two sets, one which requires reasoning on linguistic structure and the other which doesn't. Additionally, we create an out-of-distribution dataset Ref-Adv by asking crowdworkers to perturb in-domain examples such that the target object changes. Using these datasets, we empirically show that existing methods fail to exploit linguistic structure and are 12% to 23% lower in performance than the established progress for this task. We also propose two methods, one based on contrastive learning and the other based on multi-task learning, to increase the robustness of ViLBERT, the current state-of-the-art model for this task. Our datasets are publicly available at https://github.com/aws/aws-refcocog-advComment: ACL 202","In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ #AUTHOR_TAG ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ #AUTHOR_TAG ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']",0,"['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ #AUTHOR_TAG ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']"
CC468,J05-3003,Gaussian coordinates and the large scale universe,automatic fstructure annotation of treebank trees,['Anette Frank'],method,,"Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and Sadler 1999 ; #AUTHOR_TAG ; Sadler , van Genabith , and Way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept .","['The first step in the application of our methodology is the production of a treebank annotated with LFG f-structure information.', 'F-structures are attribute-value structures which represent abstract syntactic information, approximating to basic predicate-argument-modifier structures.', 'Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and Sadler 1999 ; #AUTHOR_TAG ; Sadler , van Genabith , and Way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept .', 'However, more recent work (Cahill et al. 2002;Cahill, McCarthy, et al. 2004) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank (Marcus et al. 1994), containing more than 1,000,000 words and 49,000 sentences.']",0,"['The first step in the application of our methodology is the production of a treebank annotated with LFG f-structure information.', 'Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and Sadler 1999 ; #AUTHOR_TAG ; Sadler , van Genabith , and Way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept .']"
CC469,J05-3003,Gaussian coordinates and the large scale universe,subcategorization acquisition as,['Anna Korhonen'],related work,"Evaluation of word sense disambiguation (WSD) systems is often based on machine-readable dictionaries (MRDs). Such evaluation typically employs a set of fine-grained dictionary senses and considers them all to be equally important. In this paper, we propose a novel evaluation method for WSD systems in the context of automatic subcategorization acquisition. Building on an extant subcategorization acquisition system, we show that the system would benefit from WSD and propose modifications which allow it to make use of WSD. The enhanced subcategorization acquisition system can then be used as a task-based evaluation method for WSD systems where both the notion of sense and the sense's relevance to the evaluation process is determined by the application itself. 1",Recent work by #AUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection .,"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', 'Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by #AUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection .', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']",0,"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by #AUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection .', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.']"
CC470,J05-3003,Gaussian coordinates and the large scale universe,compacting the penn treebank grammar,"['Alexander Krotov', 'Mark Hepple', 'Robert Gaizauskas', 'Yorick Wilks']",,"Treebanks, such as the Penn Treebank (PTB), offer a simple approach to obtaining a broad coverage grammar: one can simply read the grammar off the parse trees in the treebank. While such a grammar is easy to obtain, a square-root rate of growth of the rule set with corpus size suggests that the derived grammar is far from complete and that much more treebanked text would be required to obtain a complete grammar, if one exists at some limit. However, we offer an alternative explanation in terms of the underspecification of structures within the treebank. This hypothesis is explored by applying an algorithm to compact the derived grammar by eliminating redundant rules - rules whose right hand sides can be parsed by other rules. The size of the resulting compacted grammar, which is significantly less than that of the full treebank grammar, is shown to approach a limit. However, such a compacted grammar does not yield very good performance figures. A version of the compaction algorithm taking rule probabilities into account is proposed, which is argued to be more linguistically motivated. Combined with simple thresholding, this method can be used to give a 58% reduction in grammar size without significant change in parsing performance, and can produce a 69% reduction with some gain in recall, but a loss in precision.","In Charniak ( 1996 ) and #AUTHOR_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .","['The rate of accession may also be represented graphically.', 'In Charniak ( 1996 ) and #AUTHOR_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', 'We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity.', 'Figure 8 graphs the rate of induction of semantic form and CFG rule types from Penn-III (the WSJ and parse-annotated Brown corpus combined).', 'Because of the variation in the size of sections between the Brown and the WSJ, we plotted accession against word count.', 'The first part of the graph (up to 1,004,414 words)']",0,"['In Charniak ( 1996 ) and #AUTHOR_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .']"
CC471,J05-3003,Gaussian coordinates and the large scale universe,three generative lexicalised models for statistical parsing,['Michael Collins'],related work,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97","Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on Magerman ( 1994 ) and #AUTHOR_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.","['Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii (2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on Magerman ( 1994 ) and #AUTHOR_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of �inverse schemata.�']",5,"['Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on Magerman ( 1994 ) and #AUTHOR_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.']"
CC472,J05-3003,Gaussian coordinates and the large scale universe,identifying verb arguments and their syntactic function in the penn treebank,"['Alexandra Kinyon', 'Carlos Prolo']",related work,"In this paper, we present a tool that allows one to automatically extract verb argument-structure from the Penn Treebank as well as from other corpora annotated with the Penn Treebank release 2 conventions. More specifically, we examine each possible sequence of tags, both functional and categorial and determine whether such a sequence indicates an obligatory argument, an optional argument or a modifier. We argue that this approach is more fine-grained and thus more satisfactory than the existing approaches which have aimed at determining argumenthood in the Penn Treebank. The goal of this work is to provide a set of sufficiently general and fine-grained rules as well as an implementation which will be reusable and freely available to the research community. 1",#AUTHOR_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank .,"['Approaches using treebank-based data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data.', '#AUTHOR_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank .', 'This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'Each of these sequences was categorized as a modifier or argument.', 'Arguments were then mapped to traditional syntactic functions.', 'For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic function is subject.', 'In general, argumenthood was preferred over adjuncthoood.', 'As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is.', 'Sarkar and Zeman (2000) present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank (Hajic 1998).', 'Czech is a language with a freer word order than English and so configurational information cannot be relied upon.', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'Finding subcategorization frames involves filtering adjuncts from the observed frame.', 'This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score.', 'The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more).', 'Marinov and Hemming (2004) present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank (Simov, Popova, and Osenova 2002).', ""In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming's system collects both arguments and adjuncts."", 'It then uses the binomial log-likelihood ratio to filter incorrect frames.', 'The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees.', 'The work done for Bulgarian is small-scale, however, as Marinov and Hemming are working with a preliminary version of the treebank with 580 sentences.']",0,['#AUTHOR_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank .']
CC473,J05-3003,Gaussian coordinates and the large scale universe,automatic extraction of subcategorization frames from the bulgarian tree bank,"['Svetoslav Marinov', 'Cecilia Hemming']",related work,"(1) a. Teodora opened the door. b. *Arto looked the door. In (1-a) the verb open takes as an obligatory argument an NP and therefore differs from look in (1-b), which is an ill-formed sentence, because look require","#AUTHOR_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) .","['Approaches using treebank-based data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data.', 'Kinyon and Prolo (2002) describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank.', 'This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'Each of these sequences was categorized as a modifier or argument.', 'Arguments were then mapped to traditional syntactic functions.', 'For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic function is subject.', 'In general, argumenthood was preferred over adjuncthoood.', 'As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is.', 'Sarkar and Zeman (2000) present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank (Hajic 1998).', 'Czech is a language with a freer word order than English and so configurational information cannot be relied upon.', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'Finding subcategorization frames involves filtering adjuncts from the observed frame.', 'This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score.', 'The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more).', '#AUTHOR_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) .', ""In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming's system collects both arguments and adjuncts."", 'It then uses the binomial log-likelihood ratio to filter incorrect frames.', 'The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees.', 'The work done for Bulgarian is small-scale, however, as Marinov and Hemming are working with a preliminary version of the treebank with 580 sentences.']",0,"['#AUTHOR_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) .']"
CC474,J05-3003,Gaussian coordinates and the large scale universe,extracting tree adjoining grammars from bracketed corpora,['Fei Xia'],related work,"ExtractingTreeAdjoiningGrammarsfromBracketedCorp oraFeiXiaDepartmentofComputerandInformationScienceUniversityofPennsylvania3401WalnutStreet,Suite400APhiladelphiaPA19104,USAfxia@linc.cis.upenn.eduAbstractInthispap er,werep ortourorkonextractinglexi-calizedtreeadjoininggrammars(LTAGs)frompar-tiallybracketedcorp ora.Thealgorithm rstfullybracketsthecorp ora,thenextractselementarytrees(etrees),and nally ltersoutinvalidusinglinguisticknowledge.Weshowthatthesetofex-tractedetreesmaynotb ecompleteenoughtocoverthewholelanguage,butthiswillnothaveabigim-pactonparsing.1Intro ductionLexicalizedTreeAdjoiningGrammar(LAG)isatree-rewritingformalism.Itismoreexpressivethanacontext-freegrammar(CFG),1andthereforeb et-terformalismforrepresentingvariousphenomenainnaturallanguages.Inthelastdecade,ithasb eenappliedtovariousNLPtaskssuchasparsing(Srini-vas,1997),machinetranslation(Palmeretal.,1998),informationretrieval(ChandrasekarandSrinivas,1997),generation(StoneandDoran,1997;McCoyetal.,1992),andsummarizationapplications(Bald-winetal.,1997).Awide-coverageLTGforapar-ticularnaturallanguageoftencontainsthousandsoftreesandtakesyearstobuild.Therehasb eenworkonextractingCFGs(Shi-raietal.,1995;Charniak,1996;Krotovandoth-ers,1998)andlexicalizedtreegrammars(Neumann,1998;Srinivas,1997)frombracketedcorp ora.Inthispap er,weprop oseanewmetho dforlearningLTAGsfromsuchcorp ora.22PennTreebankandLAG2.1PennTreebankInthispap er,weusetheEnglishPennTreebankasourbracketedcorpus,whichincludesab out1millionTheauthorwishestothankChung-hyeHan,AravindJoshi,MarthaPalmer,CarlosProlo,Ano opSarkar,andthreeanonymousreviewersformanyhelpfulcomments.1LTAGismoreexpressivethanCFformalismb othinweakandstronggenerativecapacity,e.g.itcanhandlecrossdep endencyelegantly.2Arelatedworkis(Srinivas,1997),butitsgoalnottolearnanewLTAGbuttoextracttheusefulinformation,suchasdep endencyandfrequencyoftrees,foranexistingLTAG.",#AUTHOR_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank .,"['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing.', ""The extraction procedure utilizes a head percolation table as introduced by Magerman (1995) in combination with a variation of Collins's (1997) approach to the differentiation between complement and adjunct."", 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', '#AUTHOR_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank .', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.', 'The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman (1994) and Collins (1997).', 'Each node is subsequently assigned the relevant category based on its constituent type and surface configuration.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Unlike our approach, those of Xia (1999) and Hockenmaier, Bierner, and Baldridge (2004) include a substantial initial correction and clean-up of the Penn-II trees.', 'Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of ""inverse schemata.""']",0,"['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', '#AUTHOR_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank .', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.']"
CC475,J05-3003,Gaussian coordinates and the large scale universe,lexical functional grammar a formal system for grammatical representation,"['Ronald Kaplan', 'Joan Bresnan']",introduction,In learning their native language children develop a remarkable set of capabilities They acquire knowledge and skills that enable them to pro duce and comprehend an inde nite number of novel utterances and to make quite subtle judgments about certain of their properties The ma jor goal of psycholinguistic research is to devise an explanatory account of the mental operations that underlie these linguistic abilities In pursuing this goal we have adopted what we call the Competence Hypothesis as a methodological principle We assume that an explana tory model of human language performance will incorporate a theoreti cally justi ed representation of the native speaker s linguistic knowledge a grammar as a component separate both from the computational mech anisms that operate on it a processor and from other nongrammatical processing parameters that might in uence the processor s behavior To a certain extent the various components that we postulate can be studied independently guided where appropriate by the well established methods and evaluation standards of linguistics computer science and experimen tal psychology However the requirement that the various components ultimately must t together in a consistent and coherent model imposes even stronger constraints on their structure and operation,"In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']",0,"['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']"
CC476,J05-3003,Gaussian coordinates and the large scale universe,how verb subcategorization frequencies are affected by corpus choice,"['Douglas Roland', 'Daniel Jurafsky']",,"The probabilistic relation between verbs and their arguments plays an important role in modern statistical parsers and supertaggers, and in psychological theories of language processing. But these probabilities are computed in very different ways by the two sets of researchers. Computational linguists compute verb subcategorization probabilities from large corpora while psycholinguists compute them from psychological studies (sentence production and completion tasks). Recent studies have found differences between corpus frequencies and psycholinguistic measures. We analyze subcategorization frequencies from four different corpora: psychological sentence production data (Connine et al. 1984), written text (Brown and WSJ), and telephone conversation data (Switchboard). We find two different sources for the differences. Discourse influence is a result of how verb use is affected by different discourse types such as narrative, connected discourse, and single sentence productions. Semantic influence is a result of different corpora using different senses of verbs, which have different subcategorization frequencies. We conclude that verb sense and discourse type play an important role in the frequencies observed in different experimental and corpus based sources of verb subcategorization frequencies.",It has been shown ( #AUTHOR_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains .,"['Recently we have applied our methodology to the Penn-III Treebank, a more balanced corpus resource with a number of text genres.', 'Penn-III consists of the WSJ section from Penn-II as well as a parse-annotated subset of the Brown corpus.', 'The Brown corpus comprises 24,242 trees compiled from a variety of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor.', 'It has been shown ( #AUTHOR_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains .', 'Our aim, therefore, is to increase the scope of the induced lexicon not only in terms of the verb lemmas for which there are entries, but also in terms of the frames with which they co-occur.', 'The f-structure annotation algorithm was extended with only minor amendments to cover the parsed Brown corpus.', 'The most important of these was the way in which we distinguish between oblique and adjunct.', 'We noted in Section 4 that our method of assigning an oblique annotation in Penn-II was precise, albeit conservative.', 'Because of a change of annotation policy in Penn-III, the -CLR tag (indicating a close relationship between a PP and the local syntactic head), information which we had previously exploited, is no longer used.', 'For Penn-III the algorithm annotates all PPs which do not carry a Penn adverbial functional tag (such as -TMP or -LOC) and occur as the sisters of the verbal head of a VP as obliques.']",4,['It has been shown ( #AUTHOR_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains .']
CC477,J05-3003,Gaussian coordinates and the large scale universe,extracting tree adjoining grammars from bracketed corpora,['Fei Xia'],introduction,"ExtractingTreeAdjoiningGrammarsfromBracketedCorp oraFeiXiaDepartmentofComputerandInformationScienceUniversityofPennsylvania3401WalnutStreet,Suite400APhiladelphiaPA19104,USAfxia@linc.cis.upenn.eduAbstractInthispap er,werep ortourorkonextractinglexi-calizedtreeadjoininggrammars(LTAGs)frompar-tiallybracketedcorp ora.Thealgorithm rstfullybracketsthecorp ora,thenextractselementarytrees(etrees),and nally ltersoutinvalidusinglinguisticknowledge.Weshowthatthesetofex-tractedetreesmaynotb ecompleteenoughtocoverthewholelanguage,butthiswillnothaveabigim-pactonparsing.1Intro ductionLexicalizedTreeAdjoiningGrammar(LAG)isatree-rewritingformalism.Itismoreexpressivethanacontext-freegrammar(CFG),1andthereforeb et-terformalismforrepresentingvariousphenomenainnaturallanguages.Inthelastdecade,ithasb eenappliedtovariousNLPtaskssuchasparsing(Srini-vas,1997),machinetranslation(Palmeretal.,1998),informationretrieval(ChandrasekarandSrinivas,1997),generation(StoneandDoran,1997;McCoyetal.,1992),andsummarizationapplications(Bald-winetal.,1997).Awide-coverageLTGforapar-ticularnaturallanguageoftencontainsthousandsoftreesandtakesyearstobuild.Therehasb eenworkonextractingCFGs(Shi-raietal.,1995;Charniak,1996;Krotovandoth-ers,1998)andlexicalizedtreegrammars(Neumann,1998;Srinivas,1997)frombracketedcorp ora.Inthispap er,weprop oseanewmetho dforlearningLTAGsfromsuchcorp ora.22PennTreebankandLAG2.1PennTreebankInthispap er,weusetheEnglishPennTreebankasourbracketedcorpus,whichincludesab out1millionTheauthorwishestothankChung-hyeHan,AravindJoshi,MarthaPalmer,CarlosProlo,Ano opSarkar,andthreeanonymousreviewersformanyhelpfulcomments.1LTAGismoreexpressivethanCFformalismb othinweakandstronggenerativecapacity,e.g.itcanhandlecrossdep endencyelegantly.2Arelatedworkis(Srinivas,1997),butitsgoalnottolearnanewLTAGbuttoextracttheusefulinformation,suchasdep endencyandfrequencyoftrees,foranexistingLTAG.","Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( Chen and Vijay-Shanker 2000 ; #AUTHOR_TAG ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .","['Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( Chen and Vijay-Shanker 2000 ; #AUTHOR_TAG ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).', 'However, our approach also generalizes to CFG category-based approaches.', 'In LFG, subcategorization requirements are enforced through semantic forms specifying which grammatical functions are required by a particular predicate.', 'Our approach is based on earlier work on LFG semantic form extraction (van Genabith, Sadler, and Way 1999) and recent progress in automatically annotating the Penn-II and Penn-III Treebanks with LFG f-structures (Cahill et al. 2002;Cahill, McCarthy, et al. 2004).', 'Our technique requires a treebank annotated with LFG functional schemata.', 'In the early approach of van Genabith, Sadler, and Way (1999), this was provided by manually annotating the rules extracted from the publicly available subset of the AP Treebank to automatically produce corresponding f-structures.', 'If the f-structures are of high quality, reliable LFG semantic forms can be generated quite simply by recursively reading off the subcategorizable grammatical functions for each local PRED value at each level of embedding in the f-structures.', 'The work reported in van Genabith, Sadler, and Way (1999) was small scale (100 trees) and proof of concept and required considerable manual annotation work.', 'It did not associate frames with probabilities, discriminate between frames for active and passive constructions, properly reflect the effects of long-distance dependencies (LDDs), or include CFG category information.', 'In this article we show how the extraction process can be scaled to the complete Wall Street Journal (WSJ) section of the Penn-II Treebank, with about one million words in 50,000 sentences, based on the automatic LFG f-structure annotation algorithm described in Cahill et al. (2002) and Cahill, McCarthy, et al. (2004).', 'More recently we have extended the extraction approach to the larger, domain-diverse Penn-III Treebank.', 'Aside from the parsed WSJ section, this version of the treebank contains parses for a subsection of the Brown corpus (almost 385,000 words in 24,000 trees) taken from a variety of text genres. 1 In addition to extracting grammatical-function-For the remainder of this work, when we refer to the Penn-II Treebank, we mean the parse-annotated WSJ, and when we refer to the Penn-III Treebank, we mean the parse-annotated WSJ and Brown corpus combined.']",0,"['Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( Chen and Vijay-Shanker 2000 ; #AUTHOR_TAG ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'Our approach is based on earlier work on LFG semantic form extraction (van Genabith, Sadler, and Way 1999) and recent progress in automatically annotating the Penn-II and Penn-III Treebanks with LFG f-structures (Cahill et al. 2002;Cahill, McCarthy, et al. 2004).', 'Our technique requires a treebank annotated with LFG functional schemata.', 'In the early approach of van Genabith, Sadler, and Way (1999), this was provided by manually annotating the rules extracted from the publicly available subset of the AP Treebank to automatically produce corresponding f-structures.', 'More recently we have extended the extraction approach to the larger, domain-diverse Penn-III Treebank.', 'Aside from the parsed WSJ section, this version of the treebank contains parses for a subsection of the Brown corpus (almost 385,000 words in 24,000 trees) taken from a variety of text genres. 1 In addition to extracting grammatical-function-For the remainder of this work, when we refer to the Penn-II Treebank, we mean the parse-annotated WSJ, and when we refer to the Penn-III Treebank, we mean the parse-annotated WSJ and Brown corpus combined.']"
CC478,J05-3003,Gaussian coordinates and the large scale universe,the derivation of a grammatically indexed lexicon from the longman dictionary of contemporary english,"['Branimir Boguraev', 'Edward Briscoe', 'John Carroll', 'David Carter', 'Claire Grover']",related work,"We describe a methodology and associated software system for the construction of a large lexicon from an existing machine-readable (published) dictionary. The lexicon serves as a component of an English morphological and syntactic analyser and contains entries with grammatical definitions compatible with the word and sentence grammar employed by the analyser. We describe a software system with two integrated components. One of these is capable of extracting syntactically rich, theory-neutral lexical templates from a suitable machine-readable source. The second supports interactive and semi-automatic generation and testing of target lexical entries in order to derive a sizeable, accurate and consistent lexicon from the source dictionary which contains partial (and occasionally in-accurate) information. Finally, we evaluate the utility of the Longman Dictionary of Contemporary English as a suitable source dictionary for the target lexicon.","Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #AUTHOR_TAG ) dictionaries and adding around 30 frames found by manual inspection .","['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', 'Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #AUTHOR_TAG ) dictionaries and adding around 30 frames found by manual inspection .', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']",0,"['Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #AUTHOR_TAG ) dictionaries and adding around 30 frames found by manual inspection .', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.']"
CC479,J05-3003,Gaussian coordinates and the large scale universe,natural language parsing as statistical pattern recognition,['David Magerman'],method,"Traditional natural language parsers are based on rewrite rule systems developed in an arduous, time-consuming manner by grammarians. A majority of the grammarian's efforts are devoted to the disambiguation process, first hypothesizing rules which dictate constituent categories and relationships among words in ambiguous sentences, and then seeking exceptions and corrections to these rules.  In this work, I propose an automatic method for acquiring a statistical parser from a set of parsed sentences which takes advantage of some initial linguistic input, but avoids the pitfalls of the iterative and seemingly endless grammar development process. Based on distributionally-derived and linguistically-based features of language, this parser acquires a set of statistical decision trees which assign a probability distribution on the space of parse trees given the input sentence. These decision trees take advantage of significant amount of contextual information, potentially including all of the lexical information in the sentence, to produce highly accurate statistical models of the disambiguation process. By basing the disambiguation criteria selection on entropy reduction rather than human intuition, this parser development method is able to consider more sentences than a human grammarian can when making individual disambiguation rules.  In experiments between a parser, acquired using this statistical framework, and a grammarian's rule-based parser, developed over a ten-year period, both using the same training material and test sentences, the decision tree parser significantly outperformed the grammar-based parser on the accuracy measure which the grammarian was trying to maximize, achieving an accuracy of 78% compared to the grammar-based parser's 69%.","The annotation procedure is dependent on locating the head daughter , for which an amended version of #AUTHOR_TAG is used .","['We utilize the automatic annotation algorithm of Cahill et al. (2002) and Cahill, McCarthy, et al. (2004) to derive a version of Penn-II in which each node in each tree is annotated with LFG functional annotations in the form of attribute-value structure equations.', 'The algorithm uses categorial, configurational, local head, and Penn-II functional and trace information.', 'The annotation procedure is dependent on locating the head daughter , for which an amended version of #AUTHOR_TAG is used .', 'The head is annotated with the LFG equation ↑=↓.', 'Linguistic generalizations are provided over the left (the prefix) and the right (suffix) context of the head for each syntactic category occurring as the mother nodes of such heads.', ""To give a simple example, the rightmost NP to the left of a VP head under an S is likely to be the subject of the sentence (↑ SUBJ =↓), while the leftmost NP to the right of the V head of a VP is most probably the verb's object (↑ OBJ =↓)."", 'Cahill, McCarthy, et al. (2004) provide four classes of annotation principles: one for noncoordinate configurations, one for coordinate configurations, one for traces (long-distance dependencies), and a final ""catch all and clean up"" phase.']",5,"['We utilize the automatic annotation algorithm of Cahill et al. (2002) and Cahill, McCarthy, et al. (2004) to derive a version of Penn-II in which each node in each tree is annotated with LFG functional annotations in the form of attribute-value structure equations.', 'The algorithm uses categorial, configurational, local head, and Penn-II functional and trace information.', 'The annotation procedure is dependent on locating the head daughter , for which an amended version of #AUTHOR_TAG is used .', 'The head is annotated with the LFG equation |=|.', 'Linguistic generalizations are provided over the left (the prefix) and the right (suffix) context of the head for each syntactic category occurring as the mother nodes of such heads.', 'Cahill, McCarthy, et al. (2004) provide four classes of annotation principles: one for noncoordinate configurations, one for coordinate configurations, one for traces (long-distance dependencies), and a final ""catch all and clean up"" phase.']"
CC480,J05-3003,Gaussian coordinates and the large scale universe,lexical functional grammar volume 34 of syntax and semantics,['Mary Dalrymple'],introduction,,"In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; #AUTHOR_TAG ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; #AUTHOR_TAG ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']",0,"['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; #AUTHOR_TAG ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']"
CC481,J05-3003,Gaussian coordinates and the large scale universe,the penn treebank annotating predicate argument structure,"['Mitchell Marcus', 'Grace Kim', 'Mary Ann Marcinkiewicz', 'Robert MacIntyre', 'Mark Ferguson', 'Karen Katz', 'Britta Schasberger']",method,"The Penn Treebank has recently implemented a new syntactic annotation scheme, designed to highlight aspects of predicate-argument structure. This paper discusses the implementation of crucial aspects of this new annotation scheme. It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides a set of coindexed null elements in what can be thought of as ""underlying"" position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions, provides some non-context free annotational mechanism to allow the structure of discontinuous constituents to be easily recovered, and allows for a clear, concise tagging system for some semantic roles.","However , more recent work ( Cahill et al. 2002 ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank ( #AUTHOR_TAG ) , containing more than 1,000,000 words and 49,000 sentences .","['The first step in the application of our methodology is the production of a treebank annotated with LFG f-structure information.', 'F-structures are attribute-value structures which represent abstract syntactic information, approximating to basic predicate-argument-modifier structures.', 'Most of the early work on automatic f-structure annotation (e.g., van Genabith, Way, and Sadler 1999;Frank 2000;Sadler, van Genabith, and Way 2000) was applied only to small data sets (fewer than 200 sentences) and was largely proof of concept.', 'However , more recent work ( Cahill et al. 2002 ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank ( #AUTHOR_TAG ) , containing more than 1,000,000 words and 49,000 sentences .']",0,"['However , more recent work ( Cahill et al. 2002 ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank ( #AUTHOR_TAG ) , containing more than 1,000,000 words and 49,000 sentences .']"
CC482,J05-3003,Gaussian coordinates and the large scale universe,headdriven phrase structure grammar,"['Carl Pollard', 'Ivan Sag']",introduction,,"In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ #AUTHOR_TAG ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ #AUTHOR_TAG ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']",0,"['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ #AUTHOR_TAG ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']"
CC483,J05-3003,Gaussian coordinates and the large scale universe,extracting tree adjoining grammars from bracketed corpora,['Fei Xia'],,"ExtractingTreeAdjoiningGrammarsfromBracketedCorp oraFeiXiaDepartmentofComputerandInformationScienceUniversityofPennsylvania3401WalnutStreet,Suite400APhiladelphiaPA19104,USAfxia@linc.cis.upenn.eduAbstractInthispap er,werep ortourorkonextractinglexi-calizedtreeadjoininggrammars(LTAGs)frompar-tiallybracketedcorp ora.Thealgorithm rstfullybracketsthecorp ora,thenextractselementarytrees(etrees),and nally ltersoutinvalidusinglinguisticknowledge.Weshowthatthesetofex-tractedetreesmaynotb ecompleteenoughtocoverthewholelanguage,butthiswillnothaveabigim-pactonparsing.1Intro ductionLexicalizedTreeAdjoiningGrammar(LAG)isatree-rewritingformalism.Itismoreexpressivethanacontext-freegrammar(CFG),1andthereforeb et-terformalismforrepresentingvariousphenomenainnaturallanguages.Inthelastdecade,ithasb eenappliedtovariousNLPtaskssuchasparsing(Srini-vas,1997),machinetranslation(Palmeretal.,1998),informationretrieval(ChandrasekarandSrinivas,1997),generation(StoneandDoran,1997;McCoyetal.,1992),andsummarizationapplications(Bald-winetal.,1997).Awide-coverageLTGforapar-ticularnaturallanguageoftencontainsthousandsoftreesandtakesyearstobuild.Therehasb eenworkonextractingCFGs(Shi-raietal.,1995;Charniak,1996;Krotovandoth-ers,1998)andlexicalizedtreegrammars(Neumann,1998;Srinivas,1997)frombracketedcorp ora.Inthispap er,weprop oseanewmetho dforlearningLTAGsfromsuchcorp ora.22PennTreebankandLAG2.1PennTreebankInthispap er,weusetheEnglishPennTreebankasourbracketedcorpus,whichincludesab out1millionTheauthorwishestothankChung-hyeHan,AravindJoshi,MarthaPalmer,CarlosProlo,Ano opSarkar,andthreeanonymousreviewersformanyhelpfulcomments.1LTAGismoreexpressivethanCFformalismb othinweakandstronggenerativecapacity,e.g.itcanhandlecrossdep endencyelegantly.2Arelatedworkis(Srinivas,1997),butitsgoalnottolearnanewLTAGbuttoextracttheusefulinformation,suchasdep endencyandfrequencyoftrees,foranexistingLTAG.","Following Hockenmaier , Bierner , and Baldridge ( 2002 ) , #AUTHOR_TAG , and Miyao , Ninomiya , and Tsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ .","['In addition to evaluating the quality of our extracted semantic forms, we also examined the rate at which they are induced.', 'This can be expressed as a measure of the coverage of the induced lexicon on new data.', 'Following Hockenmaier , Bierner , and Baldridge ( 2002 ) , #AUTHOR_TAG , and Miyao , Ninomiya , and Tsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ .', 'We then compare this to a test lexicon from Section 23.', 'Table 27 shows the results of the evaluation of the coverage of an induced lexicon for verbs only.', 'There is a corresponding semantic form in the reference lexicon for 89.89% of the verbs in Section 23. 10.11% of the entries in the test lexicon did not appear in the reference lexicon.', 'Within this group, we can distinguish between known words, which have an entry in the reference lexicon, and unknown words, which do not exist at all in the reference lexicon.', 'In the same way we make the distinction  between known frames and unknown frames.', 'There are, therefore, four different cases in which an entry may not appear in the reference lexicon.', 'Table 27 shows that the most common case is that of known verbs occurring with a different, although known, subcategorization frame (7.85%).']",5,"['In addition to evaluating the quality of our extracted semantic forms, we also examined the rate at which they are induced.', 'Following Hockenmaier , Bierner , and Baldridge ( 2002 ) , #AUTHOR_TAG , and Miyao , Ninomiya , and Tsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ .']"
CC484,J05-3003,Gaussian coordinates and the large scale universe,from grammar to lexicon unsupervised learning of lexical syntax,['Michael Brent'],related work,"Imagine a language that is completely unfamiliar; the only means of studying it are an ordinary grammar book and a very large corpus of text. No dictionary is available. How can easily recognized, surface grammatical facts be used to extract from a corpus as much syntactic information as possible about individual words? This paper describes an approach based on two principles. First, rely on local morpho-syntactic cues to structure rather than trying to parse entire sentences. Second, treat these cues as probabilistic rather than absolute indicators of syntactic structure. Apply inferential statistics to the data collected using the cues, rather than drawing a categorical conclusion from a single occurrence of a cue. The effectiveness of this approach for inferring the syntactic frames of verbs is supported by experiments on an English corpus using a program called Lerner. Lerner starts out with no knowledge of content words---it bootstraps from determiners, auxiliaries, modals, prepositions, pronouns, complementizers, coordinating conjunctions, and punctuation.",#AUTHOR_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames .,"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', '#AUTHOR_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames .', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', 'Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']",0,"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', '#AUTHOR_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames .', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.']"
CC485,J05-3003,Gaussian coordinates and the large scale universe,lexical functional grammar a formal system for grammatical representation,"['Ronald Kaplan', 'Joan Bresnan']",,In learning their native language children develop a remarkable set of capabilities They acquire knowledge and skills that enable them to pro duce and comprehend an inde nite number of novel utterances and to make quite subtle judgments about certain of their properties The ma jor goal of psycholinguistic research is to devise an explanatory account of the mental operations that underlie these linguistic abilities In pursuing this goal we have adopted what we call the Competence Hypothesis as a methodological principle We assume that an explana tory model of human language performance will incorporate a theoreti cally justi ed representation of the native speaker s linguistic knowledge a grammar as a component separate both from the computational mech anisms that operate on it a processor and from other nongrammatical processing parameters that might in uence the processor s behavior To a certain extent the various components that we postulate can be studied independently guided where appropriate by the well established methods and evaluation standards of linguistics computer science and experimen tal psychology However the requirement that the various components ultimately must t together in a consistent and coherent model imposes even stronger constraints on their structure and operation,Lexical functional grammar ( #AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .,"['Lexical functional grammar ( #AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', 'C-structure takes the form of phrase structure trees and is defined in terms of CFG rules and lexical entries.', 'F-structure is produced from functional annotations on the nodes of the c-structure and implemented in terms of recursive feature structures (attribute-value matrices).', 'This is exemplified by the analysis of the string The inquiry soon focused on the judge (wsj 0267 72) using the grammar in Figure 1, which results in the annotated c-structure and f-structure in Figure 2.']",0,['Lexical functional grammar ( #AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .']
CC486,J05-3003,Gaussian coordinates and the large scale universe,automatic extraction of subcategorization frames for czech,"['Anoop Sarkar', 'Daniel Zeman']",related work,"We present some novel machine learning techniques for the identification of subcategorization information for verbs in Czech. We compare three different statistical techniques applied to this problem. We show how the learning algorithm can be used to discover previously unknown subcategorization frames from the Czech Prague Dependency Treebank. The algorithm can then be used to label dependents of a verb in the Czech treebank as either arguments or adjuncts. Using our techniques, we are able to achieve 88 % precision on unseen parsed text.",#AUTHOR_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic,"['Approaches using treebank-based data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data.', 'Kinyon and Prolo (2002) describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank.', 'This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'Each of these sequences was categorized as a modifier or argument.', 'Arguments were then mapped to traditional syntactic functions.', 'For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic function is subject.', 'In general, argumenthood was preferred over adjuncthoood.', 'As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is.', '#AUTHOR_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic', 'Czech is a language with a freer word order than English and so configurational information cannot be relied upon.', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'Finding subcategorization frames involves filtering adjuncts from the observed frame.', 'This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score.', 'The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more).', 'Marinov and Hemming (2004) present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank (Simov, Popova, and Osenova 2002).', ""In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming's system collects both arguments and adjuncts."", 'It then uses the binomial log-likelihood ratio to filter incorrect frames.', 'The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees.', 'The work done for Bulgarian is small-scale, however, as Marinov and Hemming are working with a preliminary version of the treebank with 580 sentences.']",0,"['Approaches using treebank-based data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data.', 'Kinyon and Prolo (2002) describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank.', 'This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'Each of these sequences was categorized as a modifier or argument.', 'Arguments were then mapped to traditional syntactic functions.', '#AUTHOR_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic', 'Czech is a language with a freer word order than English and so configurational information cannot be relied upon.', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'Finding subcategorization frames involves filtering adjuncts from the observed frame.', 'The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more).', 'Marinov and Hemming (2004) present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank (Simov, Popova, and Osenova 2002).', ""In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming's system collects both arguments and adjuncts."", 'The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees.']"
CC487,J05-3003,Gaussian coordinates and the large scale universe,three generative lexicalised models for statistical parsing,['Michael Collins'],related work,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97","The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and #AUTHOR_TAG .","['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing.', ""The extraction procedure utilizes a head percolation table as introduced by Magerman (1995) in combination with a variation of Collins's (1997) approach to the differentiation between complement and adjunct."", 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and #AUTHOR_TAG .', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.', 'The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman (1994) and Collins (1997).', 'Each node is subsequently assigned the relevant category based on its constituent type and surface configuration.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Unlike our approach, those of Xia (1999) and Hockenmaier, Bierner, and Baldridge (2004) include a substantial initial correction and clean-up of the Penn-II trees.', 'Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of ""inverse schemata.""']",0,"['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing.', 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and #AUTHOR_TAG .', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.']"
CC488,J05-3003,Gaussian coordinates and the large scale universe,lexical functional grammar volume 34 of syntax and semantics,['Mary Dalrymple'],method,,"#AUTHOR_TAG argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization .","['In order to capture CFG-based categorial information, we add a CAT feature to the f-structures automatically generated from the Penn-II and Penn-III Treebanks.', 'Its value is the syntactic category of the lexical item whose lemma gives rise to the PRED value at that particular level of embedding.', 'This makes it possible to classify words and their semantic forms based on their syntactic category and reduces the risk of inaccurate assignment of subcategorization frame frequencies due to POS ambiguity, distinguishing, for example, between the nominal and verbal occurrences of the lemma fight.', 'With this, the output for the verb impose in Figure 4 is impose (v,[subj, obj, obl:on]).', 'For some of our experiments, we conflate the different verbal (and other) tags used in the Penn Treebanks to a single verbal marker (Table 4).', 'As a further extension, the extraction procedure reads off the syntactic category of the head of each of the subcategorized syntactic functions: impose(v,[subj(n),obj(n),obl:on]). 3 In this way, our methodology is able to produce surface syntactic as well as abstract functional subcategorization details.', '#AUTHOR_TAG argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization .', 'In contrast to much of the work reviewed in Section 3, which limits itself to the extraction of surface syntactic subcategorization details, our system can provide this information as well as details of grammatical function.']",4,"['In order to capture CFG-based categorial information, we add a CAT feature to the f-structures automatically generated from the Penn-II and Penn-III Treebanks.', 'Its value is the syntactic category of the lexical item whose lemma gives rise to the PRED value at that particular level of embedding.', 'This makes it possible to classify words and their semantic forms based on their syntactic category and reduces the risk of inaccurate assignment of subcategorization frame frequencies due to POS ambiguity, distinguishing, for example, between the nominal and verbal occurrences of the lemma fight.', 'As a further extension, the extraction procedure reads off the syntactic category of the head of each of the subcategorized syntactic functions: impose(v,[subj(n),obj(n),obl:on]). 3 In this way, our methodology is able to produce surface syntactic as well as abstract functional subcategorization details.', '#AUTHOR_TAG argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization .', 'In contrast to much of the work reviewed in Section 3, which limits itself to the extraction of surface syntactic subcategorization details, our system can provide this information as well as details of grammatical function.']"
CC489,J05-3003,Gaussian coordinates and the large scale universe,from treebank to propbank,"['Paul Kingsbury', 'Martha Palmer']",,,"In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank ( #AUTHOR_TAG ) .","['We have presented an algorithm for the extraction of semantic forms (or subcategorization frames) from the Penn-II and Penn-III Treebanks, automatically annotated with LFG f-structures.', 'In contrast to many other approaches, ours does not predefine the subcategorization frames we extract.', ""We have applied the algorithm to the WSJ sections of Penn-II (50,000 trees) (O' Donovan et al. 2004) and to the parse-annotated Brown corpus of Penn-III (almost 25,000 additional trees)."", 'We extract syntactic-function-based subcategorization frames (LFG semantic forms) and traditional CFG category-based frames, as well as mixed-function-category-based frames.', 'Unlike many other approaches to subcategorization frame extraction, our system properly reflects the effects of long-distance dependencies.', 'Also unlike many approaches, our method distinguishes between active and passive frames.', 'Finally, our system associates conditional probabilities with the frames we extract.', 'Making the distinction between the behavior of verbs in active and passive contexts is particularly important for the accurate assignment of probabilities to semantic forms.', 'We carried out an extensive evaluation of the complete induced lexicon against the full COMLEX resource.', 'To our knowledge, this is the most extensive qualitative evaluation of subcategorization extraction in English.', 'The only evaluation of a similar scale is that carried out by Schulte im Walde (2002b) for German.', ""The results reported here for Penn-II compare favorably against the baseline and, in fact, are an improvement on those reported in O' Donovan et al. (2004)."", 'The results for the larger, more domain-diverse Penn-III lexicon are very encouraging, in some cases almost 15% above the baseline.', 'We believe our semantic forms are fine-grained, and by choosing to evaluate against COMLEX, we set our sights high: COMLEX is considerably more detailed than the OALD or LDOCE used for other earlier evaluations.', 'Our error analysis also revealed some interesting issues associated with using an external standard such as COMLEX.', 'In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank ( #AUTHOR_TAG ) .']",3,"[""We have applied the algorithm to the WSJ sections of Penn-II (50,000 trees) (O' Donovan et al. 2004) and to the parse-annotated Brown corpus of Penn-III (almost 25,000 additional trees)."", 'In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank ( #AUTHOR_TAG ) .']"
CC490,J05-3003,Gaussian coordinates and the large scale universe,how verb subcategorization frequencies are affected by corpus choice,"['Douglas Roland', 'Daniel Jurafsky']",,"The probabilistic relation between verbs and their arguments plays an important role in modern statistical parsers and supertaggers, and in psychological theories of language processing. But these probabilities are computed in very different ways by the two sets of researchers. Computational linguists compute verb subcategorization probabilities from large corpora while psycholinguists compute them from psychological studies (sentence production and completion tasks). Recent studies have found differences between corpus frequencies and psycholinguistic measures. We analyze subcategorization frequencies from four different corpora: psychological sentence production data (Connine et al. 1984), written text (Brown and WSJ), and telephone conversation data (Switchboard). We find two different sources for the differences. Discourse influence is a result of how verb use is affected by different discourse types such as narrative, connected discourse, and single sentence productions. Semantic influence is a result of different corpora using different senses of verbs, which have different subcategorization frequencies. We conclude that verb sense and discourse type play an important role in the frequencies observed in different experimental and corpus based sources of verb subcategorization frequencies.","As noted above , it is well documented ( #AUTHOR_TAG ) that subcategorization frames ( and their frequencies ) vary across domains .","['Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As noted above , it is well documented ( #AUTHOR_TAG ) that subcategorization frames ( and their frequencies ) vary across domains .', 'We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ.', 'For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon.', 'It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases.', 'This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs.', 'Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples.', 'As a generalization, Briscoe (2001) notes that lexicons such as COMLEX tend to demonstrate high precision but low recall.', 'Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX.', 'Precision was quite high (95%), but recall was low (84%).', 'This has an effect on both the precision and recall scores of our system against COMLEX.', 'In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26.', 'We randomly selected 80 false negatives (fn) and 80 false positives (fp) across a range of active frame types containing prepositional and particle detail taken from Penn-III and manually examined them in order to classify them as ""correct"" or ""incorrect.""', 'Of the 80 fps, 33 were manually judged to be legitimate subcategorization frames.', 'For example, as Table 26 shows, there are a number of correct transitive verbs ([subj,obj]) in our automatically induced lexicon which are not included in COMLEX.', 'This examination was also useful in highlighting to us the frame types on which the lexical extraction procedure was performing poorly, in our case, those containing XCOMPs and those containing OBJ2S.', 'Out of 80 fns, 14 were judged to be incorrect when manually examined.', 'These can be broken down as follows: one intransitive frame, three ditransitive frames, three frames containing a COMP, and seven frames containing an oblique were found to be invalid.']",4,"['As noted above , it is well documented ( #AUTHOR_TAG ) that subcategorization frames ( and their frequencies ) vary across domains .', 'Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX.']"
CC491,J05-3003,Gaussian coordinates and the large scale universe,automated extraction of tags from the penn treebank,"['John Chen', 'K Vijay-Shanker']",related work,"The accuracy of statistical parsing models can be improved with the use of lexical information. Statistical parsing using Lexicalized tree adjoining grammar (LTAG), a kind of lexicalized grammar, has remained relatively unexplored. We believe that is largely in part due to the absence of large corpora accurately bracketed in terms of a perspicuous yet broad coverage LTAG. Our work attempts to alleviate this difficulty. We extract different LTAGs from the Penn Treebank. We show that certain strategies yield an improved extracted LTAG in terms of compactness, broad coverage, and supertagging accuracy. Furthermore, we perform a preliminary investigation in smoothing these grammars by means of an external linguistic resource, namely, the tree families of an XTAG grammar, a hand built grammar of English.",#AUTHOR_TAG explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing .,"['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', '#AUTHOR_TAG explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing .', ""The extraction procedure utilizes a head percolation table as introduced by Magerman (1995) in combination with a variation of Collins's (1997) approach to the differentiation between complement and adjunct."", 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.', 'The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman (1994) and Collins (1997).', 'Each node is subsequently assigned the relevant category based on its constituent type and surface configuration.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Unlike our approach, those of Xia (1999) and Hockenmaier, Bierner, and Baldridge (2004) include a substantial initial correction and clean-up of the Penn-II trees.', 'Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of ""inverse schemata.""']",0,"['#AUTHOR_TAG explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing .', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).']"
CC492,J05-3003,Gaussian coordinates and the large scale universe,treebank grammars in,['Eugene Charniak'],,,"In #AUTHOR_TAG and Krotov et al. ( 1998 ) , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .","['The rate of accession may also be represented graphically.', 'In #AUTHOR_TAG and Krotov et al. ( 1998 ) , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', 'We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity.', 'Figure 8 graphs the rate of induction of semantic form and CFG rule types from Penn-III (the WSJ and parse-annotated Brown corpus combined).', 'Because of the variation in the size of sections between the Brown and the WSJ, we plotted accession against word count.', 'The first part of the graph (up to 1,004,414 words)']",0,"['In #AUTHOR_TAG and Krotov et al. ( 1998 ) , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .']"
CC493,J05-3003,Gaussian coordinates and the large scale universe,automatic acquisition of a large subcategorisation dictionary from corpora,['Christopher Manning'],introduction,,"#AUTHOR_TAG argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature .","['One important type of lexical information is the subcategorization requirements of an entry (i.e., the arguments a predicate must take in order to form a grammatical construction).', 'Lexicons, including subcategorization details, were traditionally produced by hand.', 'However, as the manual construction of lexical resources is time consuming, error prone, expensive, and rarely ever complete, it is often the case that the limitations of NLP systems based on lexicalized approaches are due to bottlenecks in the lexicon component.', 'In addition, subcategorization requirements may vary across linguistic domain or genre (Carroll and Rooth 1998).', '#AUTHOR_TAG argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature .', 'Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue.']",4,"['One important type of lexical information is the subcategorization requirements of an entry (i.e., the arguments a predicate must take in order to form a grammatical construction).', 'Lexicons, including subcategorization details, were traditionally produced by hand.', 'However, as the manual construction of lexical resources is time consuming, error prone, expensive, and rarely ever complete, it is often the case that the limitations of NLP systems based on lexicalized approaches are due to bottlenecks in the lexicon component.', 'In addition, subcategorization requirements may vary across linguistic domain or genre (Carroll and Rooth 1998).', '#AUTHOR_TAG argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature .', 'Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue.']"
CC494,J05-3003,Gaussian coordinates and the large scale universe,lexicalfunctional syntax,['Joan Bresnan'],introduction,,"In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']",0,"['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']"
CC495,J05-3003,Gaussian coordinates and the large scale universe,the automatic acquisition of frequencies of verb subcategorization frames from tagged corpora,"['Akira Ushioda', 'David Evans', 'Ted Gibson', 'Alex Waibel']",related work,"We describe a mechanism for automatically acquiring verb subcategorization frames and their frequencies in a large corpus. A tagged corpus is first partially parsed to identify noun phrases and then a finear grammar is used to estimate the appropriate subcategorization frame for each verb token in the corpus. In an experiment involving the identification of six fixed subcategorization frames, our current system showed more than 80% accuracy. In addition, a new statistical approach substantially improves the accuracy of the frequency estimation.",#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .,"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', '#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']",0,"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', '#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .', 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.']"
CC496,J05-3003,Gaussian coordinates and the large scale universe,from dictionary to corpus to selforganizing dictionary learning valency associations in the face of variation and change,['Edward Briscoe'],,"ing over specific lexically-governed particles and prepositions and specific predicate selectional preferences, but including some `derived' / `alternant' semi-productive, and therefore only semipredictable, bounded dependency constructions, such as particle or dative movement, there are at least 163 valency frames associated with verbal predicates in (current) English (Briscoe, 2000). In this paper, I will review the work that my colleagues and I have done to learn (semi-)automatically this very large number of associations between individual verbal predicates and valency frames. Access to a comprehensive and accurate valency lexicon is critical for the development of robust and accurate parsing technology capable of recovering predicate-argument relations (and thus logical forms) from free text or transcribed speech. Without this information it is possible to `chunk' input into phrases but not to distinguish arguments from adjuncts or resolve most phrasal attachment ambiguities. Furthermore, for statistical parsers it is not enough to know the associations of predicates to valency frames, it is also critical to know the relative frequency of such associations given a specific predicate. Such information is a core component of that required to `lexicalize' a probabilistic parser, and it is now well-established that lexicalization is essential for accurate disambiguation (e.g. Collins, 1997, Carroll et al, 1998). While state-of-the-art wide-coverage grammars of English, capable of recovering predicateargument structure and expressed as a unification-based phrase structure grammar, have on the order of 1000 rules, it is clear that the number of associations between valency frames and predicates needed in a lexicon for such a grammar will be much higher.","As a generalization , #AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .","['Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As noted above, it is well documented (Roland and Jurafsky 1998) that subcategorization frames (and their frequencies) vary across domains.', 'We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ.', 'For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon.', 'It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases.', 'This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs.', 'Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples.', 'As a generalization , #AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .', 'Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX.', 'Precision was quite high (95%), but recall was low (84%).', 'This has an effect on both the precision and recall scores of our system against COMLEX.', 'In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26.', 'We randomly selected 80 false negatives (fn) and 80 false positives (fp) across a range of active frame types containing prepositional and particle detail taken from Penn-III and manually examined them in order to classify them as ""correct"" or ""incorrect.""', 'Of the 80 fps, 33 were manually judged to be legitimate subcategorization frames.', 'For example, as Table 26 shows, there are a number of correct transitive verbs ([subj,obj]) in our automatically induced lexicon which are not included in COMLEX.', 'This examination was also useful in highlighting to us the frame types on which the lexical extraction procedure was performing poorly, in our case, those containing XCOMPs and those containing OBJ2S.', 'Out of 80 fns, 14 were judged to be incorrect when manually examined.', 'These can be broken down as follows: one intransitive frame, three ditransitive frames, three frames containing a COMP, and seven frames containing an oblique were found to be invalid.']",0,"['As a generalization , #AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .']"
CC497,J05-4005,Chinese Word Segmentation and Named Entity Recognition: A Pragmatic Approach,a stochastic finitestate wordsegmentation algorithm for chinese,"['Richard Sproat', 'Chilin Shih', 'William Gale', 'Nancy Chang']",related work,,"A previous work along this line is #AUTHOR_TAG , which is based on weighted finite-state transducers ( FSTs ) .","['We believe that the identification of OOV words should not be treated as a problem separate from word segmentation.', 'We propose a unified approach that solves both problems simultaneously.', 'A previous work along this line is #AUTHOR_TAG , which is based on weighted finite-state transducers ( FSTs ) .', 'Our approach is similarly motivated but is based on a different mechanism: linear mixture models.', 'As we shall see, the models provide a more flexible framework for incorporating various kinds of lexical and statistical information.', ""Many types of OOV words that are not covered in Sproat's system can be dealt with in our system."", 'The linear models we used are originally derived from linear discriminant functions widely used for pattern classification (Duda, Hart, and Stork 2001) and have been recently introduced into NLP tasks by Collins and Duffy (2001).', 'Other frameworks of Chinese word segmentation, which are similar to the linear models, include maximum entropy models (Xue 2003) and conditional random fields (Peng, Feng, and McCallum 2004).', 'They also use a unified approach to word breaking and OOV identification.']",1,"['We believe that the identification of OOV words should not be treated as a problem separate from word segmentation.', 'A previous work along this line is #AUTHOR_TAG , which is based on weighted finite-state transducers ( FSTs ) .', 'As we shall see, the models provide a more flexible framework for incorporating various kinds of lexical and statistical information.', ""Many types of OOV words that are not covered in Sproat's system can be dealt with in our system."", 'Other frameworks of Chinese word segmentation, which are similar to the linear models, include maximum entropy models (Xue 2003) and conditional random fields (Peng, Feng, and McCallum 2004).']"
CC498,J06-2002,Generating Referring Expressions that Involve Gradable Properties,computational interpretations of the gricean maximes in the generation of referring expressions,"['Robbert Dale', 'Ehud Reiter']",experiments,"We examine the problem of generating definite noun phrases that are appropriate referring expressions; that is, noun phrases that (a) successfully identify the intended referent to the hearer whilst (b) not conveying to him or her any false conversational implicatures (Grice, 1975). We review several possible computational interpretations of the conversational implicature maxims, with different computational costs, and argue that the simplest may be the best, because it seems to be closest to what human speakers do. We describe our recommended algorithm in detail, along with a specification of the resources a host system must provide in order to make use of the algorithm, and an implementation used in the natural language generation component of the IDAS system.","#AUTHOR_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) .","['Common sense (as well as the Gricean maxims; Grice 1975) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication.', 'We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented.', '#AUTHOR_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) .', 'They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 "" bolt, human assemblers ""unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt"" (Dale and Reiter 1995). 6 Our own experiments (van Deemter 2004) point in the same direction.', 'In one experiment, for example, 34 students at the University of Brighton were shown six pieces of paper, each of which showed two isosceles and approximately equilateral triangles.', 'Triangles of three sizes were shown, with bases of 5, 8, and 16 mm respectively.', 'On each sheet, one of the two triangles had been circled with a pencil.', 'We asked subjects to imagine themselves on the phone to someone who held a copy of the same sheet, but not necessarily with the same orientation (e.g., possibly upside down), and to complete the answers in the following: Q: Which triangle on this sheet was circled?', 'A: The ............ triangle.']",0,"['Common sense (as well as the Gricean maxims; Grice 1975) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication.', 'We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented.', '#AUTHOR_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) .', 'They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 "" bolt, human assemblers ""unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt"" (Dale and Reiter 1995). 6 Our own experiments (van Deemter 2004) point in the same direction.', 'Triangles of three sizes were shown, with bases of 5, 8, and 16 mm respectively.', 'On each sheet, one of the two triangles had been circled with a pencil.', 'A: The ............ triangle.']"
CC499,J06-2002,Generating Referring Expressions that Involve Gradable Properties,situations and attitudes,"['Jon Barwise', 'John Perry']",introduction,"In this provocative book, Barwise and Perry tackle the slippery subject of ""meaning, "" a subject that has long vexed linguists, language philosophers, and logicians.","Viewed in this way , gradable adjectives are an extreme example of the ""efficiency of language"" ( #AUTHOR_TAG ) : Far from meaning something concrete like ""larger than 8 cm"" -- a concept that would have very limited applicability -- or even something more general like ""larger than the average N , ""a word like large is applicable across a wide range of different situations .","['Viewed in this way , gradable adjectives are an extreme example of the ""efficiency of language"" ( #AUTHOR_TAG ) : Far from meaning something concrete like ""larger than 8 cm"" -- a concept that would have very limited applicability -- or even something more general like ""larger than the average N , ""a word like large is applicable across a wide range of different situations .']",1,"['Viewed in this way , gradable adjectives are an extreme example of the ""efficiency of language"" ( #AUTHOR_TAG ) : Far from meaning something concrete like ""larger than 8 cm"" -- a concept that would have very limited applicability -- or even something more general like ""larger than the average N , ""a word like large is applicable across a wide range of different situations .']"
CC500,J06-2002,Generating Referring Expressions that Involve Gradable Properties,projecting the adjective the syntax and semantics of gradability and comparison,['Christopher Kennedy'],introduction,,"Clearly , what it takes for the adjective to be applicable has not been cast in stone , but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX , #AUTHOR_TAG ) .","['Clearly , what it takes for the adjective to be applicable has not been cast in stone , but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX , #AUTHOR_TAG ) .', 'The numeral (whether it is implicit, as in (3), or explicit) can be construed as allowing the reader to draw inferences about the standards employed (Kyburg and Morreau 2000;DeVault and Stone 2004): (3), for example, implies a standard that counts 10 cm as large and 8 cm as not large.', 'Our own proposal will abstract away from the effects of linguistic context.', 'We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words.', 'This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form of']",0,"['Clearly , what it takes for the adjective to be applicable has not been cast in stone , but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX , #AUTHOR_TAG ) .']"
CC501,J06-2002,Generating Referring Expressions that Involve Gradable Properties,welfare economics and social choice theory,['Allan M Feldman'],experiments,"Preferences and Utility.- Barter Exchange.- Welfare Properties of Market Exchange.- Welfare Properties of ""Jungle"" Exchange.- Economies with Production.- Uncertainty in Exchange.- Externalities.- Public Goods.- Compensation Criteria.- Fairness and the Rawls Criterion.- Life and Death Choices.- Majority Voting.- Arrow's Impossibility Theorem.- Dominant-Strategy Implementation.- Nash Implementation.- Bayesian Implementation.- Epilogue.","Cases like this would be covered if the decision-theoretic property of Pareto optimality ( e.g. , #AUTHOR_TAG ) was used as the sole criterion : Formally , an object r E C has a Pareto-optimal combination of Values V iff there is no other x E C such that","['Cases like this would be covered if the decision-theoretic property of Pareto optimality ( e.g. , #AUTHOR_TAG ) was used as the sole criterion : Formally , an object r E C has a Pareto-optimal combination of Values V iff there is no other x E C such that']",0,"['Cases like this would be covered if the decision-theoretic property of Pareto optimality ( e.g. , #AUTHOR_TAG ) was used as the sole criterion : Formally , an object r E C has a Pareto-optimal combination of Values V iff there is no other x E C such that']"
CC502,J06-2002,Generating Referring Expressions that Involve Gradable Properties,ordering among premodifiers,"['James Shaw', 'Vasileios Hatzivassiloglou']",,"We present a corpus-based study of the sequential ordering among premodifiers in noun phrases. This information is important for the fluency of generated text in practical applications. We propose and evaluate three approaches to identify sequential order among premodifiers: direct evidence, transitive closure, and clustering. Our implemented system can make over 94 % of such ordering decisions correctly, as evaluated on a large, previously unseen test corpus.","One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , #AUTHOR_TAG ; Malouf 2000 ) .","['One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , #AUTHOR_TAG ; Malouf 2000 ) .', ""Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position."", 'Interestingly, vague properties tend to be realized before others.', 'Quirk et al. (1985), for example, report that ""adjectives denoting size, length, and height normally precede other nonderived adjectives"" (e.g., the small round table is usually preferred to the round small table).', 'Semantically, this does not come as a surprise.', 'In a noun phrase of the form the three small (-est) [N], for example, the words preceding N select the three smallest elements of [N].', 'It follows that, to denote the three smallest elements of the set of round tables, the only option is to say the three small round tables, rather than the three round small tables.', 'The latter would mean something else, namely, the three round ones among the n small(est) tables (where n is not specified).', ""It actually seems quite possible to say this, but only when some set of small tables is contextually salient (e.g., I don't mean those small tables, I mean the three round ones)."", 'Given that n is unspecified, the noun phrase would tend to be very unclear in any other context.']",0,"['One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , #AUTHOR_TAG ; Malouf 2000 ) .', 'Interestingly, vague properties tend to be realized before others.', 'Quirk et al. (1985), for example, report that ""adjectives denoting size, length, and height normally precede other nonderived adjectives"" (e.g., the small round table is usually preferred to the round small table).', 'Semantically, this does not come as a surprise.', 'In a noun phrase of the form the three small (-est) [N], for example, the words preceding N select the three smallest elements of [N].', 'The latter would mean something else, namely, the three round ones among the n small(est) tables (where n is not specified).', ""It actually seems quite possible to say this, but only when some set of small tables is contextually salient (e.g., I don't mean those small tables, I mean the three round ones)."", 'Given that n is unspecified, the noun phrase would tend to be very unclear in any other context.']"
CC503,J06-2002,Generating Referring Expressions that Involve Gradable Properties,modern engineering mathematics second edition,"['Glyn James', 'David Burley', 'Dick Clements', 'Phil Dyke', 'John Searl', 'Jerry Wright']",introduction,,"3 The degree of precision of the measurement ( #AUTHOR_TAG , Section 1.5 ) determines which objects can be described by the GRE algorithm , since it determines which objects count as having the same size .","['3 The degree of precision of the measurement ( #AUTHOR_TAG , Section 1.5 ) determines which objects can be described by the GRE algorithm , since it determines which objects count as having the same size .']",0,"['3 The degree of precision of the measurement ( #AUTHOR_TAG , Section 1.5 ) determines which objects can be described by the GRE algorithm , since it determines which objects count as having the same size .']"
CC504,J06-2002,Generating Referring Expressions that Involve Gradable Properties,efficient contextsensitive generation of referring expressions,"['Emiel Krahmer', 'Mari¨et Theune']",,3 A Modification of the Algorithm Based on Salience 5 3.1 Motivation: Determining the Context Set : : : : : : : : : : : : : 5 3.2 Preliminaries : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 3.3 Outline of the Modified Algorithm : : : : : : : : : : : : : : : : : 7 3.4 Examples : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 9 3.5 Discussion : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 11,"Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( Stone and Webber 1998 ; #AUTHOR_TAG ) .","['Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( Stone and Webber 1998 ; #AUTHOR_TAG ) .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', 'Consider, for instance, the list of properties L = size > 3 cm, size < 9 cm .', 'If interleaving forced us to realize the two properties in L one by one, then it would no longer be possible to combine them into, for example, the largest mouse but one (if the facts in the KB support it), or even into the mice between 3 and 9 cm (since size > 3 cm is realized before size < 9 cm).', 'Clearly, sophisticated use of gradable adjectives requires a separation between CD and linguistic realization, unless one is willing to complicate linguistic realization considerably.']",1,"['Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( Stone and Webber 1998 ; #AUTHOR_TAG ) .']"
CC505,J06-2002,Generating Referring Expressions that Involve Gradable Properties,sorites paradox,['Dominic Hyde'],,,"This is the strongest version of the sorites paradox ( e.g. , #AUTHOR_TAG ) .","['NLG has to do more than select a distinguishing description (i.e., one that unambiguously denotes its referent; Dale 1989): The selected expression should also be felicitous.', 'Consider the question, discussed in the philosophical logic literature, of whether it is legitimate, for a gradable adjective, to distinguish between ""observationally indifferent"" entities: Suppose two objects x and y, are so similar that it is impossible to distinguish their sizes; can it ever be reasonable to say that x is large and y is not?', 'A positive answer would not be psychologically plausible, since x and y are indistinguishable; but a negative answer would prohibit any binary distinction between objects that are large and objects that are not, given that one can always construct objects x and y, one of which falls just below the divide while the other falls just above it.', 'This is the strongest version of the sorites paradox ( e.g. , #AUTHOR_TAG ) .']",0,"['This is the strongest version of the sorites paradox ( e.g. , #AUTHOR_TAG ) .']"
CC506,J06-2002,Generating Referring Expressions that Involve Gradable Properties,efficient contextsensitive generation of referring expressions,"['Emiel Krahmer', 'Mari¨et Theune']",introduction,3 A Modification of the Algorithm Based on Salience 5 3.1 Motivation: Determining the Context Set : : : : : : : : : : : : : 5 3.2 Preliminaries : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 3.3 Outline of the Modified Algorithm : : : : : : : : : : : : : : : : : 7 3.4 Examples : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 9 3.5 Discussion : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 11,"We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( Beun and Cremers 1998 , #AUTHOR_TAG ) .","['We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( Beun and Cremers 1998 , #AUTHOR_TAG ) .', 'Before we do this, consider the tractability of the original IA.', 'If the running time of FindBestValue(r, A i ) is a constant times the number of Values of the Attribute A i , then the worst-case running time of IA (and IA Plur ) is O(n v n a ), where n a equals the number of Attributes in the language and n v the average number of Values of all Attributes.', 'This is because, in the worst case, all Values of all Attributes need to be attempted (van Deemter 2002).', 'As for the new algorithm, we focus on the crucial phases 2, 4, and 5.']",0,"['We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( Beun and Cremers 1998 , #AUTHOR_TAG ) .']"
CC507,J06-2002,Generating Referring Expressions that Involve Gradable Properties,what do we mean by ‘usually’,['John H Toogood'],,,"This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #AUTHOR_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) .","['Sometimes we are forced to be vague because the information we have (e.g., based on perception or verbal reports) is itself inexact.', 'Such cases can be modeled by letting NLG systems take vague information (e.g., Rain[Wednesday] = heavy) as their input.', 'We shall focus on the more challenging case where the output of the generator is less precise than the input, as is the case in FOG and DYD.', 'This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #AUTHOR_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) .', 'We shall therefore focus-unlike earlier computational accounts-on vague descriptions, that is, vague expressions in definite descriptions.', 'Here, the context tends to obliterate the vagueness associated with the adjective.', ""Suppose you enter a vet's surgery in the company of two dogs: a big one on a leash, and a tiny one in your arms."", 'The vet asks ""Who\'s the patient?"", and you answer ""the big dog.""', 'This answer will allow the vet to pick out the patient just as reliably as if you had said ""the one on the leash""; the fact that big is a vague term is irrelevant.', 'You omit the exact size of the dog, just like some of its other properties (e.g., the leash), because they do not improve the description.', 'This shows how vague properties can contribute to the precise task of identifying a referent.']",0,"['Sometimes we are forced to be vague because the information we have (e.g., based on perception or verbal reports) is itself inexact.', 'This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #AUTHOR_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) .']"
CC508,J06-2002,Generating Referring Expressions that Involve Gradable Properties,psychologie der objektbenennung,"['Tony Hermann', 'Roland Deutsch']",introduction,,"#AUTHOR_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking .","['Even if comparative properties are at the bottom of the preference order, while stronger inequalities precede weaker ones, the order is not fixed completely.', 'Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x.', 'Which of these should come first?', '#AUTHOR_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking .', ""In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say 'the tall candle' when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to 'the fat candle.'"", ""Hermann and Deutsch's findings may be implemented as follows."", 'First, the Values of the different Attributes should be normalized to make them comparable.', 'Second, preference order should be calculated dynamically (i.e., based on the current value of C, and taking the target into account), preferring larger gaps over smaller ones.', '(It is possible, e.g., that width is most suitable for singling out a black cat, but height for singling out a white cat.)', 'The rest of the algorithm remains unchanged.']",0,"['Which of these should come first?', '#AUTHOR_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking .', ""Hermann and Deutsch's findings may be implemented as follows.""]"
CC509,J06-2002,Generating Referring Expressions that Involve Gradable Properties,the effects of redundant communications on listeners when more is less child development,['Susan Sonnenschein'],experiments,,"While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; Pechmann 1989 ; #AUTHOR_TAG ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .","['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; Pechmann 1989 ; #AUTHOR_TAG ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6).', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'A similar problem is discussed in the psycholinguistics of interpretation (Sedivy et al. 1999): Interpretation is widely assumed to proceed incrementally, but vague descriptions resist strict incrementality, since an adjective in a vague description can only be fully interpreted when its comparison set is known.', 'Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']",0,"['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; Pechmann 1989 ; #AUTHOR_TAG ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']"
CC510,J06-2002,Generating Referring Expressions that Involve Gradable Properties,principles of categorization,['Eleanor Rosch'],introduction,,"The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level , #AUTHOR_TAG ) .","[""FindBestValue selects the 'best value' from among the Values of a given Attribute, assuming that these are linearly ordered in terms of specificity."", 'The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level , #AUTHOR_TAG ) .', 'IA Plur can refer to individuals as well as sets, since reference to a target individual r can be modeled as reference to the singleton set {r}.']",0,"[""FindBestValue selects the 'best value' from among the Values of a given Attribute, assuming that these are linearly ordered in terms of specificity."", 'The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level , #AUTHOR_TAG ) .', 'IA Plur can refer to individuals as well as sets, since reference to a target individual r can be modeled as reference to the singleton set {r}.']"
CC511,J06-2002,Generating Referring Expressions that Involve Gradable Properties,efficient contextsensitive generation of referring expressions,"['Emiel Krahmer', 'Mari¨et Theune']",experiments,3 A Modification of the Algorithm Based on Salience 5 3.1 Motivation: Determining the Context Set : : : : : : : : : : : : : 5 3.2 Preliminaries : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 3.3 Outline of the Modified Algorithm : : : : : : : : : : : : : : : : : 7 3.4 Examples : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 9 3.5 Discussion : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 11,"#AUTHOR_TAG have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available .","['We shall see that a natural treatment of salience falls automatically out of our treatment of vague descriptions.', 'As we shall see, this will allow us to simplify the structure of GRE algorithms, and it will explain why many definite descriptions that look as if they were distinguishing descriptions are actually ambiguous.', '9.4.1 A New Perspective on Salience.', ""#AUTHOR_TAG have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available ."", 'In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse.', 'Now suppose we let GRE treat salience just like other gradable Attributes.', 'Suppose there are ten mice, five of which are black, whose degrees of salience are 1, 1, 3, 4, and 5 (the last one being most salient), while the other objects in the domain (cats, white mice) all have a higher salience.', 'Then our algorithm might generate this list of properties: L = mouse, black, salience > 4 .', 'This is a distinguishing description for the black mouse whose salience is 5: the most salient black mouse.', 'The simpler description the black mouse can be derived by stipulating that the property of being most salient can be left implicit in English.', 'The salience Attribute has to be taken into account by CD, however, and this can be ensured in various ways.', 'For example, instead of testing whether C ∩']",0,"['We shall see that a natural treatment of salience falls automatically out of our treatment of vague descriptions.', 'As we shall see, this will allow us to simplify the structure of GRE algorithms, and it will explain why many definite descriptions that look as if they were distinguishing descriptions are actually ambiguous.', '9.4.1 A New Perspective on Salience.', ""#AUTHOR_TAG have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available ."", 'Then our algorithm might generate this list of properties: L = mouse, black, salience > 4 .', 'The simpler description the black mouse can be derived by stipulating that the property of being most salient can be left implicit in English.', 'The salience Attribute has to be taken into account by CD, however, and this can be ensured in various ways.', 'For example, instead of testing whether C ']"
CC512,J06-2002,Generating Referring Expressions that Involve Gradable Properties,children’s use of context in interpreting ”big” and ”little” child development,"['K S Ebeling', 'S A Gelman']",,,Children use vague adjectives among their first dozens of words ( Peccei 1994 ) and understand some of their intricacies as early as their 24th month ( #AUTHOR_TAG ) .,"['Gradability is especially widespread in adjectives.', 'A search of the British National Corpus (BNC), for example, shows at least seven of the ten most frequent adjectives (last, other, new, good, old, great, high, small, different, large) to be gradable.', 'Children use vague adjectives among their first dozens of words ( Peccei 1994 ) and understand some of their intricacies as early as their 24th month ( #AUTHOR_TAG ) .', 'These intricacies include what Ebeling and Gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set (e.g., Is this hat big or is it little?, when two hats of different sizes are visible).']",0,"['Children use vague adjectives among their first dozens of words ( Peccei 1994 ) and understand some of their intricacies as early as their 24th month ( #AUTHOR_TAG ) .', 'These intricacies include what Ebeling and Gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set (e.g., Is this hat big or is it little?, when two hats of different sizes are visible).']"
CC513,J06-2002,Generating Referring Expressions that Involve Gradable Properties,fitting words vague language in context linguistics and philosophy,"['Alice Kyburg', 'Michael Morreau']",introduction,,"The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #AUTHOR_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .","['Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999).', 'The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #AUTHOR_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', 'Our own proposal will abstract away from the effects of linguistic context.', 'We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words.', 'This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form of']",0,"['The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #AUTHOR_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', 'Our own proposal will abstract away from the effects of linguistic context.']"
CC514,J06-2002,Generating Referring Expressions that Involve Gradable Properties,object reference in a shared domain of conversation pragmatics and cognition,"['Robbert-Jan Beun', 'Anita Cremers']",introduction,,"We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( #AUTHOR_TAG , Krahmer and Theune 2002 ) .","['We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( #AUTHOR_TAG , Krahmer and Theune 2002 ) .', 'Before we do this, consider the tractability of the original IA.', 'If the running time of FindBestValue(r, A i ) is a constant times the number of Values of the Attribute A i , then the worst-case running time of IA (and IA Plur ) is O(n v n a ), where n a equals the number of Attributes in the language and n v the average number of Values of all Attributes.', 'This is because, in the worst case, all Values of all Attributes need to be attempted (van Deemter 2002).', 'As for the new algorithm, we focus on the crucial phases 2, 4, and 5.']",0,"['We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( #AUTHOR_TAG , Krahmer and Theune 2002 ) .']"
CC515,J06-2002,Generating Referring Expressions that Involve Gradable Properties,speaking from intention to articulation,['William J M Levelt'],introduction,"In Speaking, Willem ""Pim"" Levelt, Director of the Max-Planck-Institut fur Psycholinguistik, accomplishes the formidable task of covering the entire process of speech production, from constraints on conversational appropriateness to articulation and self-monitoring of speech. Speaking is unique in its balanced coverage of all major aspects of the production of speech, in the completeness of its treatment of the entire speech process, and in its strategy of exemplifying rather than formalizing theoretical issues.","Hermann and Deutsch ( 1976 ; also reported in #AUTHOR_TAG ) show that greater differences are most likely to be chosen , presumably because they are more striking .","['Even if comparative properties are at the bottom of the preference order, while stronger inequalities precede weaker ones, the order is not fixed completely.', 'Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x.', 'Which of these should come first?', 'Hermann and Deutsch ( 1976 ; also reported in #AUTHOR_TAG ) show that greater differences are most likely to be chosen , presumably because they are more striking .', ""In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say 'the tall candle' when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to 'the fat candle.'"", ""Hermann and Deutsch's findings may be implemented as follows."", 'First, the Values of the different Attributes should be normalized to make them comparable.', 'Second, preference order should be calculated dynamically (i.e., based on the current value of C, and taking the target into account), preferring larger gaps over smaller ones.', '(It is possible, e.g., that width is most suitable for singling out a black cat, but height for singling out a white cat.)', 'The rest of the algorithm remains unchanged.']",0,"['Which of these should come first?', 'Hermann and Deutsch ( 1976 ; also reported in #AUTHOR_TAG ) show that greater differences are most likely to be chosen , presumably because they are more striking .', ""Hermann and Deutsch's findings may be implemented as follows.""]"
CC516,J06-2002,Generating Referring Expressions that Involve Gradable Properties,computational interpretations of the gricean maximes in the generation of referring expressions,"['Robbert Dale', 'Ehud Reiter']",experiments,"We examine the problem of generating definite noun phrases that are appropriate referring expressions; that is, noun phrases that (a) successfully identify the intended referent to the hearer whilst (b) not conveying to him or her any false conversational implicatures (Grice, 1975). We review several possible computational interpretations of the conversational implicature maxims, with different computational costs, and argue that the simplest may be the best, because it seems to be closest to what human speakers do. We describe our recommended algorithm in detail, along with a specification of the resources a host system must provide in order to make use of the algorithm, and an implementation used in the natural language generation component of the IDAS system.","The account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( Appelt 1985 ; #AUTHOR_TAG ) .","['The account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( Appelt 1985 ; #AUTHOR_TAG ) .', 'But IA may be replaced by any other reasonable GRE algorithm, for example, one that always exactly minimizes the number of properties expressed, or one that always ""greedily"" selects the property that removes the maximum number of distractors.', 'Let G be any such GRE algorithm, then we can proceed as follows:']",0,"['The account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( Appelt 1985 ; #AUTHOR_TAG ) .']"
CC517,J06-2002,Generating Referring Expressions that Involve Gradable Properties,how to refer with vague descriptions,['Manfred Pinkal'],,"This paper deals with the question how reference with vague descriptions should be analysed formally, and gives a proposal for a solution. More precisely, it suggests a formal treatment for the reference identifying function of a special type of vague expressions, as they are used in a special type of definite descriptions. The results of the analysis, however, seem to be of more general importance. The paper consists of:    (i)    some remarks on vagueness and the formal treatment of one type of vagueness;          (ii)    some remarks on definite descriptions and the formal treatment of one kind of descriptions;          (iii)    a short outline of the formal frame which is used to describe both phenomena;          (iv)    an exemplary interpretation of one sentence containing a definite description with a vague predicate within this frame;          (v)    some remarks on the results of the formal analysis concerning reference, vagueness and their mutual relations1.","Following #AUTHOR_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole .","['Vague or gradable expressions pose problems to models of language, caused by their context dependence, and by the fact that they are applicable to different degrees.', 'This article focuses on gradable adjectives, also called degree adjectives. 1 More specifically, we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program.', 'Following #AUTHOR_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole .', 'It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as well as the positive or base form (large) of the adjective.', 'Vague descriptions are worth studying because they use vagueness in a comparatively transparent way, often combining clarity of reference with indeterminacy of meaning; as a result, they allow us to make inroads into the difficult area of research on vagueness.', 'Generation offers an interesting perspective because it forces one to ask when it is a good idea to use these descriptions, in addition to asking what they mean.']",5,"['Following #AUTHOR_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole .', 'Vague descriptions are worth studying because they use vagueness in a comparatively transparent way, often combining clarity of reference with indeterminacy of meaning; as a result, they allow us to make inroads into the difficult area of research on vagueness.']"
CC518,J06-2002,Generating Referring Expressions that Involve Gradable Properties,achieving incremental semantic interpretation through contextual representation,"['Julie Sedivy', 'Michael Tanenhaus', 'Craig Chambers', 'Gregory Carlson']",experiments,"While much work has been done investigating the role of context in the incremental processing of syntactic indeterminacies, relatively little is known about online semantic interpretation. The experiments in this article made use of the eye-tracking paradigm with spoken language and visual contexts in order to examine how, and when listeners make use of contextually-defined contrast in interpreting simple prenominal adjectives. Experiment 1 focused on intersective adjectives. Experiment 1A provided further evidence that intersective adjectives are processed incrementally. Experiment 1B compared response times to follow instructions such as 'Pick up the blue comb' under conditions where there were two blue objects (e.g. a blue pen and a blue comb), but only one of these objects had a contrasting member in the display. Responses were faster to objects with a contrasting member, establishing that the listeners initially assume a contrastive interpretation for intersective adjectives. Experiments 2 and 3 focused on vague scalar adjectives examining the time course with which listeners establish contrast for scalar adjectives such as tall using information provided by the head noun (e.g. glass) and information provided by the visual context. Use of head-based information was examined by manipulating the typicality of the target object (e.g. whether it was a good or poor example of a tall glass. Use of context-dependent contrast was examined by either having only a single glass in the display (the no contrast condition) or a contrasting object (e.g. a smaller glass). The pattern of results indicated that listeners interpreted the scalar adjective incrementally taking into account context-specific contrast prior to encountering the head. Moreover, the presence of a contrasting object, sharply reduced, and in some conditions completely eliminated, typicality effects. The results suggest a language processing system in which semantic interpretation, as well as syntactic processing, is conducted incrementally, with early integration of contextual information.","A similar problem is discussed in the psycholinguistics of interpretation ( #AUTHOR_TAG ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known .","['While IA is generally thought to be consistent with findings on human language production (Hermann and Deutsch 1976;Levelt 1989;Pechmann 1989;Sonnenschein 1982), the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates.', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6).', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'A similar problem is discussed in the psycholinguistics of interpretation ( #AUTHOR_TAG ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known .', 'Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']",1,"['We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'A similar problem is discussed in the psycholinguistics of interpretation ( #AUTHOR_TAG ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known .', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']"
CC519,J06-2002,Generating Referring Expressions that Involve Gradable Properties,data structures and algorithms,"['Alfred V Aho', 'John E Hopcroft', 'Jeffrey D Ullman']",introduction,"Multi-adaptive Galerkin methods are extensions of the standard continuous and discontinuous Galerkin methods for the numerical solution of initial value problems for ordinary or partial differential equations. In particular, the multi-adaptive methods allow individual and adaptive time steps to be used for different components or in different regions of space. We present algorithms for efficient multi-adaptive time-stepping, including the recursive construction of time slabs and adaptive time step selection. We also present data structures for efficient storage and interpolation of the multi-adaptive solution. The efficiency of the proposed algorithms and data structures is demonstrated for a series of benchmark problems.Comment: ACM Transactions on Mathematical Software 35(3), 24 pages (2008","Worst case , calculating the set corresponding with such a property , of the form size ( x ) = maxm , for example , involves sorting the distractors as to their size , which may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [ #AUTHOR_TAG ] Chapter 8 ) .","['If computing the intersection of two sets takes constant time then this makes the complexity of interpreting non-vague descriptions linear: O(nd), where nd is the number of properties used.', 'In a vague description, the property last added to the description is context dependent.', 'Worst case , calculating the set corresponding with such a property , of the form size ( x ) = maxm , for example , involves sorting the distractors as to their size , which may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [ #AUTHOR_TAG ] Chapter 8 ) .', 'Once again, the most time-consuming part of the calculation can be performed off-line, since it is the same for all referring expressions.']",0,"['Worst case , calculating the set corresponding with such a property , of the form size ( x ) = maxm , for example , involves sorting the distractors as to their size , which may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [ #AUTHOR_TAG ] Chapter 8 ) .']"
CC520,J06-2002,Generating Referring Expressions that Involve Gradable Properties,logic and conversation,['Paul Grice'],experiments,"Die Studie befasst sich mit dem Unterschied zwischen dem sozialen Alltagsgesprach und der experimentellen Befragungssituation. Es wird angenommen, dass das Alltagsgesprach nach bestimmten Prinzipien ('co-operative' and 'relevance' principles) ablauft, die fur beide Gesprachspartner in verbindlicher Weise einen relevanten, wahrheitsgetreuen und gehaltvollen Informationsaustausch garantieren. Diese Prinzipien werden im experimentellen Gesprach durch den Forscher verletzt. Zwei Experimente, die mit 44 Studenten an der Universitat Heidelberg und 48 Studenten der University of Illinois durchgefuhrt wurden, sollten beweisen, dass Versuchspersonen, denen weder informative noch relevante Inhalte vermittelt wurden, sich bei ihrem Antwortverhalten von den durch Psychologen vermittelten personlichen Informationen und nicht von statistischen Informationen leiten liessen, selbst wenn die statistischen Daten einen hoheren diagnostischen Wert besassen. Dies entspricht ganz den Regeln der sozialen Gesprachsfuhrung (denn nur Psychologen, nicht Computer vermogen sich den Konversationsregeln anzupassen) und kann nur in der Weise interpretiert werden, dass Versuchspersonen der ihnen dargebotenen Information eine Relevanz aus dem sozialen Kontext der Situation heraus zuordnen. (ML)'According to the co-operative principle of social discourse, listeners expect speakers to be relevant, truthful, and informative. The apparent overreliance of individuals on non-diagnostic person information at the expense of base-rate information is shown to be, in part, due to the violation of this principle in experiments on judgmental biases. In these experiments, subjects are presented information that is neither informative nor relevant, in a communicative context that suggests otherwise. Accordingly, subjects relied more on individuating personality information and less on base-rate information when the personality information was presented by a psychologist rather than compiled by a computer, presumably because a human communicator but not a computer is supposed to conform to conversational norms. Moreover, subjects relied more on individuating information when the framing of the task implied that psychologists provided correct estimates than when it implied that statisticians provided correct estimates; and when the individuating rather than the base-rate information was varied as a within subjects factor.' (author's abstract",Common sense ( as well as the Gricean maxims ; #AUTHOR_TAG ) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication .,"['Common sense ( as well as the Gricean maxims ; #AUTHOR_TAG ) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication .', 'We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented.', 'Dale and Reiter (1995), for example, discussed the transcripts of a dialogue between people who assemble a piece of garden furniture (originally recorded by Candy Sidner).', 'They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 "" bolt, human assemblers ""unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt"" (Dale and Reiter 1995). 6 Our own experiments (van Deemter 2004) point in the same direction.', 'In one experiment, for example, 34 students at the University of Brighton were shown six pieces of paper, each of which showed two isosceles and approximately equilateral triangles.', 'Triangles of three sizes were shown, with bases of 5, 8, and 16 mm respectively.', 'On each sheet, one of the two triangles had been circled with a pencil.', 'We asked subjects to imagine themselves on the phone to someone who held a copy of the same sheet, but not necessarily with the same orientation (e.g., possibly upside down), and to complete the answers in the following: Q: Which triangle on this sheet was circled?', 'A: The ............ triangle.']",0,['Common sense ( as well as the Gricean maxims ; #AUTHOR_TAG ) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication .']
CC521,J06-2002,Generating Referring Expressions that Involve Gradable Properties,two theories about adjectives,['Hans Kamp'],experiments,,Similar things hold for multifaceted properties like intelligence ( #AUTHOR_TAG ) .,"['). Multidimensionality can also slip in through the backdoor.', 'Consider big, for example, when applied to 3D shapes.', 'If there exists a formula for mapping three dimensions into one (e.g., length × width × height) then the result is one dimension (overall-size), and the algorithm of Section 4 can be applied verbatim.', 'But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situation.', 'Similar things hold for multifaceted properties like intelligence ( #AUTHOR_TAG ) .']",0,"['). Multidimensionality can also slip in through the backdoor.', 'Similar things hold for multifaceted properties like intelligence ( #AUTHOR_TAG ) .']"
CC522,J06-2002,Generating Referring Expressions that Involve Gradable Properties,the semantics of gradation,['Manfred Bierwisch'],introduction,"The meaning of a complex expression is a function of the lexical meanings of its components and the syntactic structure of the whole. Regularity of semantic composition The meaning of a syntactically regular expression derives from the meanings of its components in a regular way. PoC&gt; The doctrine&gt; Sub-compositionality&gt; Verb gradation&gt; Semantics&gt; Cognition&gt; Life 2 1.2 Syntactic composition and semantic composition Syntactic composition (in terms of constituency or in terms of dependency, or both) follows grammatical rules. * The rules of syntactic composition are in terms of syntactic types (""syntactic categories"") of expressions to be composed, and of the results. * The rules are constrained by principles which, at least partly, may be assumed to apply to syntactic composition only: constraints due to the requirements such as linearization, parsability, syntactic interpretability. (Other constraints, such as economy and faithfulness apply more generally.","For some adjectives , including the ones that #AUTHOR_TAG called evaluative ( as opposed to dimensional ) , this is clearly inadequate .","['What we said above has also disregarded elements of the ""global"" (i.e., not immediately available) context.', 'For some adjectives , including the ones that #AUTHOR_TAG called evaluative ( as opposed to dimensional ) , this is clearly inadequate .', 'He argued that evaluative adjectives (such as beautiful and its antonym ugly; smart and its antonym stupid, etc.) can be recognized by the way in which they compare with antonyms.', 'For example (after Bierwisch 1989),']",0,"['For some adjectives , including the ones that #AUTHOR_TAG called evaluative ( as opposed to dimensional ) , this is clearly inadequate .', 'For example (after Bierwisch 1989),']"
CC523,J06-2002,Generating Referring Expressions that Involve Gradable Properties,logic and conversation,['Paul Grice'],,"Die Studie befasst sich mit dem Unterschied zwischen dem sozialen Alltagsgesprach und der experimentellen Befragungssituation. Es wird angenommen, dass das Alltagsgesprach nach bestimmten Prinzipien ('co-operative' and 'relevance' principles) ablauft, die fur beide Gesprachspartner in verbindlicher Weise einen relevanten, wahrheitsgetreuen und gehaltvollen Informationsaustausch garantieren. Diese Prinzipien werden im experimentellen Gesprach durch den Forscher verletzt. Zwei Experimente, die mit 44 Studenten an der Universitat Heidelberg und 48 Studenten der University of Illinois durchgefuhrt wurden, sollten beweisen, dass Versuchspersonen, denen weder informative noch relevante Inhalte vermittelt wurden, sich bei ihrem Antwortverhalten von den durch Psychologen vermittelten personlichen Informationen und nicht von statistischen Informationen leiten liessen, selbst wenn die statistischen Daten einen hoheren diagnostischen Wert besassen. Dies entspricht ganz den Regeln der sozialen Gesprachsfuhrung (denn nur Psychologen, nicht Computer vermogen sich den Konversationsregeln anzupassen) und kann nur in der Weise interpretiert werden, dass Versuchspersonen der ihnen dargebotenen Information eine Relevanz aus dem sozialen Kontext der Situation heraus zuordnen. (ML)'According to the co-operative principle of social discourse, listeners expect speakers to be relevant, truthful, and informative. The apparent overreliance of individuals on non-diagnostic person information at the expense of base-rate information is shown to be, in part, due to the violation of this principle in experiments on judgmental biases. In these experiments, subjects are presented information that is neither informative nor relevant, in a communicative context that suggests otherwise. Accordingly, subjects relied more on individuating personality information and less on base-rate information when the personality information was presented by a psychologist rather than compiled by a computer, presumably because a human communicator but not a computer is supposed to conform to conversational norms. Moreover, subjects relied more on individuating information when the framing of the task implied that psychologists provided correct estimates than when it implied that statisticians provided correct estimates; and when the individuating rather than the base-rate information was varied as a within subjects factor.' (author's abstract","In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles ( #AUTHOR_TAG ) .","['Minimality.', 'Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form.', 'In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles ( #AUTHOR_TAG ) .']",0,"['Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form.', 'In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles ( #AUTHOR_TAG ) .']"
CC524,J06-2002,Generating Referring Expressions that Involve Gradable Properties,understanding shortcuts in nlg systems,['Chris Mellish'],experiments,,"In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( #AUTHOR_TAG ) .","['If the usefulness of NLG resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail.', 'In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( #AUTHOR_TAG ) .', 'The only practical alternative is to provide the generator with ""crisp"" (i.e., quantitative) input, allowing the generator to be hooked on to a general-purpose database.', 'It is this avenue that we have explored in this article, in combination with various (incremental and other) approaches to GRE.', 'Far from being a peculiarity of a few adjectives, vagueness is widespread.', 'We believe that our approach can be applied to a variety of situations in which vagueness affects referring expressions including, for example, r color terms (Section 9.3); r nouns that allow different degrees of strictness (Section 9.5); r degrees of salience (Section 9.4); and r imprecise pointing (Section 9.5).']",4,"['If the usefulness of NLG resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail.', 'In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( #AUTHOR_TAG ) .']"
CC525,J06-2002,Generating Referring Expressions that Involve Gradable Properties,efficient contextsensitive generation of referring expressions,"['Emiel Krahmer', 'Mari¨et Theune']",experiments,3 A Modification of the Algorithm Based on Salience 5 3.1 Motivation: Determining the Context Set : : : : : : : : : : : : : 5 3.2 Preliminaries : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 3.3 Outline of the Modified Algorithm : : : : : : : : : : : : : : : : : 7 3.4 Examples : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 9 3.5 Discussion : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 11,"CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm ( #AUTHOR_TAG , Section 8.6.2 ) : Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size ( say , size 5 ) ; if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed .","['Some generalizations of our method are fairly straightforward. For example, consider a relational description (cf., Dale and Haddock 1991) involving a gradable adjective, as in the dog in the large shed.', 'CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm ( #AUTHOR_TAG , Section 8.6.2 ) : Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size ( say , size 5 ) ; if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed .', 'Note that this is felicitous even if the shed is not the largest one in the domain, as is true for d2 in the following situation (contains-a=b means that a is contained by b):']",0,"['Some generalizations of our method are fairly straightforward. For example, consider a relational description (cf., Dale and Haddock 1991) involving a gradable adjective, as in the dog in the large shed.', 'CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm ( #AUTHOR_TAG , Section 8.6.2 ) : Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size ( say , size 5 ) ; if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed .']"
CC526,J06-2002,Generating Referring Expressions that Involve Gradable Properties,incremental speech production and referential overspecification,['Thomas Pechmann'],experiments,"Speech is often produced incrementally: speakers start to articulate an utterance before knowing exactly what they are going to say. The time devoted to articulating first parts of an utterance is simultaneously used to process further information which is to be incorporated into that utterance. Empirical evidence is presented which supports the assumption that referential noun phrases are indeed often produced incrementally. This assumption explains several phenomena: first, speakers' production of overspecified ('redundant') utterances; second, irregularities concerning the ordering ofprenominal adjectives; and third, the way acoustic stress is assigned in referential noun phrases. The basic problem that a speaker faces in what is usually called 'referential communication' is that he has to refer unambiguously to a target object in the context of other objects in such a way that a listener is able to single it out from all relevant alternatives. In order to characterize a target object unambiguously in the context of others, the speaker must determine some appropriate set of distinguishing features, that is, those features that distinguish the target from all relevant alternatives. Consider the simple example depicted in Figure 1. Suppose a referential domain consists of three objects: a black and white triangle and a white circle. Suppose the white circle is the target object on which the speaker wants to focus the listener's attention. In this case the distinguishing feature is the object class. The information 'circle' is sufficient for the listener to know which object is meant by the speaker. In contrast, color would be nondistinguishing information in this case, since color does not help the listener to differentiate the circle from the white triangle. Research on referential communication has primarily been oriented toward developmental questions (for overviews, see Dickson 1981; Glucksberg et al. 1975; Shatz 1978). For most researchers, success or failure in the child's referential-communication tasks has served as an Linguistics 27 (1989), 89-110 0024-3949/89/0027-0089 $2.00 (c) Mouton de Gruyter, Amsterdam","While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; #AUTHOR_TAG ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .","['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; #AUTHOR_TAG ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6).', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'A similar problem is discussed in the psycholinguistics of interpretation (Sedivy et al. 1999): Interpretation is widely assumed to proceed incrementally, but vague descriptions resist strict incrementality, since an adjective in a vague description can only be fully interpreted when its comparison set is known.', 'Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']",0,"['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; #AUTHOR_TAG ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .']"
CC527,J06-2002,Generating Referring Expressions that Involve Gradable Properties,games and information an introduction to game theory,['Eric Rasmusen'],experiments,"List of Figures. List of Tables. List of Games. Preface. Contents and Purpose. Changes in the Second Edition (1994). Changes in the Third Edition (2001). Changes in the Fourth Edition (2006). Using the Book. The Level of Mathematics. Other Books. Contact Information. Acknowledgements. Introduction. History. Game Theory's Method. Exemplifying Theory. This Book's Style. Notes. PART 1: GAME THEORY. 1. The Rules of the Game. Definitions. Dominated and Dominant Strategies: The Prisoner's Dilemma. Iterated Dominance: The Battle of the Bismarck Sea. Nash Equilibrium: Boxed Pigs, The Battle of the Sexes and Ranked Coordination. Focal Points. Notes. Problems. Classroom Game. 2. Information. The Strategic and Extensive Forms of a Game. Information Sets. Perfect, Certain, Symmetric, and Complete Information. The Harsanyi Transformation and Bayesian Games. Example: The Png Settlement Game. Notes. Problems. Classroom Game. 3. Mixed and Continuous Strategies. Mixed Strategies: The Welfare Game. The Payoff-equating Method and Games of Timing. Mixed Strategies with General Parameters and N Players: The Civic Duty Game. Randomizing is not Always Mixing: The Auditing Game. Continuous Strategies: The Cournot Game. Continuous Strategies: The Bertrand Game, Strategic Complements, and Strategic. Substitutes. Existence of Equilibrium. Notes. Problems. Classroom Game. 4. Dynamic Games with Symmetric Information. Subgame Perfectness. An Example of Perfectness: Entry Deterrence I. Credible Threats, Sunk Costs, and the Open-Set Problem in the Game of Nuisance Suits. Recoordination to Pareto-dominant Equilibria in Subgames: Pareto Perfection. Notes. Problems. Classroom Game. 5. Reputation and Repeated Games with Symmetric Information. Finitely Repeated Games and the Chainstore Paradox. Infinitely Repeated Games, Minimax Punishments, and the Folk Theorem. Reputation: The One-sided Prisoner's Dilemma. Product Quality in an Infinitely Repeated Game. Markov Equilibria and Overlapping Generations: Customer Switching Costs. Evolutionary Equilibrium: The Hawk-Dove Game. Notes. Problems. Classroom Game. 6. Dynamic Games with Incomplete Information. Perfect Bayesian Equilibrium: Entry Deterrence II and III. Refining Perfect Bayesian Equilibrium in the Entry Deterrence and PhD Admissions Games. The Importance of Common Knowledge: Entry Deterrence IV and V. Incomplete Information in the Repeated Prisoner's Dilemma: The Gang of Four Model. The Axelrod Tournament. Credit and the Age of the Firm: The Diamond Model. Notes. Problems. Classroom Game. PART 2: ASYMMETRIC INFORMATION. 7. Moral Hazard: Hidden Actions. Categories of Asymmetric Information Models. A Principal-agent Model: The Production Game. The Incentive Compatibility and Participation Constraints. Optimal Contracts: The Broadway Game. Notes. Problems. Classroom Game. 8. Further Topics in Moral Hazard. Efficiency Wages. Tournaments. Institutions and Agency Problems. Renegotiation: The Repossession Game. State-space Diagrams: Insurance Games I and II. Joint Production by Many Agents: The Holmstrom Teams Model. The Multitask Agency Problem. Notes. Problems. Classroom Game. 9. Adverse Selection. Introduction: Production Game VI. Adverse Selection under Certainty: Lemons I and II. Heterogeneous Tastes: Lemons III and IV. Adverse Selection under Uncertainty: Insurance Game III. Market Microstructure. A Variety of Applications. Adverse Selection and Moral Hazard Combined: Production Game VII. Notes. Problems. Classroom Game. 10. Mechanism Design and Postcontractual Hidden Knowledge. Mechanisms, Unravelling, Cross Checking, and the Revelation Principle. Myerson Mechanism Design. An Example of Postcontractual Hidden Knowledge: The Salesman Game. The Groves Mechanism. Price Discrimination. Rate-of-return Regulation and Government Procurement. Notes. Problems. Classroom Game. 11. Signalling. The Informed Player Moves First: Signalling. Variants on the Signalling Model of Education. General Comments on Signalling in Education. The Informed Player Moves Second: Screening. Two Signals: The Game of Underpricing New Stock Issues. Signal Jamming and Limit Pricing. Countersignalling. Notes. Problems. Classroom Game. PART 3: APPLICATIONS. 12. Bargaining. The Basic Bargaining Problem: Splitting a Pie. The Nash Bargaining Solution. Alternating Offers over Finite Time. Alternating Offers over Infinite Time. Incomplete Information. Setting Up a Way to Bargain: The Myerson-Satterthwaite Mechanism. Notes. Problems. Classroom Game. 13. Auctions. Values Private and Common, Continuous and Discrete. Optimal Strategies under Different Rules in Private-value Auctions. Revenue Equivalence, Risk Aversion, and Uncertainty. Reserve Prices and the Marginal Revenue Approach. Common-value Auctions and the Winner's Curse. Asymmetric Equilibria, Affiliation, and Linkage: The Wallet Game. Notes. Problems. Classroom Game. 14. Pricing. Quantities as Strategies: Cournot Equilibrium Revisited. Capacity Constraints: The Edgeworth Paradox. Location Models. Comparative Statics and Supermodular Games. Vertical Differentiation. Durable Monopoly. Notes. Problems. Classroom Game. Mathematical Appendix. Notation. The Greek Alphabet. Glossary. Formulas and Functions. Probability Distributions. Supermodularity. Fixed Point Theorems. Genericity. Discounting. Risk. References and Name Index. Subject Index","When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #AUTHOR_TAG ) .","['Generalizations to complex Boolean descriptions involving negation and disjunction (van Deemter 2004) appear to be largely straightforward, except for issues to do with opposites and markedness.', 'For example, the generator will have to decide whether to say the patients that are old or the patients that are not young.', '9.3 Multidimensionality 9.3.1 Combinations of Adjectives.', 'When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #AUTHOR_TAG ) .', 'Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective.', 'The NP the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension (a different one for a than for c, as it happens) while not being exceeded by any distractors in any dimension: Cases like this would be covered if the decision-theoretic property of Pareto optimality (e.g., Feldman 1980) was used as the sole criterion: Formally, an object r ∈ C has a Pareto-optimal combination of Values V iff there is no other x ∈ C such that 1. ∃V i ∈ V : V i (x) > V i (r) and 2. ¬∃V j ∈ V : V j (x) < V j (r)']",0,"['When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #AUTHOR_TAG ) .']"
CC528,J06-2002,Generating Referring Expressions that Involve Gradable Properties,speaking from intention to articulation,['William J M Levelt'],experiments,"In Speaking, Willem ""Pim"" Levelt, Director of the Max-Planck-Institut fur Psycholinguistik, accomplishes the formidable task of covering the entire process of speech production, from constraints on conversational appropriateness to articulation and self-monitoring of speech. Speaking is unique in its balanced coverage of all major aspects of the production of speech, in the completeness of its treatment of the entire speech process, and in its strategy of exemplifying rather than formalizing theoretical issues.","While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; #AUTHOR_TAG ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .","['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; #AUTHOR_TAG ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6).', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'A similar problem is discussed in the psycholinguistics of interpretation (Sedivy et al. 1999): Interpretation is widely assumed to proceed incrementally, but vague descriptions resist strict incrementality, since an adjective in a vague description can only be fully interpreted when its comparison set is known.', 'Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']",0,"['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; #AUTHOR_TAG ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']"
CC529,J06-2002,Generating Referring Expressions that Involve Gradable Properties,achieving incremental semantic interpretation through contextual representation,"['Julie Sedivy', 'Michael Tanenhaus', 'Craig Chambers', 'Gregory Carlson']",experiments,"While much work has been done investigating the role of context in the incremental processing of syntactic indeterminacies, relatively little is known about online semantic interpretation. The experiments in this article made use of the eye-tracking paradigm with spoken language and visual contexts in order to examine how, and when listeners make use of contextually-defined contrast in interpreting simple prenominal adjectives. Experiment 1 focused on intersective adjectives. Experiment 1A provided further evidence that intersective adjectives are processed incrementally. Experiment 1B compared response times to follow instructions such as 'Pick up the blue comb' under conditions where there were two blue objects (e.g. a blue pen and a blue comb), but only one of these objects had a contrasting member in the display. Responses were faster to objects with a contrasting member, establishing that the listeners initially assume a contrastive interpretation for intersective adjectives. Experiments 2 and 3 focused on vague scalar adjectives examining the time course with which listeners establish contrast for scalar adjectives such as tall using information provided by the head noun (e.g. glass) and information provided by the visual context. Use of head-based information was examined by manipulating the typicality of the target object (e.g. whether it was a good or poor example of a tall glass. Use of context-dependent contrast was examined by either having only a single glass in the display (the no contrast condition) or a contrasting object (e.g. a smaller glass). The pattern of results indicated that listeners interpreted the scalar adjective incrementally taking into account context-specific contrast prior to encountering the head. Moreover, the presence of a contrasting object, sharply reduced, and in some conditions completely eliminated, typicality effects. The results suggest a language processing system in which semantic interpretation, as well as syntactic processing, is conducted incrementally, with early integration of contextual information.","(For Fritz to be the stupid man, it is not enough for him to be the least intelligent male in the local context; he also has to be a fairly stupid specimen in his own right.) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans�s and Fritz�s heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see #AUTHOR_TAG 1999, discussed in Section 7.2).","['We could require that the referent of an evaluative description fall into the correct segment of the relevant dimension.', '(For Fritz to be the stupid man, it is not enough for him to be the least intelligent male in the local context; he also has to be a fairly stupid specimen in his own right.) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans�s and Fritz�s heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see #AUTHOR_TAG 1999, discussed in Section 7.2).', 'Be this as it may, we shall henceforth focus on local context, assuming that additional requirements on the global context can be made if necessary.']",0,"['We could require that the referent of an evaluative description fall into the correct segment of the relevant dimension.', '(For Fritz to be the stupid man, it is not enough for him to be the least intelligent male in the local context; he also has to be a fairly stupid specimen in his own right.) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans�s and Fritz�s heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see #AUTHOR_TAG 1999, discussed in Section 7.2).', 'Be this as it may, we shall henceforth focus on local context, assuming that additional requirements on the global context can be made if necessary.']"
CC530,J06-2002,Generating Referring Expressions that Involve Gradable Properties,cooking up referring expressions,['Robert Dale'],,"This paper describes the referring expression generation mechanisms used in EPICURE, a computer program which produces natural language descriptions of cookery recipes. Major features of the system include: an underlying ontology which permits the representation of non-singular entities; a notion of discriminatory power, to determine what properties should be used in a description; and a PATR-like unification grammar to produce surface linguistic strings.","NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; #AUTHOR_TAG ) : The selected expression should also be felicitous .","['NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; #AUTHOR_TAG ) : The selected expression should also be felicitous .', 'Consider the question, discussed in the philosophical logic literature, of whether it is legitimate, for a gradable adjective, to distinguish between ""observationally indifferent"" entities: Suppose two objects x and y, are so similar that it is impossible to distinguish their sizes; can it ever be reasonable to say that x is large and y is not?', 'A positive answer would not be psychologically plausible, since x and y are indistinguishable; but a negative answer would prohibit any binary distinction between objects that are large and objects that are not, given that one can always construct objects x and y, one of which falls just below the divide while the other falls just above it.', 'This is the strongest version of the sorites paradox (e.g., Hyde 2002).']",0,"['NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; #AUTHOR_TAG ) : The selected expression should also be felicitous .']"
CC531,J06-2002,Generating Referring Expressions that Involve Gradable Properties,the order of prenominal adjectives in natural language generation,['Rob Malouf'],,The order of prenominal adjectival modifiers in English is governed by complex and difficult to describe constraints which straddle the boundary between competence and performance. This paper describes and compares a number of statistical and machine learning techniques for ordering sequences of adjectives in the context of a natural language generation system.,"One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , Shaw and Hatzivassiloglou 1999 ; #AUTHOR_TAG ) .","['One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , Shaw and Hatzivassiloglou 1999 ; #AUTHOR_TAG ) .', ""Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position."", 'Interestingly, vague properties tend to be realized before others.', 'Quirk et al. (1985), for example, report that ""adjectives denoting size, length, and height normally precede other nonderived adjectives"" (e.g., the small round table is usually preferred to the round small table).', 'Semantically, this does not come as a surprise.', 'In a noun phrase of the form the three small (-est) [N], for example, the words preceding N select the three smallest elements of [N].', 'It follows that, to denote the three smallest elements of the set of round tables, the only option is to say the three small round tables, rather than the three round small tables.', 'The latter would mean something else, namely, the three round ones among the n small(est) tables (where n is not specified).', ""It actually seems quite possible to say this, but only when some set of small tables is contextually salient (e.g., I don't mean those small tables, I mean the three round ones)."", 'Given that n is unspecified, the noun phrase would tend to be very unclear in any other context.']",0,"['One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , Shaw and Hatzivassiloglou 1999 ; #AUTHOR_TAG ) .']"
CC532,J06-2002,Generating Referring Expressions that Involve Gradable Properties,computational interpretations of the gricean maximes in the generation of referring expressions,"['Robbert Dale', 'Ehud Reiter']",introduction,"We examine the problem of generating definite noun phrases that are appropriate referring expressions; that is, noun phrases that (a) successfully identify the intended referent to the hearer whilst (b) not conveying to him or her any false conversational implicatures (Grice, 1975). We review several possible computational interpretations of the conversational implicature maxims, with different computational costs, and argue that the simplest may be the best, because it seems to be closest to what human speakers do. We describe our recommended algorithm in detail, along with a specification of the resources a host system must provide in order to make use of the algorithm, and an implementation used in the natural language generation component of the IDAS system.","4 To turn this likelihood into a certainty , one can add a test at the end of the algorithm , which adds a type-related property if none is present yet ( cfXXX , #AUTHOR_TAG ) .","['4 To turn this likelihood into a certainty , one can add a test at the end of the algorithm , which adds a type-related property if none is present yet ( cfXXX , #AUTHOR_TAG ) .', 'VAGUE uses both of these devices.']",0,"['4 To turn this likelihood into a certainty , one can add a test at the end of the algorithm , which adds a type-related property if none is present yet ( cfXXX , #AUTHOR_TAG ) .', 'VAGUE uses both of these devices.']"
CC533,J06-2002,Generating Referring Expressions that Involve Gradable Properties,understanding complex visually referring utterances,"['Peter Gorniak', 'Deb Roy']",experiments,"We propose a computational model of visually-grounded spatial language understanding, based on a study of how people verbally describe objects in visual scenes. We describe our implementation of word level visually-grounded semantics and their embedding in a compositional parsing framework. The implemented system selects the correct referents in response to a broad range of referring expressions for a large percentage of test cases. In an analysis of the system's successes and failures we reveal how visual context influences the semantics of utterances and propose future extensions to the model that take such context into account.","The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( Nash 1950 ; cfXXX #AUTHOR_TAG ; Thorisson 1994 , for other plans ) .","['In our example, b is the only object that has a Pareto-optimal combination of Values, predicting correctly that b can be called the tall fat giraffe.', 'It seems likely, however, that people use doubly graded descriptions more liberally.', 'For example, if the example is modified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall fat giraffe.', 'Many alternative strategies are possible.', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( Nash 1950 ; cfXXX #AUTHOR_TAG ; Thorisson 1994 , for other plans ) .']",0,"['It seems likely, however, that people use doubly graded descriptions more liberally.', 'Many alternative strategies are possible.', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( Nash 1950 ; cfXXX #AUTHOR_TAG ; Thorisson 1994 , for other plans ) .']"
CC534,J06-2002,Generating Referring Expressions that Involve Gradable Properties,interpreting vague utterances in context,"['David DeVault', 'Matthew Stone']",introduction,"We use the interpretation of vague scalar predicates  like small as an illustration of how systematic  semantic models of dialogue context enable  the derivation of useful, fine-grained utterance  interpretations from radically underspecified  semantic forms. Because dialogue context  suffices to determine salient alternative scales  and relevant distinctions along these scales,  we can infer implicit standards of comparison  for vague scalar predicates through completely  general pragmatics, yet closely constrain the intended  meaning to within a natural range","The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ; #AUTHOR_TAG ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .","['Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999).', 'The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ; #AUTHOR_TAG ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', 'Our own proposal will abstract away from the effects of linguistic context.', 'We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words.', 'This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form of']",0,"['Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999).', 'The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ; #AUTHOR_TAG ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', 'Our own proposal will abstract away from the effects of linguistic context.']"
CC535,J06-2002,Generating Referring Expressions that Involve Gradable Properties,computational interpretations of the gricean maximes in the generation of referring expressions,"['Robbert Dale', 'Ehud Reiter']",introduction,"We examine the problem of generating definite noun phrases that are appropriate referring expressions; that is, noun phrases that (a) successfully identify the intended referent to the hearer whilst (b) not conveying to him or her any false conversational implicatures (Grice, 1975). We review several possible computational interpretations of the conversational implicature maxims, with different computational costs, and argue that the simplest may be the best, because it seems to be closest to what human speakers do. We describe our recommended algorithm in detail, along with a specification of the resources a host system must provide in order to make use of the algorithm, and an implementation used in the natural language generation component of the IDAS system.","Each of these Values has equal status , so the notion of a basic-level Value can not play a role ( cfXXX , #AUTHOR_TAG ) .","['The representation of inequalities is not entirely trivial.', 'For one thing, it is convenient to view properties of the form size(x) < α as belonging to a different Attribute than those of the form size(x) > α, because this causes the Values of an Attribute to be linearly ordered: Being larger than 12 cm implies being larger than 10 cm, and so on.', 'More importantly, it will now become normal for an object to have many Values for the same Attribute; c 4 , for example, has the Values > 6 cm, > 10 cm, and > 12 cm.', 'Each of these Values has equal status , so the notion of a basic-level Value can not play a role ( cfXXX , #AUTHOR_TAG ) .', ""If we abstract away from the role of basic-level Values, then Dale and Reiter's FindBestValue chooses the most general Value that removes the maximal number of distractors, as we have seen."", 'Thus, size(x) > m is preferred over size(x) > n iff m > n; conversely, size(x) < m is preferred over size(x) < n iff m < n.)', 'This is reflected by the order in which the properties are listed above: Once a sizerelated property is selected, later size-related properties do not remove any distractors and will therefore not be included in the description.']",0,"['The representation of inequalities is not entirely trivial.', 'Each of these Values has equal status , so the notion of a basic-level Value can not play a role ( cfXXX , #AUTHOR_TAG ) .', ""If we abstract away from the role of basic-level Values, then Dale and Reiter's FindBestValue chooses the most general Value that removes the maximal number of distractors, as we have seen.""]"
CC536,J06-2002,Generating Referring Expressions that Involve Gradable Properties,generating referring expressions containing relations,"['Robbert Dale', 'Nickolas Haddock']",experiments,"Recent work on the Generation of Referring Expressions has increased the generating capability of algorithms in this area. This paper asks whether the models underlying these proposals can still be used if even more complex referring expressions are generated. To discuss this issue, we will investigate a variety of referring expressions that pose difficulties to current generation algorithms. In particular, we will discuss the difficulties associated with quantified referring expressions (such as 'those women who have fewer than two children', or 'the people who work for exactly 2 employers') and explain how they can be generated by extending the inference-based approach described in (Varges, 2004).","For example , consider a relational description ( cfXXX , #AUTHOR_TAG ) involving a gradable adjective , as in the dog in the large shed .","['Some generalizations of our method are fairly straightforward.', 'For example , consider a relational description ( cfXXX , #AUTHOR_TAG ) involving a gradable adjective , as in the dog in the large shed .', 'CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm (Krahmer and Theune 2002, Section 8.6.2):', 'Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size (say, size 5); if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed.', ""Note that this is felicitous even if the shed is not the largest one in the domain, as is true for d 2 in the following situation (contains-a=b means that a is contained by b): In other words, the dog in the large shed denotes 'the dog such that there is no other shed that is equally large or larger and that contains a dog'."", 'Note that it would be odd, in the above-sketched situation, to say the dog in the largest shed.']",0,"['For example , consider a relational description ( cfXXX , #AUTHOR_TAG ) involving a gradable adjective , as in the dog in the large shed .', 'CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm (Krahmer and Theune 2002, Section 8.6.2):']"
CC537,J06-2002,Generating Referring Expressions that Involve Gradable Properties,the bargaining problem,['John Nash'],experiments,"AbstractConsider the problem of partitioning n items among d players where the utility of each player for bundles of items is additive; so, player r has utility vri for item i and the utility of that player for a bundle of items is the sum of the vir's over the items i in his/her bundle. Each partition S of the items is then associated with a d-dimensional utility vector VS whose coordinates are the utilities that the players assign to the bundles they get under S. Also, lotteries over partitions are associated with the corresponding expected utility vectors. We model the problem as a Nash bargaining game over the set of lotteries over partitions and provide methods for computing the corresponding Nash solution, to prescribed accuracy, with effort that is polynomial in n. In particular, we show that points in the pareto-optimal set of the corresponding bargaining set correspond to lotteries over partitions under which each item, with the possible exception of at most d(d-1)/2 items, is assigned in the same way","The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( #AUTHOR_TAG ; cfXXX Gorniak and Roy 2003 ; Thorisson 1994 , for other plans ) .","['In our example, b is the only object that has a Pareto-optimal combination of Values, predicting correctly that b can be called the tall fat giraffe.', 'It seems likely, however, that people use doubly graded descriptions more liberally.', 'For example, if the example is modified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall fat giraffe.', 'Many alternative strategies are possible.', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( #AUTHOR_TAG ; cfXXX Gorniak and Roy 2003 ; Thorisson 1994 , for other plans ) .']",0,"['Many alternative strategies are possible.', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( #AUTHOR_TAG ; cfXXX Gorniak and Roy 2003 ; Thorisson 1994 , for other plans ) .']"
CC538,J06-2002,Generating Referring Expressions that Involve Gradable Properties,building natural language generation systems,"['Ehud Reiter', 'Robert Dale']",experiments,"This book explains how to build Natural Language Generation (NLG) systems - computer software systems which use techniques from artificial intelligence and computational linguistics to automatically generate understandable texts in English or other human languages, either in isolation or as part of multimedia documents, Web pages, and speech output systems","The inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model ( e.g. , #AUTHOR_TAG ) .","['The inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model ( e.g. , #AUTHOR_TAG ) .', 'An example of such an inference rule is the one that transforms a list of the form mouse, >10 cm into one of the form mouse, size(x) = max 2 if only two mice are larger than 10 cm.', 'The same issues also make it difficult to interleave CD and linguistic realization as proposed by various authors, because properties may need to be combined before they are expressed.']",0,"['The inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model ( e.g. , #AUTHOR_TAG ) .', 'An example of such an inference rule is the one that transforms a list of the form mouse, >10 cm into one of the form mouse, size(x) = max 2 if only two mice are larger than 10 cm.', 'The same issues also make it difficult to interleave CD and linguistic realization as proposed by various authors, because properties may need to be combined before they are expressed.']"
CC539,J06-2002,Generating Referring Expressions that Involve Gradable Properties,should corpora texts be gold standards for nlg,"['Ehud Reiter', 'Somayajulu Sripada']",,,"A more flexible approach is used by #AUTHOR_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on .","['Some NLG systems produce gradable adjectives. The FOG weather-forecast system, for example, uses numerical input (Rain[Tuesday] = 45 mm) to generate vague output (Heavy rain fell on Tuesday, Goldberg, Driedger, and Kitteridge 1994).', 'FOG does not appear to have generic rules governing the use of gradable notions: it does not compute the meaning of a vague term based on the context, but uses fixed boundary values instead.', 'A more flexible approach is used by #AUTHOR_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on .', 'A third approach was implemented in Dial Your Disc (DYD), where the extension of a gradable adjective like famous was computed rather than specified by hand (van Deemter and Odijk 1997).', 'To determine, for example, whether one of Mozart�s piano sonatas could be called a famous sonata, the system looked up the number x of compact disc recordings of this sonata (as listed in an encyclopedia) and compared it to the average number y of CD recordings of each of Mozart�s sonatas.', 'The sonata was called a famous sonata if x >> y. Like DYD, the work reported in this article will abandon the use of fixed boundary values for gradable adjectives, letting these values depend on the context in which the adjective is used.']",0,"['Some NLG systems produce gradable adjectives. The FOG weather-forecast system, for example, uses numerical input (Rain[Tuesday] = 45 mm) to generate vague output (Heavy rain fell on Tuesday, Goldberg, Driedger, and Kitteridge 1994).', 'FOG does not appear to have generic rules governing the use of gradable notions: it does not compute the meaning of a vague term based on the context, but uses fixed boundary values instead.', 'A more flexible approach is used by #AUTHOR_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on .']"
CC540,J06-2002,Generating Referring Expressions that Involve Gradable Properties,textual economy through close coupling of syntax and semantics,"['Matthew Stone', 'Bonnie Webber']",,"We focus on the production of efficient descriptions of objects, actions and events. We define a type of  efficiency, textual economy, that exploits the hearer&apos;s recognition of inferential links to material elsewhere  within a sentence. Textual economy leads to efficient descriptions because the material that supports such  inferences has been included to satisfy independent communicative goals, and is therefore overloaded  in the sense of Pollack [18]. We argue that achieving textual economy imposes strong requirements  on the representation and reasoning used in generating sentences. The representation must support the  generator&apos;s simultaneous consideration of syntax and semantics. Reasoningmust enable the generator  to assess quickly and reliably at any stage how the hearer will interpret the current sentence, with its  &apos;-(inc6mplete)syntax and&apos;semantics. We show that these representational and reasoning requirements are  met in the SPUD system for sentence planning and realization","Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #AUTHOR_TAG ; Krahmer and Theune 2002 ) .","['Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #AUTHOR_TAG ; Krahmer and Theune 2002 ) .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', 'Consider, for instance, the list of properties L = size > 3 cm, size < 9 cm .', 'If interleaving forced us to realize the two properties in L one by one, then it would no longer be possible to combine them into, for example, the largest mouse but one (if the facts in the KB support it), or even into the mice between 3 and 9 cm (since size > 3 cm is realized before size < 9 cm).', 'Clearly, sophisticated use of gradable adjectives requires a separation between CD and linguistic realization, unless one is willing to complicate linguistic realization considerably.']",1,"['Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #AUTHOR_TAG ; Krahmer and Theune 2002 ) .']"
CC541,J06-2002,Generating Referring Expressions that Involve Gradable Properties,psychologie der objektbenennung,"['Tony Hermann', 'Roland Deutsch']",experiments,,"While IA is generally thought to be consistent with findings on human language production ( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .","['While IA is generally thought to be consistent with findings on human language production ( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6).', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'A similar problem is discussed in the psycholinguistics of interpretation (Sedivy et al. 1999): Interpretation is widely assumed to proceed incrementally, but vague descriptions resist strict incrementality, since an adjective in a vague description can only be fully interpreted when its comparison set is known.', 'Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']",0,"['While IA is generally thought to be consistent with findings on human language production ( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'A similar problem is discussed in the psycholinguistics of interpretation (Sedivy et al. 1999): Interpretation is widely assumed to proceed incrementally, but vague descriptions resist strict incrementality, since an adjective in a vague description can only be fully interpreted when its comparison set is known.', 'Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']"
CC542,J06-2002,Generating Referring Expressions that Involve Gradable Properties,basic color terms,"['Brent Berlin', 'Paul Kay']",experiments,,"A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #AUTHOR_TAG , pages 10 -- 12 ) .","['Color terms are a case apart.', 'If color is modeled in terms of saturation, hue, and luminosity, for instance, then an object a may be classified as greener than b on one dimension (e.g., saturation), but less green than b on another (e.g., hue).', 'This would considerably complicate the application of our algorithm to color terms, which is otherwise mostly straighforward (Section 9.3).', '(The green chair, said in the presence of two greenish chairs, would refer to the one that is closest to prototypical green.)', 'A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #AUTHOR_TAG , pages 10 -- 12 ) .', '(Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent.', 'Red as in red hair, e.g., differs from red as in red chair.)', 'Different attitudes towards multidimensionality are possible.', 'One possibility is to be cautious and to keep aiming for distinguishing descriptions in the strict sense.', 'In this case, the program should limit the use of vague descriptions to situations where there exists a referent that has a Pareto-optimal combination of Values.', 'Alternatively, one could allow referring expressions to be ambiguous.', 'It would be consistent with this attitude, for example, to map multiple dimensions into one overall dimension, perhaps by borrowing from principles applied in perceptual grouping, where different perceptual dimensions are mapped into one (e.g., Thorisson 1994).', 'The empirical basis of this line of work, however, is still somewhat weak, so the risk of referential unclarity looms large.', 'Also, this attitude would go against the spirit of GRE, where referring expressions have always been assumed to be distinguishing.']",0,"['Color terms are a case apart.', 'If color is modeled in terms of saturation, hue, and luminosity, for instance, then an object a may be classified as greener than b on one dimension (e.g., saturation), but less green than b on another (e.g., hue).', 'This would considerably complicate the application of our algorithm to color terms, which is otherwise mostly straighforward (Section 9.3).', 'A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #AUTHOR_TAG , pages 10 -- 12 ) .', '(Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent.']"
CC543,J06-2002,Generating Referring Expressions that Involve Gradable Properties,achieving incremental semantic interpretation through contextual representation,"['Julie Sedivy', 'Michael Tanenhaus', 'Craig Chambers', 'Gregory Carlson']",experiments,"While much work has been done investigating the role of context in the incremental processing of syntactic indeterminacies, relatively little is known about online semantic interpretation. The experiments in this article made use of the eye-tracking paradigm with spoken language and visual contexts in order to examine how, and when listeners make use of contextually-defined contrast in interpreting simple prenominal adjectives. Experiment 1 focused on intersective adjectives. Experiment 1A provided further evidence that intersective adjectives are processed incrementally. Experiment 1B compared response times to follow instructions such as 'Pick up the blue comb' under conditions where there were two blue objects (e.g. a blue pen and a blue comb), but only one of these objects had a contrasting member in the display. Responses were faster to objects with a contrasting member, establishing that the listeners initially assume a contrastive interpretation for intersective adjectives. Experiments 2 and 3 focused on vague scalar adjectives examining the time course with which listeners establish contrast for scalar adjectives such as tall using information provided by the head noun (e.g. glass) and information provided by the visual context. Use of head-based information was examined by manipulating the typicality of the target object (e.g. whether it was a good or poor example of a tall glass. Use of context-dependent contrast was examined by either having only a single glass in the display (the no contrast condition) or a contrasting object (e.g. a smaller glass). The pattern of results indicated that listeners interpreted the scalar adjective incrementally taking into account context-specific contrast prior to encountering the head. Moreover, the presence of a contrasting object, sharply reduced, and in some conditions completely eliminated, typicality effects. The results suggest a language processing system in which semantic interpretation, as well as syntactic processing, is conducted incrementally, with early integration of contextual information.",#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .,"['It is normally perhaps unlikely that people produce language on the basis of the kind of numerical representations that our algorithm has used as input.', 'Although psychological plausibility is not our aim, it is worth noting that the inequalities computed as step 2 of the algorithm of Section 4 might be psychologically more plausible, since they are essentially no more than comparisons between objects.', '#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .', 'Consider the tall cup.', 'The relevant scene would contain three distractors: (1) a less tall object of the same type as the target (e.g., a cup that is less tall), (2) a different kind of object that previous studies had shown to be intermediate in height (e.g., a pitcher that, while being taller than both cups, was neither short nor tall for a pitcher), and (3) a different type of object to which the adjective is inapplicable (e.g., a door key).', ""Across the different conditions under which the experiment was done (e.g., allowing subjects to study the domain before or after the onset of speech), it was found not to matter much whether the adjective applied 'intrinsically' to the target object (i.e., whether the target was tall for a cup): Hearers identifed the target without problems in both types of situations."", 'The time subjects took before looking at the target for the first time was measured, and although these latency times were somewhat greater when the referent were not intrinsically tall than when they were, the average difference was tiny at 554 versus 538 miliseconds.', 'Since latency times are thought to be sensitive to most of the problems that hearers may have in processing a text, these results suggest that, for dimensional adjectives, it is forgivable to disregard global context.']",0,"['It is normally perhaps unlikely that people produce language on the basis of the kind of numerical representations that our algorithm has used as input.', 'Although psychological plausibility is not our aim, it is worth noting that the inequalities computed as step 2 of the algorithm of Section 4 might be psychologically more plausible, since they are essentially no more than comparisons between objects.', '#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .']"
CC544,J06-2002,Generating Referring Expressions that Involve Gradable Properties,efficient contextsensitive generation of referring expressions,"['Emiel Krahmer', 'Mari¨et Theune']",experiments,3 A Modification of the Algorithm Based on Salience 5 3.1 Motivation: Determining the Context Set : : : : : : : : : : : : : 5 3.2 Preliminaries : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 3.3 Outline of the Modified Algorithm : : : : : : : : : : : : : : : : : 7 3.4 Examples : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 9 3.5 Discussion : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 11,"It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #AUTHOR_TAG ) .","['It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #AUTHOR_TAG ) .', 'We have seen in Section 4.3 that generation and interpretation of vague descriptions does have a slightly higher computational complexity than that of nonvague descriptions.', 'Yet, by giving gradable properties a low ranking, we might cause the algorithm to underuse them, for example, in situations where gradable properties are highly relevant to the purpose of the discourse (e.g., a fist fight between people of very different sizes).', 'Luckily, there are no semantic or algorithmic reasons for giving gradables a low ranking.', 'Let us see how things would work if they were ranked more highly.']",1,"['It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #AUTHOR_TAG ) .', 'We have seen in Section 4.3 that generation and interpretation of vague descriptions does have a slightly higher computational complexity than that of nonvague descriptions.', 'Yet, by giving gradable properties a low ranking, we might cause the algorithm to underuse them, for example, in situations where gradable properties are highly relevant to the purpose of the discourse (e.g., a fist fight between people of very different sizes).', 'Let us see how things would work if they were ranked more highly.']"
CC545,J06-2002,Generating Referring Expressions that Involve Gradable Properties,two theories about adjectives,['Hans Kamp'],introduction,,"2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #AUTHOR_TAG ; also Section 8.1 of the present article ) .","['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #AUTHOR_TAG ; also Section 8.1 of the present article ) .']",0,"['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #AUTHOR_TAG ; also Section 8.1 of the present article ) .']"
CC546,J06-2002,Generating Referring Expressions that Involve Gradable Properties,situations and attitudes,"['Jon Barwise', 'John Perry']",introduction,"In this provocative book, Barwise and Perry tackle the slippery subject of ""meaning, "" a subject that has long vexed linguists, language philosophers, and logicians.","In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #AUTHOR_TAG ; see our Section 2 ) .","['IA Plur deals with vague properties in essentially the same way as FOG: Attributes like size are treated as if they were not context dependent: Their Values always apply to the same objects, regardless of what other properties occur in the description.', 'In this way, IA could never describe the same animal as the large chihuahua and the small brown dog, for example.', 'This approach does not do justice to gradable adjectives, whether they are used in the base form, the superlative, or the comparative.', 'Suppose, for example, one set a fixed quantitative boundary, making the word large true of everything above it, and false of everything below it.', 'Then IA would tend to have little use for this property at all since, presumably, every chihuahua would be small and every alsatian large, making each of the combinations {large, chihuahua} (which denotes the empty set) and {large, alsatian} (the set of all alsatians) useless.', ""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #AUTHOR_TAG ; see our Section 2 ) .""]",0,"['IA Plur deals with vague properties in essentially the same way as FOG: Attributes like size are treated as if they were not context dependent: Their Values always apply to the same objects, regardless of what other properties occur in the description.', 'This approach does not do justice to gradable adjectives, whether they are used in the base form, the superlative, or the comparative.', 'Suppose, for example, one set a fixed quantitative boundary, making the word large true of everything above it, and false of everything below it.', 'Then IA would tend to have little use for this property at all since, presumably, every chihuahua would be small and every alsatian large, making each of the combinations {large, chihuahua} (which denotes the empty set) and {large, alsatian} (the set of all alsatians) useless.', ""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #AUTHOR_TAG ; see our Section 2 ) .""]"
CC547,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,the wellbuilt clinical question a key to evidencebased decisions,"['W Scott Richardson', 'Mark C Wilson', 'James Nishikawa', 'Robert S Hayward']",,,The following four components have been identified as the key elements of a question related to patient care ( #AUTHOR_TAG ) :,"['The second facet is independent of the clinical task and pertains to the structure of a well-built clinical question.', 'The following four components have been identified as the key elements of a question related to patient care ( #AUTHOR_TAG ) :']",0,"['The second facet is independent of the clinical task and pertains to the structure of a well-built clinical question.', 'The following four components have been identified as the key elements of a question related to patient care ( #AUTHOR_TAG ) :']"
CC548,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,modern information retrieval,"['Ricardo Baeza-Yates', 'Berthier Ribeiro-Neto']",,"Digital technologies can help scholars to navigate the vast quantities of musical data and source materials now available to them, but an imaginative leap is needed in order to conceptualize the kinds of musicological research questions we might ask of electronic corpora. In particular, our data-rich digital world offers enormous potential for the exploration of musical transmission and relatedness.  In this article, we explore the 16th- and 17th-century instrumental battaglia (battle piece), a genre with a very distinctive collective identity arising from the use of numerous shared ingredients (including melodic, motivic, textural, harmonic and rhythmic features). However, a battaglia is not defined by the presence of a core set of essential features, and exact concordance between these pieces is often remarkably low. This kind of musical 'family resemblance' (formulated after Wittgenstein) poses a serious challenge to both traditional musicological apparatus (for example, finding aids such as thematic catalogues) and Music Information Retrieval (which has often privileged melodic similarity at the expense of other kinds of musical relatedness).  This case study provides a stimulus for rethinking the complex nature of musical similarity. In doing this, we outline a set of requirements for digital tools that could support the discovery, exploration and representation of these kinds of relationships",Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved ( #AUTHOR_TAG ).,"['Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved ( #AUTHOR_TAG ).', 'It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall.']",5,"['Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved ( #AUTHOR_TAG ).', 'It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall.']"
CC549,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,formulating the question,['Andrew Booth'],related work,To know something -- as the first part of our inquiry showed -- is to designate facts by means of judgments in such a way as to obtain a unique correlation while using the smallest possible number of concepts.,"Although originally developed as a tool to assist in query formulation , #AUTHOR_TAG pointed out that PICO frames can be employed to structure IR results for improving precision .","['The idea that clinical information systems should be sensitive to the practice of evidence-based medicine is not new.', 'Based on analyses of 4,000 MEDLINE citations, Mendonça and Cimino (2001) have studied MeSH terms associated with the four basic clinical tasks of therapy, diagnosis, prognosis, and etiology.', 'The goal was to automatically classify citations for task-specific retrieval, similar in spirit to the Hedges Project (Haynes et al. 1994;Wilczynski, McKibbon, and Haynes 2001).', 'Cimino and Mendonça reported good performance for etiology, diagnosis, and in particular therapy, but not prognosis.', 'Although originally developed as a tool to assist in query formulation , #AUTHOR_TAG pointed out that PICO frames can be employed to structure IR results for improving precision .', 'PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al. 1989).', 'The work of Hearst (1996) demonstrates that faceted queries can be converted into simple filtering constraints to boost precision.']",0,"['Based on analyses of 4,000 MEDLINE citations, Mendonca and Cimino (2001) have studied MeSH terms associated with the four basic clinical tasks of therapy, diagnosis, prognosis, and etiology.', 'Cimino and Mendonca reported good performance for etiology, diagnosis, and in particular therapy, but not prognosis.', 'Although originally developed as a tool to assist in query formulation , #AUTHOR_TAG pointed out that PICO frames can be employed to structure IR results for improving precision .', 'The work of Hearst (1996) demonstrates that faceted queries can be converted into simple filtering constraints to boost precision.']"
CC550,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,a comparative study on feature selection in text categorization,"['Yiming Yang', 'Jan O Pedersen']",,This paper is a comparative study of feature selection methods in statistical learning of text categorization The focus is on aggres sive dimensionality reduction Five meth ods were evaluated including term selection based on document frequency DF informa tion gain IG mutual information MI a test CHI and term strength TS We found IG and CHI most e ective in our ex periments Using IG thresholding with a k nearest neighbor classi er on the Reuters cor pus removal of up to removal of unique terms actually yielded an improved classi cation accuracy measured by average preci sion DF thresholding performed similarly Indeed we found strong correlations between the DF IG and CHI values of a term This suggests that DF thresholding the simplest method with the lowest cost in computation can be reliably used instead of IG or CHI when the computation of these measures are too expensive TS compares favorably with the other methods with up to vocabulary reduction but is not competitive at higher vo cabulary reduction levels In contrast MI had relatively poor performance due to its bias towards favoring rare terms and its sen sitivity to probability estimation errors,"We first identified the most informative unigrams and bigrams using the information gain measure ( #AUTHOR_TAG ) , and then selected only the positive outcome predictors using odds ratio ( Mladenic and Grobelnik 1999 ) .","['The n-gram based classifier is also a naive Bayes classifier, but it operates on a different set of features.', 'We first identified the most informative unigrams and bigrams using the information gain measure ( #AUTHOR_TAG ) , and then selected only the positive outcome predictors using odds ratio ( Mladenic and Grobelnik 1999 ) .', 'Diseasespecific terms, such as rheumatoid arthritis, were then manually removed.', 'Finally, the list of features was revised by the registered nurse who participated in the annotation effort.', 'This classifier also outputs the probability of a class assignment.']",5,"['The n-gram based classifier is also a naive Bayes classifier, but it operates on a different set of features.', 'We first identified the most informative unigrams and bigrams using the information gain measure ( #AUTHOR_TAG ) , and then selected only the positive outcome predictors using odds ratio ( Mladenic and Grobelnik 1999 ) .', 'This classifier also outputs the probability of a class assignment.']"
CC551,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,customization in a unified framework for summarizing medical literature,"['Noemie Elhadad', 'Min-Yen Kan', 'Judith Klavans', 'Kathleen McKeown']",related work,"Objective: We present the summarization system in the PErsonalized Retrieval and Summarization of Images, Video and Language (PERSIVAL) medical digital library. Although we discuss the context of our summarization research within the PERSIVAL platform, the primary focus of this article is on strategies to define and generate customized summaries. Methods and material: Our summarizer employs a unified user model to create a tailored summary of relevant documents for either a physician or lay person. The approach takes advantage of regularities in medical literature text structure and content to fulfill identified user needs. Results: The resulting summaries combine both machine-generated text and extracted text that comes from multiple input documents. Customization includes both group-based modeling for two classes of users, physician and lay person, and individually driven models based on a patient record. Conclusions: Our research shows that customization is feasible in a medical digital library","The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and Hatzivassiloglou 2003 ; #AUTHOR_TAG ) .","['In addition to question answering, multi-document summarization provides a complementary approach to addressing clinical information needs.', ""The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and Hatzivassiloglou 2003 ; #AUTHOR_TAG ) ."", 'Although the system incorporates both a user and a task model, it does not explicitly capture the principles of evidence-based medicine.', 'Patient information is no doubt important to answering clinical questions, and our work could certainly benefit from experiences gained in the PERSIVAL project.']",1,"['In addition to question answering, multi-document summarization provides a complementary approach to addressing clinical information needs.', ""The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and Hatzivassiloglou 2003 ; #AUTHOR_TAG ) .""]"
CC552,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,generative content models for structural analysis of medical abstracts,"['Jimmy Lin', 'Damianos Karakos', 'Dina Demner-Fushman', 'Sanjeev Khudanpur']",related work,"The ability to accurately model the content structure of text is important for many natural language processing applications. This paper describes experiments with generative models for analyzing the discourse structure of medical abstracts, which generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"". We demonstrate that Hidden Markov Models are capable of accurately capturing the structure of such texts, and can achieve classification accuracy comparable to that of discriminative techniques. In addition, generative approaches provide advantages that may make them preferable to discriminative techniques such as Support Vector Machines under certain conditions. Our work makes two contributions: at the application level, we report good performance on an interesting task in an important domain; more generally, our results contribute to an ongoing discussion regarding the tradeoffs between generative and discriminative techniques.","For example , McKnight and Srinivasan ( 2003 ) describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also #AUTHOR_TAG ) .","['The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes.', 'For example , McKnight and Srinivasan ( 2003 ) describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also #AUTHOR_TAG ) .', 'Tbahriti et al. (2006) have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance.', 'Note, however, that such labels are orthogonal to PICO frame elements, and hence are not directly relevant to knowledge extraction for clinical question answering.', 'In a similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of speculative statements in MEDLINE abstracts, but once again, this work is not directly applicable to clinical question answering.']",0,"['The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes.', 'For example , McKnight and Srinivasan ( 2003 ) describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also #AUTHOR_TAG ) .']"
CC553,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,automatically evaluating answers to definition questions,"['Jimmy Lin', 'Dina Demner-Fushman']",,"Following recent developments in the automatic evaluation of machine translation and document summarization, we present a similar approach, implemented in a measure called Pourpre, for automatically evaluating answers to definition questions. Until now, the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system's response. The lack of automatic methods for scoring system output is an impediment to progress in the field, which we address with this work. Experiments with the TREC 2003 and TREC 2004 QA tracks indicate that rankings produced by our metric correlate highly with official rankings, and that Pourpre outperforms direct application of existing metrics.","We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization ( #AUTHOR_TAGb ) , but these features are also beyond the capabilities of current summarization systems .","['The most important characteristic of answers, as recommended by Ely et al. (2005) in their study of real-world physicians, is that they focus on bottom-line clinical advice-information that physicians can directly act on.', 'Ideally, answers should integrate information from multiple clinical studies, pointing out both similarities and differences.', 'The system should collate concurrences, that is, if multiple abstracts arrive at the same conclusion-it need not be repeated unless the physician wishes to ""drill down""; the system should reconcile contradictions, for example, if two abstracts disagree on a particular treatment because they studied different patient populations.', 'We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization ( #AUTHOR_TAGb ) , but these features are also beyond the capabilities of current summarization systems .']",1,"['We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization ( #AUTHOR_TAGb ) , but these features are also beyond the capabilities of current summarization systems .']"
CC554,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,using argumentation to retrieve articles with similar citations an inquiry into improving related articles search in the medline digital library,"['Imad Tbahriti', 'Christine Chichester', 'Fr´ed´erique Lisacek', 'Patrick Ruch']",related work,"The aim of this study is to investigate the relationships between citations and the scientific argumentation found abstracts. We design a related article search task and observe how the argumentation can affect the search results. We extracted citation lists from a set of 3200 full-text papers originating from a narrow domain. In parallel, we recovered the corresponding MEDLINE records for analysis of the argumentative moves. Our argumentative model is founded on four classes: PURPOSE, METHODS, RESULTS and CONCLUSION. A Bayesian classifier trained on explicitly structured MEDLINE abstracts generates these argumentative categories. The categories are used to generate four different argumentative indexes. A fifth index contains the complete abstract, together with the title and the list of Medical Subject Headings (MeSH) terms. To appraise the relationship of the moves to the citations, the citation lists were used as the criteria for determining relatedness of articles, establishing a benchmark; it means that two articles are considered as ""related"" if they share a significant set of co-citations. Our results show that the average precision of queries with the PURPOSE and CONCLUSION features is the highest, while the precision of the RESULTS and METHODS features was relatively low. A linear weighting combination of the moves is proposed, which significantly improves retrieval of related articles.",#AUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance .,"['The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes.', 'For example, McKnight and Srinivasan (2003) describe a machine learning approach to automatically label sentences as belonging to introduction, methods, results, or conclusion using structured abstracts as training data (see also Lin et al. 2006).', '#AUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance .', 'Note, however, that such labels are orthogonal to PICO frame elements, and hence are not directly relevant to knowledge extraction for clinical question answering.', 'In a similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of speculative statements in MEDLINE abstracts, but once again, this work is not directly applicable to clinical question answering.']",0,"['The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes.', '#AUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance .', 'In a similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of speculative statements in MEDLINE abstracts, but once again, this work is not directly applicable to clinical question answering.']"
CC555,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,answering clinical questions,"['M Lee Chambliss', 'Jennifer Conley']",introduction,"The combination of recent developments in question-answering research and the availability of unparalleled resources developed specifically for automatic semantic processing of text in the medical domain provides a unique opportunity to explore complex question answering in the domain of clinical medicine. This article presents a system designed to satisfy the information needs of physicians practicing evidence-based medicine. We have developed a series of knowledge extractors, which employ a combination of knowledge-based and statistical techniques, for automatically identifying clinically relevant aspects of MEDLINE abstracts. These extracted elements serve as the input to an algorithm that scores the relevance of citations with respect to structured representations of information needs, in accordance with the principles of evidencebased medicine. Starting with an initial list of citations retrieved by PubMed, our system can bring relevant abstracts into higher ranking positions, and from these abstracts generate responses that directly answer physicians ' questions. We describe three separate evaluations: one focused on the accuracy of the knowledge extractors, one conceptualized as a document reranking task, and finally, an evaluation of answers by two physicians. Experiments on a collection of real-world clinical questions show that our approach significantly outperforms the already competitive PubMed baseline. 1","However , studies have shown that existing systems for searching MEDLINE ( such as PubMed , the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and Wykoff 1994 ; #AUTHOR_TAG ) .","['Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985;Gorman, Ash, and Wykoff 1994;Ely et al. 1999Ely et al. , 2005.', ""MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians' questions, and is commonly used in that capacity (Cogdill and Moore 1997;De Groote and Dorsch 2003)."", 'However , studies have shown that existing systems for searching MEDLINE ( such as PubMed , the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and Wykoff 1994 ; #AUTHOR_TAG ) .', 'Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts.', 'Not only is clinical question answering interesting from a research perspective, it also represents a potentially high-impact, real-world application of language processing and information retrieval technology-better information systems to provide decision support for physicians have the potential to improve the quality of health care.']",0,"['Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985;Gorman, Ash, and Wykoff 1994;Ely et al. 1999Ely et al. , 2005.', ""MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians' questions, and is commonly used in that capacity (Cogdill and Moore 1997;De Groote and Dorsch 2003)."", 'However , studies have shown that existing systems for searching MEDLINE ( such as PubMed , the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and Wykoff 1994 ; #AUTHOR_TAG ) .', 'Not only is clinical question answering interesting from a research perspective, it also represents a potentially high-impact, real-world application of language processing and information retrieval technology-better information systems to provide decision support for physicians have the potential to improve the quality of health care.']"
CC556,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,the nlm indexing initiative’s medical text indexer,"['Alan R Aronson', 'James G Mork', 'Clifford W Gay', 'Susanne M Humphrey', 'Willie J Rogers']",,,"Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content ( #AUTHOR_TAG ) .","['Additional metadata are associated with each MEDLINE citation.', 'The most important of these is the controlled vocabulary terms assigned by human indexers.', ""NLM's controlled vocabulary thesaurus, Medical Subject Headings (MeSH), 2 contains approximately 23,000 descriptors arranged in a hierarchical structure and more than 151,000 Supplementary Concept Records (additional chemical substance names) within a separate thesaurus."", ""Indexing is performed by approximately 100 indexers with at least bachelor's degrees in life sciences and formal training in indexing provided by NLM."", 'Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content ( #AUTHOR_TAG ) .', 'Nevertheless, the indexing process remains firmly human-centered.']",0,"['Additional metadata are associated with each MEDLINE citation.', 'The most important of these is the controlled vocabulary terms assigned by human indexers.', ""NLM's controlled vocabulary thesaurus, Medical Subject Headings (MeSH), 2 contains approximately 23,000 descriptors arranged in a hierarchical structure and more than 151,000 Supplementary Concept Records (additional chemical substance names) within a separate thesaurus."", ""Indexing is performed by approximately 100 indexers with at least bachelor's degrees in life sciences and formal training in indexing provided by NLM."", 'Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content ( #AUTHOR_TAG ) .', 'Nevertheless, the indexing process remains firmly human-centered.']"
CC557,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,what makes a good answer the role of context in question answering,"['Jimmy Lin', 'Dennis Quan', 'Vineet Sinha', 'Karun Bakshi', 'David Huynh', 'Boris Katz', 'David R Karger']",conclusion,"Question answering systems have proven to be helpful to users because they can provide succinct answers that do not require users to wade through a large number of documents. However, despite recent advances in the underlying question answering technology, the problem of designing effective interfaces has been largely unexplored. We conducted a user study to investigate this area and discovered that, overall, users prefer paragraph-sized chunks of text over just an exact phrase as the answer to their questions. Furthermore, users generally prefer answers embedded in context, regardless of the perceived reliability of the source documents. When researching a topic, increasing the amount of text returned to users significantly decreases the number of queries that they pose to the system, suggesting that users utilize supporting text to answer related questions. We believe that these results can serve to guide future developments in question answering interfaces.","Previously , a user study ( #AUTHOR_TAG ) has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance .","['The design and implementation of our current system leaves many open avenues for future exploration, one of which concerns our assumptions about the query interface.', 'Previously , a user study ( #AUTHOR_TAG ) has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance .', 'We have argued that a query interface based on structured PICO frames will yield better-formulated queries, although it is unclear whether physicians would invest the upfront effort necessary to accomplish this.', 'Issuing extremely short queries appears to be an ingrained habit of information seekers today, and the dominance of World Wide Web searches reinforce this behavior.', 'Given these trends, physicians may actually prefer the rapid back-and-forth interaction style that comes with short queries.', 'We believe that if systems can produce noticeably better results with richer queries, users will make more of an effort to formulate them.', 'This, however, presents a chicken-and-egg problem: One possible solution is to develop models that can automatically fill query frames given a couple of keywords-this would serve to kick-start the query generation process.']",1,"['Previously , a user study ( #AUTHOR_TAG ) has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance .']"
CC558,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,towards a medical questionanswering system a feasibility study,"['Pierre Jacquemart', 'Pierre Zweigenbaum']",related work,,"The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , #AUTHOR_TAG , Rinaldi et al. 2004 ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .","['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , #AUTHOR_TAG , Rinaldi et al. 2004 ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', 'Our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine.']",0,"['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , #AUTHOR_TAG , Rinaldi et al. 2004 ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .']"
CC559,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,the interaction of domain knowledge and linguistic structure in natural language processing interpreting hypernymic propositions in biomedical text,"['Thomas C Rindflesch', 'Marcelo Fiszman']",introduction,"Interpretation of semantic propositions in free-text documents such as MEDLINE citations would provide valuable support for biomedical applications, and several approaches to semantic interpretation are being pursued in the biomedical informatics community. In this paper, we describe a methodology for interpreting linguistic structures that encode hypernymic propositions, in which a more specific concept is in a taxonomic relationship with a more general concept. In order to effectively process these constructions, we exploit underspecified syntactic analysis and structured domain knowledge from the Unified Medical Language System (UMLS). After introducing the syntactic processing on which our system depends, we focus on the UMLS knowledge that supports interpretation of hypernymic propositions. We first use semantic groups from the Semantic Network to ensure that the two concepts involved are compatible; hierarchical information in the Metathesaurus then determines which concept is more general and which more specific. A preliminary evaluation of a sample based on the semantic group Chemicals and Drugs provides 83% precision. An error analysis was conducted and potential solutions to the problems encountered are presented. The research discussed here serves as a paradigm for investigating the interaction between domain knowledge and linguistic structure in natural language processing, and could also make a contribution to research on automatic processing of discourse structure. Additional implications of the system we present include its integration in advanced semantic interpretation processors for biomedical text and its use for information extraction in specific domains. The approach has the potential to support a range of applications, including information retrieval and ontology engineering.","Second , software for utilizing this ontology already exists : MetaMap ( Aronson 2001 ) identifies concepts in free text , and SemRep ( #AUTHOR_TAG ) extracts relations between the concepts .","['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'This domain is well-suited for exploring the posed research questions for several reasons.', 'First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'Second , software for utilizing this ontology already exists : MetaMap ( Aronson 2001 ) identifies concepts in free text , and SemRep ( #AUTHOR_TAG ) extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', 'The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.', 'Third, the paradigm of evidence-based medicine (Sackett et al. 2000) provides a task-based model of the clinical information-seeking process.', 'The PICO framework (Richardson et al. 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system.', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']",0,"['First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'Second , software for utilizing this ontology already exists : MetaMap ( Aronson 2001 ) identifies concepts in free text , and SemRep ( #AUTHOR_TAG ) extracts relations between the concepts .']"
CC560,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,issues in stacked generalization,"['Kai Ming Ting', 'Ian H Witten']",,"Stacked generalization is a general method of using a high-level model to combine lower-level models to achieve greater predictive accuracy. In this paper we address two crucial issues which have been considered to be a 'black art' in classification tasks ever since the introduction of stacked generalization in 1992 by Wolpert: the type of generalizer that is suitable to derive the higher-level model, and the kind of attributes that should be used as its input. We find that best results are obtained when the higher-level model combines the confidence (and not just the predictions) of the lower-level ones.    We demonstrate the effectiveness of stacked generalization for combining three different types of learning algorithms for classification tasks. We also compare the performance of stacked generalization with majority vote and published results of arcing and bagging.","The second involved a more principled method using confidence values generated by the base classifiers and least squares linear regression adapted for classification ( #AUTHOR_TAG ) , which can be described by the following equation:","['We attempted two approaches for assigning these weights. The first method relied on ad hoc weight selection based on intuition.', 'The second involved a more principled method using confidence values generated by the base classifiers and least squares linear regression adapted for classification ( #AUTHOR_TAG ) , which can be described by the following equation:']",5,"['The second involved a more principled method using confidence values generated by the base classifiers and least squares linear regression adapted for classification ( #AUTHOR_TAG ) , which can be described by the following equation:']"
CC561,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,analysis of semantic classes in medical text for question answering,"['Yun Niu', 'Graeme Hirst']",related work,"To answer questions from clinical-evidence texts, we identify occurrences of the semantic classes -- disease, medication, patient outcome -- that are candidate elements of the answer, and the relations among them. Additionally, we determine whether an outcome is positive or negative.",The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by #AUTHOR_TAG .,"['The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by #AUTHOR_TAG .', 'Their study also illustrates the importance of semantic classes and relations.', 'However, extraction of outcome statements from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations because secondary sources represent knowledge that has already been distilled by humans (which may limit its scope).', 'Because secondary sources are often more consistently organized, it is possible to depend on certain surface cues for reliable extraction (which is not possible for MEDLINE abstracts in general).', 'Our study tackles outcome identification in primary medical sources and demonstrates that respectable performance is possible with a feature-combination approach.']",1,"['The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by #AUTHOR_TAG .', 'Their study also illustrates the importance of semantic classes and relations.', 'Because secondary sources are often more consistently organized, it is possible to depend on certain surface cues for reliable extraction (which is not possible for MEDLINE abstracts in general).']"
CC562,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,effective mapping of biomedical text to the umls metathesaurus the metamap program,['Alan R Aronson'],introduction,"The UMLS (r) Metathesaurus(r), the largest thesaurus in the biomedical domain, provides a representation of biomedical knowledge consisting of concepts classi-fied by semantic type and both hierarchical and non-hierarchical relationships among the concepts. This knowledge has proved useful for many applications including decision support systems, management of patient records, information retrieval (IR) and data mining. Gaining effective access to the knowledge is critical to the success of these applications. This paper describes MetaMap, a program developed at the National Library of Medicine (NLM) to map bio-medical text to the Metathesaurus or, equivalently, to discover Metathesaurus concepts referred to in text. MetaMap uses a knowledge intensive approach based on symbolic, natural language processing (NLP) and computational linguistic techniques. Besides being applied for both IR and data mining applications, MetaMap is one of the foundations of NLM's Indexing Initiative System which is being applied to both semi-automatic and fully automatic indexing of the biomed-ical literature at the library","Second , software for utilizing this ontology already exists : MetaMap ( #AUTHOR_TAG ) identifies concepts in free text , and SemRep ( Rindflesch and Fiszman 2003 ) extracts relations between the concepts .","['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'This domain is well-suited for exploring the posed research questions for several reasons.', 'First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'Second , software for utilizing this ontology already exists : MetaMap ( #AUTHOR_TAG ) identifies concepts in free text , and SemRep ( Rindflesch and Fiszman 2003 ) extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', 'The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.', 'Third, the paradigm of evidence-based medicine (Sackett et al. 2000) provides a task-based model of the clinical information-seeking process.', 'The PICO framework (Richardson et al. 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system.', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']",0,"['First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'Second , software for utilizing this ontology already exists : MetaMap ( #AUTHOR_TAG ) identifies concepts in free text , and SemRep ( Rindflesch and Fiszman 2003 ) extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', 'The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.']"
CC563,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,improving fulltext precision on short queries using simple constraints,['Marti A Hearst'],related work,"We show that two simple constraints, when applied to short user queries (on the order of 5{10 words) can yield precision scores comparable to or better than those achieved using long queries (50{85 words) at low document cuto levels. These constraints are meant to detect documents that have subtopic passages that includes the most important components of the query. The constraints are: (i) a simple Boolean constraint which requires the user to specify the query as a list of topics; this list is converted into a conjunct of disjuncts by the system, and (ii) a subtopic-sized proximity constraint imposed over the Boolean constraint. The vector space model is used to rank the documents that satisfy both constraints. Experiments run over 45 TREC queries show signi cant, almost consistent improvements over rankings that use no constraints. These results have important rami cations for interactive systems intended for casual users, such as those searching on the World Wide Web.",The work of #AUTHOR_TAG demonstrates that faceted queries can be converted into simple filtering constraints to boost precision .,"['The idea that clinical information systems should be sensitive to the practice of evidence-based medicine is not new.', 'Based on analyses of 4,000 MEDLINE citations, Mendonça and Cimino (2001) have studied MeSH terms associated with the four basic clinical tasks of therapy, diagnosis, prognosis, and etiology.', 'The goal was to automatically classify citations for task-specific retrieval, similar in spirit to the Hedges Project (Haynes et al. 1994;Wilczynski, McKibbon, and Haynes 2001).', 'Cimino and Mendonça reported good performance for etiology, diagnosis, and in particular therapy, but not prognosis.', 'Although originally developed as a tool to assist in query formulation, Booth (2000) pointed out that PICO frames can be employed to structure IR results for improving precision.', 'PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al. 1989).', 'The work of #AUTHOR_TAG demonstrates that faceted queries can be converted into simple filtering constraints to boost precision .']",0,['The work of #AUTHOR_TAG demonstrates that faceted queries can be converted into simple filtering constraints to boost precision .']
CC564,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,automatically identifying health outcome information in medline records,"['Dina Demner-Fushman', 'Barbara Few', 'Susan E Hauser', 'George Thoma']",,"Objective: Understanding the effect of a given intervention on the patient's health outcome is one of the key elements in providing optimal patient care. This study presents a methodology for automatic identification of outcomes-related information in medical text and evaluates its potential in satisfying clinical information needs related to health care outcomes. Design: An annotation scheme based on an evidence-based medicine model for critical appraisal of evidence was developed and used to annotate 633 MEDLINE citations. Textual, structural, and meta-information features essential to outcome identification were learned from the created collection and used to develop an automatic system. Accuracy of automatic outcome identification was assessed in an intrinsic evaluation and in an extrinsic evaluation, in which ranking of MEDLINE search results obtained using PubMed Clinical Queries relied on identified outcome statements. Measurements: The accuracy and positive predictive value of outcome identification were calculated. Effectiveness of the outcome-based ranking was measured using mean average precision and precision at rank 10. Results: Automatic outcome identification achieved 88% to 93% accuracy. The positive predictive value of individual sentences identified as outcomes ranged from 30% to 37%. Outcome-based ranking improved retrieval accuracy, tripling mean average precision and achieving 389% improvement in precision at rank 10. Conclusion: Preliminary results in outcome-based document ranking show potential validity of the evidence-based medicine-model approach in timely delivery of information critical to clinical decision support at the point of service","The extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine ( #AUTHOR_TAG ) .","['The extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine ( #AUTHOR_TAG ) .', 'As will be described herein, the population, problem, and the intervention extractors are based largely on recognition of semantic types and a few manually constructed rules; the outcome extrac-tor, in contrast, is implemented as an ensemble of classifiers trained using supervised machine learning techniques ).', 'These two very different approaches can be attributed to differences in the nature of the frame elements: Whereas problems and interventions can be directly mapped to UMLS concepts, and populations easily mapped to patterns that include UMLS concepts, outcome statements follow no predictable pattern.', 'The initial goal of the annotation effort was to identify outcome statements in abstract text.', 'A physician, two registered nurses, and an engineering researcher manually identified sentences that describe outcomes in 633 MEDLINE abstracts; a post hoc analysis demonstrates good agreement (κ = 0.77).', 'The annotated abstracts were retrieved using PubMed and attempted to model different user behaviors ranging from naive to expert (where advanced search features were employed).', 'With the exception of 50 citations retrieved to answer a question about childhood immunization, the rest of the results were retrieved by querying on a disease, for example, diabetes.', 'Of the 633 citations, 100 abstracts were also fully annotated with population, problems, and interventions.', 'These 100 abstracts were set aside as a held-out test set.', 'Of the remaining citations, 275 were used for training and rule derivation, as described in the following sections.']",5,"['The extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine ( #AUTHOR_TAG ) .']"
CC565,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,analysis of questions asked by family doctors regarding patient care,"['John W Ely', 'Jerome A Osheroff', 'Mark H Ebell', 'George R Bergus', 'Barcey T Levy', 'M Lee Chambliss', 'Eric R Evans']",introduction,"Abstract Objectives: To characterise the information needs of family doctors by collecting the questions they asked about patient care during consultations and to classify these in ways that would be useful to developers of knowledge bases. Design: Observational study in which investigators visited doctors for two half days and collected their questions. Taxonomies were developed to characterise the clinical topic and generic type of information sought for each question. Setting: Eastern Iowa. Participants: Random sample of 103 family doctors. Main outcome measures: Number of questions posed, pursued, and answered; topic and generic type of information sought for each question; time spent pursuing answers; information resources used. Results: Participants asked a total of 1101 questions. Questions about drug prescribing, obstetrics and gynaecology, and adult infectious disease were most common and comprised 36% of all questions. The taxonomy of generic questions included 69 categories; the three most common types, comprising 24% of all questions, were ""What is the cause of symptom X?"" ""What is the dose of drug X?"" and ""How should I manage disease or finding X?"" Answers to most questions (702, 64%) were not immediately pursued, but, of those pursued, most (318, 80%) were answered Doctors spent an average of less than 2 minutes pursuing an answer, and they used readily available print and human resources Only two questions led to a formal literature search. Conclusions: Family doctors in this study did not pursue answers to most of their questions. Questions about patient care can be organised into a limited number of generic types, which could help guide the efforts of knowledge base developers. Key messages Questions that doctors have about the care of their patients could help guide the content of medical information sources and medical training In this study of US family doctors, participants frequently had questions about patient care but did not pursue answers to most questions (64%) On average, participants spent less than 2 minutes seeking an answer to a question The most common resources used to answer questions included textbooks and colleagues; formal literature searches were rarely performed The most common generic questions were ""What is the cause of symptom X?"" ""What is the dose of drug X?"" and ""How should I manage disease or finding X?""","Furthermore , the need to answer questions related to patient care at the point of service has been well studied and documented ( Covell , Uman , and Manning 1985 ; Gorman , Ash , and Wykoff 1994 ; #AUTHOR_TAG , 2005 ) .","['Furthermore , the need to answer questions related to patient care at the point of service has been well studied and documented ( Covell , Uman , and Manning 1985 ; Gorman , Ash , and Wykoff 1994 ; #AUTHOR_TAG , 2005 ) .', ""MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians' questions, and is commonly used in that capacity (Cogdill and Moore 1997;De Groote and Dorsch 2003)."", 'However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994;Chambliss and Conley 1996).', 'Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts.', 'Not only is clinical question answering interesting from a research perspective, it also represents a potentially high-impact, real-world application of language processing and information retrieval technology-better information systems to provide decision support for physicians have the potential to improve the quality of health care.']",0,"['Furthermore , the need to answer questions related to patient care at the point of service has been well studied and documented ( Covell , Uman , and Manning 1985 ; Gorman , Ash , and Wykoff 1994 ; #AUTHOR_TAG , 2005 ) .']"
CC566,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,answer extraction semantic clustering and extractive summarization for clinical question answering,"['Dina Demner-Fushman', 'Jimmy Lin']",,"This paper presents a hybrid approach to question answering in the clinical domain that combines techniques from summarization and information retrieval. We tackle a frequently-occurring class of questions that takes the form ""What is the best drug treatment for X?"" Starting from an initial set of MEDLINE citations, our system first identifies the drugs under study. Abstracts are then clustered using semantic classes from the UMLS ontology. Finally, a short extractive summary is generated for each abstract to populate the clusters. Two evaluations---a manual one focused on short answers and an automatic one focused on the supporting abstract---demonstrate that our system compares favorably to PubMed, the search system most widely used by physicians today.","We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see #AUTHOR_TAG and Lin (2006).","['Finally, answer generation remains an area that awaits further exploration, although we would have to first define what a good answer should be.', 'We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see #AUTHOR_TAG and Lin (2006).', 'Furthermore, the current answer generator does not handle complex issues such as contradictory and inconsistent statements.', 'To address these very difficult challenges, finer-grained semantic analysis of medical texts is required.']",0,"['We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see #AUTHOR_TAG and Lin (2006).', 'Furthermore, the current answer generator does not handle complex issues such as contradictory and inconsistent statements.', 'To address these very difficult challenges, finer-grained semantic analysis of medical texts is required.']"
CC567,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,automatically identifying health outcome information in medline records,"['Dina Demner-Fushman', 'Barbara Few', 'Susan E Hauser', 'George Thoma']",,"Objective: Understanding the effect of a given intervention on the patient's health outcome is one of the key elements in providing optimal patient care. This study presents a methodology for automatic identification of outcomes-related information in medical text and evaluates its potential in satisfying clinical information needs related to health care outcomes. Design: An annotation scheme based on an evidence-based medicine model for critical appraisal of evidence was developed and used to annotate 633 MEDLINE citations. Textual, structural, and meta-information features essential to outcome identification were learned from the created collection and used to develop an automatic system. Accuracy of automatic outcome identification was assessed in an intrinsic evaluation and in an extrinsic evaluation, in which ranking of MEDLINE search results obtained using PubMed Clinical Queries relied on identified outcome statements. Measurements: The accuracy and positive predictive value of outcome identification were calculated. Effectiveness of the outcome-based ranking was measured using mean average precision and precision at rank 10. Results: Automatic outcome identification achieved 88% to 93% accuracy. The positive predictive value of individual sentences identified as outcomes ranged from 30% to 37%. Outcome-based ranking improved retrieval accuracy, tripling mean average precision and achieving 389% improvement in precision at rank 10. Conclusion: Preliminary results in outcome-based document ranking show potential validity of the evidence-based medicine-model approach in timely delivery of information critical to clinical decision support at the point of service","After much exploration , #AUTHOR_TAG discovered that it was not practical to annotate PICO entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues .","['After much exploration , #AUTHOR_TAG discovered that it was not practical to annotate PICO entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues .', 'Consider the following segment:']",0,"['After much exploration , #AUTHOR_TAG discovered that it was not practical to annotate PICO entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues .']"
CC568,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,feature selection for unbalanced class distribution and naive bayes,"['Dunja Mladenic', 'Marko Grobelnik']",,"This paper describes an approach to feature subset selection that takes into account problem speciics and learning algorithm characteristics. It is developed for the Naive Bayesian classiier applied on text data, since it combines well with the addressed learning problems. We focus on domains with many features that also have a highly unbalanced class distribution and asymmetric misclassii-cation costs given only implicitly in the problem. By asymmetric misclassiication costs we mean that one of the class values is the target class value for which we want to get predictions and we prefer false positive over false negative. Our example problem is automatic document categorization using machine learning, where we want to identify documents relevant for the selected category. Usually, only about 1%-10% of examples belong to the selected category. Our experimental comparison of eleven feature scoring measures show that considering domain and algorithm characteristics signiicantly improves the results of classiication.","We first identified the most informative unigrams and bigrams using the information gain measure ( Yang and Pedersen 1997 ) , and then selected only the positive outcome predictors using odds ratio ( #AUTHOR_TAG ) .","['The n-gram based classifier is also a naive Bayes classifier, but it operates on a different set of features.', 'We first identified the most informative unigrams and bigrams using the information gain measure ( Yang and Pedersen 1997 ) , and then selected only the positive outcome predictors using odds ratio ( #AUTHOR_TAG ) .', 'Diseasespecific terms, such as rheumatoid arthritis, were then manually removed.', 'Finally, the list of features was revised by the registered nurse who participated in the annotation effort.', 'This classifier also outputs the probability of a class assignment.']",5,"['The n-gram based classifier is also a naive Bayes classifier, but it operates on a different set of features.', 'We first identified the most informative unigrams and bigrams using the information gain measure ( Yang and Pedersen 1997 ) , and then selected only the positive outcome predictors using odds ratio ( #AUTHOR_TAG ) .', 'This classifier also outputs the probability of a class assignment.']"
CC569,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,answer extraction semantic clustering and extractive summarization for clinical question answering,"['Dina Demner-Fushman', 'Jimmy Lin']",,"This paper presents a hybrid approach to question answering in the clinical domain that combines techniques from summarization and information retrieval. We tackle a frequently-occurring class of questions that takes the form ""What is the best drug treatment for X?"" Starting from an initial set of MEDLINE citations, our system first identifies the drugs under study. Abstracts are then clustered using semantic classes from the UMLS ontology. Finally, a short extractive summary is generated for each abstract to populate the clusters. Two evaluations---a manual one focused on short answers and an automatic one focused on the supporting abstract---demonstrate that our system compares favorably to PubMed, the search system most widely used by physicians today.","Perhaps some variation of multi-level bulleted lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , #AUTHOR_TAG .","['It is clear that the type of answers desired by physicians require a level of semantic analysis that is beyond the current state of the art, even with the aid of existing medical ontologies.', 'For example, even the seemingly straightforward task of identifying similarities and differences in outcome statements is rendered exceedingly complex by the tremendous amount of background medical knowledge that must be brought to bear in interpreting clinical results and subtle differences in study design, objectives, and results; the closest analogous task in computational linguistics-redundancy detection for multi-document summarization-seems easy by comparison.', 'Furthermore, it is unclear if textual strings make ""good answers.""', 'Perhaps a graphical rendering of the semantic predicates present in relevant abstracts might more effectively convey the desired information; see, for example, Fiszman, Rindflesch, and Kilicoglu (2004).', 'Perhaps some variation of multi-level bulleted lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , #AUTHOR_TAG .']",3,"['It is clear that the type of answers desired by physicians require a level of semantic analysis that is beyond the current state of the art, even with the aid of existing medical ontologies.', 'For example, even the seemingly straightforward task of identifying similarities and differences in outcome statements is rendered exceedingly complex by the tremendous amount of background medical knowledge that must be brought to bear in interpreting clinical results and subtle differences in study design, objectives, and results; the closest analogous task in computational linguistics-redundancy detection for multi-document summarization-seems easy by comparison.', 'Furthermore, it is unclear if textual strings make ""good answers.""', 'Perhaps a graphical rendering of the semantic predicates present in relevant abstracts might more effectively convey the desired information; see, for example, Fiszman, Rindflesch, and Kilicoglu (2004).', 'Perhaps some variation of multi-level bulleted lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , #AUTHOR_TAG .']"
CC570,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,overview of the trec,['Ellen M Voorhees'],related work,,"Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other '' questions ( #AUTHOR_TAG ) .","['Finally, the evaluation of answers to complex questions remains an open research problem.', 'Although it is clear that measures designed for open-domain factoid questions are not appropriate, the community has not agreed on a methodology that will allow meaningful comparisons of results from different systems.', 'In Sections 9 and 10, we have discussed many of these issues.', 'Recently, there is a growing consensus that an evaluation methodology based on the notion of ""information nuggets"" may provide an appropriate framework for assessing the quality of answers to complex questions.', ""Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other '' questions ( #AUTHOR_TAG ) ."", 'A number of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcomings of the original nugget scoring model, although a number of these issues have been recently addressed Demner-Fushman 2005a, 2006b).', 'However, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken.']",0,"['Finally, the evaluation of answers to complex questions remains an open research problem.', 'In Sections 9 and 10, we have discussed many of these issues.', 'Recently, there is a growing consensus that an evaluation methodology based on the notion of ""information nuggets"" may provide an appropriate framework for assessing the quality of answers to complex questions.', ""Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other '' questions ( #AUTHOR_TAG ) ."", 'A number of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcomings of the original nugget scoring model, although a number of these issues have been recently addressed Demner-Fushman 2005a, 2006b).']"
CC571,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,the role of knowledge in conceptual retrieval a study in the domain of clinical medicine,"['Jimmy Lin', 'Dina Demner-Fushman']",related work,"Despite its intuitive appeal, the hypothesis that retrieval at the level of ""concepts"" should outperform purely term-based approaches remains unverified empirically. In addition, the use of ""knowledge"" has not consistently resulted in performance gains. After identifying possible reasons for previous negative results, we present a novel framework for ""conceptual retrieval"" that articulates the types of knowledge that are important for information seeking. We instantiate this general framework in the domain of clinical medicine based on the principles of evidence-based medicine (EBM). Experiments show that an EBM-based scoring algorithm dramatically outperforms a state-of-the-art baseline that employs only term statistics. Ablation studies further yield a better understanding of the performance contributions of different components. Finally, we discuss how other domains can benefit from knowledge-based approaches.","In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #AUTHOR_TAGa ) for a brief overview .","['Clinical question answering is an emerging area of research that has only recently begun to receive serious attention.', 'As a result, there exist relatively few points of comparison to our own work, as the research space is sparsely populated.', 'In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems.', 'For an overview of systems designed to answer open-domain factoid questions, the TREC QA track overview papers are a good place to start (Voorhees and Tice 1999).', 'In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #AUTHOR_TAGa ) for a brief overview .']",0,"['In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems.', 'In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #AUTHOR_TAGa ) for a brief overview .']"
CC572,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,automatically evaluating answers to definition questions,"['Jimmy Lin', 'Dina Demner-Fushman']",related work,"Following recent developments in the automatic evaluation of machine translation and document summarization, we present a similar approach, implemented in a measure called Pourpre, for automatically evaluating answers to definition questions. Until now, the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system's response. The lack of automatic methods for scoring system output is an impediment to progress in the field, which we address with this work. Experiments with the TREC 2003 and TREC 2004 QA tracks indicate that rankings produced by our metric correlate highly with official rankings, and that Pourpre outperforms direct application of existing metrics.","A number of studies ( e.g. , Hildebrandt , Katz , and Lin 2004 ) have pointed out shortcomings of the original nugget scoring model , although a number of these issues have been recently addressed ( #AUTHOR_TAGa , 2006b ) .","['Finally, the evaluation of answers to complex questions remains an open research problem.', 'Although it is clear that measures designed for open-domain factoid questions are not appropriate, the community has not agreed on a methodology that will allow meaningful comparisons of results from different systems.', 'In Sections 9 and 10, we have discussed many of these issues.', 'Recently, there is a growing consensus that an evaluation methodology based on the notion of ""information nuggets"" may provide an appropriate framework for assessing the quality of answers to complex questions.', 'Nugget F-score has been employed as a metric in the TREC question-answering track since 2003, to evaluate so-called definition and ""other"" questions (Voorhees 2003).', 'A number of studies ( e.g. , Hildebrandt , Katz , and Lin 2004 ) have pointed out shortcomings of the original nugget scoring model , although a number of these issues have been recently addressed ( #AUTHOR_TAGa , 2006b ) .', 'However, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken.']",0,"['Finally, the evaluation of answers to complex questions remains an open research problem.', 'Although it is clear that measures designed for open-domain factoid questions are not appropriate, the community has not agreed on a methodology that will allow meaningful comparisons of results from different systems.', 'In Sections 9 and 10, we have discussed many of these issues.', 'Recently, there is a growing consensus that an evaluation methodology based on the notion of ""information nuggets"" may provide an appropriate framework for assessing the quality of answers to complex questions.', 'Nugget F-score has been employed as a metric in the TREC question-answering track since 2003, to evaluate so-called definition and ""other"" questions (Voorhees 2003).', 'A number of studies ( e.g. , Hildebrandt , Katz , and Lin 2004 ) have pointed out shortcomings of the original nugget scoring model , although a number of these issues have been recently addressed ( #AUTHOR_TAGa , 2006b ) .']"
CC573,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,strength of recommendation taxonomy sort a patientcentered approach to grading evidence in the medical literature,"['Mark H Ebell', 'Jay Siwek', 'Barry D Weiss', 'Steven H Woolf', 'Jeffrey Susman', 'Bernard Ewigman', 'Marjorie Bowman']",,"A large number of taxonomies are used to rate the quality of an individual study and the strength of a recommendation based on a body of evidence. We have developed a new grading scale that will be used by several family medicine and primary care journals (required or optional), with the goal of allowing readers to learn one taxonomy that will apply to many sources of evidence. Our scale is called the Strength of Recommendation Taxonomy. It addresses the quality, quantity, and consistency of evidence and allows authors to rate individual studies or bodies of evidence. The taxonomy is built around the information mastery framework, which emphasizes the use of patient-oriented outcomes that measure changes in morbidity or mortality. An A-level recommendation is based on consistent and good-quality patient-oriented evidence; a B-level recommendation is based on inconsistent or limited-quality patient-oriented evidence; and a C-level recommendation is based on consensus, usual practice, opinion, disease-oriented evidence, or case series for studies of diagnosis, treatment, prevention, or screening. Levels of evidence from 1 to 3 for individual studies also are defined. We hope that consistent use of this taxonomy will improve the ability of authors and readers to communicate about the translation of research into practice.",Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy ( #AUTHOR_TAG ) .,"['The potential highest level of the strength of evidence for a given citation can be identified using the Publication Type (a metadata field) and MeSH terms pertaining to the type of the clinical study.', 'Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy ( #AUTHOR_TAG ) .']",5,['Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy ( #AUTHOR_TAG ) .']
CC574,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,firstyear medical students’ information needs and resource selection responses to a clinical scenario,"['Keith W Cogdill', 'Margaret E Moore']",introduction,"Etude ayant pour but une meilleure comprehension des besoins en information des etudiants en medecine de premiere annee, et de leurs perceptions des ressources appropriees( ressources telles que livres , MEDLINE ... )","MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians ' questions , and is commonly used in that capacity ( #AUTHOR_TAG ; De Groote and Dorsch 2003 ) .","['Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985;Gorman, Ash, and Wykoff 1994;Ely et al. 1999Ely et al. , 2005.', ""MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians ' questions , and is commonly used in that capacity ( #AUTHOR_TAG ; De Groote and Dorsch 2003 ) ."", 'However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994;Chambliss and Conley 1996).', 'Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts.', 'Not only is clinical question answering interesting from a research perspective, it also represents a potentially high-impact, real-world application of language processing and information retrieval technology-better information systems to provide decision support for physicians have the potential to improve the quality of health care.']",0,"['Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985;Gorman, Ash, and Wykoff 1994;Ely et al. 1999Ely et al. , 2005.', ""MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians ' questions , and is commonly used in that capacity ( #AUTHOR_TAG ; De Groote and Dorsch 2003 ) ."", 'However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994;Chambliss and Conley 1996).', 'Not only is clinical question answering interesting from a research perspective, it also represents a potentially high-impact, real-world application of language processing and information retrieval technology-better information systems to provide decision support for physicians have the potential to improve the quality of health care.']"
CC575,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,the trec8 question answering track evaluation,"['Ellen M Voorhees', 'Dawn M Tice']",related work,,"For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start ( #AUTHOR_TAG ) .","['Clinical question answering is an emerging area of research that has only recently begun to receive serious attention.', 'As a result, there exist relatively few points of comparison to our own work, as the research space is sparsely populated.', 'In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems.', 'For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start ( #AUTHOR_TAG ) .', 'In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006a) for a brief overview.']",0,"['For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start ( #AUTHOR_TAG ) .']"
CC576,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,evidencebased medicine how to practice and teach ebm second edition churchill livingstone,"['David L Sackett', 'Sharon E Straus', 'W Scott Richardson', 'William Rosenberg', 'R Brian Haynes']",introduction,,"Third , the paradigm of evidence-based medicine ( #AUTHOR_TAG ) provides a task-based model of the clinical information-seeking process .","['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'This domain is well-suited for exploring the posed research questions for several reasons.', 'First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'Second, software for utilizing this ontology already exists: MetaMap (Aronson 2001) identifies concepts in free text, and SemRep (Rindflesch and Fiszman 2003) extracts relations between the concepts.', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', 'The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.', 'Third , the paradigm of evidence-based medicine ( #AUTHOR_TAG ) provides a task-based model of the clinical information-seeking process .', 'The PICO framework (Richardson et al. 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system.', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']",0,"['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'Third , the paradigm of evidence-based medicine ( #AUTHOR_TAG ) provides a task-based model of the clinical information-seeking process .']"
CC577,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,natural language question answering the view from here,"['Lynette Hirschman', 'Robert Gaizauskas']",experiments,"As users struggle to navigate the wealth of on-line information now available, the need for automated question answering systems becomes more urgent. We need systems that allow a user to ask a question in everyday language and receive an answer quickly and succinctly, with sufficient context to validate the answer. Current search engines can return ranked lists of documents, but they do not deliver answers to the user. Question answering systems address this problem. Recent successes have been reported in a series of question-answering evaluations that started in 1999 as part of the Text Retrieval Conference (TREC). The best systems are now able to answer more than two thirds of factual questions in this evaluation.","As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( Voorhees and Tice 1999 ; #AUTHOR_TAG ) .","['Ideally, we would like to match structured representations derived from the question with those derived from MEDLINE citations (taking into consideration other EBMrelevant factors).', 'However, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results.', 'As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( Voorhees and Tice 1999 ; #AUTHOR_TAG ) .']",1,"['As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( Voorhees and Tice 1999 ; #AUTHOR_TAG ) .']"
CC578,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,the trec8 question answering track evaluation,"['Ellen M Voorhees', 'Dawn M Tice']",experiments,,"As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #AUTHOR_TAG ; Hirschman and Gaizauskas 2001 ) .","['Ideally, we would like to match structured representations derived from the question with those derived from MEDLINE citations (taking into consideration other EBMrelevant factors).', 'However, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results.', 'As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #AUTHOR_TAG ; Hirschman and Gaizauskas 2001 ) .']",1,"['As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #AUTHOR_TAG ; Hirschman and Gaizauskas 2001 ) .']"
CC579,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,semantic characteristics of medline citations useful for therapeutic decisionmaking,"['Charles Sneiderman', 'Dina Demner-Fushman', 'Marcelo Fiszman', 'Thomas C Rindflesch']",,MEDLINE retrieval using several information retrieval algorithms was characterized for relevance to point-of-care therapeutic decisions for a sample of clinical queries in family practice. Evaluation methodology is described and preliminary results are presented.,"For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in #AUTHOR_TAG .","['Although our problem extractor returns a list of clinical problems, we only evaluate performance on identification of the primary problem.', ""For some abstracts, MeSH headings can be used as ground truth, because one of the human indexers' tasks in assigning terms is to identify the main topic of the article (sometimes a disorder)."", 'For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in #AUTHOR_TAG .']",5,"['For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in #AUTHOR_TAG .']"
CC580,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,answering questions in the genomics domain,"['Fabio Rinaldi', 'James Dowdall', 'Gerold Schneider', 'Andreas Persidis']",related work,"In this paper we describe current efforts aimed at adapting an existing Question Answering system to a new document set, namely research papers in the genomics domain. The system has been originally developed for another restricted domain, however it has already proved its portability. Nevertheless, the process is not painless, and the specific purpose of  this paper is to describe the problems encountered.","The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .","['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', 'Our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine.']",0,"['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .']"
CC581,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,effective mapping of biomedical text to the umls metathesaurus the metamap program,['Alan R Aronson'],,"The UMLS (r) Metathesaurus(r), the largest thesaurus in the biomedical domain, provides a representation of biomedical knowledge consisting of concepts classi-fied by semantic type and both hierarchical and non-hierarchical relationships among the concepts. This knowledge has proved useful for many applications including decision support systems, management of patient records, information retrieval (IR) and data mining. Gaining effective access to the knowledge is critical to the success of these applications. This paper describes MetaMap, a program developed at the National Library of Medicine (NLM) to map bio-medical text to the Metathesaurus or, equivalently, to discover Metathesaurus concepts referred to in text. MetaMap uses a knowledge intensive approach based on symbolic, natural language processing (NLP) and computational linguistic techniques. Besides being applied for both IR and data mining applications, MetaMap is one of the foundations of NLM's Indexing Initiative System which is being applied to both semi-automatic and fully automatic indexing of the biomed-ical literature at the library","Our knowledge extractors rely extensively on MetaMap ( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .","['Our knowledge extractors rely extensively on MetaMap ( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .', 'Many of our algorithms operate at the level of coarser-grained semantic types called Semantic Groups (McCray, Burgun, and Bodenreider 2001), which capture higher-level generalizations about entities (e.g., CHEMICALS & DRUGS).', 'An additional feature we take advantage of (when present) is explicit section markers present in some abstracts.', 'These so-called structured abstracts were recommended by the Ad Hoc Working Group for Critical Appraisal of the Medical Literature (1987) to help humans assess the reliability and content of a publication and to facilitate the indexing and retrieval processes.', 'These abstracts loosely adhere to the introduction, methods, results, and conclusions format common in scientific writing, and delineate a study using explicitly marked sections with variations of the above headings.', 'Although many core clinical journals require structured abstracts, there is a great deal of variation in the actual headings.', 'Even when present, the headings are not organized in a manner focused on patient care.', 'In addition, abstracts of much high-quality work remain unstructured.', 'For these reasons, explicit section markers are not entirely reliable indicators for the various semantic elements we seek to extract, but must be considered along with other sources of evidence.']",5,"['Our knowledge extractors rely extensively on MetaMap ( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .']"
CC582,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,knowledge extraction for clinical question answering preliminary results,"['Dina Demner-Fushman', 'Jimmy Lin']",,"The combination of recent developments in question an-swering research and the unparalleled resources devel-oped specifically for automatic semantic processing of text in the medical domain provides a unique opportu-nity to explore complex question answering in the clin-ical domain. In this paper, we attempt to operationalize major aspects of evidence-based medicine in the form of knowledge extractors that serve as the fundamental building blocks of a clinical question answering sys-tem. Our evaluations demonstrate that domain-specific knowledge can be effectively leveraged to extract PICO frame elements from MEDLINE abstracts. Clinical in-formation systems in support of physicians ' decision-making process have the potential to improve the qual-ity of patient care in real-world settings","This section , which elaborates on preliminary results reported in #AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .","['The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.', 'This section , which elaborates on preliminary results reported in #AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .', 'For an example of a completely annotated abstract, see Figure 2.', 'Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases.']",2,"['The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.', 'This section , which elaborates on preliminary results reported in #AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .', 'For an example of a completely annotated abstract, see Figure 2.', 'Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases.']"
CC583,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,the nlm indexing initiative’s medical text indexer,"['Alan R Aronson', 'James G Mork', 'Clifford W Gay', 'Susanne M Humphrey', 'Willie J Rogers']",,,"Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #AUTHOR_TAG ) .","['The function α(t) maps a MeSH term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task.', 'Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #AUTHOR_TAG ) .']",3,"['Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #AUTHOR_TAG ) .']"
CC584,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,the wellbuilt clinical question a key to evidencebased decisions,"['W Scott Richardson', 'Mark C Wilson', 'James Nishikawa', 'Robert S Hayward']",introduction,,The PICO framework ( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .,"['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'This domain is well-suited for exploring the posed research questions for several reasons.', 'First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'Second, software for utilizing this ontology already exists: MetaMap (Aronson 2001) identifies concepts in free text, and SemRep (Rindflesch and Fiszman 2003) extracts relations between the concepts.', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', 'The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.', 'Third, the paradigm of evidence-based medicine (Sackett et al. 2000) provides a task-based model of the clinical information-seeking process.', 'The PICO framework ( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']",0,"['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'Third, the paradigm of evidence-based medicine (Sackett et al. 2000) provides a task-based model of the clinical information-seeking process.', 'The PICO framework ( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']"
CC585,J08-1003,Wide-Coverage Deep Statistical Parsing Using Automatic Dependency Structure Annotation,speed and accuracy in shallow and deep stochastic parsing,"['Ron Kaplan', 'Stefan Riezler', 'Tracy Holloway King', 'John T Maxwell', 'Alexander Vasserman', 'Richard Crouch']",introduction,"Abstract : This paper reports some experiments that Compare the accuracy and performance of two stochastic parsing systems. The currently popular Collins parser is a shallow parser whose output contains more detailed semantically relevant information than other such parsers. The XLE parser is a deep-parsing system that couples a Lexical Functional Grammar to a log- linear disambiguation component and provides much richer representations theory. We measured the accuracy of both systems against a gold standard of the PARC 700 dependency bank, and also measured their processing times. We found the deep-parsing system to be more accurate than the Collins parser with only a slight reduction in parsing speed.","Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; Preiss 2003 ; #AUTHOR_TAG ; Miyao and Tsujii 2004 ) .","['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; Preiss 2003 ; #AUTHOR_TAG ; Miyao and Tsujii 2004 ) .', 'Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation.']",4,"['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; Preiss 2003 ; #AUTHOR_TAG ; Miyao and Tsujii 2004 ) .']"
CC586,J08-1003,Wide-Coverage Deep Statistical Parsing Using Automatic Dependency Structure Annotation,the parc 700 dependency bank,"['Tracy Holloway King', 'Richard Crouch', 'Stefan Riezler', 'Mary Dalrymple', 'Ron Kaplan']",introduction,"An automatic method for annotating the Penn-II Treebank (Marcus et al., 1994) with high-level Lexical Functional Grammar (Kaplan and Bresnan, 1982; Bresnan, 2001; Dalrymple, 2001) f-structure representations is described in (Cahill et al., 2002; Cahill et al., 2004a; Cahill et al., 2004b; O'Donovan et al., 2004). The annotation algorithm and the automatically-generated f-structures are the basis for the automatic acquisition of wide-coverage and robust probabilistic approximations of LFG grammars (Cahill et al., 2002; Cahill et al., 2004a) and for the induction of LFG semantic forms (O'Donovan et al., 2004). The quality of the annotation algorithm and the f-structures it generates is, therefore, extremely important. To date, annotation quality has been measured in terms of precision and recall against the DCU 105. The annotation algorithm currently achieves an f-score of 96.57% for complete f-structures and 94.3% for preds-only f-structures. There are a number of problems with evaluating against a gold standard of this size, most notably that of overfitting. There is a risk of assuming that the gold standard is a complete and balanced representation of the linguistic phenomena in a language and basing design decisions on this. It is, therefore, preferable to evaluate against a more extensive, external standard. Although the DCU 105 is publicly available, 1 a larger well-established external standard can provide a more widely-recognised benchmark against which the quality of the f-structure annotation algorithm can be evaluated. For these reasons, we present an evaluation of the f-structure annotation algorithm of (Cahill et al., 2002; Cahill et al., 2004a; Cahill et al., 2004b; O'Donovan et al., 2004) against the PARC 700 Dependency Bank (King et al., 2003). Evaluation against an external gold standard is a non-trivial task as linguistic analyses may differ systematically between the gold standard and the output to be evaluated as regards feature geometry and nomenclature. We present conversion software to automatically account for many (but not all) of the systematic differences. Currently, we achieve an f-score of 87.31% for the f-structures generated from the original Penn-II trees and an f-score of 81.79% for f-structures from parse trees produced by Charniak's (2000) parser in our pipeline parsing architecture against the PARC 700","Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; #AUTHOR_TAG ; Preiss 2003 ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .","['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; #AUTHOR_TAG ; Preiss 2003 ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .', 'Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation.']",4,"['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; #AUTHOR_TAG ; Preiss 2003 ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .', 'Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation.']"
CC587,J08-1003,Wide-Coverage Deep Statistical Parsing Using Automatic Dependency Structure Annotation,using grammatical relations to compare parsers,['Judita Preiss'],introduction,"We use the grammatical relations (GRs) described in Carroll et al. (1998) to compare a number of parsing algorithms. A first ranking of the parsers is provided by comparing the extracted GRs to a gold standard GR annotation of 500 Susanne sentences: this required an implementation of GR extraction software for Penn Treebank style parsers. In addition, we perform an experiment using the extracted GRs as input to the Lappin and Leass (1994) anaphora resolution algorithm. This produces a second ranking of the parsers, and we investigate the number of errors that are caused by the incorrect 'GRs.","Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; #AUTHOR_TAG ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .","['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; #AUTHOR_TAG ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .', 'Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation.']",4,"['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; #AUTHOR_TAG ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .', 'Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation.']"
CC588,J08-2002,A Global Joint Model for Semantic Role Labeling,discriminative reranking for natural language parsing,['Michael Collins'],introduction,"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation","Our re-ranking approach , like the approach to parse re-ranking of #AUTHOR_TAG , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes .","['Such a rich graphical model can represent many dependencies but there are two dangers-one is that the computational complexity of training the model and searching for the most likely labeling given the tree can be prohibitive, and the other is that if too many dependencies are encoded, the model will over-fit the training data and will not generalize well.', 'We propose a model which circumvents these two dangers and achieves significant performance gains over a similar local model that does not add any dependency arcs among the random variables.', 'To tackle the efficiency problem, we adopt dynamic programming and re-ranking algorithms.', 'To avoid overfitting we encode only a small set of linguistically motivated dependencies in features over sets of the random variables.', 'Our re-ranking approach , like the approach to parse re-ranking of #AUTHOR_TAG , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes .', 'The joint model is restricted to these n assignments and does not have to search the exponentially large space of all possible joint labelings.']",1,"['Our re-ranking approach , like the approach to parse re-ranking of #AUTHOR_TAG , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes .']"
CC589,J08-2002,A Global Joint Model for Semantic Role Labeling,discriminative reranking for natural language parsing,['Michael Collins'],,"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation","Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #AUTHOR_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions .","['Motivation for Re-Ranking.', 'For argument identification, the number of possible assignments for a parse tree with n nodes is 2 n .', 'This number can run into the hundreds of billions for a normal-sized tree.', 'For argument labeling, the number of possible assignments is ≈ 20 m , if m is the number of arguments of a verb (typically between 2 and 5), and 20 is the approximate number of possible labels if considering both core and modifying arguments.', 'Training a model which has such a huge number of classes is infeasible if the model does not factorize due to strong independence assumptions.', 'Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #AUTHOR_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions .', 'We utilize the top n assignments of our local semantic role labeling model P SRL to generate likely assignments.', 'As can be seen from Figure 8(a), for relatively small values of n, our re-ranking approach does not present a serious bottleneck to performance.', 'We used a value of n = 10 for training.', 'In Figure 8(a) we can see that if we could pick, using an oracle, the best assignment out of the top 10 assignments according to the local model, we would achieve an F-Measure of 97.3 on all arguments.', 'Increasing the number of n to 30 results in a very small gain in the upper bound on performance and a large increase in memory requirements.', 'We therefore selected n = 10 as a good compromise.']",5,"['Motivation for Re-Ranking.', 'For argument identification, the number of possible assignments for a parse tree with n nodes is 2 n .', 'For argument labeling, the number of possible assignments is  20 m , if m is the number of arguments of a verb (typically between 2 and 5), and 20 is the approximate number of possible labels if considering both core and modifying arguments.', 'Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #AUTHOR_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions .', 'We utilize the top n assignments of our local semantic role labeling model P SRL to generate likely assignments.', 'As can be seen from Figure 8(a), for relatively small values of n, our re-ranking approach does not present a serious bottleneck to performance.']"
CC590,J09-1003,Evaluating Centering for Information Ordering Using Corpora,stochastic text structuring using the principle of continuity,"['Nikiforos Karamanis', 'Hisar Maruli Manurung']",,"This paper explores the feasibility of implementing an evolutionary algorithm for text structuring using the heuristic of continuity as a fitness function, chosen over other more complicated metrics of text coherence. Using MCGONAGALL (Manurung et al., 2000) as our experimental platform, we show that by employing an elitist strategy for stochastic search it is possible to quickly reach the global optimum of minimal violations of continuity.","Following our previous work ( #AUTHOR_TAG ; Althaus , Karamanis , and Koller 2004 ) , the input to information ordering is an unordered set of informationbearing items represented as CF lists .","['Following our previous work ( #AUTHOR_TAG ; Althaus , Karamanis , and Koller 2004 ) , the input to information ordering is an unordered set of informationbearing items represented as CF lists .', 'A set of candidate orderings is produced by creating different permutations of these lists.', 'A metric of coherence uses features from centering to compute a score for each candidate ordering and select the highest scoring ordering as the output. 9', ""A wide range of metrics of coherence can be defined in centering's terms, simply on the basis of the work we reviewed in Section 3. To exemplify this, let us first assume that the ordering in Example (3), which is analyzed as a sequence of CF lists in Table 5, is a candidate ordering."", 'Table 6 summarizes the NOCBs, the violations of COHERENCE, SALIENCE, and CHEAPNESS, and the centering transitions for this ordering. 10', 'he candidate ordering contains two NOCBs in sentences (3e) and (3f).', 'Its score according to M.NOCB, the metric used by Karamanis and Manurung (2002) and Althaus, Karamanis, and Koller (2004), is 2. Another ordering with fewer NOCBs (should such an ordering exist) will be preferred over this candidate as the selected output of information ordering if M.NOCB is used to guide this process.', 'M.NOCB relies only on CONTINUITY.', 'Because satisfying this principle is a prerequisite for the computation of every other centering feature, M.NOCB is the simplest possible centering-based metric and will be used as the baseline in our experiments.']",2,"['Following our previous work ( #AUTHOR_TAG ; Althaus , Karamanis , and Koller 2004 ) , the input to information ordering is an unordered set of informationbearing items represented as CF lists .', 'A set of candidate orderings is produced by creating different permutations of these lists.', 'A metric of coherence uses features from centering to compute a score for each candidate ordering and select the highest scoring ordering as the output. 9', ""A wide range of metrics of coherence can be defined in centering's terms, simply on the basis of the work we reviewed in Section 3. To exemplify this, let us first assume that the ordering in Example (3), which is analyzed as a sequence of CF lists in Table 5, is a candidate ordering."", 'Table 6 summarizes the NOCBs, the violations of COHERENCE, SALIENCE, and CHEAPNESS, and the centering transitions for this ordering. 10', 'M.NOCB relies only on CONTINUITY.']"
CC591,J09-1005,Unsupervised Type and Token Identification of Idiomatic Expressions,automatic identification of noncompositional phrases,['Dekang Lin'],,,"Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #AUTHOR_TAG .","['Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #AUTHOR_TAG .', 'We use the automatically built thesaurus of Lin (1998) to find words similar to each constituent, in order to automatically generate variants.', 'Variants are generated by replacing either the noun or the verb constituent of a pair with a semantically (and syntactically) similar word. 3', 'xamples of automatically generated variants for the pair spill, bean are pour, bean , stream, bean , spill, corn , and spill, rice .']",4,"['Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #AUTHOR_TAG .', 'We use the automatically built thesaurus of Lin (1998) to find words similar to each constituent, in order to automatically generate variants.', 'Variants are generated by replacing either the noun or the verb constituent of a pair with a semantically (and syntactically) similar word. 3']"
CC592,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,decision graphs—an extension of decision trees,['J J Oliver'],,"In this paper, we examine Decision Graphs, a generalization of decision trees. We present an inference scheme to construct decision graphs using the Minimum Message Length Principle. Empirical tests demonstrate that this scheme compares favourably with other decision tree inference schemes. This work provides a metric for comparing the relative merit of the decision tree and decision graph formalisms for a particular domain.","We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs ( #AUTHOR_TAG ) .","['In principle, we could have used a classification method to predict clusters from the values of the confidence measures for unseen cases.', 'We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs ( #AUTHOR_TAG ) .']",0,"['We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs ( #AUTHOR_TAG ) .']"
CC593,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,knowledge harvesting articulation and delivery the hewlettpackard journal,"['K A Delic', 'D Lahaix']",,,"Such technologies require significant human input , and are difficult to create and maintain ( #AUTHOR_TAG ) .","['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997).', 'Such technologies require significant human input , and are difficult to create and maintain ( #AUTHOR_TAG ) .', 'In contrast, the techniques examined in this article are corpus-based and data-driven.', 'The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus.']",0,"['Such technologies require significant human input , and are difficult to create and maintain ( #AUTHOR_TAG ) .']"
CC594,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,bridging the lexical chasm statistical approaches to answerfinding,"['A Berger', 'R Caruana', 'D Cohn', 'D Freitag', 'V Mittal']",method,,"â¢ Only an automatic evaluation was performed , which relied on having model responses ( Berger and Mittal 2000 ; #AUTHOR_TAG ) .","['â\x80¢ Only an automatic evaluation was performed , which relied on having model responses ( Berger and Mittal 2000 ; #AUTHOR_TAG ) .']",1,"['â\x80¢ Only an automatic evaluation was performed , which relied on having model responses ( Berger and Mittal 2000 ; #AUTHOR_TAG ) .']"
CC595,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,an introduction to modern information retrieval,"['G Salton', 'M J McGill']",method,"Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd introduction to modern information retrieval as the choice of reading, you can find here.","This method follows a traditional Information Retrieval paradigm ( #AUTHOR_TAG ) , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query .","['As stated herein, we studied two document-based methods: Document Retrieval and Document Prediction.', '(Doc-Ret).', 'This method follows a traditional Information Retrieval paradigm ( #AUTHOR_TAG ) , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query .', 'In our case, the query is a new request e-mail to be addressed by the system, and we have considered three views of the documents in the corpus:']",5,"['This method follows a traditional Information Retrieval paradigm ( #AUTHOR_TAG ) , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query .']"
CC596,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,an introduction to modern information retrieval,"['G Salton', 'M J McGill']",method,"Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd introduction to modern information retrieval as the choice of reading, you can find here.","For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #AUTHOR_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) .","['We carried out a preliminary experiment in order to compare the three variants of the Doc-Ret method.', 'The evaluation is performed by considering each request e-mail in turn, removing it and its response from the corpus, carrying out the retrieval process, and then comparing the retrieved response with the actual response (if there are several similar responses in the corpus, an appropriate response can still be retrieved).', 'The results of this experiment are shown in Table 1.', 'The first column shows which document retrieval variant is being evaluated.', 'The second column shows the proportion of requests for which one or more documents were retrieved (using our applicability threshold).', 'We see that matching on requests yields more retrieved documents than matching on responses, and that matching on request-response pairs yields even more retrieved documents.', 'For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #AUTHOR_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) .', 'The third column in Table 1 shows the proportion of requests for which this similarity is non-zero.', 'Again, the third variant (matching on request-response pairs) retrieves the highest proportion of responses that bear some similarity to the real responses.', 'The fourth column shows the average similarity between the top retrieved response and the real response for the cases where retrieval took place.', 'Here too the third variant yields the best similarity score (0.52).']",5,"['The results of this experiment are shown in Table 1.', 'The first column shows which document retrieval variant is being evaluated.', 'For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #AUTHOR_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) .', 'Here too the third variant yields the best similarity score (0.52).']"
CC597,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,an introduction to modern information retrieval,"['G Salton', 'M J McGill']",,"Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd introduction to modern information retrieval as the choice of reading, you can find here.","This situation suggests a response-automation approach that follows the document retrieval paradigm ( #AUTHOR_TAG ) , where a new request is matched with existing response documents ( e-mails ) .","['The example in Figure 1(b) illustrates a situation where specific words in the request (docking station and install) are also mentioned in the response.', 'This situation suggests a response-automation approach that follows the document retrieval paradigm ( #AUTHOR_TAG ) , where a new request is matched with existing response documents ( e-mails ) .', 'However, specific words in the request do not always match a response well, and sometimes do not match a response at all, as demonstrated by the examples in Figures 1(a) and 1(c), respectively.']",0,"['This situation suggests a response-automation approach that follows the document retrieval paradigm ( #AUTHOR_TAG ) , where a new request is matched with existing response documents ( e-mails ) .', 'However, specific words in the request do not always match a response well, and sometimes do not match a response at all, as demonstrated by the examples in Figures 1(a) and 1(c), respectively.']"
CC598,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,expert systems a technology before its time ai expert available at wwwstanfordedugroup scipavsgtexpertsystemsaiexperthtml,"['A Barr', 'S Tessler']",introduction,,It is therefore no surprise that early attempts at response automation were knowledge-driven ( #AUTHOR_TAG ; Watson 1997 ; Delic and Lahaix 1998 ) .,"['However, even the automation of responses to the ""easy"" problems is a difficult task.', 'Although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( #AUTHOR_TAG ; Watson 1997 ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']",0,"['However, even the automation of responses to the ""easy"" problems is a difficult task.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( #AUTHOR_TAG ; Watson 1997 ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']"
CC599,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,statistical and inductive inference by minimum message length,['C S Wallace'],,,We then use the program Snob ( Wallace and Boulton 1968 ; #AUTHOR_TAG ) to cluster these experiences .,"['We train the system by clustering the ""experiences"" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively).', 'We then use the program Snob ( Wallace and Boulton 1968 ; #AUTHOR_TAG ) to cluster these experiences .', 'Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.', 'shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15', 'These clusters were chosen because they illustrate clearly three situations of interest.']",5,"['We train the system by clustering the ""experiences"" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively).', 'We then use the program Snob ( Wallace and Boulton 1968 ; #AUTHOR_TAG ) to cluster these experiences .', 'Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.', 'shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15', 'These clusters were chosen because they illustrate clearly three situations of interest.']"
CC600,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,expert systems a technology before its time ai expert available at wwwstanfordedugroup scipavsgtexpertsystemsaiexperthtml,"['A Barr', 'S Tessler']",,,"The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #AUTHOR_TAG ) and case-based reasoning ( Watson 1997 ) .","['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #AUTHOR_TAG ) and case-based reasoning ( Watson 1997 ) .', 'Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998).', 'In contrast, the techniques examined in this article are corpus-based and data-driven.', 'The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus.']",1,"['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #AUTHOR_TAG ) and case-based reasoning ( Watson 1997 ) .', 'Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998).', 'In contrast, the techniques examined in this article are corpus-based and data-driven.']"
CC601,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,libsvm a library for support vector machines software available at httpwwwcsie ntuedutw∼cjlinlibsvm,"['C C Chang', 'C J Lin']",method,,7 We employed the LIBSVM package ( #AUTHOR_TAG ) .,"[""We use a Support Vector Machine (SVM) with a Radial Basis Function kernel to predict SCs from users' requests. 7"", 'A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request.', 'During the For Sent-Pred we also experimented with grammatical and sentence-based syntactic features, such as number of syntactic phrases, grammatical mood, and grammatical person (Marom and Zukerman 2006), but the simple binary bag-of-lemmas representation yielded similar results.', '7 We employed the LIBSVM package ( #AUTHOR_TAG ) .', 'prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3.', 'We then apply the following steps.']",5,['7 We employed the LIBSVM package ( #AUTHOR_TAG ) .']
CC602,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,statistical and inductive inference by minimum message length,['C S Wallace'],method,,"In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ; #AUTHOR_TAG ) .","[""The idea behind the Doc-Pred method is similar to Bickel and Scheffer's (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request's features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected."", 'In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ; #AUTHOR_TAG ) .', 'We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2).', 'The input to Snob is a set of binary vectors, one vector per response document.', 'The values of a vector correspond to the presence or absence of each (lemmatized) corpus word in the document in question (after removing stop-words and words with very low frequency).', 'The predictive model is a Decision Graph (Oliver 1993), which, like Snob, is based on the MML principle.', 'The Decision Graph is trained on unigram and bigram lemmas in the request as input features, 5 and the identifier of the response cluster that contains the actual response for the request as the target feature.', 'The model predicts which response cluster is most suitable for a given request, and returns the probability that this prediction is correct.', 'This probability is our indicator of whether the Doc-Pred method can address a new request.', 'As for the Doc-Ret method, an applicability threshold for this parameter is currently determined empirically (Table 3).']",5,"['In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ; #AUTHOR_TAG ) .']"
CC603,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,an introduction to modern information retrieval,"['G Salton', 'M J McGill']",method,"Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd introduction to modern information retrieval as the choice of reading, you can find here.",We use two measures from Information Retrieval to determine the quality of an automatically generated response : precision and F-score ( van Rijsbergen 1979 ; #AUTHOR_TAG ) .,"['We use two measures from Information Retrieval to determine the quality of an automatically generated response : precision and F-score ( van Rijsbergen 1979 ; #AUTHOR_TAG ) .', 'Precision measures how much of the information in an automatically generated response is correct (i.e., appears in the model response), and F-score measures the overall similarity between the automatically generated response and the model response.', 'F-score is the harmonic mean of precision and recall, which measures how much of the information in the model response appears in the generated response.', 'We consider precision separately because it does not penalize missing information, enabling us to better assess our sentence-based methods.', 'Precision, recall, and F-score are calculated as follows using a word-by-word comparison (stop-words are excluded). 13', 'ecision =']",5,"['We use two measures from Information Retrieval to determine the quality of an automatically generated response : precision and F-score ( van Rijsbergen 1979 ; #AUTHOR_TAG ) .', 'F-score is the harmonic mean of precision and recall, which measures how much of the information in the model response appears in the generated response.', 'Precision, recall, and F-score are calculated as follows using a word-by-word comparison (stop-words are excluded). 13']"
CC604,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,applying casebased reasoning techniques for enterprise systems,['I Watson'],,"Chapter 1: What Is Case-Based Reasoning? Chapter 2: Understanding CBR Chapter 3: The Application of CBR Chapter 4: Industrial Applications of CBR Chapter 5: CBR and Customer Service Chapter 6: CBR Software Tools Chapter 7: Building a Diagnostic Case-Base Chapter 8: Building, Testing, and Maintaining Case-Bases Chapter 9: Conclusion Chapter 10: Bibliography","The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( Barr and Tessler 1995 ) and case-based reasoning ( #AUTHOR_TAG ) .","['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( Barr and Tessler 1995 ) and case-based reasoning ( #AUTHOR_TAG ) .', 'Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998).', 'In contrast, the techniques examined in this article are corpus-based and data-driven.', 'The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus.']",1,"['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( Barr and Tessler 1995 ) and case-based reasoning ( #AUTHOR_TAG ) .', 'In contrast, the techniques examined in this article are corpus-based and data-driven.']"
CC605,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,mercure towards an automatic email followup system,"['G Lapalme', 'L Kosseim']",,,"#AUTHOR_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering .","['There are very few reported attempts at corpus-based automation of help-desk responses (Carmel, Shtalhaim, and Soffer 2000;Lapalme and Kosseim 2003;Bickel and Scheffer 2004;Malik, Subramaniam, and Kaushik 2007).', 'eResponder, the system developed by Carmel, Shtalhaim, and Soffer (2000), retrieves a list of request-response pairs and presents a ranked list of responses to the user.', 'If the user is unsatisfied with this list, an operator is asked to generate a new response.', 'The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses.', 'However, there is no attempt to automatically generate a single response.', 'Bickel and Scheffer (2004) compared the performance of document retrieval and document prediction for generating help-desk responses.', 'Their retrieval technique, which is similar to our request-to-request Doc-Ret method, matches user questions to the questions in a database of question-answer pairs.', 'Their prediction method, which is similar to Doc-Pred, is based on clustering the responses in the corpus into semantically equivalent answers, and then training a classifier to match a query with one of these classes.', 'The generated response is the answer that is closest to the centroid of the cluster.', ""Bickel and Scheffer's results are consistent with ours, in the sense that the performance of the Doc-Ret method is significantly worse than that of Doc-Pred."", 'However, it is worth noting that their corpus is significantly smaller than ours (805 question-answer pairs), their questions seem to be much simpler and shorter than those in our corpus, and the replies shorter and more homogeneous.', 'Malik, Subramaniam, and Kaushik (2007) developed a system that builds questionanswer pairs from help-center e-mails, and then maps new questions to existing questions in order to retrieve an answer.', 'This part of their approach resembles our Doc-Ret method, but instead of retrieving entire response documents, they retrieve individual sentences.', 'In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates.', '#AUTHOR_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering .', 'Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed.', 'The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches.', 'However, the personalization component of the case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individuals and companies in texts).']",1,"['#AUTHOR_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering .']"
CC606,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,the design implementation and use of computational linguistics volume 35 number 4 the ngram statistics package in cicling,"['S Banerjee', 'T Pedersen']",method,,"5 Significant bigrams are obtained using the n-gram statistics package NSP ( #AUTHOR_TAG ) , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation ) .","['5 Significant bigrams are obtained using the n-gram statistics package NSP ( #AUTHOR_TAG ) , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation ) .']",5,"['5 Significant bigrams are obtained using the n-gram statistics package NSP ( #AUTHOR_TAG ) , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation ) .']"
CC607,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,mercure towards an automatic email followup system,"['G Lapalme', 'L Kosseim']",introduction,,"Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .","['In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches.', 'An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries.', 'This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary.', 'Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', 'A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues.', 'Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses.', 'Our study is based on a large corpus of requestresponse e-mail dialogues between customers and operators at Hewlett-Packard.', 'Observations from this corpus have led us to consider several methods that implement different types of corpus-based strategies.', 'Specifically, we have investigated two types of methods (retrieval and prediction) applied at two levels of granularity (document and sentence).', 'In this article, we present these methods and compare their performance.']",0,"['An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries.', 'Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', 'A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues.', 'Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses.', 'Our study is based on a large corpus of requestresponse e-mail dialogues between customers and operators at Hewlett-Packard.', 'Observations from this corpus have led us to consider several methods that implement different types of corpus-based strategies.']"
CC608,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,automatic question answering using the web beyond the factoid,"['R Soricut', 'E Brill']",,"In this paper we describe and evaluate a Question Answering (QA) system that goes beyond answering factoid questions. Our approach to QA assumes no restrictions on the type of questions that are handled, and no assumption that the answers to be provided are factoids. We present an unsupervised approach for collecting question and answer pairs from FAQ pages, which we use to collect a corpus of 1 million question/answer pairs from FAQ pages available on the Web. This corpus is used to train various statistical models employed by our QA system: a statistical chunker used to transform a natural language-posed question into a phrase-based query to be submitted for exact match to an off-the-shelf search engine; an answer/question translation model, used to assess the likelihood that a proposed answer is indeed an answer to the posed question; and an answer language model, used to assess the likelihood that a proposed answer is a well-formed answer. We evaluate our QA system in a modular fashion, by comparing the performance of baseline algorithms against our proposed algorithms for various modules in our QA system. The evaluation shows that our system achieves reasonable performance in terms of answer accuracy for a large variety of complex, non-factoid questions.","Two applications that, like help-desk, deal with question�answer pairs are: sum- marization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; Berger et al. 2000; Jijkoun and de Rijke 2005;  #AUTHOR_TAG ) .","['Two applications that, like help-desk, deal with question�answer pairs are: sum- marization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; Berger et al. 2000; Jijkoun and de Rijke 2005;  #AUTHOR_TAG ) .', 'An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries.', 'In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information.', 'Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses.']",1,"['Two applications that, like help-desk, deal with question�answer pairs are: sum- marization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; Berger et al. 2000; Jijkoun and de Rijke 2005;  #AUTHOR_TAG ) .']"
CC609,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,mercure towards an automatic email followup system,"['G Lapalme', 'L Kosseim']",method,,â¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( #AUTHOR_TAG ; Roy and Subramaniam 2006 ) .,['â\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( #AUTHOR_TAG ; Roy and Subramaniam 2006 ) .'],1,['â\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( #AUTHOR_TAG ; Roy and Subramaniam 2006 ) .']
CC610,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,assessing agreement on classification tasks the kappa statistic,['J Carletta'],method,"Currently, computational linguists and cognitive scientists working in the area of discourse and dialogue argue that their subjective judgments are reliable using several different statistics, none of which are easily interpretable or comparable to each other. Meanwhile, researchers in content analysis have already experienced the same difficulties and come up with a solution in the kappa statistic. We discuss what is wrong with reliability measures as they are currently used for discourse and dialogue work in computational linguistics and cognitive science, and argue that we would be better off as a field adopting techniques from content analysis.","Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures ( #AUTHOR_TAG ) .","['Each evaluation set contained 80 cases, randomly selected from the corpus in proportion to the size of each data set, where a case contained a request e-mail, the model response, and the responses generated by the two methods being compared.', 'Each judge was given of these cases, and was asked to assess the generated responses on the four criteria listed previously. 14', 'e maximized the coverage of this study by allocating different cases to each judge, thus avoiding a situation where a particularly good or bad set of cases is evaluated by all judges.', ""In addition, we tried to ensure that the sets of cases shown to the judges were of similar quality, so that the judges' assessments would be comparable."", 'Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures ( #AUTHOR_TAG ) .', 'However, it is still necessary to We asked the judges to leave a question unanswered if they felt they did not have the technical knowledge to make a judgment, but this did not occur.']",5,"['Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures ( #AUTHOR_TAG ) .']"
CC611,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,multidocument summarization by sentence extraction,"['J Goldstein', 'V Mittal', 'J Carbonell', 'M Kantrowitz']",method,,"In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( #AUTHOR_TAG ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and McKeown 2005 ) .","['As discussed in Section 2, there are situations that cannot be addressed by a document-level approach, because requests only predict or match portions of responses.', 'An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response.', 'This task can be cast as extractive multi-document summarization.', 'Unlike a document reuse approach, sentence-level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable.', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( #AUTHOR_TAG ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and McKeown 2005 ) .']",0,"['This task can be cast as extractive multi-document summarization.', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( #AUTHOR_TAG ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and McKeown 2005 ) .']"
CC612,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,hybrid recommender systems user modeling and useradapted interaction,['R Burke'],,,A common way to combine different models consists of selecting the model that is most confident regarding its decision ( #AUTHOR_TAG ) .,"['A common way to combine different models consists of selecting the model that is most confident regarding its decision ( #AUTHOR_TAG ) .', 'However, in our case, the individual confidence (applicability) measures employed by our response-generation methods are not comparable (e.g., the retrieval score in Doc-Ret is different in nature from the prediction probability in Doc-Pred).', 'Hence, prior to selecting the most confident method, we need to find a way to compare the different measures of confidence.', 'Because the performances of the different methods are comparable, we do this by establishing a link between confidence and performance.', 'In other words, our meta-level process learns to predict the performance of the different methods from their confidence levels on the basis of previous experience.', 'These predictions enable our system to recommend a particular method for handling a new (unseen) request (Marom, Zukerman, and Japkowicz 2007).']",1,"['A common way to combine different models consists of selecting the model that is most confident regarding its decision ( #AUTHOR_TAG ) .', 'However, in our case, the individual confidence (applicability) measures employed by our response-generation methods are not comparable (e.g., the retrieval score in Doc-Ret is different in nature from the prediction probability in Doc-Pred).', 'Because the performances of the different methods are comparable, we do this by establishing a link between confidence and performance.', 'In other words, our meta-level process learns to predict the performance of the different methods from their confidence levels on the basis of previous experience.', 'These predictions enable our system to recommend a particular method for handling a new (unseen) request (Marom, Zukerman, and Japkowicz 2007).']"
CC613,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,queryrelevant summarization using faqs,"['A Berger', 'V Mittal']",,"This paper introduces a statistical model for query-relevant summarization: succinctly characterizing the relevance of a document to a query. Learning parameter values for the proposed model requires a large collection of summarized documents, which we do not have, but as a proxy, we use a collection of FAQ (frequently-asked question) documents. Taking a learning approach enables a principled, quantitative evaluation of the proposed system, and the results of some initial experiments---on a collection of Usenet FAQs and on a FAQ-like set of customer-submitted questions to several large retail companies---suggest the plausibility of learning for summarization.","Two applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004;Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) ( #AUTHOR_TAG; Soricut and Brill 2006).","['Two applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004;Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) ( #AUTHOR_TAG; Soricut and Brill 2006).', 'An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries.', 'In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information.', 'Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses.']",1,"['Two applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004;Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) ( #AUTHOR_TAG; Soricut and Brill 2006).']"
CC614,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,building effective question answering characters,"['A Leuski', 'R Patel', 'D Traum', 'B Kennedy']",method,"In this paper, we describe methods for building and evaluation of limited domain question-answering characters. Several classification techniques are tested, including text classification using support vector machines, language-model based retrieval, and cross-language information retrieval techniques, with the latter having the highest success rate. We also evaluated the effect of speech recognition errors on performance with users, finding that retrieval is robust until recognition reaches over 50 % WER.","â¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( Feng et al. 2006 ; #AUTHOR_TAG ) .","['â\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( Feng et al. 2006 ; #AUTHOR_TAG ) .', 'The representativeness of the sample size was not discussed in any of these studies.']",1,"['â\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( Feng et al. 2006 ; #AUTHOR_TAG ) .']"
CC615,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,queryrelevant summarization using faqs,"['A Berger', 'V Mittal']",method,"This paper introduces a statistical model for query-relevant summarization: succinctly characterizing the relevance of a document to a query. Learning parameter values for the proposed model requires a large collection of summarized documents, which we do not have, but as a proxy, we use a collection of FAQ (frequently-asked question) documents. Taking a learning approach enables a principled, quantitative evaluation of the proposed system, and the results of some initial experiments---on a collection of Usenet FAQs and on a FAQ-like set of customer-submitted questions to several large retail companies---suggest the plausibility of learning for summarization.","â¢ Only an automatic evaluation was performed , which relied on having model responses ( #AUTHOR_TAG ; Berger et al. 2000 ) .","['In Marom and Zukerman (2007a) we identified several systems that resemble ours in that they provide answers to queries.', 'These systems addressed the evaluation issue as follows.', 'â\x80¢ Only an automatic evaluation was performed , which relied on having model responses ( #AUTHOR_TAG ; Berger et al. 2000 ) .']",1,"['In Marom and Zukerman (2007a) we identified several systems that resemble ours in that they provide answers to queries.', 'These systems addressed the evaluation issue as follows.', 'â\x80¢ Only an automatic evaluation was performed , which relied on having model responses ( #AUTHOR_TAG ; Berger et al. 2000 ) .']"
CC616,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,evaluation of a largescale email response system,"['Y Marom', 'I Zukerman']",method,,In #AUTHOR_TAGa ) we identified several systems that resemble ours in that they provide answers to queries .,"['In #AUTHOR_TAGa ) we identified several systems that resemble ours in that they provide answers to queries .', 'These systems addressed the evaluation issue as follows.', 'r Only an automatic evaluation was performed, which relied on having model responses .']",0,['In #AUTHOR_TAGa ) we identified several systems that resemble ours in that they provide answers to queries .']
CC617,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,an information measure for classification,"['C S Wallace', 'D M Boulton']",method,"1. The class to which each thing belongs. 2. The average properties of each class. 3. The deviations of each thing from the average properties of its parent class. If the things are found to be concentrated in a small area of the region of each class in the measurement space then the deviations will be small, and with reference to the average class properties most of the information about a thing is given by naming the class to which it belongs. In this case the information may be recorded much more briefly than if a classification had not been used. We suggest that the best classification is that which results in the briefest recording of all the attribute information. In this context, we will regard the measurements of each thing as being a message about that thing. Shannon (1948) showed that where messages may be regarded as each nominating the occurrence of a particular event among a universe of possible events, the information needed to record a series of such messages is minimised if the messages are encoded so that the length of each message is proportional to minus the logarithm of the relative frequency of occurrence of the event which it nominates. The information required is greatest when all frequencies are equal. The messages here nominate the positions in measurement space of the 5 1 points representing the attributes of the things. If the expected density of points in the measurement space is everywhere uniform, the positions of the points cannot be encoded more briefly than by a simple list of the measured values. However, if the expected density is markedly non-uniform, application","In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #AUTHOR_TAG ; Wallace 2005 ) .","[""The idea behind the Doc-Pred method is similar to Bickel and Scheffer's (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request's features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected."", 'In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #AUTHOR_TAG ; Wallace 2005 ) .', 'We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2).', 'The input to Snob is a set of binary vectors, one vector per response document.', 'The values of a vector correspond to the presence or absence of each (lemmatized) corpus word in the document in question (after removing stop-words and words with very low frequency).', 'The predictive model is a Decision Graph (Oliver 1993), which, like Snob, is based on the MML principle.', 'The Decision Graph is trained on unigram and bigram lemmas in the request as input features, 5 and the identifier of the response cluster that contains the actual response for the request as the target feature.', 'The model predicts which response cluster is most suitable for a given request, and returns the probability that this prediction is correct.', 'This probability is our indicator of whether the Doc-Pred method can address a new request.', 'As for the Doc-Ret method, an applicability threshold for this parameter is currently determined empirically (Table 3).']",5,"['In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #AUTHOR_TAG ; Wallace 2005 ) .', 'The predictive model is a Decision Graph (Oliver 1993), which, like Snob, is based on the MML principle.']"
CC618,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,queryrelevant summarization using faqs,"['A Berger', 'V Mittal']",,"This paper introduces a statistical model for query-relevant summarization: succinctly characterizing the relevance of a document to a query. Learning parameter values for the proposed model requires a large collection of summarized documents, which we do not have, but as a proxy, we use a collection of FAQ (frequently-asked question) documents. Taking a learning approach enables a principled, quantitative evaluation of the proposed system, and the results of some initial experiments---on a collection of Usenet FAQs and on a FAQ-like set of customer-submitted questions to several large retail companies---suggest the plausibility of learning for summarization.","In FAQs , #AUTHOR_TAG employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document .","['In FAQs , #AUTHOR_TAG employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document .', 'They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.', 'compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models).', 'Jijkoun and de Rijke ( 2005) compared different variants of retrieval techniques.', 'Soricut and Brill (2006) compared a predictive approach (statistical translation), a retrieval approach based on a language-model, and a hybrid approach which combines statistical chunking and traditional retrieval.', 'Two significant differences between help-desk and FAQs are the following.']",0,"['In FAQs , #AUTHOR_TAG employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document .', 'compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models).', 'Jijkoun and de Rijke ( 2005) compared different variants of retrieval techniques.']"
CC619,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,an intelligent discussionbot for answering student queries in threaded discussions,"['D Feng', 'E Shaw', 'J Kim', 'E Hovy']",method,"This paper describes a discussion-bot that provides answers to students' discussion board questions in an unobtrusive and human-like way. Using information retrieval and natural language processing techniques, the discussion-bot identifies the questioner's interest, mines suitable answers from an annotated corpus of 1236 archived threaded discussions and 279 course documents and chooses an appropriate response. A novel modeling approach was designed for the analysis of archived threaded discussions to facilitate answer extraction. We compare a self-out and an all-in evaluation of the mined answers. The results show that the discussion-bot can begin to meet students' learning requests. We discuss directions that might be taken to increase the effectiveness of the question matching and answer extraction algorithms. The research takes place in the context of an undergraduate computer science course.","â¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( #AUTHOR_TAG ; Leuski et al. 2006 ) .","['â\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( #AUTHOR_TAG ; Leuski et al. 2006 ) .', 'The representativeness of the sample size was not discussed in any of these studies.']",1,"['â\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( #AUTHOR_TAG ; Leuski et al. 2006 ) .']"
CC620,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,learning from message pairs for automatic email answering,"['S Bickel', 'T Scheffer']",introduction,"We consider the problem of learning a mapping from question to answer messages. The training data for this problem consist of pairs of messages that have been received and sent in the past. We formulate the problem setting, discuss appropriate performance metrics, develop a solution and describe two baseline methods for comparison. We present a case study based on emails received and answered by the service center of a large online store.","Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .","['In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches.', 'An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries.', 'This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary.', 'Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', 'A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues.', 'Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses.', 'Our study is based on a large corpus of requestresponse e-mail dialogues between customers and operators at Hewlett-Packard.', 'Observations from this corpus have led us to consider several methods that implement different types of corpus-based strategies.', 'Specifically, we have investigated two types of methods (retrieval and prediction) applied at two levels of granularity (document and sentence).', 'In this article, we present these methods and compare their performance.']",0,"['An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries.', 'Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', 'Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses.', 'Our study is based on a large corpus of requestresponse e-mail dialogues between customers and operators at Hewlett-Packard.', 'Observations from this corpus have led us to consider several methods that implement different types of corpus-based strategies.', 'Specifically, we have investigated two types of methods (retrieval and prediction) applied at two levels of granularity (document and sentence).']"
CC621,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,statistical learning theory,['V N Vapnik'],method,"The traditional approach of statistical physics to supervised learning routinely assumes unrealistic generative models for the data: usually inputs are independent random variables, uncorrelated with their labels. Only recently, statistical physicists started to explore more complex forms of data, such as equally-labelled points lying on (possibly low dimensional) object manifolds. Here we provide a bridge between this recently-established research area and the framework of statistical learning theory, a branch of mathematics devoted to inference in machine learning. The overarching motivation is the inadequacy of the classic rigorous results in explaining the remarkable generalization properties of deep learning. We propose a way to integrate physical models of data into statistical learning theory, and address, with both combinatorial and statistical mechanics methods, the computation of the Vapnik-Chervonenkis entropy, which counts the number of different binary classifications compatible with the loss class. As a proof of concept, we focus on kernel machines and on two simple realizations of data structure introduced in recent physics literature: $k$-dimensional simplexes with prescribed geometric relations and spherical manifolds (equivalent to margin classification). Entropy, contrary to what happens for unstructured data, is nonmonotonic in the sample size, in contrast with the rigorous bounds. Moreover, data structure induces a novel transition beyond the storage capacity, which we advocate as a proxy of the nonmonotonicity, and ultimately a cue of low generalization error. The identification of a synaptic volume vanishing at the transition allows a quantification of the impact of data structure within replica theory, applicable in cases where combinatorial methods are not available, as we demonstrate for margin learning.Comment: 19 pages, 3 figure","Specifically , we used Decision Graphs ( Oliver 1993 ) for Doc-Pred , and SVMs ( #AUTHOR_TAG ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .","['The focus of our work is on the general applicability of the different response automa- tion methods, rather than on comparing the performance of particular implementa- tion techniques.', 'Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research.', 'Specifically , we used Decision Graphs ( Oliver 1993 ) for Doc-Pred , and SVMs ( #AUTHOR_TAG ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .', 'Because this variation was uniformly implemented for both approaches, it does not affect their relative performance.', 'These methodological variations are summarized in Table 2.']",5,"['Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research.', 'Specifically , we used Decision Graphs ( Oliver 1993 ) for Doc-Pred , and SVMs ( #AUTHOR_TAG ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .', 'Because this variation was uniformly implemented for both approaches, it does not affect their relative performance.', 'These methodological variations are summarized in Table 2.']"
CC622,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,eventbased extractive summarization,"['E Filatova', 'V Hatzivassiloglou']",method,,"After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #AUTHOR_TAG to penalize redundant sentences in cohesive clusters .","['Removing redundant sentences.', 'After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #AUTHOR_TAG to penalize redundant sentences in cohesive clusters .', 'This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we retain one sentence as a reference sentence-i.e., its score is not decremented).', 'Specifically, given a sentence s k in cluster SC l which contains a sentence with a higher or equal score, the contribution of SC l to Score(s k ) (= Pr(SC l ) × Pr(s k |SC l )) is subtracted from Score(s k ).', 'After applying these penalties, we retain only the sentences whose adjusted score is greater than zero (for a highly cohesive cluster, typically only one sentence remains).']",5,"['Removing redundant sentences.', 'After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #AUTHOR_TAG to penalize redundant sentences in cohesive clusters .']"
CC623,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,decision graphs—an extension of decision trees,['J J Oliver'],method,"In this paper, we examine Decision Graphs, a generalization of decision trees. We present an inference scheme to construct decision graphs using the Minimum Message Length Principle. Empirical tests demonstrate that this scheme compares favourably with other decision tree inference schemes. This work provides a metric for comparing the relative merit of the decision tree and decision graph formalisms for a particular domain.","The predictive model is a Decision Graph ( #AUTHOR_TAG ) , which , like Snob , is based on the MML principle .","['The idea behind the Doc-Pred method is similar to Bickel and Scheffer�s (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request�s features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected.', 'In our case, the clustering is performed by the program Snob, which implements mixture model- ing combined with model selection based on the Minimum Message Length (MML) criterion (Wallace and Boulton 1968; Wallace 2005).', 'We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2).', 'The input to Snob is a set of binary vectors, one vector per response document.', 'The values of a vector correspond to the presence or absence of each (lem- matized) corpus word in the document in question (after removing stop-words and words with very low frequency).4', 'The predictive model is a Decision Graph ( #AUTHOR_TAG ) , which , like Snob , is based on the MML principle .', 'The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature.', 'The model predicts which response cluster is most suitable for a given re- quest, and returns the probability that this prediction is correct.', 'This probability is our indicator of whether the Doc-Pred method can address a new request.', 'As for the Doc-Ret method, an applicability threshold for this parameter is currently determined empirically (Table 3).']",5,"['The predictive model is a Decision Graph ( #AUTHOR_TAG ) , which , like Snob , is based on the MML principle .']"
CC624,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,knowledge harvesting articulation and delivery the hewlettpackard journal,"['K A Delic', 'D Lahaix']",introduction,,It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; Watson 1997 ; #AUTHOR_TAG ) .,"['However, even the automation of responses to the ""easy"" problems is a difficult task.', 'Although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; Watson 1997 ; #AUTHOR_TAG ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']",0,['It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; Watson 1997 ; #AUTHOR_TAG ) .']
CC625,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,mercure towards an automatic email followup system,"['G Lapalme', 'L Kosseim']",,,"There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; #AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .","['There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; #AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', 'eResponder, the system developed by Carmel, Shtalhaim, and Soffer (2000), retrieves a list of request-response pairs and presents a ranked list of responses to the user.', 'If the user is unsatisfied with this list, an operator is asked to generate a new response.', 'The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses.', 'However, there is no attempt to automatically generate a single response.', 'Bickel and Scheffer (2004) compared the performance of document retrieval and document prediction for generating help-desk responses.', 'Their retrieval technique, which is similar to our request-to-request Doc-Ret method, matches user questions to the questions in a database of question-answer pairs.', 'Their prediction method, which is similar to Doc-Pred, is based on clustering the responses in the corpus into semantically equivalent answers, and then training a classifier to match a query with one of these classes.', 'The generated response is the answer that is closest to the centroid of the cluster.', ""Bickel and Scheffer's results are consistent with ours, in the sense that the performance of the Doc-Ret method is significantly worse than that of Doc-Pred."", 'However, it is worth noting that their corpus is significantly smaller than ours (805 question-answer pairs), their questions seem to be much simpler and shorter than those in our corpus, and the replies shorter and more homogeneous.', 'Malik, Subramaniam, and Kaushik (2007) developed a system that builds questionanswer pairs from help-center e-mails, and then maps new questions to existing questions in order to retrieve an answer.', 'This part of their approach resembles our Doc-Ret method, but instead of retrieving entire response documents, they retrieve individual sentences.', 'In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates.', 'Lapalme and Kosseim (2003) investigated three approaches to the automatic generation of response e-mails: text classification, case-based reasoning, and question answering.', 'Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed.', 'The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches.', 'However, the personalization component of the case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individuals and companies in texts).']",1,"['There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; #AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', 'Lapalme and Kosseim (2003) investigated three approaches to the automatic generation of response e-mails: text classification, case-based reasoning, and question answering.', 'Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed.']"
CC626,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,learning from message pairs for automatic email answering,"['S Bickel', 'T Scheffer']",,"We consider the problem of learning a mapping from question to answer messages. The training data for this problem consist of pairs of messages that have been received and sent in the past. We formulate the problem setting, discuss appropriate performance metrics, develop a solution and describe two baseline methods for comparison. We present a case study based on emails received and answered by the service center of a large online store.","There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .","['There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', 'eResponder, the system developed by Carmel, Shtalhaim, and Soffer (2000), retrieves a list of request-response pairs and presents a ranked list of responses to the user.', 'If the user is unsatisfied with this list, an operator is asked to generate a new response.', 'The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses.', 'However, there is no attempt to automatically generate a single response.', 'Bickel and Scheffer (2004) compared the performance of document retrieval and document prediction for generating help-desk responses.', 'Their retrieval technique, which is similar to our request-to-request Doc-Ret method, matches user questions to the questions in a database of question-answer pairs.', 'Their prediction method, which is similar to Doc-Pred, is based on clustering the responses in the corpus into semantically equivalent answers, and then training a classifier to match a query with one of these classes.', 'The generated response is the answer that is closest to the centroid of the cluster.', ""Bickel and Scheffer's results are consistent with ours, in the sense that the performance of the Doc-Ret method is significantly worse than that of Doc-Pred."", 'However, it is worth noting that their corpus is significantly smaller than ours (805 question-answer pairs), their questions seem to be much simpler and shorter than those in our corpus, and the replies shorter and more homogeneous.', 'Malik, Subramaniam, and Kaushik (2007) developed a system that builds questionanswer pairs from help-center e-mails, and then maps new questions to existing questions in order to retrieve an answer.', 'This part of their approach resembles our Doc-Ret method, but instead of retrieving entire response documents, they retrieve individual sentences.', 'In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates.', 'Lapalme and Kosseim (2003) investigated three approaches to the automatic generation of response e-mails: text classification, case-based reasoning, and question answering.', 'Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed.', 'The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches.', 'However, the personalization component of the case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individuals and companies in texts).']",1,"['There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', 'However, there is no attempt to automatically generate a single response.', 'Bickel and Scheffer (2004) compared the performance of document retrieval and document prediction for generating help-desk responses.', 'Their prediction method, which is similar to Doc-Pred, is based on clustering the responses in the corpus into semantically equivalent answers, and then training a classifier to match a query with one of these classes.', 'The generated response is the answer that is closest to the centroid of the cluster.', 'However, it is worth noting that their corpus is significantly smaller than ours (805 question-answer pairs), their questions seem to be much simpler and shorter than those in our corpus, and the replies shorter and more homogeneous.', 'Malik, Subramaniam, and Kaushik (2007) developed a system that builds questionanswer pairs from help-center e-mails, and then maps new questions to existing questions in order to retrieve an answer.', 'Lapalme and Kosseim (2003) investigated three approaches to the automatic generation of response e-mails: text classification, case-based reasoning, and question answering.']"
CC627,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,automatic question answering using the web beyond the factoid,"['R Soricut', 'E Brill']",,"In this paper we describe and evaluate a Question Answering (QA) system that goes beyond answering factoid questions. Our approach to QA assumes no restrictions on the type of questions that are handled, and no assumption that the answers to be provided are factoids. We present an unsupervised approach for collecting question and answer pairs from FAQ pages, which we use to collect a corpus of 1 million question/answer pairs from FAQ pages available on the Web. This corpus is used to train various statistical models employed by our QA system: a statistical chunker used to transform a natural language-posed question into a phrase-based query to be submitted for exact match to an off-the-shelf search engine; an answer/question translation model, used to assess the likelihood that a proposed answer is indeed an answer to the posed question; and an answer language model, used to assess the likelihood that a proposed answer is a well-formed answer. We evaluate our QA system in a modular fashion, by comparing the performance of baseline algorithms against our proposed algorithms for various modules in our QA system. The evaluation shows that our system achieves reasonable performance in terms of answer accuracy for a large variety of complex, non-factoid questions.","#AUTHOR_TAG compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval .","['In FAQs,  employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document.', 'They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.', 'compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models).', 'Jijkoun and de Rijke ( 2005) compared different variants of retrieval techniques.', '#AUTHOR_TAG compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval .', 'Two significant differences between help-desk and FAQs are the following.']",1,"['In FAQs,  employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document.', 'compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models).', 'Jijkoun and de Rijke ( 2005) compared different variants of retrieval techniques.', '#AUTHOR_TAG compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval .']"
CC628,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,automating helpdesk responses a comparative study of informationgathering approaches,"['Y Marom', 'I Zukerman']",method,,"6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person ( #AUTHOR_TAG ) , but the simple binary bag-of-lemmas representation yielded similar results .","[""We use a Support Vector Machine (SVM) with a Radial Basis Function kernel to predict SCs from users' requests. 7"", 'A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request.', '6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person ( #AUTHOR_TAG ) , but the simple binary bag-of-lemmas representation yielded similar results .', '7 We employed the LIBSVM package (Chang and Lin 2001).', 'prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3.', 'We then apply the following steps.']",5,"[""We use a Support Vector Machine (SVM) with a Radial Basis Function kernel to predict SCs from users' requests. 7"", '6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person ( #AUTHOR_TAG ) , but the simple binary bag-of-lemmas representation yielded similar results .', '7 We employed the LIBSVM package (Chang and Lin 2001).', 'We then apply the following steps.']"
CC629,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,hybrid recommender systems user modeling and useradapted interaction,['R Burke'],,,"They also proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by #AUTHOR_TAG as follows .","['In addition to the different response-generation methods, we have proposed a metalevel strategy to combine them.', 'This kind of meta-learning is referred to as stacking by the Data Mining community (Witten and Frank 2000).', 'Lekakos and Giaglis (2007) implemented a supervised version of this approach for a recommender system, as opposed to our unsupervised version.', 'They also proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by #AUTHOR_TAG as follows .', ""The merging category corresponds to techniques where the individual methods affect each other in different ways (this category encompasses Burke's feature combination, cascade, feature augmentation, and meta-level sub-categories)."", ""The ensemble category corresponds to techniques where the predictions of the individual methods are combined to produce a final prediction (this category encompasses Burke's weighted, switching, and mixed sub-categories).""]",0,"['Lekakos and Giaglis (2007) implemented a supervised version of this approach for a recommender system, as opposed to our unsupervised version.', 'They also proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by #AUTHOR_TAG as follows .']"
CC630,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,evaluation of a largescale email response system,"['Y Marom', 'I Zukerman']",method,,"In Section 5 , we discuss the difficulties associated with such user studies , and describe a human-based evaluation we conducted for a small subset of the responses generated by our system ( #AUTHOR_TAGb ) .","['Our experimental set-up is designed to evaluate the ability of the different responsegeneration methods to address unseen request e-mails.', 'In particular, we want to determine the applicability of our methods to different situations, namely, whether different requests are addressed only by some methods, or whether there is a significant overlap between the methods.', 'Our evaluation is performed by measuring the quality of the generated responses.', 'Quality is a subjective measure, which is best judged by the users of the system (i.e., the help-desk customers or operators).', 'In Section 5 , we discuss the difficulties associated with such user studies , and describe a human-based evaluation we conducted for a small subset of the responses generated by our system ( #AUTHOR_TAGb ) .', 'However, our more comprehensive evaluation is an automatic one that treats the responses generated by the help-desk operators as model responses, and performs text-based comparisons between the model responses and the automatically generated ones.']",5,"['Our experimental set-up is designed to evaluate the ability of the different responsegeneration methods to address unseen request e-mails.', 'In Section 5 , we discuss the difficulties associated with such user studies , and describe a human-based evaluation we conducted for a small subset of the responses generated by our system ( #AUTHOR_TAGb ) .']"
CC631,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,applying casebased reasoning techniques for enterprise systems,['I Watson'],introduction,"Chapter 1: What Is Case-Based Reasoning? Chapter 2: Understanding CBR Chapter 3: The Application of CBR Chapter 4: Industrial Applications of CBR Chapter 5: CBR and Customer Service Chapter 6: CBR Software Tools Chapter 7: Building a Diagnostic Case-Base Chapter 8: Building, Testing, and Maintaining Case-Bases Chapter 9: Conclusion Chapter 10: Bibliography",It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; #AUTHOR_TAG ; Delic and Lahaix 1998 ) .,"['However, even the automation of responses to the ""easy"" problems is a difficult task.', 'Although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; #AUTHOR_TAG ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']",0,"['It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; #AUTHOR_TAG ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']"
CC632,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,bridging the lexical chasm statistical approaches to answerfinding,"['A Berger', 'R Caruana', 'D Cohn', 'D Freitag', 'V Mittal']",,,#AUTHOR_TAG compared two retrieval approaches ( TF.IDF and query expansion ) and two predictive approaches ( statistical translation and latent variable models ) .,"['In FAQs,  employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document.', 'They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.', '#AUTHOR_TAG compared two retrieval approaches ( TF.IDF and query expansion ) and two predictive approaches ( statistical translation and latent variable models ) .', 'Jijkoun and de Rijke ( 2005) compared different variants of retrieval techniques.', 'Soricut and Brill (2006) compared a predictive approach (statistical translation), a retrieval approach based on a language-model, and a hybrid approach which combines statistical chunking and traditional retrieval.', 'Two significant differences between help-desk and FAQs are the following.']",0,"['In FAQs,  employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document.', '#AUTHOR_TAG compared two retrieval approaches ( TF.IDF and query expansion ) and two predictive approaches ( statistical translation and latent variable models ) .', 'Soricut and Brill (2006) compared a predictive approach (statistical translation), a retrieval approach based on a language-model, and a hybrid approach which combines statistical chunking and traditional retrieval.']"
CC633,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,a hybrid approach for improving predictive accuracy of collaborative filtering algorithms user modeling and useradapted interaction,"['G Lekakos', 'G M Giaglis']",,,"Following #AUTHOR_TAG , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases .","['Following #AUTHOR_TAG , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases .', 'However, in our situation, there is not always one single winner (two methods can perform similarly well for a given request), and there are different ways to pick winners (for example, based on F-score or precision).', 'Therefore, such an approach would require the utilization of subjective heuristics for creating labels, which would significantly influence what is being learned.', 'Instead, we adopt an unsupervised approach that finds patterns in the data-confidence values coupled with performance scores (Section 6.1)-and then attempts to fit unseen data to these patterns (Section 6.2).', 'Heuristics are still needed in order to decide which response-generation method to apply to an unseen case, but they are applied only after the learning is complete (Section 6.3).', 'In other words, the subjective process of setting performance criteria (which should be conducted by the organization running the helpdesk) does not influence the machine learning process.']",1,"['Following #AUTHOR_TAG , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases .', 'However, in our situation, there is not always one single winner (two methods can perform similarly well for a given request), and there are different ways to pick winners (for example, based on F-score or precision).', 'Therefore, such an approach would require the utilization of subjective heuristics for creating labels, which would significantly influence what is being learned.', 'Instead, we adopt an unsupervised approach that finds patterns in the data-confidence values coupled with performance scores (Section 6.1)-and then attempts to fit unseen data to these patterns (Section 6.2).', 'Heuristics are still needed in order to decide which response-generation method to apply to an unseen case, but they are applied only after the learning is complete (Section 6.3).', 'In other words, the subjective process of setting performance criteria (which should be conducted by the organization running the helpdesk) does not influence the machine learning process.']"
CC634,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,automatic generation of domain models for callcenters from noisy transcriptions,"['S Roy', 'L V Subramaniam']",method,"Call centers handle customer queries from various domains such as computer sales and support, mobile phones, car rental, etc. Each such domain generally has a domain model which is essential to handle customer complaints. These models contain common problem categories, typical customer issues and their solutions, greeting styles. Currently these models are manually created over time. Towards this, we propose an unsupervised technique to generate domain models automatically from call transcriptions. We use a state of the art Automatic Speech Recognition system to transcribe the calls between agents and customers, which still results in high word error rates (40%) and show that even from these noisy transcriptions of calls we can automatically build a domain model. The domain model is comprised of primarily a topic taxonomy where every node is characterized by topic(s), typical Questions-Answers (Q&As), typical actions and call statistics. We show how such a domain model can be used for topic identification of unseen calls. We also propose applications for aiding agents while handling calls and for agent monitoring based on the domain model.",â¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #AUTHOR_TAG ) .,['â\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #AUTHOR_TAG ) .'],1,['â\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #AUTHOR_TAG ) .']
CC635,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,sentence fusion for multidocument news summarization,"['R Barzilay', 'K R McKeown']",method,"A system that can produce informative summaries, highlighting common information found in many online documents, will help Web users to pinpoint information that they need without extensive reading. In this article, we introduce sentence fusion, a novel text-to-text generation technique for synthesizing common information across documents. Sentence fusion involves bottom-up local multisequence alignment to identify phrases conveying similar information and statistical generation to combine common phrases into a sentence. Sentence fusion moves the summarization field from the use of purely extractive methods to the generation of abstracts that contain sentences not found in any of the input documents and can synthesize information across sources.","In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #AUTHOR_TAG ) .","['As discussed in Section 2, there are situations that cannot be addressed by a document-level approach, because requests only predict or match portions of responses.', 'An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response.', 'This task can be cast as extractive multi-document summarization.', 'Unlike a document reuse approach, sentence-level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable.', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #AUTHOR_TAG ) .']",0,"['This task can be cast as extractive multi-document summarization.', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #AUTHOR_TAG ) .']"
CC636,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,detection of questionanswer pairs in email conversations,"['L Shrestha', 'K R McKeown']",,,"Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).","['Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).', 'An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries.', 'In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information.', 'Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses.']",1,"['Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).']"
CC637,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,decision graphs—an extension of decision trees,['J J Oliver'],method,"In this paper, we examine Decision Graphs, a generalization of decision trees. We present an inference scheme to construct decision graphs using the Minimum Message Length Principle. Empirical tests demonstrate that this scheme compares favourably with other decision tree inference schemes. This work provides a metric for comparing the relative merit of the decision tree and decision graph formalisms for a particular domain.","Specifically , we used Decision Graphs ( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .","['The focus of our work is on the general applicability of the different response automation methods, rather than on comparing the performance of particular implementation techniques.', 'Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research.', 'Specifically , we used Decision Graphs ( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .', 'Because this variation was uniformly implemented for both approaches, it does not affect their relative performance.', 'These methodological variations are summarized in Table 2.']",5,"['Specifically , we used Decision Graphs ( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .']"
CC638,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,automatic evaluation of summaries using ngram cooccurrence statistics,"['C Y Lin', 'E H Hovy']",method,,"13 We also employed sequence-based measures using the ROUGE tool set ( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures .","['13 We also employed sequence-based measures using the ROUGE tool set ( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures .']",5,"['13 We also employed sequence-based measures using the ROUGE tool set ( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures .']"
CC639,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,an information measure for classification,"['C S Wallace', 'D M Boulton']",,"1. The class to which each thing belongs. 2. The average properties of each class. 3. The deviations of each thing from the average properties of its parent class. If the things are found to be concentrated in a small area of the region of each class in the measurement space then the deviations will be small, and with reference to the average class properties most of the information about a thing is given by naming the class to which it belongs. In this case the information may be recorded much more briefly than if a classification had not been used. We suggest that the best classification is that which results in the briefest recording of all the attribute information. In this context, we will regard the measurements of each thing as being a message about that thing. Shannon (1948) showed that where messages may be regarded as each nominating the occurrence of a particular event among a universe of possible events, the information needed to record a series of such messages is minimised if the messages are encoded so that the length of each message is proportional to minus the logarithm of the relative frequency of occurrence of the event which it nominates. The information required is greatest when all frequencies are equal. The messages here nominate the positions in measurement space of the 5 1 points representing the attributes of the things. If the expected density of points in the measurement space is everywhere uniform, the positions of the points cannot be encoded more briefly than by a simple list of the measured values. However, if the expected density is markedly non-uniform, application",We then use the program Snob ( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .,"['We train the system by clustering the ""experiences"" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively).', 'We then use the program Snob ( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .', 'Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.', 'shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15', 'These clusters were chosen because they illustrate clearly three situations of interest.']",5,"['We train the system by clustering the ""experiences"" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively).', 'We then use the program Snob ( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .', 'Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.', 'shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15', 'These clusters were chosen because they illustrate clearly three situations of interest.']"
CC640,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,evaluation of a largescale email response system,"['Y Marom', 'I Zukerman']",method,,"In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #AUTHOR_TAGa ) .","['In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #AUTHOR_TAGa ) .', 'Our judges were instructed to position themselves as help-desk customers who know that they are receiving an automated response, and that such a response is likely to arrive quicker than a response composed by an operator.', 'Our user study assessed the response-generation methods from the following perspectives, which yield information that is beyond the F-score and precision measures obtained in the automatic evaluation.']",5,"['In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #AUTHOR_TAGa ) .']"
CC641,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,in question answering two heads are better than one,"['J Chu-Carroll', 'K Czuba', 'J M Prager', 'A Ittycheriah']",,"Motivated by the success of ensemble methods  in machine learning and other areas of natural  language processing, we developed a multistrategy  and multi-source approach to question  answering which is based on combining the results  from different answering agents searching  for answers in multiple corpora. The answering  agents adopt fundamentally different strategies,  one utilizing primarily knowledge-based  mechanisms and the other adopting statistical  techniques. We present our multi-level answer  resolution algorithm that combines results from  the answering agents at the question, passage,  and/or answer levels. Experiments evaluating  the effectiveness of our answer resolution algorithm  show a 35.0% relative improvement over  our baseline system in the number of questions  correctly answered, and a 32.8% improvement  according to the average precision metric","The question answering system developed by #AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) .","['Our system falls into the ensemble category, because it combines the results of the various methods into a single outcome.', ""More specifically, it belongs to Burke's switching sub-category, where a single method is selected on a case-by-case basis."", ""A similar approach is taken in Rotaru and Litman's (2005) reading comprehension system, but their system does not perform any learning."", 'Instead it uses a voting mechanism to select the answer given by the majority of methods.', ""The question answering system developed by #AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) ."", 'Because the results of all the methods are comparable, no learning is required: At each stage of the ""cascade of methods,"" the method that performs best is selected.', 'In contrast to these two systems, our system employs methods that are not comparable, because they use different metrics.', 'Therefore, we need to learn from experience when to use each method.']",1,"[""The question answering system developed by #AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) .""]"
CC642,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,paraphrasing with bilingual parallel corpora,"['Colin Bannard', 'Chris Callison-Burch']",introduction,"Previous work has used monolingual parallel corpora to extract and generate paraphrases. We show that this task can be done using bilingual parallel corpora, a much more commonly available resource. Using alignment techniques from phrase-based statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot. We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account. We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments.","But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( #AUTHOR_TAG ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .","['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al. 2004;Chiang et al. 2005]) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( #AUTHOR_TAG ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']",4,"['But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( #AUTHOR_TAG ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']"
CC643,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,nonlinear programming 2nd edition athena scientific,['Dimitri P Bertsekas'],,,"Here , PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient ( #AUTHOR_TAG ) ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable .","['Hence the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ.', 'We optimize the dual objective using the gradient based methods shown in Algorithm 1.', ""Here η is an optimization precision, α is a step size chosen with the strong Wolfe's rule (Nocedal and Wright 1999)."", 'Here , PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient ( #AUTHOR_TAG ) ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable .', 'In practice this only happens at the start of optimization and we use a sub-gradient for the first direction.']",5,"['We optimize the dual objective using the gradient based methods shown in Algorithm 1.', ""Here e is an optimization precision, a is a step size chosen with the strong Wolfe's rule (Nocedal and Wright 1999)."", 'Here , PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient ( #AUTHOR_TAG ) ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable .', 'In practice this only happens at the start of optimization and we use a sub-gradient for the first direction.']"
CC644,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,wordbased alignment phrasebased translation whats the link,"['Adam Lopez', 'Philip Resnik']",,,See #AUTHOR_TAG for further discussion .,"['We now investigate whether our alignments produce improvements in an end-to-end phrase-based machine translation system.', 'We use a state-of-the-art machine translation system,5 and follow the experimental setup used for the 2008 shared task on machine translation (ACL 2008 Third Workshop on Statistical Machine Translation).', 'The full pipeline consists of the following steps: (1) prepare the data (lowercase, tokenize, and filter long sentences); (2) build language models; (3) create word alignments in each direction; (4) symmetrize directional word alignments; (5) build phrase table; (6) tune weights for the phrase table.', 'For more details consult the shared task description.6', 'To evaluate the quality of the produced alignments, we keep the pipeline unchanged, and use the models described earlier to generate the word alignments in Step 3.', 'For Step 4, we use the soft union symmetrization heuristic.', 'Symmetrization has almost no effect on alignments produced by S-HMM, but we use it for uniformity in the experiments.', 'We tested three values of the threshold (0.2, 0.4, 0.6) which try to capture different tradeoffs of precision vs. recall, and pick the best according to the translation performance on development data.', 'Table 2 summarizes the results for the different corpora.', 'For refer- ence we include IBM Model 4 as suggested in the task description.', 'PR training always outperforms EM training and outperforms IBM Model 4 in all but one experiment.', 'Differences in BLEU range from 0.2 to 0.9.', 'The two constraints help to a different extent for different corpora and translation directions, in a somewhat unpredictable manner.', 'In general our impression is that the connection between alignment quality and BLEU scores is complicated, and changes are difficult to explain and justify.', 'The number of iterations for MERT optimization to converge varied from 2 to 28; and the best choice of threshold on the development set did not always correspond to the best on the test set.', 'Contrary to conventional wisdom in the MT community, bigger phrase tables did not always perform better.', 'In 14 out of 18 cases, the threshold picked was 0.4 (medium size phrase tables) and the other four times 0.2 was picked (smaller phrase tables).', 'When we include only high confidence alignments, more phrases are extracted but many of these are erroneous.', 'Potentially this leads to a poor estimate of the phrase probabilities.', 'See #AUTHOR_TAG for further discussion .']",0,"['We now investigate whether our alignments produce improvements in an end-to-end phrase-based machine translation system.', 'We use a state-of-the-art machine translation system,5 and follow the experimental setup used for the 2008 shared task on machine translation (ACL 2008 Third Workshop on Statistical Machine Translation).', 'The full pipeline consists of the following steps: (1) prepare the data (lowercase, tokenize, and filter long sentences); (2) build language models; (3) create word alignments in each direction; (4) symmetrize directional word alignments; (5) build phrase table; (6) tune weights for the phrase table.', 'For more details consult the shared task description.6', 'To evaluate the quality of the produced alignments, we keep the pipeline unchanged, and use the models described earlier to generate the word alignments in Step 3.', 'Symmetrization has almost no effect on alignments produced by S-HMM, but we use it for uniformity in the experiments.', 'We tested three values of the threshold (0.2, 0.4, 0.6) which try to capture different tradeoffs of precision vs. recall, and pick the best according to the translation performance on development data.', 'Table 2 summarizes the results for the different corpora.', 'The two constraints help to a different extent for different corpora and translation directions, in a somewhat unpredictable manner.', 'Contrary to conventional wisdom in the MT community, bigger phrase tables did not always perform better.', 'In 14 out of 18 cases, the threshold picked was 0.4 (medium size phrase tables) and the other four times 0.2 was picked (smaller phrase tables).', 'When we include only high confidence alignments, more phrases are extracted but many of these are erroneous.', 'Potentially this leads to a poor estimate of the phrase probabilities.', 'See #AUTHOR_TAG for further discussion .']"
CC645,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,a systematic comparison of various statistical alignment models,"['Franz Josef Och', 'Hermann Ney']",introduction,"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.","Many researchers use the GIZA + + software package ( #AUTHOR_TAG ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency .","['IBM Models 1 and 2 and the HMM are simple and tractable probabilistic models, which produce the target sentence one target word at a time by choosing a source word and generating its translation.', 'IBM Models 3, 4, and 5 attempt to capture fertility (the tendency of each source word to generate several target words), resulting in probabilistically deficient, intractable models that require local heuristic search and are difficult to implement and extend.', 'Many researchers use the GIZA + + software package ( #AUTHOR_TAG ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency .', 'All of the models are asymmetric (switching target and source languages produces drastically different results) and the simpler models (IBM Models 1, 2, and HMM) do not enforce bijectivity (the majority of words translating as a single word).', 'Although there are systematic translation phenomena where one cannot hope to obtain 1-to-1 alignments, we observe that in over 6 different European language pairs the majority of alignments are in fact 1-to-1 (86-98%).', 'This leads to the common practice of post-processing heuristics for intersecting directional alignments to produce nearly bijective and symmetric results (Koehn, Och, and Marcu 2003).']",0,"['Many researchers use the GIZA + + software package ( #AUTHOR_TAG ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency .']"
CC646,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,numerical optimization,"['Jorge Nocedal', 'Stephen J Wright']",,"Numerical Optimization presents a comprehensive and up-to-date description of the most effective methods in continuous optimization. It responds to the growing interest in optimization in engineering, science, and business by focusing on the methods that are best suited to practical problems. For this new edition the book has been thoroughly updated throughout. There are new chapters on nonlinear interior methods and derivative-free methods for optimization, both of which are used widely in practice and the focus of much current research. Because of the emphasis on practical methods, as well as the extensive illustrations and exercises, the book is accessible to a wide audience. It can be used as a graduate text in engineering, operations research, mathematics, computer science, and business. It also serves as a handbook for researchers and practitioners in the field. The authors have strived to produce a text that is pleasant to read, informative, and rigorous - one that reveals both the beautiful nature of the discipline and its practical side.","Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule ( #AUTHOR_TAG ) .","['Hence the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ.', 'We optimize the dual objective using the gradient based methods shown in Algorithm 1.', ""Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule ( #AUTHOR_TAG ) ."", 'Here, β∇(λ) represents an ascent direction chosen as follows: For inequality constraints, it is the projected gradient (Bertsekas 1999); for equality constraints with slack, we use conjugate gradient (Nocedal and Wright 1999), noting that when λ = 0, the objective is not differentiable.', 'In practice this only happens at the start of optimization and we use a sub-gradient for the first direction.']",5,"['We optimize the dual objective using the gradient based methods shown in Algorithm 1.', ""Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule ( #AUTHOR_TAG ) ."", 'Here, b(l) represents an ascent direction chosen as follows: For inequality constraints, it is the projected gradient (Bertsekas 1999); for equality constraints with slack, we use conjugate gradient (Nocedal and Wright 1999), noting that when l = 0, the objective is not differentiable.', 'In practice this only happens at the start of optimization and we use a sub-gradient for the first direction.']"
CC647,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,getting the structure right for word alignment leaf,"['Alexander Fraser', 'Daniel Marcu']",,"Word alignment is the problem of annotating parallel text with translational correspondence. Previous generative word alignment models have made structural assumptions such as the 1-to-1, 1-to-N, or phrase-based consecutive word assumptions, while previous discriminative models have either made such an assumption directly or used features derived from a generative model making one of these assumptions. We present a new generative alignment model which avoids these structural limitations, and show that it is effective when trained using both unsupervised and semi-supervised training methods.","This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model ( #AUTHOR_TAG ) .","['Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments.', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model ( #AUTHOR_TAG ) .', 'Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.', ""Instead, we propose to use a learning framework called Posterior Regularization (Graça, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model's posteriors."", 'The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model).', 'Because in most applications what we are interested in are the latent variables (in this case the alignments), constraining the posteriors allows a more direct way to achieve the desired behavior.', 'On the other hand, constraining the expected value of the features instead of adding them to the model allows us to express features that would otherwise make the model intractable.', 'For example, enforcing that each hidden state of an HMM model should be used at most once per sentence would break the Markov property and make the model intractable.', 'In contrast, we will show how to enforce the constraint that each hidden state is used at most once in expectation.', 'The underlying model remains unchanged, but the learning method changes.', 'During learning, our method is similar to the EM algorithm with the addition of solving an optimization problem similar to a maximum entropy problem inside the E Step.', 'The following subsections present the Posterior Regularization framework, followed by a description of how to encode two pieces of prior information aimed at solving the problems described at the end of Section 2.']",1,"['Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments.', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model ( #AUTHOR_TAG ) .', 'Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.', ""Instead, we propose to use a learning framework called Posterior Regularization (Graca, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model's posteriors."", 'The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model).', 'Because in most applications what we are interested in are the latent variables (in this case the alignments), constraining the posteriors allows a more direct way to achieve the desired behavior.', 'On the other hand, constraining the expected value of the features instead of adding them to the model allows us to express features that would otherwise make the model intractable.', 'For example, enforcing that each hidden state of an HMM model should be used at most once per sentence would break the Markov property and make the model intractable.', 'In contrast, we will show how to enforce the constraint that each hidden state is used at most once in expectation.', 'The underlying model remains unchanged, but the learning method changes.']"
CC648,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,moses open source toolkit for statistical machine translation,"['Hieu Hoang', 'Alexandra Birch', 'Chris Callison-Burch', 'Richard Zens', 'Rwth Aachen', 'Alexandra Constantin', 'Marcello Federico', 'Nicola Bertoldi', 'Chris Dyer', 'Brooke Cowan', 'Wade Shen', 'Christine Moran', 'Ondrej Bojar']",,"We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.",5 The open source Moses ( #AUTHOR_TAG ) toolkit from www.statmt.org/moses/ .,['5 The open source Moses ( #AUTHOR_TAG ) toolkit from www.statmt.org/moses/ .'],5,['5 The open source Moses ( #AUTHOR_TAG ) toolkit from www.statmt.org/moses/ .']
CC649,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,building a multilingual parallel subtitle corpus,['J¨org Tiedemann'],,"In this paper on-going work of creating an extensive multilingual parallel corpus of movie subtitles is presented. The corpus currently contains roughly 23,000 pairs of aligned subtitles covering about 2,700 movies in 29 languages. Subtitles mainly consist of transcribed speech, sometimes in a very condensed way. Insertions, deletions and paraphrases are very frequent which makes them a challenging data set to work with especially when applying automatic sentence alignment. Standard alignment approaches rely on translation consistency either in terms of length or term translations or a combination of both. In the paper, we show that these approaches are not applicable for subtitles and we propose a new alignment approach based on time overlaps specifically designed for subtitles. In our experiments we obtain a significant improvement of alignment accuracy compared to standard length-based","results are based on a corpus of movie subtitles ( #AUTHOR_TAG ) , and are consequently shorter sentences , whereas the En â Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) .","['In this section, we compare the different alignments produced with and without PR based on how well they can be used for transfer of linguistic resources across languages.', 'We used the system proposed by Ganchev, Gillenwater, and Taskar (2009).', 'This system uses a word-aligned corpus and a parser for a resource-rich language (source language) in order to create a parser for a resource-poor language (target language).', 'We consider a parse tree on the source language as a set of dependency edges to be transferred.', 'For each such edge, if both end points are aligned to words in the target language, then the edge is transferred.', 'These edges are then used as weak supervision when training a generative or discriminative dependency parser.', 'In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised parse trees on the target side.', 'By changing the threshold in MBR decoding of alignments, we can trade off accuracy of the transferred edges vs. transferring more edges.', 'We generated supervised parses using the first-order model from the MST parser (McDonald, Crammer, and Pereira 2005) trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and Spanish.', 'Following Ganchev, Gillenwater, and Taskar (2009), we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En→Bg) and from English to Spanish (En→Es).', 'results are based on a corpus of movie subtitles ( #AUTHOR_TAG ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) .', 'We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM.']",5,"['In this section, we compare the different alignments produced with and without PR based on how well they can be used for transfer of linguistic resources across languages.', 'results are based on a corpus of movie subtitles ( #AUTHOR_TAG ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) .']"
CC650,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,the hiero machine translation system extensions evaluation and analysis,"['David Chiang', 'Adam Lopez', 'Nitin Madnani', 'Christof Monz', 'Philip Resnik', 'Michael Subotin']",introduction,"Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has been largely absent from the best performing machine translation systems in recent community-wide evaluations. In this paper, we discuss a new hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems.","Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ Galley et al. 2004 ; #AUTHOR_TAG ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).","['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ Galley et al. 2004 ; #AUTHOR_TAG ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001;Hwa et al. 2005;Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008).']",0,"['Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ Galley et al. 2004 ; #AUTHOR_TAG ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).']"
CC651,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,minimum bayesrisk word alignments of bilingual texts,"['Shankar Kumar', 'William Byrne']",introduction,"We present Minimum Bayes-Risk word alignment for machine translation. This statistical, model-based approach attempts to minimize the expected risk of alignment errors under loss functions that measure alignment quality. We describe various loss functions, including some that incorporate linguistic analysis as can be obtained from parse trees, and show that these approaches can improve alignments of the English-French Hansards.","Another possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding ( #AUTHOR_TAG ; Liang , Taskar , and Klein 2006 ; Ganchev , and Taskar 2007 ) .","['Another possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding ( #AUTHOR_TAG ; Liang , Taskar , and Klein 2006 ; Ganchev , and Taskar 2007 ) .', 'Using this decoding we include an alignment link i − j if the posterior probability that word i aligns to word j is above some threshold.', 'This allows the accumulation of probability from several low-scoring alignments that agree on one alignment link.', 'The threshold is tuned on some small amount of labeled data-in our case the development set-to minimize some loss.', 'Kumar and Byrne (2002) study different loss functions that incorporate linguistic knowledge, and show significant improvement over likelihood decoding.', 'Note that this could potentially result in an alignment having zero probability under the model, as many-to-many alignments can be produced in this way.', 'MBR decoding has several advantages over Viterbi decoding.', 'First, independently of the particular choice of the loss function, by picking a specific threshold we can trade off precision and recall of the predicted word alignments.', 'In fact, in this work when comparing different alignment sets we do not commit to any loss function but instead compare precision vs recall curves, by generating alignments for different thresholds (0..1).', 'Second, with this method we can ignore the null word probabilities, which tend to be poorly estimated.']",5,"['Another possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding ( #AUTHOR_TAG ; Liang , Taskar , and Klein 2006 ; Ganchev , and Taskar 2007 ) .', 'Using this decoding we include an alignment link i - j if the posterior probability that word i aligns to word j is above some threshold.', 'This allows the accumulation of probability from several low-scoring alignments that agree on one alignment link.', 'The threshold is tuned on some small amount of labeled data-in our case the development set-to minimize some loss.', 'Kumar and Byrne (2002) study different loss functions that incorporate linguistic knowledge, and show significant improvement over likelihood decoding.', 'Note that this could potentially result in an alignment having zero probability under the model, as many-to-many alignments can be produced in this way.', 'First, independently of the particular choice of the loss function, by picking a specific threshold we can trade off precision and recall of the predicted word alignments.', 'In fact, in this work when comparing different alignment sets we do not commit to any loss function but instead compare precision vs recall curves, by generating alignments for different thresholds (0..1).', 'Second, with this method we can ignore the null word probabilities, which tend to be poorly estimated.']"
CC652,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,a systematic comparison of various statistical alignment models,"['Franz Josef Och', 'Hermann Ney']",,"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.","We used a standard implementation of IBM Model 4 ( #AUTHOR_TAG ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves .","['The intent of this experimental section is to evaluate the gains from using constraints during learning, hence the main comparison is between HMM trained with normal EM vs. trained with PR plus constraints.', 'We also report results for IBM Model 4, because it is often used as the default word alignment model, and can be used as a reference.', 'However, we would like to note that IBM Model 4 is a more complex model, able to capture more structure, albeit at the cost of intractable inference.', 'Because our approach is orthogonal to the base model used, the constraints described here could be applied in principle to IBM Model 4 if exact inference was efficient, hopefully yielding similar improvements.', 'We used a standard implementation of IBM Model 4 ( #AUTHOR_TAG ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves .', 'We trained IBM Model 4 using the default configuration of the']",5,"['We also report results for IBM Model 4, because it is often used as the default word alignment model, and can be used as a reference.', 'However, we would like to note that IBM Model 4 is a more complex model, able to capture more structure, albeit at the cost of intractable inference.', 'Because our approach is orthogonal to the base model used, the constraints described here could be applied in principle to IBM Model 4 if exact inference was efficient, hopefully yielding similar improvements.', 'We used a standard implementation of IBM Model 4 ( #AUTHOR_TAG ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves .', 'We trained IBM Model 4 using the default configuration of the']"
CC653,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,unsupervised multilingual learning for morphological segmentation,"['Benjamin Snyder', 'Regina Barzilay']",introduction,"For centuries, the deep connection between languages has brought about major discoveries about human communication. In this paper we investigate how this powerful source of information can be exploited for unsupervised language learning. In particular, we study the task of morphological segmentation of multiple languages. We present a nonparametric Bayesian model that jointly induces morpheme segmentations of each language under consideration and at the same time identifies cross-lingual morpheme patterns, or abstract morphemes. We apply our model to three Semitic languages: Arabic, Hebrew, Aramaic, as well as to English. Our results demonstrate that learning morphological models in tandem reduces error by up to 24% relative to monolingual models. Furthermore, we provide evidence that our joint model achieves better performance when applied to languages from the same family.","But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( #AUTHOR_TAG ) .","['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al. 2004;Chiang et al. 2005]) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( #AUTHOR_TAG ) .']",4,"['But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( #AUTHOR_TAG ) .']"
CC654,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,a systematic comparison of various statistical alignment models,"['Franz Josef Och', 'Hermann Ney']",,"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.",The standard approach is to train two models independently and then intersect their predictions ( #AUTHOR_TAG ) .,"['The directional nature of the generative models used to recover word alignments conflicts with their interpretation as translations.', 'In practice, we see that the choice of which language is source versus target matters and changes the mistakes made by the model (the first row of panels in Figure 1).', 'The standard approach is to train two models independently and then intersect their predictions ( #AUTHOR_TAG ) .', 'However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree.', 'Let the directional models be defined as: − → p ( − → z ) (source-target) and ← − p ( ← − z ) (target-source).', 'We suppress dependence on x and y for brevity.', 'Define z to range over the union of all possible directional alignments − → Z ∪ ← − Z .', 'We define a mixture model p']",1,"['The directional nature of the generative models used to recover word alignments conflicts with their interpretation as translations.', 'The standard approach is to train two models independently and then intersect their predictions ( #AUTHOR_TAG ) .', 'However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree.', 'We suppress dependence on x and y for brevity.', 'Define z to range over the union of all possible directional alignments - - Z  - - Z .']"
CC655,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,a new view of the em algorithm that justifies incremental sparse and other variants,"['Radford M Neal', 'Geoffrey E Hinton']",introduction,"The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible.","EM maximizes G ( 0 ) via block-coordinate ascent on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y ) ( #AUTHOR_TAG ) :","['Because of the latent alignment variables z, the log-likelihood function for the HMM model is not concave, and the model is fit using the Expectation Maximization (EM) algorithm (Dempster, Laird, and Rubin 1977).', 'EM maximizes G ( 0 ) via block-coordinate ascent on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y ) ( #AUTHOR_TAG ) :']",5,"['Because of the latent alignment variables z, the log-likelihood function for the HMM model is not concave, and the model is fit using the Expectation Maximization (EM) algorithm (Dempster, Laird, and Rubin 1977).', 'EM maximizes G ( 0 ) via block-coordinate ascent on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y ) ( #AUTHOR_TAG ) :']"
CC656,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,a systematic comparison of various statistical alignment models,"['Franz Josef Och', 'Hermann Ney']",,"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.","This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #AUTHOR_TAG ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) .","['Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments.', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #AUTHOR_TAG ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) .', 'Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.', ""Instead, we propose to use a learning framework called Posterior Regularization (Graça, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model's posteriors."", 'The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model).', 'Because in most applications what we are interested in are the latent variables (in this case the alignments), constraining the posteriors allows a more direct way to achieve the desired behavior.', 'On the other hand, constraining the expected value of the features instead of adding them to the model allows us to express features that would otherwise make the model intractable.', 'For example, enforcing that each hidden state of an HMM model should be used at most once per sentence would break the Markov property and make the model intractable.', 'In contrast, we will show how to enforce the constraint that each hidden state is used at most once in expectation.', 'The underlying model remains unchanged, but the learning method changes.', 'During learning, our method is similar to the EM algorithm with the addition of solving an optimization problem similar to a maximum entropy problem inside the E Step.', 'The following subsections present the Posterior Regularization framework, followed by a description of how to encode two pieces of prior information aimed at solving the problems described at the end of Section 2.']",1,"['Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #AUTHOR_TAG ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) .', 'Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.', ""Instead, we propose to use a learning framework called Posterior Regularization (Graca, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model's posteriors."", 'The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model).', 'Because in most applications what we are interested in are the latent variables (in this case the alignments), constraining the posteriors allows a more direct way to achieve the desired behavior.', 'On the other hand, constraining the expected value of the features instead of adding them to the model allows us to express features that would otherwise make the model intractable.', 'In contrast, we will show how to enforce the constraint that each hidden state is used at most once in expectation.', 'The underlying model remains unchanged, but the learning method changes.', 'During learning, our method is similar to the EM algorithm with the addition of solving an optimization problem similar to a maximum entropy problem inside the E Step.', 'The following subsections present the Posterior Regularization framework, followed by a description of how to encode two pieces of prior information aimed at solving the problems described at the end of Section 2.']"
CC657,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,hmm word and phrase alignment for statistical machine translation,"['Yonggang Deng', 'William Byrne']",related work,"Estimation and alignment procedures for word and phrase alignment hidden Markov models (HMMs) are developed for the alignment of parallel text. The development of these models is motivated by an analysis of the desirable features of IBM Model 4, one of the original and most effective models for word alignment. These models are formulated to capture the desirable aspects of Model 4 in an HMM alignment formalism. Alignment behavior is analyzed and compared to human-generated reference alignments, and the ability of these models to capture different types of alignment phenomena is evaluated. In analyzing alignment performance, Chinese-English word alignments are shown to be comparable to those of IBM Model 4 even when models are trained over large parallel texts. In translation performance, phrase-based statistical machine translation systems based on these HMM alignments can equal and exceed systems based on Model 4 alignments, and this is shown in Arabic-English and Chinese-English translation. These alignment models can also be used to generate posterior statistics over collections of parallel text, and this is used to refine and extend phrase translation tables with a resulting improvement in translation quality.","In the context of word alignment , #AUTHOR_TAG use a state-duration HMM in order to model word-to-phrase translations .","['The idea of introducing constraints over a model to better guide the learning process has appeared before.', 'In the context of word alignment , #AUTHOR_TAG use a state-duration HMM in order to model word-to-phrase translations .', 'The fertility of each source word is implicitly encoded in the durations of the HMM states.', 'Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant η > 1.', 'This encourages more transitions and hence shorter phrases.', 'For the task of unsupervised dependency parsing, Smith and Eisner (2006) add a constraint of the form ""the average length of dependencies should be X"" to capture the locality of syntax (at least half of the dependencies are between adjacent words), using a scheme they call structural annealing.', ""They modify the model's distribution over trees p θ (y) by a penalty term as: p θ (y) ∝ p θ (y)e (δ e∈y length(e)) , where length(e) is the surface length of edge e."", 'The factor δ changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training.', 'These two approaches also have the goal of controlling unsupervised learning, and the form of the modified distributions is reminiscent of the form that the projected posteriors take.', 'However, the approaches differ substantially from PR. Smith and Eisner (2006) make a statement of the form ""scale the total length of edges"", which depending on the value of δ will prefer to have more shorter/longer edges.', 'Such statements are not data dependent.', 'Depending on the value of δ, for instance if δ ≤ 0, even if the data is such that the model already uses too many short edges on average, this value of δ will push for more short edges.', 'By contrast the statements we can make in PR are of the form ""there should be more short edges than long edges"".', 'Such a statement is data-dependent in the sense that if the model satisfies the constraints then we do not need to change it; if it is far from satisfying it we might need to make very dramatic changes.']",0,"['In the context of word alignment , #AUTHOR_TAG use a state-duration HMM in order to model word-to-phrase translations .']"
CC658,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,europarl a parallel corpus for statistical machine translation,['Philipp Koehn'],,"We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web. This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT). We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead.","results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En â Es results are based on a corpus of parliamentary proceedings ( #AUTHOR_TAG ) .","['In this section, we compare the different alignments produced with and without PR based on how well they can be used for transfer of linguistic resources across languages.', 'We used the system proposed by Ganchev, Gillenwater, and Taskar (2009).', 'This system uses a word-aligned corpus and a parser for a resource-rich language (source language) in order to create a parser for a resource-poor language (target language).', 'We consider a parse tree on the source language as a set of dependency edges to be transferred.', 'For each such edge, if both end points are aligned to words in the target language, then the edge is transferred.', 'These edges are then used as weak supervision when training a generative or discriminative dependency parser.', 'In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised parse trees on the target side.', 'By changing the threshold in MBR decoding of alignments, we can trade off accuracy of the transferred edges vs. transferring more edges.', 'We generated supervised parses using the first-order model from the MST parser (McDonald, Crammer, and Pereira 2005) trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and Spanish.', 'Following Ganchev, Gillenwater, and Taskar (2009), we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En→Bg) and from English to Spanish (En→Es).', 'results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings ( #AUTHOR_TAG ) .', 'We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM.']",5,"['In this section, we compare the different alignments produced with and without PR based on how well they can be used for transfer of linguistic resources across languages.', 'This system uses a word-aligned corpus and a parser for a resource-rich language (source language) in order to create a parser for a resource-poor language (target language).', 'We generated supervised parses using the first-order model from the MST parser (McDonald, Crammer, and Pereira 2005) trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and Spanish.', 'results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings ( #AUTHOR_TAG ) .']"
CC659,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,tailoring word alignments to syntactic machine translation,"['John DeNero', 'Dan Klein']",,"Extracting tree transducer rules for syntactic MT systems can be hindered by word alignment errors that violate syntactic correspondences. We propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model. Our model's predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality. We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments.",This heuristic is called soft union ( #AUTHOR_TAG ) .,"['As discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair.', 'Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment.', 'For MT the most commonly used heuristic is called grow diagonal final (Och and Ney 2003).', 'This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points.', 'The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.', 'In syntax transfer the intersection heuristic is normally used, because one wants to have high precision links to transfer knowledge between languages.', 'One pitfall of these symmetrization heuristics is that they can obfuscate the link between the original alignment and the ones used for a specific task, making errors more difficult to analyze.', 'Because they are heuristics tuned for a particular phrasebased translation system, it is not clear when they will help and when they will hinder system performance.', 'In this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model.', 'We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold.', 'This heuristic is called soft union ( #AUTHOR_TAG ) .', 'Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.', 'The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization.', 'This should not be very surprising, because the soft union symmetrization can be viewed as an approximation of our symmetry constraint applied only at decode time.', 'Applying the symmetrization to the model with symmetry constraints does not affect performance.']",5,"['As discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair.', 'Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment.', 'For MT the most commonly used heuristic is called grow diagonal final (Och and Ney 2003).', 'The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.', 'In syntax transfer the intersection heuristic is normally used, because one wants to have high precision links to transfer knowledge between languages.', 'One pitfall of these symmetrization heuristics is that they can obfuscate the link between the original alignment and the ones used for a specific task, making errors more difficult to analyze.', 'Because they are heuristics tuned for a particular phrasebased translation system, it is not clear when they will help and when they will hinder system performance.', 'We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold.', 'This heuristic is called soft union ( #AUTHOR_TAG ) .', 'Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.', 'The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization.']"
CC660,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,bootstrapping parsers via syntactic projection across parallel texts natural language engineering,"['Rebecca Hwa', 'Philip Resnik', 'Amy Weinberg', 'Clara Cabezas', 'Okan Kolak']",introduction,"Broad coverage, high quality parsers are available for only a handful of languages. A prerequisite for developing broad coverage parsers for more languages is the annotation of text with the desired linguistic representations (also known as ""treebanking""). However, syntactic annotation is a labor intensive and time-consuming process, and it is difficult to find linguistically annotated text in sufficient quantities. In this article, we explore using parallel text to help solving the problem of creating syntactic annotation in more languages. The central idea is to annotate the English side of a parallel corpus, project the analysis to the second language, and then train a stochastic analyzer on the resulting noisy annotations. We discuss our background assumptions, describe an initial study on the ""projectability"" of syntactic relations, and then present two experiments in which stochastic parsers are developed with minimal human intervention via projection from English.","But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; #AUTHOR_TAG ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .","['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al. 2004;Chiang et al. 2005]) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; #AUTHOR_TAG ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']",4,"['But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; #AUTHOR_TAG ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']"
CC661,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,a systematic comparison of various statistical alignment models,"['Franz Josef Och', 'Hermann Ney']",introduction,"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.","Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #AUTHOR_TAG ) .","['A word alignment for a parallel sentence pair represents the correspondence between words in a source language and their translations in a target language (Brown et al. 1993b).', 'There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, auxiliary verbs used in one language but not the other (e.g., English He walked and French Il est allé), articles required in one language but optional in the other (e.g., English Cars use gas and Portuguese Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expressions translated indirectly.', 'Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #AUTHOR_TAG ) .', 'The top row of Figure 1 shows two word alignments between an English-French sentence pair.', 'We use the following notation: the alignment on the left (right) will be referenced as source-target (target-source) and contains source (target) words as rows and target (source) words as columns.', 'Each entry in the matrix corresponds to a source-target word pair, and is the candidate for an alignment link.', 'Sure links are represented as squares with borders, and possible links']",0,"['A word alignment for a parallel sentence pair represents the correspondence between words in a source language and their translations in a target language (Brown et al. 1993b).', 'Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #AUTHOR_TAG ) .', 'We use the following notation: the alignment on the left (right) will be referenced as source-target (target-source) and contains source (target) words as rows and target (source) words as columns.', 'Each entry in the matrix corresponds to a source-target word pair, and is the candidate for an alignment link.']"
CC662,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,a systematic comparison of various statistical alignment models,"['Franz Josef Och', 'Hermann Ney']",,"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.",For MT the most commonly used heuristic is called grow diagonal final ( #AUTHOR_TAG ) .,"['As discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair.', 'Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment.', 'For MT the most commonly used heuristic is called grow diagonal final ( #AUTHOR_TAG ) .', 'This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points.', 'The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.', 'In syntax transfer the intersection heuristic is normally used, because one wants to have high precision links to transfer knowledge between languages.', 'One pitfall of these symmetrization heuristics is that they can obfuscate the link between the original alignment and the ones used for a specific task, making errors more difficult to analyze.', 'Because they are heuristics tuned for a particular phrasebased translation system, it is not clear when they will help and when they will hinder system performance.', 'In this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model.', 'We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold.', 'This heuristic is called soft union (DeNero and Klein 2007).', 'Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.', 'The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization.', 'This should not be very surprising, because the soft union symmetrization can be viewed as an approximation of our symmetry constraint applied only at decode time.', 'Applying the symmetrization to the model with symmetry constraints does not affect performance.']",1,"['As discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair.', 'Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment.', 'For MT the most commonly used heuristic is called grow diagonal final ( #AUTHOR_TAG ) .', 'This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points.', 'The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.', 'One pitfall of these symmetrization heuristics is that they can obfuscate the link between the original alignment and the ones used for a specific task, making errors more difficult to analyze.', 'Because they are heuristics tuned for a particular phrasebased translation system, it is not clear when they will help and when they will hinder system performance.', 'In this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model.', 'We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold.', 'This heuristic is called soft union (DeNero and Klein 2007).', 'Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.', 'The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization.', 'Applying the symmetrization to the model with symmetry constraints does not affect performance.']"
CC663,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,simple robust scalable semisupervised learning via expectation regularization,"['G Mann', 'A McCallum']",related work,"Although semi-supervised learning has been an active area of research, its use in deployed applications is still relatively rare because the methods are often difficult to implement, fragile in tuning, or lacking in scalability. This paper presents expectation regularization, a semi-supervised learning method for exponential family parametric models that augments the traditional conditional label-likelihood objective function with an additional term that encourages model predictions on unlabeled data to match certain expectations---such as label priors. The method is extremely easy to implement, scales as well as logistic regression, and can handle non-independent features. We present experiments on five different data sets, showing accuracy improvements over other semi-supervised methods.","PR is closely related to the work of #AUTHOR_TAG , 2008 ) , who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning .","['PR is closely related to the work of #AUTHOR_TAG , 2008 ) , who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning .', 'They call their method generalized expectation (GE) constraints or alternatively expectation regularization.', 'In the original GE framework, the posteriors of the model on unlabeled data are regularized directly.', 'They train a discriminative model, using conditional likelihood on labeled data and an ""expectation regularization"" penalty term on the unlabeled data:']",0,"['PR is closely related to the work of #AUTHOR_TAG , 2008 ) , who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning .', 'They call their method generalized expectation (GE) constraints or alternatively expectation regularization.', 'In the original GE framework, the posteriors of the model on unlabeled data are regularized directly.', 'They train a discriminative model, using conditional likelihood on labeled data and an ""expectation regularization"" penalty term on the unlabeled data:']"
CC664,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,inducing multilingual pos taggers and np bracketers via robust projection across aligned corpora,"['David Yarowsky', 'Grace Ngai']",introduction,"This paper investigates the potential for projecting linguistic annotations including part-of-speech tags and base noun phrase bracketings from one language to another via automatically word-aligned parallel corpora. First, experiments assess the accuracy of unmodified direct transfer of tags and brackets from the source language English to the target languages French and Chinese, both for noisy machine-aligned sentences and for clean hand-aligned sentences. Performance is then substantially boosted over both of these baselines by using training techniques optimized for very noisy data, yielding 94-96% core French part-of-speech tag accuracy and 90% French bracketing F-measure for stand-alone monolingual tools trained without the need for any human-annotated data in the given language.","But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #AUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .","['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al. 2004;Chiang et al. 2005]) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #AUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']",4,"['But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #AUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']"
CC665,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,what’s in a translation rule,"['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']",introduction,"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.","Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).","['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1�5) for statistical machine translation and the concept of �word-by- word� alignment, the correspondence between words in source and target languages.', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al. 2005; Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008).']",0,"['Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).']"
CC666,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,annealing structural bias in multilingual weighted grammar induction,"['Noah A Smith', 'Jason Eisner']",related work,"We first show how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004). Next, by annealing the free parameter that controls this bias, we achieve further improvements. We then describe an alternative kind of structural bias, toward ""broken "" hypotheses consisting of partial structures over segmented sentences, and show a similar pattern of improvement. We relate this approach to contrastive estimation (Smith and Eisner, 2005a), apply the latter to grammar induction in six languages, and show that our new approach improves accuracy by 1-17 % (absolute) over CE (and 8-30% over EM), achieving to our knowledge the best results on this task to date. Our method, structural annealing, is a general technique with broad applicability to hidden-structure discovery problems.","For the task of unsupervised dependency parsing , #AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing .","['The idea of introducing constraints over a model to better guide the learning process has appeared before.', 'In the context of word alignment, Deng and Byrne (2005) use a state-duration HMM in order to model word-to-phrase translations.', 'The fertility of each source word is implicitly encoded in the durations of the HMM states.', 'Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant η > 1.', 'This encourages more transitions and hence shorter phrases.', ""For the task of unsupervised dependency parsing , #AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing ."", ""They modify the model's distribution over trees p θ (y) by a penalty term as: p θ (y) ∝ p θ (y)e (δ e∈y length(e)) , where length(e) is the surface length of edge e."", 'The factor δ changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training.', 'These two approaches also have the goal of controlling unsupervised learning, and the form of the modified distributions is reminiscent of the form that the projected posteriors take.', 'However, the approaches differ substantially from PR. Smith and Eisner (2006) make a statement of the form ""scale the total length of edges"", which depending on the value of δ will prefer to have more shorter/longer edges.', 'Such statements are not data dependent.', 'Depending on the value of δ, for instance if δ ≤ 0, even if the data is such that the model already uses too many short edges on average, this value of δ will push for more short edges.', 'By contrast the statements we can make in PR are of the form ""there should be more short edges than long edges"".', 'Such a statement is data-dependent in the sense that if the model satisfies the constraints then we do not need to change it; if it is far from satisfying it we might need to make very dramatic changes.']",0,"[""For the task of unsupervised dependency parsing , #AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing .""]"
CC667,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,a corpus for modeling morphosyntactic agreement in arabic gender number and rationality,"['Sarah Alkuhlani', 'Nizar Habash']",conclusion,"We present an enriched version of the Penn Arabic Treebank (Maamouri et al., 2004), where latent features necessary for modeling morpho-syntactic agreement in Arabic are manually annotated. We describe our process for efficient annotation, and present the first quantitative analysis of Arabic morpho-syntactic phenomena.","We use the agreement checker code developed by #AUTHOR_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference .","['Grammaticality of parse trees.', 'We now turn to our second type of error analysis, the evaluation of the grammaticality of the parse trees in terms of gender and number agreement patterns.', 'We use the agreement checker code developed by #AUTHOR_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference .', 'The agreement checker verifies, for all verb-nominal subject relations and noun-adjective relations found in the tree, whether the agreement conditions are met or not.', 'The accuracy number reflects the percentage of such relations found which meet the agreement criteria.', 'Note that we use the syntax given by the tree, not the gold syntax.', 'For all three trees, however, we used gold morphological features for this evaluation even when those features were not used in the parsing task.', 'This is because we want to see to what extent the predicted morphological features help find the correct syntactic relations, not whether the predicted trees are intrinsically coherent given possibly false predicted morphology.', 'The results can be found in Table 18.', 'We note that the grammaticality of the gold corpus is not 100%; this is approximately equally due to errors in the checking script and to annotation errors in the gold standard.', 'We take the given grammaticality of the gold corpus as a topline for this analysis.', 'Nominal modification has a smaller error band between baseline and gold compared with subject-verb agreement.', 'We assume this is because subject-verb agreement is more complex (it depends on their relative order), and because nominal modification can have multiple structural targets, only one of which is correct, although all, however, are plausible from the point of view of agreement.', 'The error reduction relative to the gold topline is 62% and 76% for nominal agreement and verb agreement, respectively.', 'Thus, we see that our second hypothesis-that the use of morphological features will reduce grammaticality errors in the resulting parse trees with respect to agreement phenomena-is borne out.']",5,"['Grammaticality of parse trees.', 'We now turn to our second type of error analysis, the evaluation of the grammaticality of the parse trees in terms of gender and number agreement patterns.', 'We use the agreement checker code developed by #AUTHOR_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference .', 'The agreement checker verifies, for all verb-nominal subject relations and noun-adjective relations found in the tree, whether the agreement conditions are met or not.', 'Note that we use the syntax given by the tree, not the gold syntax.', 'For all three trees, however, we used gold morphological features for this evaluation even when those features were not used in the parsing task.', 'This is because we want to see to what extent the predicted morphological features help find the correct syntactic relations, not whether the predicted trees are intrinsically coherent given possibly false predicted morphology.', 'We note that the grammaticality of the gold corpus is not 100%; this is approximately equally due to errors in the checking script and to annotation errors in the gold standard.', 'We take the given grammaticality of the gold corpus as a topline for this analysis.', 'Nominal modification has a smaller error band between baseline and gold compared with subject-verb agreement.', 'We assume this is because subject-verb agreement is more complex (it depends on their relative order), and because nominal modification can have multiple structural targets, only one of which is correct, although all, however, are plausible from the point of view of agreement.', 'The error reduction relative to the gold topline is 62% and 76% for nominal agreement and verb agreement, respectively.', 'Thus, we see that our second hypothesis-that the use of morphological features will reduce grammaticality errors in the resulting parse trees with respect to agreement phenomena-is borne out.']"
CC668,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,buckwalter arabic morphological analyzer version 20 linguistic data consortium,['Timothy A Buckwalter'],experiments,,"In comparison, the tag set of the Buckwalter Morphological Analyzer ( #AUTHOR_TAG ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8","['Linguistically, words have associated POS tags, e.g., �verb� or �noun,� which further abstract over morphologically and syntactically similar lexemes.', 'Traditional Arabic grammars often describe a very general three-way distinction into verbs, nominals, and particles.', 'In comparison, the tag set of the Buckwalter Morphological Analyzer ( #AUTHOR_TAG ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8', 'Cross-linguistically, a core set containing around 12 tags is often assumed as a �universal tag set� (Rambow et al. 2006; Petrov, Das, and McDonald 2012).', 'We have adapted the list from Rambow et al. (2006) for Arabic, and call it here CORE12.', 'It contains the following tags: verb (V), noun (N), adjective (AJ), adverb (AV), proper noun (PN), pronoun (PRO), relative pronoun (REL), preposition (P), conjunction (C), particle (PRT), abbreviation (AB), and punctuation (PNX).', 'The CATIB6 tag set can be viewed as a further reduction, with the exception that CATIB6 contains a passive voice tag (a mor- phological feature); this tag constitutes only 0.5% of the tags in the training, however.']",1,"['Traditional Arabic grammars often describe a very general three-way distinction into verbs, nominals, and particles.', 'In comparison, the tag set of the Buckwalter Morphological Analyzer ( #AUTHOR_TAG ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8']"
CC669,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,better arabic parsing baselines evaluations and analysis,"['Spence Green', 'Christopher D Manning']",conclusion,"In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design. First, we identify sources of syntactic ambiguity understudied in the existing parsing literature. Second, we show that although the Penn Arabic Treebank is similar to other tree-banks in gross statistical terms, annotation consistency remains problematic. Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG. Fourth, we show how to build better models for three different parsers. Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2--5% F1.","For better comparison with work of others , we adopt the suggestion made by #AUTHOR_TAG to evaluate the parsing quality on sentences up to 70 tokens long .","['For better comparison with work of others , we adopt the suggestion made by #AUTHOR_TAG to evaluate the parsing quality on sentences up to 70 tokens long .', 'We report these filtered results in Table 14.', 'Filtered results are consistently higher (as expected).', 'Results are about 0.9% absolute higher on the development set, and about 0.6% higher on the test set.', 'The contribution of the RAT feature across sets is negligible (or small and unstable), resulting in less than 0.1% absolute loss on the dev set, but about 0.15% gain on the test set.', 'For clarity and conciseness, we only show the best model (with RAT) in Table 14.']",5,"['For better comparison with work of others , we adopt the suggestion made by #AUTHOR_TAG to evaluate the parsing quality on sentences up to 70 tokens long .']"
CC670,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,algorithms for deterministic incremental dependency parsing,['Joakim Nivre'],related work,"Abstract Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.","As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; #AUTHOR_TAG ) and the CATiB ( Habash and Roth 2009 ) .","['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; #AUTHOR_TAG ) and the CATiB ( Habash and Roth 2009 ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']",0,"['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; #AUTHOR_TAG ) and the CATiB ( Habash and Roth 2009 ) .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']"
CC671,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,catib the columbia arabic treebank,"['Nizar Habash', 'Ryan Roth']",experiments,"The Columbia Arabic Treebank (CATiB) is a resource for Arabic parsing. CATiB contrasts with previous efforts on Arabic treebanking and treebanking of morphologically rich languages in that it encodes less linguistic information in the interest of speedier annotation of large amounts of text. This paper describes CATiB's representation and annotation procedure, and reports on achieved inter-annotator agreement and annotation speed",We use the Columbia Arabic Treebank ( CATiB ) ( #AUTHOR_TAG ) .,"['We use the Columbia Arabic Treebank ( CATiB ) ( #AUTHOR_TAG ) .', 'Specifically, we use the portion converted from Part 3 of the PATB to the CATiB format, which enriches the CATiB dependency trees with full PATB morphological information.', ""CATiB's dependency representation is based on traditional Arabic grammar and emphasizes syntactic case relations."", 'It has a reduced POS tag set consisting of six tags only (henceforth CATIB6).', 'The tags are: NOM (non-proper nominals including nouns, pronouns, adjectives, and adverbs), PROP (proper nouns), VRB (active-voice verbs), VRB-PASS (passive-voice verbs), PRT (particles such as prepositions or conjunctions), and PNX (punctuation).', 'CATiB uses a standard set of eight dependency relations: SBJ and OBJ for subject and (direct or indirect) object, respectively (whether they appear pre-or postverbally); IDF for the idafa (possessive) relation; MOD for most other modifications; and other less common relations that we will not discuss here.', 'For other PATB-based POS tag sets, see Sections 2.6 and 2.7.']",5,['We use the Columbia Arabic Treebank ( CATiB ) ( #AUTHOR_TAG ) .']
CC672,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,a statistical parser for czech,"['Michael Collins', 'Jan Hajic', 'Lance Ramshaw', 'Christoph Tillmann']",introduction,"This paper considers statistical parsing of Czech, which differs radically from English in at least two respects: (1) it is a highly inflected language, and (2) it has relatively free word order. These differences are likely to pose new problems for techniques that have been developed on English. We describe our experience in building on the parsing model of (Collins 97). Our final results- 80% dependency accuracy - represent good progress towards the 91% accuracy of the parser on English (Wall Street Journal) text.","For example , modeling CASE in Czech improves Czech parsing ( #AUTHOR_TAG ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .","['Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors.', 'In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.', 'For example , modeling CASE in Czech improves Czech parsing ( #AUTHOR_TAG ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .', 'It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;.']",4,"['Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors.', 'In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.', 'For example , modeling CASE in Czech improves Czech parsing ( #AUTHOR_TAG ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .']"
CC673,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,the penn arabic treebank building a largescale annotated arabic corpus,"['Mohamed Maamouri', 'Ann Bies', 'Timothy A Buckwalter', 'Wigdan Mekki']",related work,"From our three year experience of developing a large-scale corpus of annotated Arabic text, our paper will address the following: (a) review pertinent Arabic language issues as they relate to methodology choices, (b) explain our choice to use the Penn English Treebank style of guidelines, (requiring the Arabic-speaking annotators to deal with a new grammatical system) rather than doing the annotation in a more traditional Arabic grammar style (requiring NLP researchers to deal with a new system); (c) show several ways in which human annotation is important and automatic analysis difficult, including the handling of orthographic ambiguity by both the morphological analyzer and human annotators; (d) give an illustrative example of the Arabic Treebank methodology, focusing on a particular construction in both morphological analysis and tagging and syntactic analysis and following it in detail through the entire annotation process, and finally, (e) conclude with what has been achieved so far and what remains to be done.","For all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1 ( #AUTHOR_TAG ) , converted to the CATiB Treebank format , as mentioned in Section 2.5 .","['For all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1 ( #AUTHOR_TAG ) , converted to the CATiB Treebank format , as mentioned in Section 2.5 .', 'We used the same training / devtest split as in Zitouni, Sorensen, and Sarikaya (2006); and we further split the devtest into two equal parts: a development (dev) set and a blind test set.', 'For all experiments, unless specified otherwise, we used the dev set. 10 We kept the test unseen (""blind"") during training and model development.', 'Statistics about this split (after conversion to the CATiB dependency format) are given in Table 1.']",5,"['For all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1 ( #AUTHOR_TAG ) , converted to the CATiB Treebank format , as mentioned in Section 2.5 .']"
CC674,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,a corpus for modeling morphosyntactic agreement in arabic gender number and rationality,"['Sarah Alkuhlani', 'Nizar Habash']",related work,"We present an enriched version of the Penn Arabic Treebank (Maamouri et al., 2004), where latent features necessary for modeling morpho-syntactic agreement in Arabic are manually annotated. We describe our process for efficient annotation, and present the first quantitative analysis of Arabic morpho-syntactic phenomena.","18 In this article , we use a newer version of the corpus by #AUTHOR_TAG than the one we used in Marton , Habash , and Rambow ( 2011 ) .","['18 In this article , we use a newer version of the corpus by #AUTHOR_TAG than the one we used in Marton , Habash , and Rambow ( 2011 ) .']",5,"['18 In this article , we use a newer version of the corpus by #AUTHOR_TAG than the one we used in Marton , Habash , and Rambow ( 2011 ) .']"
CC675,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,identifying broken plurals irregular gender and rationality in arabic text,"['Sarah Alkuhlani', 'Nizar Habash']",experiments,"Arabic morphology is complex, partly because of its richness, and partly because of common irregular word forms, such as broken plurals (which resemble singular nouns), and nouns with irregular gender (feminine nouns that look masculine and vice versa). In addition, Arabic morpho-syntactic agreement interacts with the lexical semantic feature of rationality, which has no morphological realization. In this paper, we present a series of experiments on the automatic prediction of the latent linguistic features of functional gender and number, and rationality in Arabic. We compare two techniques, using simple maximum likelihood (MLE) with back-off and a support vector machine based sequence tagger (Yamcha). We study a number of orthographic, morphological and syntactic learning features. Our results show that the MLE technique is preferred for words seen in the training data, while the Yam-cha technique is optimal for unseen words, which are our real target. Furthermore, we show that for unseen words, morphological features help beyond orthographic features and that syntactic features help even more. A combination of the two techniques improves overall performance even further.","In this article , we use an in-house system which provides functional gender , number , and rationality features ( #AUTHOR_TAG ) .","['Some inflectional features, specifically gender and number, are expressed using different morphemes in different words (even within the same POS).', 'There are four sound gender-number suffixes in Arabic: 5 +φ (null morpheme) for masculine singular, + + for feminine singular, + +wn for masculine plural, and + +At for feminine plural.', 'Form-based GENDER and NUMBER feature values are set only according to these four morphemes (and a few others, ignored for simplicity).', 'There are exceptions and alternative ways to express GENDER and NUMBER, however, and functional feature values take them into account: Depending on the lexeme, plurality can be expressed using sound plural suffixes or using a pattern change together with singular suffixes.', ""A sound plural example is the word pair / Hafiyd+a /Hafiyd+At ('granddaughter/granddaughters.)"", ""On the other hand, the plural of the inflectionally and morphemically feminine singular word madras+a ('school') is the word madAris+φ ('schools'), which is feminine and plural inflectionally, but has a masculine singular suffix."", 'This irregular inflection, known as broken plural, is similar to the English mouse/mice, but is much more common in Arabic (over 50% of plurals in our training data).', ""A similar inconsistency appears in feminine nominals that are not inflected using sound gender suffixes, for example, the feminine form of the masculine singular adjective Âzraq+φ ('blue') is zarqA'+φ not * *Âzraq+a ."", 'To address this inconsistency in the correspondence between inflectional features and morphemes, and inspired by Smrž (2007), we distinguish between two types of inflectional features: formbased (a.k.a.', 'surface, or illusory) features and functional features. 6', 'ost available Arabic NLP tools and resources model morphology using formbased (""surface"") inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005;Habash, Rambow, and Roth 2012).', 'The Elixir-FM analyzer (Smrž 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality.', 'In this article , we use an in-house system which provides functional gender , number , and rationality features ( #AUTHOR_TAG ) .', 'See Section 5.2 for more details.']",5,"['Some inflectional features, specifically gender and number, are expressed using different morphemes in different words (even within the same POS).', 'There are four sound gender-number suffixes in Arabic: 5 +ph (null morpheme) for masculine singular, + + for feminine singular, + +wn for masculine plural, and + +At for feminine plural.', 'There are exceptions and alternative ways to express GENDER and NUMBER, however, and functional feature values take them into account: Depending on the lexeme, plurality can be expressed using sound plural suffixes or using a pattern change together with singular suffixes.', ""On the other hand, the plural of the inflectionally and morphemically feminine singular word madras+a ('school') is the word madAris+ph ('schools'), which is feminine and plural inflectionally, but has a masculine singular suffix."", 'This irregular inflection, known as broken plural, is similar to the English mouse/mice, but is much more common in Arabic (over 50% of plurals in our training data).', ""A similar inconsistency appears in feminine nominals that are not inflected using sound gender suffixes, for example, the feminine form of the masculine singular adjective Azraq+ph ('blue') is zarqA'+ph not * *Azraq+a ."", 'ost available Arabic NLP tools and resources model morphology using formbased (""surface"") inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005;Habash, Rambow, and Roth 2012).', 'In this article , we use an in-house system which provides functional gender , number , and rationality features ( #AUTHOR_TAG ) .']"
CC676,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,catib the columbia arabic treebank,"['Nizar Habash', 'Ryan Roth']",related work,"The Columbia Arabic Treebank (CATiB) is a resource for Arabic parsing. CATiB contrasts with previous efforts on Arabic treebanking and treebanking of morphologically rich languages in that it encodes less linguistic information in the interest of speedier annotation of large amounts of text. This paper describes CATiB's representation and annotation procedure, and reports on achieved inter-annotator agreement and annotation speed","As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( #AUTHOR_TAG ) .","['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( #AUTHOR_TAG ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']",0,"['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( #AUTHOR_TAG ) .']"
CC677,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,an efficient algorithm for easyfirst nondirectional dependency parsing,"['Yoav Goldberg', 'Michael Elhadad']",related work,,"Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #AUTHOR_TAG ) ( Section 6 ) .","['Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', 'We go beyond previous work, however, and explore additional lexical and inflectional features.', 'Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;).', 'Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task.', 'Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #AUTHOR_TAG ) ( Section 6 ) .', 'Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '9 We expect this kind of feature to yield lower gains for Arabic, unless: r one uses functional feature values (such as those used here for the first time in Arabic NLP), r one uses yet another representation level to account for the otherwise non-identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as VS (where the verb precedes its subject), and r one adequately represents the otherwise ""inverse"" number agreement (a phenomenon common to other Semitic languages, such as Hebrew, too).']",5,"['Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #AUTHOR_TAG ) ( Section 6 ) .', 'Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.']"
CC678,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,a corpus for modeling morphosyntactic agreement in arabic gender number and rationality,"['Sarah Alkuhlani', 'Nizar Habash']",related work,"We present an enriched version of the Penn Arabic Treebank (Maamouri et al., 2004), where latent features necessary for modeling morpho-syntactic agreement in Arabic are manually annotated. We describe our process for efficient annotation, and present the first quantitative analysis of Arabic morpho-syntactic phenomena.","To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality ( #AUTHOR_TAG ) .18 This is the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender ) .","['The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values.', 'To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality ( #AUTHOR_TAG ) .18 This is the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender ) .', 'We conducted experiments with gold features to assess the potential of these features, and with predicted fea- tures, obtained from training a simple maximum likelihood estimation classifier on this resource (Alkuhlani and Habash 2012).', '19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input).', 'The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts.', 'The fourth part of the table shows the combination of these functional features with the other features that participated in the best combination so far (LMM, the extended DET2, and PERSON); without RAT, this combination is at least as useful as its form-based counterpart, in both gold and predicted input; adding RAT to this combination yields 0.4% (absolute) gain in gold, offering further support to the relevance of the rationality feature, but a slight decrease in predicted input, presumably due to insufficient accuracy again.', 'The last part of the table revalidates the gains achieved with the best controlled feature combination, using CATIBEX�the best performing tag set with predicted in- put.', 'Note, however, that the 1% (absolute) advantage of CATIBEX (without additional features) over the morphology-free CORE12 on machine-predicted input (Table 2) has shrunk with these functional feature combinations to 0.3%.', 'We take it as further support to the relevance of our functional morphology features, and their partial redundancy with the form-based morphological information embedded in the CATIBEX POS tags.']",5,"['The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values.', 'To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality ( #AUTHOR_TAG ) .18 This is the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender ) .', 'The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts.']"
CC679,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,arabic tokenization partofspeech tagging and morphological disambiguation in one fell swoop,"['Nizar Habash', 'Owen Rambow']",related work,"We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including part-of-speech tagging) Arabic words in one process. We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from the output of the analyzer. We obtain accuracy rates on all tasks in the high nineties.","Put differently, we are interested in the tradeoff between relevance and accu- racy.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit ( #AUTHOR_TAG ; Habash, Rambow, and Roth 2012)15 (see Table 2, columns 5�7).","['So far we discussed optimal (gold) conditions.', 'But in prac- tice, POS tags are annotated by automatic taggers, so parsers get predicted POS tags as input, as opposed to gold (human-annotated) tags.14', 'The more informative the tag set, the less accurate the tag prediction might be, so the effect on overall parsing quality is unclear.', 'Put differently, we are interested in the tradeoff between relevance and accu- racy.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit ( #AUTHOR_TAG ; Habash, Rambow, and Roth 2012)15 (see Table 2, columns 5�7).', 'It turned out that BW, the best gold performer but with lowest POS pre- diction accuracy (81.8%), suffered the biggest drop (11.4%) and was the worst performer with predicted tags.', 'The simplest tag set, CATIB6, and its extension, CATIBEX, benefited from the highest POS prediction accuracy (97.7%), and their performance suffered the least.', 'CATIBEX was the best performer with predicted POS tags.', 'Performance drop and POS prediction accuracy are given in columns 8 and 9.']",5,"['But in prac- tice, POS tags are annotated by automatic taggers, so parsers get predicted POS tags as input, as opposed to gold (human-annotated) tags.14', 'The more informative the tag set, the less accurate the tag prediction might be, so the effect on overall parsing quality is unclear.', 'Put differently, we are interested in the tradeoff between relevance and accu- racy.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit ( #AUTHOR_TAG ; Habash, Rambow, and Roth 2012)15 (see Table 2, columns 5�7).', 'CATIBEX was the best performer with predicted POS tags.']"
CC680,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,algorithms for deterministic incremental dependency parsing,['Joakim Nivre'],related work,"Abstract Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.","For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #AUTHOR_TAG .","['All results are reported mainly in terms of labeled attachment accuracy score (the parent word and the type of dependency relation to it, abbreviated as LAS), which is also used for greedy (hill-climbing) decisions for feature combination.', 'Unlabeled attachment accuracy score (UAS) and label accuracy (dependency relation regardless of parent, LS) are also given.', ""For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #AUTHOR_TAG ."", 'We denote p < 0.05 and p < 0.01 with + and ++ , respectively.']",5,"[""For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #AUTHOR_TAG .""]"
CC681,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,parsing indian languages with maltparser,['Joakim Nivre'],related work,"This paper describes the application of MaltParser, a transition-based dependency parser, to three Indian languages - Bangla, Hindi and Telugu - in the context of the NLP Tools Contest at ICON 2009. In the final evaluation, MaltParser was ranked second among the participating systems and achieved an unlabeled attachment score close to 90% for Bangla and Hindi, and over 85% for Telugu, while the labeled attachment score was 15-25 percentage points lower. It is likely that the high unlabeled accuracy is achieved thanks to a relatively low syntactic complexity in the data sets, while the low labeled accuracy is due to the limited amounts of training data.","For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( Nivre 2003 , 2008 ; KÃ¼bler , McDonald , and #AUTHOR_TAG ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).","['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( Nivre 2003 , 2008 ; KÃ¼bler , McDonald , and #AUTHOR_TAG ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).', 'We do not perform further weight optimization (which, if done, is done on a separate ""tuning set"").', 'to predict the next state in the parse derivation.', 'All experiments were done using the Nivre ""eager"" algorithm.', '11 There are five default attributes in the MaltParser terminology for each token in the text: word ID (ordinal position in the sentence), word-form, POS tag, head (parent word ID), and deprel (the dependency relation between the current word and its parent).', 'There are default MaltParser features (in the machine learning sense), 12 which are the values of functions over these attributes, serving as input to the MaltParser internal classifiers.', 'The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.).', 'Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on.', 'Kübler, McDonald, and Nivre (2009) describe a ""typical"" MaltParser model configuration of attributes and features.', '13 Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags.', 'For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0].', 'We did not change the features for deprel: stk[0], ldep(stk[0]), rdep(stk[0]), ldep(buf[0]), rdep(buf[0]) (where ldep and rdep are the left and right, respectively, dependents of the specified argument).', 'This new MaltParser configuration resulted in gains of 0.3-1.1% in labeled attachment accuracy (depending on the POS tag set) over the default MaltParser configuration.', 'We also experimented with using normalized word-forms (Alif Maqsura conversion to Ya, and Hamza removal from each Alif ) as is common in parsing and statistical machine translation literature, but it resulted in a small decrease in performance, so we settled on using non-normalized word-forms.', 'All experiments reported here were conducted using this new configuration.', 'To recap, it has the following MaltParser attributes (machine learning features): 4 word-form attributes, POS tag attributes, and 5 deprel attributes (some of which are not useful for the Nivre ""eager"" algorithm), totaling 16 attributes and two more for every new feature described in Section 4.3 and on (e.g., CASE).']",5,"['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( Nivre 2003 , 2008 ; KÃ¼bler , McDonald , and #AUTHOR_TAG ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).']"
CC682,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,identifying broken plurals irregular gender and rationality in arabic text,"['Sarah Alkuhlani', 'Nizar Habash']",related work,"Arabic morphology is complex, partly because of its richness, and partly because of common irregular word forms, such as broken plurals (which resemble singular nouns), and nouns with irregular gender (feminine nouns that look masculine and vice versa). In addition, Arabic morpho-syntactic agreement interacts with the lexical semantic feature of rationality, which has no morphological realization. In this paper, we present a series of experiments on the automatic prediction of the latent linguistic features of functional gender and number, and rationality in Arabic. We compare two techniques, using simple maximum likelihood (MLE) with back-off and a support vector machine based sequence tagger (Yamcha). We study a number of orthographic, morphological and syntactic learning features. Our results show that the MLE technique is preferred for words seen in the training data, while the Yam-cha technique is optimal for unseen words, which are our real target. Furthermore, we show that for unseen words, morphological features help beyond orthographic features and that syntactic features help even more. A combination of the two techniques improves overall performance even further.","We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #AUTHOR_TAG ) .19","['The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values.', 'To address this issue, we use a version of the PATB3 training and dev sets manually annotated with functional gender, number, and rationality (Alkuhlani and Habash 2011).18', 'This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender).', 'We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #AUTHOR_TAG ) .19', 'The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input).', 'The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts.', 'The fourth part of the table shows the combination of these functional features with the other features that participated in the best combination so far (LMM, the extended DET2, and PERSON); without RAT, this combination is at least as useful as its form-based counterpart, in both gold and predicted input; adding RAT to this combination yields 0.4% (absolute) gain in gold, offering further support to the relevance of the rationality feature, but a slight decrease in predicted input, presumably due to insufficient accuracy again.', 'The last part of the table revalidates the gains achieved with the best controlled feature combination, using CATIBEX�the best performing tag set with predicted in- put.', 'Note, however, that the 1% (absolute) advantage of CATIBEX (without additional features) over the morphology-free CORE12 on machine-predicted input (Table 2) has shrunk with these functional feature combinations to 0.3%.', 'We take it as further support to the relevance of our functional morphology features, and their partial redundancy with the form-based morphological information embedded in the CATIBEX POS tags.']",5,"['The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values.', 'To address this issue, we use a version of the PATB3 training and dev sets manually annotated with functional gender, number, and rationality (Alkuhlani and Habash 2011).18', 'This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender).', 'We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #AUTHOR_TAG ) .19', 'The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input).', 'The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts.', 'We take it as further support to the relevance of our functional morphology features, and their partial redundancy with the form-based morphological information embedded in the CATIBEX POS tags.']"
CC683,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,the penn arabic treebank building a largescale annotated arabic corpus,"['Mohamed Maamouri', 'Ann Bies', 'Timothy A Buckwalter', 'Wigdan Mekki']",experiments,"From our three year experience of developing a large-scale corpus of annotated Arabic text, our paper will address the following: (a) review pertinent Arabic language issues as they relate to methodology choices, (b) explain our choice to use the Penn English Treebank style of guidelines, (requiring the Arabic-speaking annotators to deal with a new grammatical system) rather than doing the annotation in a more traditional Arabic grammar style (requiring NLP researchers to deal with a new system); (c) show several ways in which human annotation is important and automatic analysis difficult, including the handling of orthographic ambiguity by both the morphological analyzer and human annotators; (d) give an illustrative example of the Arabic Treebank methodology, focusing on a particular construction in both morphological analysis and tagging and syntactic analysis and following it in detail through the entire annotation process, and finally, (e) conclude with what has been achieved so far and what remains to be done.","Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) ( #AUTHOR_TAG ) , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005;  Habash, Rambow, and Roth 2012).","['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) ( #AUTHOR_TAG ) , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005;  Habash, Rambow, and Roth 2012).', 'The Elixir-FM analyzer (Smr_ 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality.', 'In this article, we use an in-house system which provides functional gender, number, and rationality features (Alkuhlani and Habash 2012).', 'See Section 5.2 for more details.']",1,"['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) ( #AUTHOR_TAG ) , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005;  Habash, Rambow, and Roth 2012).']"
CC684,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,arabic tokenization partofspeech tagging and morphological disambiguation in one fell swoop,"['Nizar Habash', 'Owen Rambow']",experiments,"We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including part-of-speech tagging) Arabic words in one process. We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from the output of the analyzer. We obtain accuracy rates on all tasks in the high nineties.","Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit ( #AUTHOR_TAG ;  Habash, Rambow, and Roth 2012).","['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit ( #AUTHOR_TAG ;  Habash, Rambow, and Roth 2012).', 'The Elixir-FM analyzer (Smr_ 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality.', 'In this article, we use an in-house system which provides functional gender, number, and rationality features (Alkuhlani and Habash 2012).', 'See Section 5.2 for more details.']",1,"['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit ( #AUTHOR_TAG ;  Habash, Rambow, and Roth 2012).']"
CC685,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,catib the columbia arabic treebank,"['Nizar Habash', 'Ryan Roth']",experiments,"The Columbia Arabic Treebank (CATiB) is a resource for Arabic parsing. CATiB contrasts with previous efforts on Arabic treebanking and treebanking of morphologically rich languages in that it encodes less linguistic information in the interest of speedier annotation of large amounts of text. This paper describes CATiB's representation and annotation procedure, and reports on achieved inter-annotator agreement and annotation speed","The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( #AUTHOR_TAG ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( Buckwalter 2004 ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both outperforming it : ( d ) Kulick , Gabbard , and Marcus ( 2006 ) 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 .","[""The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( #AUTHOR_TAG ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( Buckwalter 2004 ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both outperforming it : ( d ) Kulick , Gabbard , and Marcus ( 2006 ) 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 .""]",5,"[""The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( #AUTHOR_TAG ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( Buckwalter 2004 ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both outperforming it : ( d ) Kulick , Gabbard , and Marcus ( 2006 ) 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 .""]"
CC686,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,buckwalter arabic morphological analyzer version 20 linguistic data consortium,['Timothy A Buckwalter'],experiments,,"Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer ( #AUTHOR_TAG ) , and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012).","['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer ( #AUTHOR_TAG ) , and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012).', 'The Elixir-FM analyzer (Smr_ 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality.', 'In this article, we use an in-house system which provides functional gender, number, and rationality features (Alkuhlani and Habash 2012).', 'See Section 5.2 for more details.']",1,"['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer ( #AUTHOR_TAG ) , and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012).']"
CC687,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,conllx shared task on multilingual dependency parsing,"['Sabine Buchholz', 'Erwin Marsi']",related work,"Each year the Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their systems on exactly the same data sets, in order to better compare systems. The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing. In this paper, we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured. We also give an overview of the parsing approaches that participants took and the results that they achieved. Finally, we try to draw general conclusions about multi-lingual parsing: What makes a particular language, treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser?","As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( #AUTHOR_TAG ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .","['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( #AUTHOR_TAG ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']",0,"['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( #AUTHOR_TAG ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']"
CC688,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,parsing indian languages with maltparser,['Joakim Nivre'],introduction,"This paper describes the application of MaltParser, a transition-based dependency parser, to three Indian languages - Bangla, Hindi and Telugu - in the context of the NLP Tools Contest at ICON 2009. In the final evaluation, MaltParser was ranked second among the participating systems and achieved an unlabeled attachment score close to 90% for Bangla and Hindi, and over 85% for Telugu, while the labeled attachment score was 15-25 percentage points lower. It is likely that the high unlabeled accuracy is achieved thanks to a relatively low syntactic complexity in the data sets, while the low labeled accuracy is due to the limited amounts of training data.","It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .","['Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors.', 'In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.', 'For example, modeling CASE in Czech improves Czech parsing (Collins et al. 1999): CASE is relevant, not redundant, and can be predicted with sufficient accuracy.', 'It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .']",4,"['It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .']"
CC689,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,algorithms for deterministic incremental dependency parsing,['Joakim Nivre'],introduction,"Abstract Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.",The result holds for both the MaltParser ( #AUTHOR_TAG ) and the Easy-First Parser ( Goldberg and Elhadad 2010 ) .,"['results, assignment features, specifically CASE, are very helpful in MSA, though only under gold conditions: Because CASE is rarely explicit in the typically undiacritized written MSA, it has a dismal accuracy rate, which makes it useless when used in a machine-predicted (real, non-gold) condition.', 'In contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions.', 'This is likely a result of MSA having a rich agreement system, covering both verb-subject and noun-adjective relations.', 'The result holds for both the MaltParser ( #AUTHOR_TAG ) and the Easy-First Parser ( Goldberg and Elhadad 2010 ) .']",5,['The result holds for both the MaltParser ( #AUTHOR_TAG ) and the Easy-First Parser ( Goldberg and Elhadad 2010 ) .']
CC690,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,buckwalter arabic morphological analyzer version 20 linguistic data consortium,['Timothy A Buckwalter'],experiments,,"The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( Habash and Roth 2009 ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( #AUTHOR_TAG ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both outperforming it : ( d ) Kulick , Gabbard , and Marcus ( 2006 ) 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 .","[""The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( Habash and Roth 2009 ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( #AUTHOR_TAG ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both outperforming it : ( d ) Kulick , Gabbard , and Marcus ( 2006 ) 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 .""]",5,"[""The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( Habash and Roth 2009 ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( #AUTHOR_TAG ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both outperforming it : ( d ) Kulick , Gabbard , and Marcus ( 2006 ) 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 .""]"
CC691,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,a statistical parser for czech,"['Michael Collins', 'Jan Hajic', 'Lance Ramshaw', 'Christoph Tillmann']",related work,"This paper considers statistical parsing of Czech, which differs radically from English in at least two respects: (1) it is a highly inflected language, and (2) it has relatively free word order. These differences are likely to pose new problems for techniques that have been developed on English. We describe our experience in building on the parsing model of (Collins 97). Our final results- 80% dependency accuracy - represent good progress towards the 91% accuracy of the parser on English (Wall Street Journal) text.",#AUTHOR_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) .,"['Much work has been done on the use of morphological features for parsing of morphologically rich languages.', '#AUTHOR_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) .', 'This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size ≈3000+).', 'They also report that the use of gender, number, and person features did not yield any improvements.', 'The results for Czech are the opposite of our results for Arabic, as we will see.', 'This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajič and Vidová-Hladká 1998) compared with Arabic (≈14.0%,', 'see Table 3).', 'Similarly, Cowan and Collins (2005) report that the use of a subset of Spanish morphological features (number for adjectives, determiners, nouns, pronouns, and verbs; and mode for verbs) outperforms other combinations.', 'Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.', 'We also find that the number feature helps for Arabic.', ""Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima'an (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing.""]",0,['#AUTHOR_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) .']
CC692,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,getting more from morphology in multilingual dependency parsing,"['Matt Hohensee', 'Emily M Bender']",related work,"We propose a linguistically motivated set of features to capture morphological agreement and add them to the MSTParser dependency parser. Compared to the built-in morphological feature set, ours is both much smaller and more accurate across a sample of 20 morphologically annotated treebanks. We find increases in accuracy of up to 5.3% absolute. While some of this results from the feature set capturing information unrelated to morphology, there is still significant improvement, up to 4.6% absolute, due to the agreement model.","9 We do not relate to specific results in their study because it has been brought to our attention that #AUTHOR_TAG are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication ) .","['9 We do not relate to specific results in their study because it has been brought to our attention that #AUTHOR_TAG are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication ) .']",1,"['9 We do not relate to specific results in their study because it has been brought to our attention that #AUTHOR_TAG are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication ) .']"
CC693,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,an efficient algorithm for easyfirst nondirectional dependency parsing,"['Yoav Goldberg', 'Michael Elhadad']",introduction,,The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser ( #AUTHOR_TAG ) .,"['results, assignment features, specifically CASE, are very helpful in MSA, though only under gold conditions: Because CASE is rarely explicit in the typically undiacritized written MSA, it has a dismal accuracy rate, which makes it useless when used in a machine-predicted (real, non-gold) condition.', 'In contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions.', 'This is likely a result of MSA having a rich agreement system, covering both verb-subject and noun-adjective relations.', 'The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser ( #AUTHOR_TAG ) .']",5,['The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser ( #AUTHOR_TAG ) .']
CC694,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,introduction to arabic natural language processing,['Nizar Habash'],experiments,"Abstract This book provides system developers and researchers in natural language processing and computational linguistics with the necessary background information for working with the Arabic language. The goal is to introduce Arabic linguistic phenomena and review the state-of-the-art in Arabic processing. The book discusses Arabic script, phonology, orthography, morphology, syntax and semantics, with a final chapter on machine translation issues. The chapter sizes correspond more or less to what is linguistically distinctive about Arabic, with morphology getting the lion's share, followed by Arabic script. No previous knowledge of Arabic is needed. This book is designed for computer scientists and linguists alike. The focus of the book is on Modern Standard Arabic; however, notes on practical issues related to Arabic dialects and languages written in the Arabic script are presented in different chapters. Table of Contents: What is ""Arabic""? / Arabic Script / Arabic Phonology and Orthography / Arabic Mo...",A more detailed discussion of the various available Arabic tag sets can be found in #AUTHOR_TAG .,"['The notion of ""POS tag set"" in natural language processing usually does not refer to a core set.', 'Instead, the Penn English Treebank (PTB) uses a set of 46 tags, including not only the core POS, but also the complete set of morphological features (this tag set is still fairly small since English is morphologically impoverished).', 'In PATB-tokenized MSA, the corresponding type of tag set (core POS extended with a complete description of morphology) would contain upwards of 2,000 tags, many of which are extremely rare (in our training corpus of about 300,000 words, we encounter only POS tags with complete morphology).', 'Therefore, researchers have proposed tag sets for MSA whose size is similar to that of the English PTB tag set, as this has proven to be a useful size computationally.', 'These tag sets are hybrids in the sense that they are neither simply the core POS, nor the complete morphologically enriched tag set, but instead they selectively enrich the core POS tag set with only certain morphological features.', 'A more detailed discussion of the various available Arabic tag sets can be found in #AUTHOR_TAG .']",0,['A more detailed discussion of the various available Arabic tag sets can be found in #AUTHOR_TAG .']
CC695,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,catib the columbia arabic treebank,"['Nizar Habash', 'Ryan Roth']",experiments,"The Columbia Arabic Treebank (CATiB) is a resource for Arabic parsing. CATiB contrasts with previous efforts on Arabic treebanking and treebanking of morphologically rich languages in that it encodes less linguistic information in the interest of speedier annotation of large amounts of text. This paper describes CATiB's representation and annotation procedure, and reports on achieved inter-annotator agreement and annotation speed","For more information on CATiB , see #AUTHOR_TAG and Habash , Faraj , and Roth ( 2009 ) .","['The CATiB Treebank uses the word segmentation of the PATB.', ""It splits off several categories of orthographic clitics, but not the definite article + Al+ ('the')."", 'In all of the experiments reported in this article, we use the gold segmentation.', 'Tokenization involves further decisions on the segmented token forms, such as spelling normalization, which we only briefly touch on here (in Section 4.1).', 'An example CATiB dependency tree is shown in Figure 1.', 'For the corpus statistics, see Table 1.', 'For more information on CATiB , see #AUTHOR_TAG and Habash , Faraj , and Roth ( 2009 ) .']",0,"['The CATiB Treebank uses the word segmentation of the PATB.', 'An example CATiB dependency tree is shown in Figure 1.', 'For the corpus statistics, see Table 1.', 'For more information on CATiB , see #AUTHOR_TAG and Habash , Faraj , and Roth ( 2009 ) .']"
CC696,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,an efficient algorithm for easyfirst nondirectional dependency parsing,"['Yoav Goldberg', 'Michael Elhadad']",experiments,,"Some researchers , however , including #AUTHOR_TAG , train on predicted feature values instead .","['So far, we have only evaluated models trained on gold POS tag set and morphological feature values.', 'Some researchers , however , including #AUTHOR_TAG , train on predicted feature values instead .', 'It makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test.', 'But we argue that it also makes sense that training on a combination of gold and predicted features (one copy of each) might do even better, because good predictions of feature values are reinforced (since they repeat the gold patterns), whereas noisy predicted feature values are still represented in training (in patterns that do not repeat the gold). 21', 'To test our hypothesis, we start this section by comparing three variations:']",1,"['So far, we have only evaluated models trained on gold POS tag set and morphological feature values.', 'Some researchers , however , including #AUTHOR_TAG , train on predicted feature values instead .', 'To test our hypothesis, we start this section by comparing three variations:']"
CC697,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,identifying broken plurals irregular gender and rationality in arabic text,"['Sarah Alkuhlani', 'Nizar Habash']",related work,"Arabic morphology is complex, partly because of its richness, and partly because of common irregular word forms, such as broken plurals (which resemble singular nouns), and nouns with irregular gender (feminine nouns that look masculine and vice versa). In addition, Arabic morpho-syntactic agreement interacts with the lexical semantic feature of rationality, which has no morphological realization. In this paper, we present a series of experiments on the automatic prediction of the latent linguistic features of functional gender and number, and rationality in Arabic. We compare two techniques, using simple maximum likelihood (MLE) with back-off and a support vector machine based sequence tagger (Yamcha). We study a number of orthographic, morphological and syntactic learning features. Our results show that the MLE technique is preferred for words seen in the training data, while the Yam-cha technique is optimal for unseen words, which are our real target. Furthermore, we show that for unseen words, morphological features help beyond orthographic features and that syntactic features help even more. A combination of the two techniques improves overall performance even further.","19 The paper by #AUTHOR_TAG presents additional , more sophisticated models that we do not use in this article .","['19 The paper by #AUTHOR_TAG presents additional , more sophisticated models that we do not use in this article .']",1,"['19 The paper by #AUTHOR_TAG presents additional , more sophisticated models that we do not use in this article .']"
CC698,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,towards an optimal pos tag set for modern standard arabic processing,['Mona Diab'],related work,,"As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; #AUTHOR_TAG ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .","['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; #AUTHOR_TAG ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']",0,"['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; #AUTHOR_TAG ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .']"
CC699,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,algorithms for deterministic incremental dependency parsing,['Joakim Nivre'],related work,"Abstract Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.","11 #AUTHOR_TAG reports that non-projective and pseudo-projective algorithms outperform the `` eager  projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies .","['11 #AUTHOR_TAG reports that non-projective and pseudo-projective algorithms outperform the `` eager  projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies .', 'The Nivre �standard� algorithm is also reported there to do better on Arabic, but in a preliminary experimentation, it did slightly worse than the �eager� one, perhaps due to the high percentage of right branching (left headed structures) in our Arabic training set�an observation already noted in Nivre (2008).']",1,"['11 #AUTHOR_TAG reports that non-projective and pseudo-projective algorithms outperform the `` eager  projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies .']"
CC700,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,parsing indian languages with maltparser,['Joakim Nivre'],related work,"This paper describes the application of MaltParser, a transition-based dependency parser, to three Indian languages - Bangla, Hindi and Telugu - in the context of the NLP Tools Contest at ICON 2009. In the final evaluation, MaltParser was ranked second among the participating systems and achieved an unlabeled attachment score close to 90% for Bangla and Hindi, and over 85% for Telugu, while the labeled attachment score was 15-25 percentage points lower. It is likely that the high unlabeled accuracy is achieved thanks to a relatively low syntactic complexity in the data sets, while the low labeled accuracy is due to the limited amounts of training data.","Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K�bler, McDonald, and #AUTHOR_TAG describe a �typical� MaltParser model configuration of attributes and features.13","['There are five default attributes in the MaltParser terminology for each token in the text: word ID (ordinal position in the sentence), word-form, POS tag, head (parent word ID), and deprel (the dependency relation between the current word and its parent).', 'There are default MaltParser features (in the machine learning sense),12 which are the values of functions over these attributes, serving as input to the MaltParser internal classifiers.', 'The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.).', 'Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K�bler, McDonald, and #AUTHOR_TAG describe a �typical� MaltParser model configuration of attributes and features.13', 'Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags.', 'For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0].', 'We did not change the features for deprel: stk[0], ldep(stk[0]), rdep(stk[0]), ldep(buf[0]), rdep(buf[0]) (where ldep and rdep are the left and right, respectively, dependents of the specified argument).', 'This new MaltParser configuration resulted in gains of 0.3�1.1% in labeled attachment accuracy (depending on the POS tag set) over the default MaltParser configuration.', 'We also experimented with using normalized word-forms (Alif Maqsura conversion to Ya, and Hamza removal from each Alif ) as is common in parsing and statistical machine translation literature, but it resulted in a small decrease in performance, so we settled on using non-normalized word-forms.', 'All experiments reported here were conducted using this new configuration. To recap, it has the following MaltParser attributes (machine learning features): 4 word-form attributes, 7 POS tag attributes, and 5 deprel attributes (some of which are not useful for the Nivre �eager� algorithm), totaling 16 attributes and two more for every new feature described in Section 4.3 and on (e.g., CASE).']",5,"['Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K�bler, McDonald, and #AUTHOR_TAG describe a �typical� MaltParser model configuration of attributes and features.13', 'All experiments reported here were conducted using this new configuration. To recap, it has the following MaltParser attributes (machine learning features): 4 word-form attributes, 7 POS tag attributes, and 5 deprel attributes (some of which are not useful for the Nivre eager algorithm), totaling 16 attributes and two more for every new feature described in Section 4.3 and on (e.g., CASE).']"
CC701,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,better arabic parsing baselines evaluations and analysis,"['Spence Green', 'Christopher D Manning']",related work,"In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design. First, we identify sources of syntactic ambiguity understudied in the existing parsing literature. Second, we show that although the Penn Arabic Treebank is similar to other tree-banks in gross statistical terms, annotation consistency remains problematic. Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG. Fourth, we show how to build better models for three different parsers. Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2--5% F1.","As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .","['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']",0,"['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.']"
CC702,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,an efficient algorithm for projective dependency parsing,['Joakim Nivre'],related work,"This paper presents a deterministic parsing algorithm for projective dependency grammar. The running time of the algorithm is linear in the length of the input string, and the dependency graph produced is guaranteed to be projective and acyclic. The algorithm has been experimentally evaluated in parsing unrestricted Swedish text, achieving an accuracy above 85% with a very simple grammar.","For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #AUTHOR_TAG , 2008 ; KÃ¼bler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).","['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #AUTHOR_TAG , 2008 ; KÃ¼bler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).', 'We do not perform further weight optimization (which, if done, is done on a separate ""tuning set"").', 'to predict the next state in the parse derivation.', 'All experiments were done using the Nivre ""eager"" algorithm.', '11 There are five default attributes in the MaltParser terminology for each token in the text: word ID (ordinal position in the sentence), word-form, POS tag, head (parent word ID), and deprel (the dependency relation between the current word and its parent).', 'There are default MaltParser features (in the machine learning sense), 12 which are the values of functions over these attributes, serving as input to the MaltParser internal classifiers.', 'The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.).', 'Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on.', 'Kübler, McDonald, and Nivre (2009) describe a ""typical"" MaltParser model configuration of attributes and features.', '13 Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags.', 'For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0].', 'We did not change the features for deprel: stk[0], ldep(stk[0]), rdep(stk[0]), ldep(buf[0]), rdep(buf[0]) (where ldep and rdep are the left and right, respectively, dependents of the specified argument).', 'This new MaltParser configuration resulted in gains of 0.3-1.1% in labeled attachment accuracy (depending on the POS tag set) over the default MaltParser configuration.', 'We also experimented with using normalized word-forms (Alif Maqsura conversion to Ya, and Hamza removal from each Alif ) as is common in parsing and statistical machine translation literature, but it resulted in a small decrease in performance, so we settled on using non-normalized word-forms.', 'All experiments reported here were conducted using this new configuration.', 'To recap, it has the following MaltParser attributes (machine learning features): 4 word-form attributes, POS tag attributes, and 5 deprel attributes (some of which are not useful for the Nivre ""eager"" algorithm), totaling 16 attributes and two more for every new feature described in Section 4.3 and on (e.g., CASE).']",5,"['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #AUTHOR_TAG , 2008 ; KÃ¼bler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).']"
CC703,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,atanas chanev gulsen eryigit sandra kubler svetoslav marinov and erwin marsi,"['Joakim Nivre', 'Johan Hall', 'Jens Nilsson']",related work,,"Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( #AUTHOR_TAG ) , trained on the PADT .","['As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006;Diab 2007;Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006;Nivre 2008) and the CATiB .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( #AUTHOR_TAG ) , trained on the PADT .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']",0,"['As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006;Diab 2007;Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006;Nivre 2008) and the CATiB .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( #AUTHOR_TAG ) , trained on the PADT .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']"
CC704,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,morphology and reranking for the statistical parsing of spanish,"['Brooke Cowan', 'Michael Collins']",related work,"We present two methods for incorporating detailed features in a Spanish parser, building on a baseline model that is a lexicalized PCFG. The first method exploits Spanish morphology, and achieves an F1 constituency score of 83.6%. This is an improvement over 81.2 % accuracy for the baseline, which makes little or no use of morphological information. The second model uses a reranking approach to add arbitrary global features of parse trees to the morphological model. The reranking model reaches 85.1 % F1 accuracy on the Spanish parsing task. The resulting model for Spanish parsing combines an approach that specifically targets morphological information with an approach that makes use of general structural features","Similarly , #AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .","['Much work has been done on the use of morphological features for parsing of morphologically rich languages.', 'Collins et al. (1999) report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature (when applicable).', 'This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size �3000+).', 'They also report that the use of gender, number, and person features did not yield any improvements.', 'The results for Czech are the opposite of our results for Arabic, as we will see.', 'This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajic� and Vidov�-Hladk� 1998) compared with Arabic (�14.0%, see Table 3).', 'Similarly , #AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .', 'Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.', 'We also find that the number feature helps for Arabic.', 'Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima�an (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing.']",0,"['Much work has been done on the use of morphological features for parsing of morphologically rich languages.', 'Collins et al. (1999) report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature (when applicable).', 'Similarly , #AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .']"
CC705,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,introduction to arabic natural language processing,['Nizar Habash'],experiments,"Abstract This book provides system developers and researchers in natural language processing and computational linguistics with the necessary background information for working with the Arabic language. The goal is to introduce Arabic linguistic phenomena and review the state-of-the-art in Arabic processing. The book discusses Arabic script, phonology, orthography, morphology, syntax and semantics, with a final chapter on machine translation issues. The chapter sizes correspond more or less to what is linguistically distinctive about Arabic, with morphology getting the lion's share, followed by Arabic script. No previous knowledge of Arabic is needed. This book is designed for computer scientists and linguists alike. The focus of the book is on Modern Standard Arabic; however, notes on practical issues related to Arabic dialects and languages written in the Arabic script are presented in different chapters. Table of Contents: What is ""Arabic""? / Arabic Script / Arabic Phonology and Orthography / Arabic Mo...","7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG , p. 102 ) .","['7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG , p. 102 ) .']",0,"['7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG , p. 102 ) .']"
CC706,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,an efficient algorithm for easyfirst nondirectional dependency parsing,"['Yoav Goldberg', 'Michael Elhadad']",experiments,,"In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #AUTHOR_TAG ) .","['In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #AUTHOR_TAG ) .', 'As in Section 4, all models are evaluated on both gold and non-gold (machine-predicted) feature values.', 'The Easy-First Parser is a shift-reduce parser (as is MaltParser).', 'Unlike MaltParser, however, it does not attempt to attach arcs ""eagerly"" as early as possible (as in previous sections), or at the latest possible stage (an option we abandoned early on in preliminary experiments).', 'Instead, the Easy-First Parser keeps a stack of partially built treelets, and attaches them to one another in order of confidence (from high confidence, ""easy"" attachment, to low, as estimated by the classifier).', 'Labeling the relation arcs is done in a second pass, with a separate training step, after all attachments have been decided (the code for which was added after the publication of Goldberg and Elhadad (2010), which only included an unlabeled attachment version).']",5,"['In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #AUTHOR_TAG ) .', 'The Easy-First Parser is a shift-reduce parser (as is MaltParser).', 'Instead, the Easy-First Parser keeps a stack of partially built treelets, and attaches them to one another in order of confidence (from high confidence, ""easy"" attachment, to low, as estimated by the classifier).']"
CC707,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,parsing indian languages with maltparser,['Joakim Nivre'],related work,"This paper describes the application of MaltParser, a transition-based dependency parser, to three Indian languages - Bangla, Hindi and Telugu - in the context of the NLP Tools Contest at ICON 2009. In the final evaluation, MaltParser was ranked second among the participating systems and achieved an unlabeled attachment score close to 90% for Bangla and Hindi, and over 85% for Telugu, while the labeled attachment score was 15-25 percentage points lower. It is likely that the high unlabeled accuracy is achieved thanks to a relatively low syntactic complexity in the data sets, while the low labeled accuracy is due to the limited amounts of training data.","Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .","['Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', 'We go beyond previous work, however, and explore additional lexical and inflectional features.', 'Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .', 'Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task.', 'Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6).', 'Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '9 We expect this kind of feature to yield lower gains for Arabic, unless: r one uses functional feature values (such as those used here for the first time in Arabic NLP), r one uses yet another representation level to account for the otherwise non-identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as VS (where the verb precedes its subject), and r one adequately represents the otherwise ""inverse"" number agreement (a phenomenon common to other Semitic languages, such as Hebrew, too).']",1,"['Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .', 'Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.']"
CC708,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,getting more from morphology in multilingual dependency parsing,"['Matt Hohensee', 'Emily M Bender']",related work,"We propose a linguistically motivated set of features to capture morphological agreement and add them to the MSTParser dependency parser. Compared to the built-in morphological feature set, ours is both much smaller and more accurate across a sample of 20 morphologically annotated treebanks. We find increases in accuracy of up to 5.3% absolute. While some of this results from the feature set capturing information unrelated to morphology, there is still significant improvement, up to 4.6% absolute, due to the agreement model.",#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .,"['Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', 'We go beyond previous work, however, and explore additional lexical and inflectional features.', 'Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;).', 'Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task.', 'Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6).', '#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '9 We expect this kind of feature to yield lower gains for Arabic, unless: r one uses functional feature values (such as those used here for the first time in Arabic NLP), r one uses yet another representation level to account for the otherwise non-identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as VS (where the verb precedes its subject), and r one adequately represents the otherwise ""inverse"" number agreement (a phenomenon common to other Semitic languages, such as Hebrew, too).']",0,['#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .']
CC709,J14-2004,Unsupervised Event Coreference Resolution,unsupervised event coreference resolution with rich linguistic features,"['Cosmin Adrian Bejan', 'Sanda Harabagiu']",related work,"This paper examines how a new class of nonparametric Bayesian models can be effectively applied to an open-domain event coreference task. Designed with the purpose of clustering complex linguistic objects, these models consider a potentially infinite number of features and categorical outcomes. The evaluation performed for solving both within- and cross-document event coreference shows significant improvements of the models when compared against two baselines for this task.",This article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ; #AUTHOR_TAG ) .,"['This article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ; #AUTHOR_TAG ) .', 'In this work, we present more details on the problem of solving both within-and cross-document event coreference as well as describe a generic framework for solving this type of problem in an unsupervised way.', 'As data sets, we consider three different resources, including our own corpus (which is the only corpus available that encodes event coreference annotations across and within documents).', 'In the next section, we provide additional information on how we performed the annotation of this corpus.', 'Another major contribution of this article is an extended description of the unsupervised models for solving event coreference.', 'In particular, we focused on providing further explanations about the implementation of the mIBP framework as well as its integration into the HDP and iHMM models.', 'Finally, in this work, we significantly extended the experimental results section, which also includes a novel set of experiments performed over the OntoNotes English corpus (LDC-ON 2007).']",2,['This article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ; #AUTHOR_TAG ) .']
CC710,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,the harpy speech understanding system in lea,"['B Lowerre', 'R Reddy']",,,"A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , #AUTHOR_TAG , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , #AUTHOR_TAG , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']",1,"['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , #AUTHOR_TAG , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .']"
CC711,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,scripts plans goals and understanding lawrence erlbaum associates,"['R Schank', 'R Abelson']",experiments,,"Thus , for example , it can acquire a `` script '' such as the one for going to a restaurant as defined in #AUTHOR_TAG .","['Though the implemented system is limited to matrix-oriented problems, the theoretical system is capable of learning a wide range of problem types.', 'The only requirement on the problem or situation is that it can be entered into the expectation system in the form of examples.', ""Thus , for example , it can acquire a `` script '' such as the one for going to a restaurant as defined in #AUTHOR_TAG .""]",0,"[""Thus , for example , it can acquire a `` script '' such as the one for going to a restaurant as defined in #AUTHOR_TAG .""]"
CC712,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,algorithmic program debugging,['E Shapiro'],,"The thesis lays a theoretical framework for program debugging, with the goal of partly mechanizing this activity. In particular, we formalize and develop algorithmic solutions to the following two questions: (1) How do we identify a bug in a program that behaves incorrectly? (2) How do we fix a bug, once one is identified?  We develop interactive diagnosis algorithms that identify a bug in a program that behaves incorrectly, and implement them in Prolog for the diagnosis of Prolog programs. Their performance suggests that they can be the backbone of debugging aids that go far beyond what is offered by current programming environments.  We develop an inductive inference algorithm that synthesizes logic programs from examples of their behavior. The algorithm incorporates the diagnosis algorithms as a component. It is incremental, and progresses by debugging a program with respect to the examples. The Model Inference System is a Prolog implementation of the algorithm. Its range of applications and efficiency is comparable to existing systems for program synthesis from examples and grammatical inference.  We develop an algorithm that can fix a bug that has been identified, and integrate it with the diagnosis algorithms to form an interactive debugging system. By restricting the class of bugs we attempt to correct, the system can debug programs that are too complex for the Model Inference System to synthesize.",There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. ( 1984 ) and the PROLOG synthesis method of #AUTHOR_TAG .,"['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984).', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert (1969), assertional statements as in Michalski (1980), or semantic nets as in Winston (1975).', 'That is, the current system learns procedures rather than data structures.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. ( 1984 ) and the PROLOG synthesis method of #AUTHOR_TAG .']",1,"['The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert (1969), assertional statements as in Michalski (1980), or semantic nets as in Winston (1975).', 'That is, the current system learns procedures rather than data structures.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. ( 1984 ) and the PROLOG synthesis method of #AUTHOR_TAG .']"
CC713,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,computer control via limited natural language,"['P Fink', 'A Sigmon', 'A Biermann']",conclusion,"A natural language processor is described for control of a machine in task-oriented situations. Particular emphasis is given to issues related to flow-of-control statements in dialogue. These include branching constructs, as in `if row 1 contains a positive entry, then . . .' and looping constructs, as in `repeat for all other rows'. Special problems are discussed concerning the processing of deeply nested control structures, pronoun resolution, and the handling of conjunctions. An experiment is described in which the robustness of the conditional feature was tested with a group of computer naive subjects. It was found that subjects could discover and use the feature effectively in solving problems even though the fact of its existence was systematically withheld during the training session.","â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by Biermann and Krishnaswamy ( 1976 ) , â¢ a `` conditioning '' facility as described by #AUTHOR_TAG , â¢ implementation of new types of paraphrasing , â¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â¢ examining inter-speaker dialogue patterns .","[""â\x80¢ use of low level knowledge from the speech recognition phase , â\x80¢ use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described by Biermann and Krishnaswamy ( 1976 ) , â\x80¢ a `` conditioning '' facility as described by #AUTHOR_TAG , â\x80¢ implementation of new types of paraphrasing , â\x80¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â\x80¢ examining inter-speaker dialogue patterns .""]",3,"[""â\x80¢ use of low level knowledge from the speech recognition phase , â\x80¢ use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described by Biermann and Krishnaswamy ( 1976 ) , â\x80¢ a `` conditioning '' facility as described by #AUTHOR_TAG , â\x80¢ implementation of new types of paraphrasing , â\x80¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â\x80¢ examining inter-speaker dialogue patterns .""]"
CC714,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,the nomad system expectationbased detection and correction of errors during understanding of syntactically illformed text,['R Granger'],,,"The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , #AUTHOR_TAG , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .","['The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , #AUTHOR_TAG , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'However, these methodologies have not used historical information at the dialogue level as described here.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by Ho (1984).', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']",1,"['The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , #AUTHOR_TAG , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.']"
CC715,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,linguistic analysis of natural language communication with computers,['B Thompson'],,"Interaction with computers in natural  language requires a language that is flexible  and suited to the task. This study of natural  dialogue was undertaken to reveal those characteristics  which can make computer English more  natural. Experiments were made in three modes  of communication: face-to-face, terminal-to-terminal  and human-to-computer, involving over  80 subjects, over 80,000 words and over 50  hours. They showed some striking similarities,  especially in sentence length and proportion of  words in sentences. The three modes also share  the use of fragments, typical of dialogue.  Detailed statistical analysis and comparisons  are given. The nature and relative frequency of  fragments, which have been classified into  twelve categories, is shown in all modes. Special  characteristics of the face-to-face mode  are due largely to these fragments (which  include phatics employed to keep the channel of  communication open). Special characteristics of  the computational mode include other fragments,  namely definitions, which are absent from other  modes. Inclusion of fragments in computational  grammar is considered a major factor in improving  computer naturalness.    The majority of experiments involved a real  life task of loading Navy cargo ships. The  peculiarities of face-to-face mode were similar  in this task to results of earlier experiments  involving another task. It was found that in  task oriented situations the syntax of interactions  is influenced in all modes by this context  in the direction of simplification, resulting in  short sentences (about 7 words long). Users  seek to maximize efficiency In solving the problem.  When given a chance, in the computational  mode, to utilize special devices facilitating  the solution of the problem, they all resort to  them.    Analyses of the special characteristics of  the computational mode, including the analysis  of the subjects"" errors, provide guidance for  the improvement of the habitability of such systems.  The availability of the REL System, a  high performance natural language system, made  the experiments possible and meaningful. The  indicated improvements in habitability are now  being embodied in the POL (Problem Oriented  Language) System, a successor to REL.","The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , #AUTHOR_TAG , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .","['The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , #AUTHOR_TAG , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'However, these methodologies have not used historical information at the dialogue level as described here.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by Ho (1984).', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']",1,"['The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , #AUTHOR_TAG , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'However, these methodologies have not used historical information at the dialogue level as described here.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by Ho (1984).', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.']"
CC716,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,toward natural language computation,"['A Biermann', 'B Ballard']",experiments,"A computer programming system called the ""Natural Language Computer"" (NLC) is described which allows a user to type English commands while watching them executed on sample data appearing on a display screen. Direct visual feedback enables the user to detect most misinterpretation errors as they are made so that incorrect or ambiguous commands can be retyped or clarified immediately. A sequence of correctly executed commands may be given a name and used as a subroutine, thus extending the set of available operations and allowing larger English-language programs to be constructed hierarchically. In addition to discussing the transition network syntax and procedural semantics of the system, special attention is devoted to the following topics: the nature of imperative sentences in the matrix domain; the processing of non-trivial noun phrases; conjunction; pronominals; and programming constructs such as ""if"", ""repeat"", and procedure definition.","An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #AUTHOR_TAG ) .","['This section has given an overview of the approach to history-based expectation processing.', 'The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented.', 'The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve.', 'The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.', 'An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #AUTHOR_TAG ) .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).', '[The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.]']",0,"['An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #AUTHOR_TAG ) .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).']"
CC717,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,learning structural descriptions from examples in,['P Winston'],,Massachusetts Institute of Technology. Dept. of Electrical Engineering. Thesis. 1970. Ph.D.,"The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in Michalski ( 1980 ) , or semantic nets as in #AUTHOR_TAG .","['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984).', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in Michalski ( 1980 ) , or semantic nets as in #AUTHOR_TAG .', 'That is, the current system learns procedures rather than data structures.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. (1984) and the PROLOG synthesis method of Shapiro (1982).']",1,"['The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in Michalski ( 1980 ) , or semantic nets as in #AUTHOR_TAG .', 'That is, the current system learns procedures rather than data structures.']"
CC718,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,the dialogue designing dialogue system dissertation,['T-P Ho'],,,Another dialogue acquisition system has been developed by #AUTHOR_TAG .,"['The problem of handling ill-formed input has been studied by Carbonell and Hayes (1983), Granger (1983), Jensen et al. (1983), Kwasny and Sondheimer (1981), Riesbeek and Schank (1976), Thompson (1980), Weischedel and Black (1980), and Weischedel and Sondheimer (1983.', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'However, these methodologies have not used historical information at the dialogue level as described here.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by #AUTHOR_TAG .', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']",1,['Another dialogue acquisition system has been developed by #AUTHOR_TAG .']
CC719,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,the hwim speech understanding system in lea,"['J Wolf', 'W Woods']",,,"A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and #AUTHOR_TAG ) .","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and #AUTHOR_TAG ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']",1,"['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and #AUTHOR_TAG ) .']"
CC720,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,relaxation techniques for parsing grammatically illformed input in natural language understanding systems,"['S Kwasny', 'N Sondheimer']",,"This paper investigates several language phenomena either considered deviant by linguistic standards or insufficiently addressed by existing approaches. These include co-occurrence violations, some forms of ellipsis and extraneous forms, and conjunction. Relaxation techniques for their treatment in Natural Language Understanding Systems are discussed. These techniques, developed within the Augmented Transition Network (ATN) model, are shown to be adequate to handle many of these cases.","The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , Jensen et al. ( 1983 ) , #AUTHOR_TAG , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .","['The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , Jensen et al. ( 1983 ) , #AUTHOR_TAG , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'However, these methodologies have not used historical information at the dialogue level as described here.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by Ho (1984).', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']",1,"['The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , Jensen et al. ( 1983 ) , #AUTHOR_TAG , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.']"
CC721,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,pattern recognition as ruleguided inductive inference,['R Michalski'],,"The determination of pattern recognition rules is viewed as a problem of inductive inference, guided by generalization rules, which control the generalization process, and problem knowledge rules, which represent the underlying semantics relevant to the recognition problem under consideration. The paper formulates the theoretical framework and a method for inferring general and optimal (according to certain criteria) descriptions of object classes from examples of classification or partial descriptions. The language for expressing the class descriptions and the guidance rules is an extension of the first-order predicate calculus, called variable-valued logic calculus VL21. VL21 involves typed variables and contains several new operators especially suited for conducting inductive inference, such as selector, internal disjunction, internal conjunction, exception, and generalization. Important aspects of the theory include: 1) a formulation of several kinds of generalization rules; 2) an ability to uniformly and adequately handle descriptors (i.e., variables, functions, and predicates) of different type (nominal, linear, and structured) and of different arity (i.e., different number of arguments); 3) an ability to generate new descriptors, which are derived from the initial descriptors through a rule-based system (i.e., an ability to conduct the so called constructive induction); 4) an ability to use the semantics underlying the problem under consideration. An experimental computer implementation of the method is briefly described and illustrated by an example.","The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in #AUTHOR_TAG , or semantic nets as in Winston ( 1975 ) .","['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984).', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in #AUTHOR_TAG , or semantic nets as in Winston ( 1975 ) .', 'That is, the current system learns procedures rather than data structures.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. (1984) and the PROLOG synthesis method of Shapiro (1982).']",1,"['The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in #AUTHOR_TAG , or semantic nets as in Winston ( 1975 ) .', 'That is, the current system learns procedures rather than data structures.']"
CC722,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,construction of programs from example computations,"['A Biermann', 'R Krishnaswamy']",,"An autoprogrammer is an interactive computer programming system which automatically constructs computer programs from example computations executed by the user. The example calculations are done in a scratch pad fashion at a computer display using a light pen or other graphic input device, and the system stores a detailed history of all of the steps executed in the process. Then the system automatically synthesizes the shortest possible program which is capable of executing the observed examples. The paper describes the computational environment provided by the system, proves that the program synthesis technique is both ""sound"" and ""complete,"" describes the design of the system, and gives some programs it was used to create.",The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #AUTHOR_TAG where program flowcharts were constructed from traces of their behaviors .,"['The problem of handling ill-formed input has been studied by Carbonell and Hayes (1983), Granger (1983), Jensen et al. (1983), Kwasny and Sondheimer (1981), Riesbeek and Schank (1976), Thompson (1980), Weischedel and Black (1980), and Weischedel and Sondheimer (1983.', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'However, these methodologies have not used historical information at the dialogue level as described here.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #AUTHOR_TAG where program flowcharts were constructed from traces of their behaviors .', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by Ho (1984).', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']",1,"['The problem of handling ill-formed input has been studied by Carbonell and Hayes (1983), Granger (1983), Jensen et al. (1983), Kwasny and Sondheimer (1981), Riesbeek and Schank (1976), Thompson (1980), Weischedel and Black (1980), and Weischedel and Sondheimer (1983.', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #AUTHOR_TAG where program flowcharts were constructed from traces of their behaviors .', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']"
CC723,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,scripts plans goals and understanding lawrence erlbaum associates,"['R Schank', 'R Abelson']",experiments,,"The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph ( #AUTHOR_TAG ) , a deep parse of Si , or some other representation .","['We denote the meaning of each sentence Si with the notation M(Si).', 'The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph ( #AUTHOR_TAG ) , a deep parse of Si , or some other representation .', 'A user behavior is represented by a network, or directed graph, of such meanings.', 'At the beginning of a task, the state of the interaction is represented by the start state of the graph.']",0,"['We denote the meaning of each sentence Si with the notation M(Si).', 'The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph ( #AUTHOR_TAG ) , a deep parse of Si , or some other representation .', 'A user behavior is represented by a network, or directed graph, of such meanings.']"
CC724,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,the sperry univac system for continuous speech recognition in,['M Medress'],,,"A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , #AUTHOR_TAG , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , #AUTHOR_TAG , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']",1,"['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , #AUTHOR_TAG , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .']"
CC725,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,the hearsayii speech understanding system integrating knowledge to resolve uncertainty,"['L Erman', 'F Hayes-Roth', 'V Lesser', 'D Reddy']",,"The Hearsay-II system, developed during the DARPA-sponsored five-year speech-understanding research program, represents both a specific solution to the speech-understanding problem and a general framework for coordinating independent processes to achieve cooperative problem-solving behavior. As a computational problem, speech understanding reflects a large number of intrinsically interesting issues. Spoken sounds are achieved by a long chain of successive transformations, from intentions, through semantic and syntactic structuring, to the eventually resulting audible acoustic waves. As a consequence, interpreting speech means effectively inverting these transformations to recover the speaker's intention from the sound. At each step in the interpretive process, ambiguity and uncertainty arise.    The Hearsay-II problem-solving framework reconstructs an intention from hypothetical interpretations formulated at various levels of abstraction. In addition, it allocates limited processing resources first to the most promising incremental actions. The final configuration of the Hearsay-II system comprises problem-solving components to generate and evaluate speech hypotheses, and a focus-of-control mechanism to identify potential actions of greatest value. Many of these specific procedures reveal novel approaches to speech problems. Most important, the system successfully integrates and coordinates all of these independent activities to resolve uncertainty and control combinatorics. Several adaptations of the Hearsay-II framework have already been undertaken in other problem domains, and it is anticipated that this trend will continue; many future systems necessarily will integrate diverse sources of knowledge to solve complex problems cooperatively.    Discussed in this paper are the characteristics of the speech problem in particular, the special kinds of problem-solving uncertainty in that domain, the structure of the Hearsay-II system developed to cope with that uncertainty, and the relationship between Hearsay-II's structure and those of other speech-understanding systems. The paper is intended for the general computer science audience and presupposes no speech or artificial intelligence background.","A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , #AUTHOR_TAG , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , #AUTHOR_TAG , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']",1,"['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , #AUTHOR_TAG , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.']"
CC726,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,parse fitting and prose fixing getting a hold on illformedness,"['K Jensen', 'G Heidorn', 'L Miller', 'Y Ravin']",,"Processing syntactically ill-formed language is an important mission of the EPISTLE system, Ill-formed input is treated by this system in various ways. Misspellings are highlighted by a standard spelling checker; syntactic errors are detected and corrections are suggested; and stylistic infelicities are called to the user's attention.Central to the EPISTLE processing strategy is its technique of fitted parsing. When the rules of a conventional syntactic grammar are unable to produce a parse for an input string, this technique can be used to produce a reasonable approximate parse that can serve as input to the remaining stages of processing.This paper first describes the fitting process and gives examples of ill-formed language situations where it is called into play. We then show how a fitted parse allows EPISTLE to carry on its text-critiquing mission where conventional grammars would fail either because of input problems or because of limitations in the grammars themselves. Some inherent difficulties of the fitting technique are also discussed. In addition, we explore how style critiquing relates to the handling of ill-formed input, and how a fitted parse can be used in style checking.","The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , #AUTHOR_TAG , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .","['The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , #AUTHOR_TAG , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'However, these methodologies have not used historical information at the dialogue level as described here.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by Ho (1984).', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']",1,"['The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , #AUTHOR_TAG , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.']"
CC727,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,natural language with discrete speech as a mode for humantomachine communication,"['A Biermann', 'R Rodman', 'D Rubin', 'J Heidlage']",experiments,"A voice interactive natural language system, which allows users to solve problems with spoken English commands, has been constructed. The system utilizes a commercially available discrete speech recognizer which requires that each word be followed by approximately a 300 millisecond pause. In a test of the system, subjects were able to learn its use after about two hours of training. The system correctly processed about 77 percent of the over 6000 input sentences spoken in problem-solving sessions. Subjects spoke at the rate of about three sentences per minute and were able to effectively use the system to complete the given tasks. Subjects found the system relatively easy to learn and use, and gave a generally positive report of their experience.","[ The current system should be distinguished from an earlier voice system ( VNLC , #AUTHOR_TAG ) , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ]","['This section has given an overview of the approach to history-based expectation processing.', 'The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented.', 'The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve.', 'The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.', 'An off-the-shelf speech recognition device, a Nippon Electric Corporation DP-200, was added to an existing natural language processing system, the Natural Language Computer (NLC) (Ballard 1979, Biermann andBallard 1980).', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).', '[ The current system should be distinguished from an earlier voice system ( VNLC , #AUTHOR_TAG ) , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ]']",1,"['The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.', 'An off-the-shelf speech recognition device, a Nippon Electric Corporation DP-200, was added to an existing natural language processing system, the Natural Language Computer (NLC) (Ballard 1979, Biermann andBallard 1980).', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).', '[ The current system should be distinguished from an earlier voice system ( VNLC , #AUTHOR_TAG ) , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ]']"
CC728,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,transition network grammars for natural language analysis,['W Woods'],experiments,"The use of augmented transition network grammars for the analysis of natural language sentences is described. Structure-building actions associated with the arcs of the grammar network allow for the reordering, restructuring, and copying of constituents necessary to produce deep-structure representations of the type normally obtained from a transformational analysis, and conditions on the arcs allow for a powerful selectivity which can rule out meaningless analyses and take advantage of semantic information to guide the parsing. The advantages of this model for natural language analysis are discussed in detail and illustrated by examples. An implementation of an experimental parsing system for transition network grammars is briefly described.",The expectation parser uses an ATN-like representation for its grammar ( #AUTHOR_TAG ) .,"['The expectation parser uses an ATN-like representation for its grammar ( #AUTHOR_TAG ) .', 'Its strategy is top-down.', 'The types of sentences accepted are essentially those accepted by the original NLC grammar, imperative sentences with nested noun groups and conjunctions (Ballard 1979).', 'An attempt has been made to build as deep a parse as possible so that sentences with the same meaning result in identical parses.', 'Sentences have the same ""meaning"" if they ""result in identical tasks being performed.', 'The various sentence structures that']",5,"['The expectation parser uses an ATN-like representation for its grammar ( #AUTHOR_TAG ) .', 'The various sentence structures that']"
CC729,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,the acquisition and use of dialogue expectation in speech recognition,['P Fink'],experiments,,"The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , #AUTHOR_TAG ) .","['This section has given an overview of the approach to history-based expectation processing.', 'The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented.', 'The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve.', 'The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.', 'An off-the-shelf speech recognition device, a Nippon Electric Corporation DP-200, was added to an existing natural language processing system, the Natural Language Computer (NLC) (Ballard 1979, Biermann andBallard 1980).', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , #AUTHOR_TAG ) .', '[The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.]']",0,"['The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , #AUTHOR_TAG ) .']"
CC730,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,the sdc speech understanding system in lea,"['J Barnett', 'M Berstein', 'R Gillman', 'I Kameny']",,,"A number of speech understanding systems have been developed during the past fifteen years ( #AUTHOR_TAG , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .","['A number of speech understanding systems have been developed during the past fifteen years ( #AUTHOR_TAG , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']",1,"['A number of speech understanding systems have been developed during the past fifteen years ( #AUTHOR_TAG , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.']"
CC731,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,the acquisition and use of dialogue expectation in speech recognition,['P Fink'],,,A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #AUTHOR_TAG .,"['A number of speech understanding systems have been developed during the past fifteen years (Barnett et al. 1980, Dixon and Martin 1979, Erman et al. 1980, Haton and Pierrel 1976, Lea 1980, Lowerre and Reddy 1980, Medress 1980, Reddy 1976, Walker 1978, and Wolf and Woods 1980.', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #AUTHOR_TAG .']",0,"['A number of speech understanding systems have been developed during the past fifteen years (Barnett et al. 1980, Dixon and Martin 1979, Erman et al. 1980, Haton and Pierrel 1976, Lea 1980, Lowerre and Reddy 1980, Medress 1980, Reddy 1976, Walker 1978, and Wolf and Woods 1980.', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #AUTHOR_TAG .']"
CC732,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,construction of programs from example computations,"['A Biermann', 'R Krishnaswamy']",conclusion,"An autoprogrammer is an interactive computer programming system which automatically constructs computer programs from example computations executed by the user. The example calculations are done in a scratch pad fashion at a computer display using a light pen or other graphic input device, and the system stores a detailed history of all of the steps executed in the process. Then the system automatically synthesizes the shortest possible program which is capable of executing the observed examples. The paper describes the computational environment provided by the system, proves that the program synthesis technique is both ""sound"" and ""complete,"" describes the design of the system, and gives some programs it was used to create.","â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by #AUTHOR_TAG , â¢ a `` conditioning '' facility as described by Fink et al. ( 1985 ) , â¢ implementation of new types of paraphrasing , â¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â¢ examining inter-speaker dialogue patterns .","[""â\x80¢ use of low level knowledge from the speech recognition phase , â\x80¢ use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described by #AUTHOR_TAG , â\x80¢ a `` conditioning '' facility as described by Fink et al. ( 1985 ) , â\x80¢ implementation of new types of paraphrasing , â\x80¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â\x80¢ examining inter-speaker dialogue patterns .""]",3,"[""â\x80¢ use of low level knowledge from the speech recognition phase , â\x80¢ use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described by #AUTHOR_TAG , â\x80¢ a `` conditioning '' facility as described by Fink et al. ( 1985 ) , â\x80¢ implementation of new types of paraphrasing , â\x80¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â\x80¢ examining inter-speaker dialogue patterns .""]"
CC733,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,recovery strategies for parsing extragrammatical language,"['J Carbonell', 'P Hayes']",,"Practical natural language interfaces must exhibit robust behaviour in the presence of extragrammatical user input. This paper classifies different types of grammatical deviations and related phenomena at the lexical, sentential and dialogue levels and presents recovery strategies tailored to specific phenomena in the classification. Such strategies constitute a tool chest of computationally tractable methods for coping with extragrammatieality in restricted domain natural language. Some of the strategies have been tested and proven viable in existing parsers.  </p","The problem of handling ill-formed input has been studied by #AUTHOR_TAG , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .","['The problem of handling ill-formed input has been studied by #AUTHOR_TAG , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'However, these methodologies have not used historical information at the dialogue level as described here.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by Ho (1984).', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']",1,"['The problem of handling ill-formed input has been studied by #AUTHOR_TAG , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.']"
CC734,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,perceptrons,"['M Minsky', 'S Papert']",,"Ashkin-Teller type perceptron models are introduced. Their maximal capacity per number of couplings is calculated within a first-step replica-symmetry-breaking Gardner approach. The results are compared with extensive numerical simulations using several algorithms.Comment: 8 pages in Latex with 2 eps figures, RSB1 calculations has been adde","The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #AUTHOR_TAG , assertional statements as in Michalski ( 1980 ) , or semantic nets as in Winston ( 1975 ) .","['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984).', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #AUTHOR_TAG , assertional statements as in Michalski ( 1980 ) , or semantic nets as in Winston ( 1975 ) .', 'That is, the current system learns procedures rather than data structures.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. (1984) and the PROLOG synthesis method of Shapiro (1982).']",1,"['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984).', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #AUTHOR_TAG , assertional statements as in Michalski ( 1980 ) , or semantic nets as in Winston ( 1975 ) .', 'That is, the current system learns procedures rather than data structures.']"
CC735,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,semantic processing for a natural language programming system,['B Ballard'],experiments,,"An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #AUTHOR_TAG , Biermann and Ballard 1980 ) .","['This section has given an overview of the approach to history-based expectation processing.', 'The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented.', 'The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve.', 'The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.', 'An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #AUTHOR_TAG , Biermann and Ballard 1980 ) .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).', '[The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.]']",0,"['An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #AUTHOR_TAG , Biermann and Ballard 1980 ) .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).']"
CC736,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,speech recognition by machine a review,['D Reddy'],,"This paper presents a brief survey on Automatic Speech Recognition and discusses the major themes and advances made in the past 60 years of research, so as to provide a technological perspective and an appreciation of the fundamental progress that has been accomplished in this important area of speech communication. After years of research and development the accuracy of automatic speech recognition remains one of the important research challenges (e.g., variations of the context, speakers, and environment).The design of Speech Recognition system requires careful attentions to the following issues: Definition of various types of speech classes, speech representation, feature extraction techniques, speech classifiers, database and performance evaluation. The problems that are existing in ASR and the various techniques to solve these problems constructed by various research workers have been presented in a chronological order. Hence authors hope that this work shall be a contribution in the area of speech recognition. The objective of this review paper is to summarize and compare some of the well known methods used in various stages of speech recognition system and identify research topic and applications which are at the forefront of this exciting and challenging field.","A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , #AUTHOR_TAG , Walker 1978 , and Wolf and Woods 1980 ) .","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , #AUTHOR_TAG , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']",1,"['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , #AUTHOR_TAG , Walker 1978 , and Wolf and Woods 1980 ) .']"
CC737,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,semantic processing for a natural language programming system,['B Ballard'],experiments,,"The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions ( #AUTHOR_TAG ) .","['The expectation parser uses an ATN-like representation for its grammar (Woods 1970).', 'Its strategy is top-down.', 'The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions ( #AUTHOR_TAG ) .', 'An attempt has been made to build as deep a parse as possible so that sentences with the same meaning result in identical parses.', 'Sentences have the same ""meaning"" if they ""result in identical tasks being performed.', 'The various sentence structures that']",0,"['The expectation parser uses an ATN-like representation for its grammar (Woods 1970).', 'The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions ( #AUTHOR_TAG ) .', 'The various sentence structures that']"
CC738,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,the acquisition and use of dialogue expectation in speech recognition,['P Fink'],experiments,,How it is done is beyond the scope of this paper but is explained in detail in #AUTHOR_TAG .,"[""In such a situation, the user's intentions may be reflected more correctly by the following expected sentence set: double (rARG) 1.0 which signifies that any row may be referred to."", 'However, though this simplified expected sentence set may be a good generalization of the pattern observed, it has ramifications for error correction.', 'Specifically, it will be unable to fill in a row number should that value be missing in the incoming sentence.', 'The first option also has its drawbacks.', 'In this case, should the row number be missing in the sentence, the expectation parser will error correct the sentence to the most probable value, or the first one in the set if the probabilities are equal, here the value one for row 1.', 'Thus, both options are imperfect in terms of the error correction capabilities that they can provide.', 'The comparison that must be made to determine which option is better in a given situation is how often the first will error correct incorrectly as opposed to how much error correcting power we will lose by using the second.', 'How it is done is beyond the scope of this paper but is explained in detail in #AUTHOR_TAG .']",0,"[""In such a situation, the user's intentions may be reflected more correctly by the following expected sentence set: double (rARG) 1.0 which signifies that any row may be referred to."", 'However, though this simplified expected sentence set may be a good generalization of the pattern observed, it has ramifications for error correction.', 'Specifically, it will be unable to fill in a row number should that value be missing in the incoming sentence.', 'The first option also has its drawbacks.', 'In this case, should the row number be missing in the sentence, the expectation parser will error correct the sentence to the most probable value, or the first one in the set if the probabilities are equal, here the value one for row 1.', 'The comparison that must be made to determine which option is better in a given situation is how often the first will error correct incorrectly as opposed to how much error correcting power we will lose by using the second.', 'How it is done is beyond the scope of this paper but is explained in detail in #AUTHOR_TAG .']"
CC739,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,automatic program construction techniques,"['A Biermann', 'G Guiho', 'Y Kodratoff', 'Eds']",,"The purpose of Avignon'86 is to provide a forum for presentat ion of new implementat ions of expert systems and basic tools and techniques for building expert systems. Aimed at developers and users of expert systems, the conference and exhibit ion will o f fer an assessment of available tools and techniques; will provide practical guidelines for making decisions concerning the application of expert system technology; and will help define, clarify, and make sense of the claims, promises, and realit ies of practical expert system applications.",There is some literature on procedure acquisition such as the LISP synthesis work described in #AUTHOR_TAG and the PROLOG synthesis method of Shapiro ( 1982 ) .,"['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984).', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert (1969), assertional statements as in Michalski (1980), or semantic nets as in Winston (1975).', 'That is, the current system learns procedures rather than data structures.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in #AUTHOR_TAG and the PROLOG synthesis method of Shapiro ( 1982 ) .']",1,"['That is, the current system learns procedures rather than data structures.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in #AUTHOR_TAG and the PROLOG synthesis method of Shapiro ( 1982 ) .']"
CC740,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,organization and operation of a connected speech understanding system at lexical syntactic and semantic levels,"['J Haton', 'J Pierrel']",,"This paper describes a connected speech understanding system being implemented in Nancy, thanks to the work done in automatic speech recognition since 1968. This system is made up of four parts : an acoustic recognizer which gives a string of phoneme-like segments from a spoken sentence, a syntactic parser which controls the recognition process, a word recognizer working on words predicted by the parser and a dialog procedure which takes in account semantic constraints in order to avoid some of the errors and ambiguities. Some original features of the system are pointed out : modularily (e.g. the language used is considered as a parameter), possibility of processing slightly syntactically incorrect sentences, ... The application both in data management and in oral control of a telephone center has given very promising results. Work is in progress for generalizing our model : extension of the vocabulary and of the grammar, multi-speaker operation, etc.","A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #AUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #AUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']",1,"['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #AUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.']"
CC741,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,processing dictionary definitions with phrasal pattern hierarchies in this issue,['Hiyan Alshawi'],introduction,"This paper shows how dictionary word sense definitions can be analysed by applying a hierarchy of phrasal patterns. An experimental system embodying this mechanism has been implemented for processing definitions from the Longman Dictionary of Contemporary English. A property of this dictionary, exploited by the system, is that it uses a restricted vocabulary in its word sense definitions. The structures generated by the experimental system are intended to be used for the classification of new word senses in terms of the senses of words in the restricted vocabulary. Examples illustrating the output generated are presented, and some qualitative performance results and problems that were encountered are discussed. The analysis process applies successively more specific phrasal analysis rules as determined by a hierarchy of patterns in which less specific patterns dominate more specific ones. This ensures that reasonable incomplete analyses of the definitions are produced when more complete analyses are not possible, resulting in a relatively robust analysis mechanism. Thus the work reported addresses two robustness problems faced by current experimental natural language processing systems: coping with an incomplete lexicon and with incomplete knowledge of phrasal constructions.",In this paper we focus on the exploitation of the LDOCE grammar coding system ; Alshawi et al. ( 1985 ) and #AUTHOR_TAG describe further research in Cambridge utilising different types of information available in LDOCE .,"['We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", '(Michiels (1982) contains further description and discussion of LDOCE.)', 'In this paper we focus on the exploitation of the LDOCE grammar coding system ; Alshawi et al. ( 1985 ) and #AUTHOR_TAG describe further research in Cambridge utilising different types of information available in LDOCE .']",0,"['We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", 'In this paper we focus on the exploitation of the LDOCE grammar coding system ; Alshawi et al. ( 1985 ) and #AUTHOR_TAG describe further research in Cambridge utilising different types of information available in LDOCE .']"
CC742,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,exploiting a large dictionary data base,['Archibal Michiels'],,,#AUTHOR_TAG and Akkerman et al. ( 1985 ) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .,"['The criteria for assignment of capital letters to verbs is not made explicit, but is influenced by the syntactic and semantic relations which hold between the verb and its arguments; for example, I5, L5 and T5 can all be assigned to verbs which take a NP subject and a sentential complement, but L5 will only be assigned if there is a fairly close semantic link between the two arguments and T5 will be used in preference to I5 if the verb is felt to be semantically two place rather than one place, such as know versus appear.', 'On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic.', '#AUTHOR_TAG and Akkerman et al. ( 1985 ) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .', 'Ingria (1984) comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated.']",0,['#AUTHOR_TAG and Akkerman et al. ( 1985 ) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .']
CC743,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,a parser for generalised phrase structure grammars,"['John Phillips', 'Henry Thompson']",introduction,,"The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; Russell et al. , 1986 ; #AUTHOR_TAG ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .","['Recent developments in linguistics, and especially on grammatical theory --for example, Generalised Phrase Structure Grammar (GPSG) (Gazdar et al., 1985), Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982) --and on natural language parsing frameworks for example, Functional Unification Grammar (FUG) (Kay, 1984a), PATR-II (Shieber, 1984) --make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; Russell et al. , 1986 ; #AUTHOR_TAG ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', 'One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.']",0,"['Recent developments in linguistics, and especially on grammatical theory --for example, Generalised Phrase Structure Grammar (GPSG) (Gazdar et al., 1985), Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982) --and on natural language parsing frameworks for example, Functional Unification Grammar (FUG) (Kay, 1984a), PATR-II (Shieber, 1984) --make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; Russell et al. , 1986 ; #AUTHOR_TAG ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', 'One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.']"
CC744,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,machinereadable dictionaries lexical data bases and the lexical system,['Nicoletta Calzolari'],,,"However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #AUTHOR_TAG for further discussion ) .","['The lispified LDOCE file retains the broad structure of the typesetting tape and divides each entry into a number of fields --head word, pronunciation, grammar codes, definitions, examples, and so forth.', 'However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #AUTHOR_TAG for further discussion ) .', 'For this purpose the formatting codes on the typesetting tape are crucial since they provide clues to the correct structure of this information.', 'For example, word senses are largely defined in terms of the 2000 word core vocabulary, however, in some cases other words (themselves defined elsewhere in terms of this vocabulary) are used.', 'These words always appear in small capitals and can therefore be recognised because they will be preceded by a font change control character.', 'In Figure 1 above the definition of rivet as verb includes the noun definition of ""RIVET 1\'\', as signalled by the font change and the numerical superscript which indicates that it is the first (i.e.', 'noun entry) homograph; additional notation exists for word senses within homographs.', 'On the typesetting tape, font control characters are indicated by hexadecimal numbers within curly brackets.', 'In addition, there is a further complication because this sense is used in the plural and the plural morpheme must be removed before RIVET can be associated with a dictionary entry.', 'However, the restructuring program can achieve this because such morphology is always italicised, so the program knows that, in the context of non-core vocabulary items, the italic font control character signals the Figure 3 occurrence of a morphological variant of a LDOCE head entry.']",0,"['The lispified LDOCE file retains the broad structure of the typesetting tape and divides each entry into a number of fields --head word, pronunciation, grammar codes, definitions, examples, and so forth.', 'However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #AUTHOR_TAG for further discussion ) .', 'For this purpose the formatting codes on the typesetting tape are crucial since they provide clues to the correct structure of this information.', 'For example, word senses are largely defined in terms of the 2000 word core vocabulary, however, in some cases other words (themselves defined elsewhere in terms of this vocabulary) are used.', 'These words always appear in small capitals and can therefore be recognised because they will be preceded by a font change control character.', 'noun entry) homograph; additional notation exists for word senses within homographs.', 'On the typesetting tape, font control characters are indicated by hexadecimal numbers within curly brackets.', 'In addition, there is a further complication because this sense is used in the plural and the plural morpheme must be removed before RIVET can be associated with a dictionary entry.', 'However, the restructuring program can achieve this because such morphology is always italicised, so the program knows that, in the context of non-core vocabulary items, the italic font control character signals the Figure 3 occurrence of a morphological variant of a LDOCE head entry.']"
CC745,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,complement types in english,['Robert Ingria'],,"223 p.Thesis (Ph.D.)--University of Illinois at Urbana-Champaign, 1971.U of I OnlyRestricted to the U of I community idenfinitely during batch ingest of legacy ETD",#AUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated .,"['The criteria for assignment of capital letters to verbs is not made explicit, but is influenced by the syntactic and semantic relations which hold between the verb and its arguments; for example, I5, L5 and T5 can all be assigned to verbs which take a NP subject and a sentential complement, but L5 will only be assigned if there is a fairly close semantic link between the two arguments and T5 will be used in preference to I5 if the verb is felt to be semantically two place rather than one place, such as know versus appear.', 'On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic.', 'Michiels (1982) and Akkerman et al. (1985) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description.', '#AUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated .']",1,['#AUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated .']
CC746,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,syntactic argumentation and the structure of english,"['D M Perlmutter', 'S Soames']",,"Syntactic Argumentation and the Structure of English (SASE) presents the major theoretical developments in generative syntax and the empirical arguments motivating them. Beautifully and lucidly written, it is an invaluable resource for working linguists as well as a pedagogical tool of unequaled depth and breadth. The chief focus of the book is syntactic argumentation. Beginning with the fundamentals of generative syntax, it proceeds by a series of gradually unfolding arguments to analyses of some of the most sophisticated proposals. It includes a wide variety of problems that guide the reader in constructing arguments deciding between alternative analyses of syntactic constructions and alternative theoretical formulations. Someone who has worked through the problems and arguments in this book will be able to apply the skills in argumentation it develops to novel issues in syntax. While teaching syntactic argumentation, SASE covers the major empirical results of generative syntax. Its contents include: Transformations in single-clause sentences; Complementation and multi-clause transformations; Universal principles governing rule interaction: the cycle and strict cyclicity; Movement rules; Ross' constraints; Pronominal reference and anaphora. SASE is an important book for several different audiences: for students, it is an introduction to syntax that teaches argumentation as well as a wide range of empirical results in the field; for linguists, it is a sourcebook of classical analyses and arguments, with some new arguments bearing on classical issues; and, for scholars, teachers, and students in related fields, it is a comprehensive guide to the major empirical and theoretical developments in generative syntax. SASE contains enough material for a two-semester or three-quarler sequence in syntax. Because it assumes no previous background, it can be used as the main text in an introduction to syntax. Since it covers a wide range of material not available in other texts, it is also suitable for intermediate and advanced syntax courses and as a supplementary source in more specialized courses and courses in other disciplines. A storehouse of classical and original arguments, SASE will prove to be of lasting value to the teacher, the student, and researchers in both linguistics and related fields.","Clearly, there are other syntactic and semantic tests for this distinction, (see eg. #AUTHOR_TAG:472 ), but these are the only ones which are explicit in the LDOCE coding system.","['Clearly, there are other syntactic and semantic tests for this distinction, (see eg. #AUTHOR_TAG:472 ), but these are the only ones which are explicit in the LDOCE coding system.']",0,"['Clearly, there are other syntactic and semantic tests for this distinction, (see eg. #AUTHOR_TAG:472 ), but these are the only ones which are explicit in the LDOCE coding system.']"
CC747,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,word formation in natural language processing systems,['Roy Byrd'],introduction,,"Two exceptions to this generalisation are the Linguistic String Project ( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; #AUTHOR_TAG ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .","['The idea of using the machine readable source of a published dictionary has occurred to a wide range of researchers, for spelling correction, lexical analysis, thesaurus construction, and machine translation, to name but a few applications.', 'Most of the work on automated dictionaries has concentrated on extracting lexical or other information, essentially by batch processing (eg. Amsler, 1981 ;Walker and Amsler, 1986), or Copyright 1987 by the Association for Computational Linguistics.', 'Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.', 'To copy otherwise, or to republish, requires a fee and/or specific permission.', '0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b).', 'Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.', 'Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand.', 'Two exceptions to this generalisation are the Linguistic String Project ( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; #AUTHOR_TAG ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .', 'In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details).', 'However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.', 'In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.']",1,"['Two exceptions to this generalisation are the Linguistic String Project ( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; #AUTHOR_TAG ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .']"
CC748,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,a dictionary and morphological analyser for english,"['Graham Russell', 'Steve Pulman', 'Graeme Ritchie', 'Alan Black']",,"This paper describes the current state of a three-year project aimed at the development of software for use in handling large quantit ies of dictionary information within natural language processing systems. 1 The project was accepted for funding by SERC/Alvey commencing ill June 1984, and is being carried out by Graeme Ritchie and Alan Black at the Universi ty of Edinburgh and Steve Puhnan and Graham Russell at the Universi ty of Cambridge. It is one of three closely related projects funded under the Alvey IKBS Programme (Natural Language Tlleme); a parser is under development at Edinburgh by Henry Thompson and John Phillips, and a sentence grammar is being devised by Ted Briscoe and Clare Grover at Lancaster and Bran Boguraev and John Carroll at Cambridge. It is intended tha t the software and rules produced by all three projects wil l be directly compatible and capable of functioning in an integrated system. Realistic and useful na tura l language processing systems such as database f ront-ends require large numbers of words, together wi th associated syntactic and semantic Information, to be efficiently stored in machine-readable form. Our system is Intended to provide the necessary facilities, being designed to store a large number (at least 10,000) of words and to perform morphological analysis on them, covering both Inflectional and derlvatlonal morphology. In pursuit of these objectives, the dictionary associates wi th each word information concerning its morphosyntactlc properties. Users are free to modify the system In a number of ways; they may add to the lexical entries Lisp functions tha t perform semantic manipulatlons, and tailor the dictionary to the particular subject mat ter they are interested in (different databases, for example). I t Is also hoped that the system is general enough to be of use to linguists wishing to Investigate the morphology of English and other languages. Contents of the basle data files may be altered or replaced: 1. A 'Word Grammar ' file contains rules assigning internal s t ructure to complex words, 2. A 'Lexicon' file holds the morpheme entries which include syntactic and other Information associated wi th stems and affixes. 3. A 'Spelling Rules' file contains rules governing permissible correspondences between the form of morphemes listed in the lextcon and complex words consisting of sequences of these morphemes. Once these data flies have been prepared, they are compiled using a number of pre-processtng functions tha t operate to produce a set of output files. These constitute a fu l ly expanded and cross-Indexed dictionary which can then be accessed from within LISP. The process of morphological analysis consists of parslng a sequence of Input morphemes wi th respect to the word grammar, It Is Implemented as an active chart parser (Thompson & Rltchle (1984)), and builds a s t ructure in the form of a tree in which each node has two","No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and #AUTHOR_TAG ) .","['The program which transforms the LDOCE grammar codes into lexical entries utilisable by a parser takes as input the decompacted codes and produces a relatively theory neutral representation of the lexical entry for a particular word, in the sense that this representation could be further transformed into a format suitable for most current parsing systems.', 'For example, if the input were the third sense of believe, as in Figure 4, the program would generate the (partial) entry shown in Figure 8  Figure 8 At the time of writing, rules for producing adequate entries to drive a parsing system have only been developed for verb codes.', 'In what follows we will describe the overall transformation strategy and the particular rules we have developed for the verb codes.', 'Extending the system to handle nouns, adjectives and adverbs would present no problems of principle.', 'However, the LDOCE coding of verbs is more comprehensive than elsewhere, so verbs are the obvious place to start in an evaluation of the usefulness of the coding system.', 'No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and #AUTHOR_TAG ) .']",0,"['No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and #AUTHOR_TAG ) .']"
CC749,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,towards a lexicon support environment for real time parsing,"['Hiyan Alshawi', 'Branimir Boguraev', 'Ted Briscoe']",introduction,,In this paper we focus on the exploitation of the LDOCE grammar coding system ; #AUTHOR_TAG and Alshawi ( 1987 ) describe further research in Cambridge utilising different types of information available in LDOCE .,"['We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", '(Michiels (1982) contains further description and discussion of LDOCE.)', 'In this paper we focus on the exploitation of the LDOCE grammar coding system ; #AUTHOR_TAG and Alshawi ( 1987 ) describe further research in Cambridge utilising different types of information available in LDOCE .']",0,"['We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", 'In this paper we focus on the exploitation of the LDOCE grammar coding system ; #AUTHOR_TAG and Alshawi ( 1987 ) describe further research in Cambridge utilising different types of information available in LDOCE .']"
CC750,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,a critical assessment of the ldoce coding system to appear in,['Erik Akkerman'],,,"One approach to this problem is that taken by the ASCOT project ( Akkerman et al. , 1985 ; #AUTHOR_TAG ) .","['This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see Michiels, 1982, for further comment).', 'One approach to this problem is that taken by the ASCOT project ( Akkerman et al. , 1985 ; #AUTHOR_TAG ) .', 'In this project, a new lexicon is being manually derived from LDOCE.', 'The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power.', 'More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis.', 'In the medium term this approach, though time consuming, will be of some utility for producing more reliable lexicons for natural language processing.']",0,"['This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see Michiels, 1982, for further comment).', 'One approach to this problem is that taken by the ASCOT project ( Akkerman et al. , 1985 ; #AUTHOR_TAG ) .', 'The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power.', 'More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis.']"
CC751,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,the design of a computer language for linguistic information,['S Shieber'],,"A considerable body of accumulated knowledge about the design of languages for communicating information to computers has been derived from the subfields of programming language design and semantics. It has been the goal of the PATR group at SRI to utilize a relevant portion of this knowledge in implementing tools to facilitate communication of linguistic information to computers. The PATR-II formalism is our current computer language for encoding linguistic information. This paper, a brief overview of that formalism, attempts to explicate our design decisions in terms of a set of properties that effective computer languages should incorporate.Engineering and Applied Science",To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system ( #AUTHOR_TAG and references therein ) .,"['The output of the transformation program can be used to derive entries which are appropriate for particular grammatical formalisms.', 'To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system ( #AUTHOR_TAG and references therein ) .', 'PATR-II was chosen because it has been reimplemented in Cambridge and was therefore, available; however, the task would be nearly identical if we were constructing entries for a system based on GPSG, FUG or LFG.', 'We The latter employs a grammatical formalism based on GPSG; the comparatively theory neutral lexical entries that we construct from LDOCE should translate straightforwardly into this framework as well.']",5,"['The output of the transformation program can be used to derive entries which are appropriate for particular grammatical formalisms.', 'To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system ( #AUTHOR_TAG and references therein ) .', 'We The latter employs a grammatical formalism based on GPSG; the comparatively theory neutral lexical entries that we construct from LDOCE should translate straightforwardly into this framework as well.']"
CC752,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,designing a computerised lexicon for linguistic purposes,"['Erik Akkerman', 'Pieter Masereeuw', 'Willem Meijs']",,,"One approach to this problem is that taken by the ASCOT project ( #AUTHOR_TAG ; Akkerman , 1986 ) .","['This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see Michiels, 1982, for further comment).', 'One approach to this problem is that taken by the ASCOT project ( #AUTHOR_TAG ; Akkerman , 1986 ) .', 'In this project, a new lexicon is being manually derived from LDOCE.', 'The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power.', 'More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis.', 'In the medium term this approach, though time consuming, will be of some utility for producing more reliable lexicons for natural language processing.']",0,"['This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see Michiels, 1982, for further comment).', 'One approach to this problem is that taken by the ASCOT project ( #AUTHOR_TAG ; Akkerman , 1986 ) .', 'In this project, a new lexicon is being manually derived from LDOCE.', 'The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power.', 'More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis.', 'In the medium term this approach, though time consuming, will be of some utility for producing more reliable lexicons for natural language processing.']"
CC753,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,natural language information processing,['N Sager'],introduction,"Our aim is to extract information about literary characters in unstructured texts. We employ natural language processing and reasoning on domain ontologies. The first task is to identify the main characters and the parts of the story where these characters are described or act. We illustrate the system in a scenario in the folktale domain. The system relies on a folktale ontology that we have developed based on Propp's model for folktales morphology.Comment: IEEE 11 International Conference on Intelligent Computer   Communication and Processing (ICCP2015), Cluj-Napoca, Romania, 3-5 September   201","Two exceptions to this generalisation are the Linguistic String Project ( #AUTHOR_TAG ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .","['The idea of using the machine readable source of a published dictionary has occurred to a wide range of researchers, for spelling correction, lexical analysis, thesaurus construction, and machine translation, to name but a few applications.', 'Most of the work on automated dictionaries has concentrated on extracting lexical or other information, essentially by batch processing (eg. Amsler, 1981 ;Walker and Amsler, 1986), or Copyright 1987 by the Association for Computational Linguistics.', 'Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.', 'To copy otherwise, or to republish, requires a fee and/or specific permission.', '0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b).', 'Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.', 'Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand.', 'Two exceptions to this generalisation are the Linguistic String Project ( #AUTHOR_TAG ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .', 'In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details).', 'However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.', 'In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.']",1,"['Two exceptions to this generalisation are the Linguistic String Project ( #AUTHOR_TAG ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .']"
CC754,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,database design for a dictionary of the future,['Frank Tompa'],,,"Lisp is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. , #AUTHOR_TAG ) ; on the other hand a method of access was clearly required , which was flexible enough to support a range of applications intending to make use of the LDOCE tape .","['There is a well recognised problem with providing computational support for machine readable dictionaries, in particular where issues of access are concerned.', ""On the one hand, dictionaries exhibit far too much structure for conventional techniques for managing 'flat' text to apply to them."", 'On the other hand, the equally large amounts of free text in dictionary entries, as well as the implicitly marked relationships commonly used to encode linguistic information, makes a dictionary difficult to represent as a structured database of a standard, eg.', 'relational, type.', 'In addition, in order to link the machine readable version of LDOCE to our development environment, and eventually to our natural language processing systems, we need to provide fast access from Lisp to data held in secondary storage.', 'Lisp is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. , #AUTHOR_TAG ) ; on the other hand a method of access was clearly required , which was flexible enough to support a range of applications intending to make use of the LDOCE tape .']",0,"['There is a well recognised problem with providing computational support for machine readable dictionaries, in particular where issues of access are concerned.', ""On the one hand, dictionaries exhibit far too much structure for conventional techniques for managing 'flat' text to apply to them."", 'On the other hand, the equally large amounts of free text in dictionary entries, as well as the implicitly marked relationships commonly used to encode linguistic information, makes a dictionary difficult to represent as a structured database of a standard, eg.', 'relational, type.', 'In addition, in order to link the machine readable version of LDOCE to our development environment, and eventually to our natural language processing systems, we need to provide fast access from Lisp to data held in secondary storage.', 'Lisp is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. , #AUTHOR_TAG ) ; on the other hand a method of access was clearly required , which was flexible enough to support a range of applications intending to make use of the LDOCE tape .']"
CC755,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,computer exploitation of ldoces grammatical codes paper presented at a conference on survey of english language,"['A Moulin', 'J Jansen', 'A Michiels']",,,"In addition , #AUTHOR_TAG note that our Object Raising rule would assign mean to this category incorrectly .","['The four verbs which are misclassified as Object Equi and which do not have T5 codes anywhere in their entries are elect, love, represent and require.', 'None of these verbs take sentential complements and therefore they appear to be counterexamples to our Object Raising rule.', 'In addition , #AUTHOR_TAG note that our Object Raising rule would assign mean to this category incorrectly .', 'Mean is assigned both a V3 and a T5 category in the code field associated with sense 2 (i.e.', '""intend""), however, when it is used in this sense it must be treated as an Object Equi verb.']",1,"['In addition , #AUTHOR_TAG note that our Object Raising rule would assign mean to this category incorrectly .']"
CC756,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,and forthcoming machine readable dictionaries and research in computational linguistics,['Branimir Boguraev'],introduction,,"In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #AUTHOR_TAG for details ) .","['The idea of using the machine readable source of a published dictionary has occurred to a wide range of researchers, for spelling correction, lexical analysis, thesaurus construction, and machine translation, to name but a few applications.', 'Most of the work on automated dictionaries has concentrated on extracting lexical or other information, essentially by batch processing (eg. Amsler, 1981 ;Walker and Amsler, 1986), or Copyright 1987 by the Association for Computational Linguistics.', 'Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.', 'To copy otherwise, or to republish, requires a fee and/or specific permission.', '0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b).', 'Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.', 'Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand.', 'Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources.', 'In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #AUTHOR_TAG for details ) .', 'However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.', 'In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.']",0,"['In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #AUTHOR_TAG for details ) .', 'However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.']"
CC757,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,lexicalfunctional grammar a formal system for grammatical representation in jbresnan ed the mental representation of grammatical relations,"['Ronald Kaplan', 'Joan Bresnan']",introduction,,"Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( #AUTHOR_TAG ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( Shieber , 1984 ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .","['Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( #AUTHOR_TAG ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( Shieber , 1984 ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'The research described below is taking place in the context of three collaborative projects (Boguraev, 1987;Russell et al., 1986;Phillips and Thompson, 1986) to develop a general-purpose, wide coverage morphological and syntactic analyser for English.', 'One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.']",0,"['Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( #AUTHOR_TAG ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( Shieber , 1984 ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .']"
CC758,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,a key to the brandeis verb catalog unpublished mimeo under nsf grant ist8420073 quotinformation structure of a natural language lexiconquot,"['Ray Jackendoff', 'Jane Grimshaw']",,,This deficiency is rectified in the verb classification system employed by #AUTHOR_TAG in the Brandeis verb catalogue .,"['Prefer is misclassified as Object Raising, rather than as Object Equi, because the relevant code field contains a T5 code, as well as a V3 code.', 'The T5 code is marked as \'rare\', and the occurrence of prefer with a tensed sentential complement, as opposed to with an infinitive, is certainly marginal:  This example also highlights a deficiency in the LDOCE coding system since prefer occurs much more naturally with a sentential complement if it collocates with a modal such as ""would"".', 'This deficiency is rectified in the verb classification system employed by #AUTHOR_TAG in the Brandeis verb catalogue .']",1,['This deficiency is rectified in the verb classification system employed by #AUTHOR_TAG in the Brandeis verb catalogue .']
CC759,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,a dictionary and morphological analyser for english,"['Graham Russell', 'Steve Pulman', 'Graeme Ritchie', 'Alan Black']",introduction,"This paper describes the current state of a three-year project aimed at the development of software for use in handling large quantit ies of dictionary information within natural language processing systems. 1 The project was accepted for funding by SERC/Alvey commencing ill June 1984, and is being carried out by Graeme Ritchie and Alan Black at the Universi ty of Edinburgh and Steve Puhnan and Graham Russell at the Universi ty of Cambridge. It is one of three closely related projects funded under the Alvey IKBS Programme (Natural Language Tlleme); a parser is under development at Edinburgh by Henry Thompson and John Phillips, and a sentence grammar is being devised by Ted Briscoe and Clare Grover at Lancaster and Bran Boguraev and John Carroll at Cambridge. It is intended tha t the software and rules produced by all three projects wil l be directly compatible and capable of functioning in an integrated system. Realistic and useful na tura l language processing systems such as database f ront-ends require large numbers of words, together wi th associated syntactic and semantic Information, to be efficiently stored in machine-readable form. Our system is Intended to provide the necessary facilities, being designed to store a large number (at least 10,000) of words and to perform morphological analysis on them, covering both Inflectional and derlvatlonal morphology. In pursuit of these objectives, the dictionary associates wi th each word information concerning its morphosyntactlc properties. Users are free to modify the system In a number of ways; they may add to the lexical entries Lisp functions tha t perform semantic manipulatlons, and tailor the dictionary to the particular subject mat ter they are interested in (different databases, for example). I t Is also hoped that the system is general enough to be of use to linguists wishing to Investigate the morphology of English and other languages. Contents of the basle data files may be altered or replaced: 1. A 'Word Grammar ' file contains rules assigning internal s t ructure to complex words, 2. A 'Lexicon' file holds the morpheme entries which include syntactic and other Information associated wi th stems and affixes. 3. A 'Spelling Rules' file contains rules governing permissible correspondences between the form of morphemes listed in the lextcon and complex words consisting of sequences of these morphemes. Once these data flies have been prepared, they are compiled using a number of pre-processtng functions tha t operate to produce a set of output files. These constitute a fu l ly expanded and cross-Indexed dictionary which can then be accessed from within LISP. The process of morphological analysis consists of parslng a sequence of Input morphemes wi th respect to the word grammar, It Is Implemented as an active chart parser (Thompson & Rltchle (1984)), and builds a s t ructure in the form of a tree in which each node has two","The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; #AUTHOR_TAG ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .","['Recent developments in linguistics, and especially on grammatical theory --for example, Generalised Phrase Structure Grammar (GPSG) (Gazdar et al., 1985), Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982) --and on natural language parsing frameworks for example, Functional Unification Grammar (FUG) (Kay, 1984a), PATR-II (Shieber, 1984) --make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; #AUTHOR_TAG ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', 'One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.']",0,"['The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; #AUTHOR_TAG ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .']"
CC760,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,exploiting a large dictionary data base,['Archibal Michiels'],,,"There are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see #AUTHOR_TAG , for further details ) .","['There are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see #AUTHOR_TAG , for further details ) .', 'However, exploiting this information to the full would be a non-trivial task, because it would require accessing the relevant knowledge about the words contained in the qualifier fields from their LDOCE entries.']",0,"['There are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see #AUTHOR_TAG , for further details ) .', 'However, exploiting this information to the full would be a non-trivial task, because it would require accessing the relevant knowledge about the words contained in the qualifier fields from their LDOCE entries.']"
CC761,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,the automatic grammatical tagging of the lob corpus,"['Geoffrey Leech', 'Roger Garside', 'Erik Atwell']",conclusion,"In collaboration with the English Department, University of Oslo, and the Nowegian Computing Centre for the Humanities, Bergen we have been engaged in the automatic grammatical tagging of the LOB (LancasterOslo/Bergen) Corpus of British English. The computer programs for this task are running at a success rate of approximately 96.7% and a substantial part of the 1,000,000-word corpus has already been tagged. The purpose of this paper is to give an account of the project, with special reference to the methods of tagging we have adopted.","In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #AUTHOR_TAG ) .","['In the longer term, it is clear that neither the contents nor form of any existing published dictionary meet all the requirements of a natural language processing system.', 'A substantial component of the research reported above has been devoted to restructuring LDOCE to make it more suitable for automatic analysis.', 'However, even after this process much of the information in LDOCE remains difficult to access, essentially because it is aimed at a human reader, as opposed to a computer system.', 'This suggests that the automatic construction of dictionaries from published sources intended for other purposes will have a limited life unless lexicography is heavily influenced by the requirements of automated natural language analysis.', 'In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #AUTHOR_TAG ) .', 'However, in the short term, the approach outlined in this paper will allow us to produce a relatively sophisticated and useful dictionary rapidly.']",3,"['In the longer term, it is clear that neither the contents nor form of any existing published dictionary meet all the requirements of a natural language processing system.', 'In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #AUTHOR_TAG ) .']"
CC762,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,diagram a grammar for dialogues,['Jane Robinson'],introduction,,"#AUTHOR_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand .","['The idea of using the machine readable source of a published dictionary has occurred to a wide range of researchers, for spelling correction, lexical analysis, thesaurus construction, and machine translation, to name but a few applications.', 'Most of the work on automated dictionaries has concentrated on extracting lexical or other information, essentially by batch processing (eg. Amsler, 1981 ;Walker and Amsler, 1986), or Copyright 1987 by the Association for Computational Linguistics.', 'Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.', 'To copy otherwise, or to republish, requires a fee and/or specific permission.', '0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b).', 'Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.', '#AUTHOR_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand .', 'Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources.', 'In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details).', 'However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.', 'In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.']",1,"['Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.', '#AUTHOR_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand .', 'Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources.', 'In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.']"
CC763,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,predication linguistic inquiry,['E S Williams'],,,"Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see #AUTHOR_TAG , for further discussion ) .","['The solution we have adopted is to derive a semantic classification of the particular sense of the verb under consideration on the basis of the complete set of codes assigned to that sense.', 'In any subcategorisation frame which involves a predicate complement there will be a non-transparent relationship between the superficial syntactic form and the underlying logical relations in the sentence.', 'In these situations the parser can use the semantic type of the verb to compute this relationship.', 'Expanding on a suggestion of Michiels (1982), we classify verbs as Subject Equi, Object Equi, Subject Raising or Object Raising for each sense which has a predicate complement code associated with it.', 'These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation.', 'For example, labelling believe(3) (Type 20Raising) indicates that this is a two place predicate and that, if believe(3) occurs with a syntactic direct object, as in ( 1) John believes the Earth to be round it will function as the logical subject of the predicate complement.', 'Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see #AUTHOR_TAG , for further discussion ) .']",0,"['In any subcategorisation frame which involves a predicate complement there will be a non-transparent relationship between the superficial syntactic form and the underlying logical relations in the sentence.', 'In these situations the parser can use the semantic type of the verb to compute this relationship.', 'Expanding on a suggestion of Michiels (1982), we classify verbs as Subject Equi, Object Equi, Subject Raising or Object Raising for each sense which has a predicate complement code associated with it.', 'For example, labelling believe(3) (Type 20Raising) indicates that this is a two place predicate and that, if believe(3) occurs with a syntactic direct object, as in ( 1) John believes the Earth to be round it will function as the logical subject of the predicate complement.', 'Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see #AUTHOR_TAG , for further discussion ) .']"
CC764,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,exploiting a large dictionary data base,['Archibal Michiels'],introduction,,( #AUTHOR_TAG contains further description and discussion of LDOCE . ),"['We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", '( #AUTHOR_TAG contains further description and discussion of LDOCE . )', 'In this paper we focus on the exploitation of the LDOCE grammar coding system; Alshawi et al. (1985) and Alshawi (1987) describe further research in Cambridge utilising different types of information available in LDOCE.']",0,"['We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", '( #AUTHOR_TAG contains further description and discussion of LDOCE . )', 'In this paper we focus on the exploitation of the LDOCE grammar coding system; Alshawi et al. (1985) and Alshawi (1987) describe further research in Cambridge utilising different types of information available in LDOCE.']"
CC765,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,exploiting a large dictionary data base,['Archibal Michiels'],,,"Expanding on a suggestion of #AUTHOR_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it .","['The solution we have adopted is to derive a semantic classification of the particular sense of the verb under consideration on the basis of the complete set of codes assigned to that sense.', 'In any subcategorisation frame which involves a predicate complement there will be a non-transparent relationship between the superficial syntactic form and the underlying logical relations in the sentence.', 'In these situations the parser can use the semantic type of the verb to compute this relationship.', 'Expanding on a suggestion of #AUTHOR_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it .', 'These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation.', 'For example, labelling believe(3) (Type 20Raising) indicates that this is a two place predicate and that, if believe(3) occurs with a syntactic direct object, as in ( 1) John believes the Earth to be round it will function as the logical subject of the predicate complement.', 'Michiels proposed rules for doing this for infinitive complement codes; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP, AP and PP predication (see Williams (1980), for further discussion).']",2,"['The solution we have adopted is to derive a semantic classification of the particular sense of the verb under consideration on the basis of the complete set of codes assigned to that sense.', 'In these situations the parser can use the semantic type of the verb to compute this relationship.', 'Expanding on a suggestion of #AUTHOR_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it .', 'These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation.']"
CC766,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,an information theoretic analysis of phonetic dictionary access computer speech and language,['David Carter'],,,"In addition to headwords , dictionary search through the pronunciation field is available ; #AUTHOR_TAG has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( Huttenlocher and Zue , 1983 ) .","['From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'In addition to headwords , dictionary search through the pronunciation field is available ; #AUTHOR_TAG has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( Huttenlocher and Zue , 1983 ) .', 'In addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information (Boguraev et al., 1987).', 'Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample.']",5,"['In addition to headwords , dictionary search through the pronunciation field is available ; #AUTHOR_TAG has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( Huttenlocher and Zue , 1983 ) .']"
CC767,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,a grammar of contemporary english longman group limited,"['Randolph Quirk', 'Sidney Greenbaum', 'Geoffrey Leech', 'Jan Svartvik']",,,"The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of #AUTHOR_TAG , 1985 ) .","['Once the grammar codes have been restructured, it still remains to be shown that the information they encode is going to be of some utility for natural language processing.', 'The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of #AUTHOR_TAG , 1985 ) .', 'The codes are doubly articulated; capital letters represent the grammatical relations which hold between a verb and its arguments and numbers represent subcategorisation frames which a verb can appear in.', 'Most of the subcategorisation frames are specified by syntactic category, but some are very ill-specified; for instance, 9 is defined as ""needs a descriptive word or phrase"".', 'In practice many adverbial and predicative complements will satisfy this code, when attached to a verb; for example, put [xg] where the code marks a locative adverbial prepositional phrase vs. make under sense 14 (hereafter written make( 14)) is coded IX9] where it marks a predicative noun phrase or prepositional phrase.']",2,"['The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of #AUTHOR_TAG , 1985 ) .']"
CC768,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,automatic analysis of texts in informatics 7,['Archibal Michiels'],,,"Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English ( #AUTHOR_TAG ) .","['Given that we were targeting all envisaged access routes from LDOCE to systems implemented in Lisp, and since the natural data structure for Lisp is the s-expression, we adopted the approach of converting the tape source into a set of list structures, one per entry.', 'Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English ( #AUTHOR_TAG ) .']",0,"['Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English ( #AUTHOR_TAG ) .']"
CC769,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,designing a computerised lexicon for linguistic purposes,"['Erik Akkerman', 'Pieter Masereeuw', 'Willem Meijs']",,,Michiels ( 1982 ) and #AUTHOR_TAG provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .,"['The criteria for assignment of capital letters to verbs is not made explicit, but is influenced by the syntactic and semantic relations which hold between the verb and its arguments; for example, I5, L5 and T5 can all be assigned to verbs which take a NP subject and a sentential complement, but L5 will only be assigned if there is a fairly close semantic link between the two arguments and T5 will be used in preference to I5 if the verb is felt to be semantically two place rather than one place, such as know versus appear.', 'On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic.', 'Michiels ( 1982 ) and #AUTHOR_TAG provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .', 'Ingria (1984) comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated.']",0,['Michiels ( 1982 ) and #AUTHOR_TAG provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .']
CC770,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,the design of a computer language for linguistic information,['S Shieber'],introduction,"A considerable body of accumulated knowledge about the design of languages for communicating information to computers has been derived from the subfields of programming language design and semantics. It has been the goal of the PATR group at SRI to utilize a relevant portion of this knowledge in implementing tools to facilitate communication of linguistic information to computers. The PATR-II formalism is our current computer language for encoding linguistic information. This paper, a brief overview of that formalism, attempts to explicate our design decisions in terms of a set of properties that effective computer languages should incorporate.Engineering and Applied Science","Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( Kaplan and Bresnan , 1982 ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( #AUTHOR_TAG ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .","['Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( Kaplan and Bresnan , 1982 ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( #AUTHOR_TAG ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'The research described below is taking place in the context of three collaborative projects (Boguraev, 1987;Russell et al., 1986;Phillips and Thompson, 1986) to develop a general-purpose, wide coverage morphological and syntactic analyser for English.', 'One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.']",0,"['Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( Kaplan and Bresnan , 1982 ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( #AUTHOR_TAG ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.']"
CC771,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,phonotactic and lexical constraints in speech recognition,"['Daniel Huttenlocher', 'Victor Zue']",,"We demonstrate a method for partitioning a large lexicon into small equivalence classes, based on sequential phonetic and prosodic constraints. The representation is attractive for speech recognition systems because it allows all but a small number of word candidates to be excluded, using only gross phonetic and prosodic information. The approach is a robust one in that the representation is relatively insensitive to phonetic variability and recognition error.","In addition to headwords , dictionary search through the pronunciation field is available ; Carter ( 1987 ) has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( #AUTHOR_TAG ) .","['From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'In addition to headwords , dictionary search through the pronunciation field is available ; Carter ( 1987 ) has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( #AUTHOR_TAG ) .', 'In addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information (Boguraev et al., 1987).', 'Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample.']",0,"['In addition to headwords , dictionary search through the pronunciation field is available ; Carter ( 1987 ) has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( #AUTHOR_TAG ) .']"
CC772,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,a comprehensive grammar of english longman group limited,"['Randolph Quirk', 'Sidney Greenbaum', 'Geoffrey Leech', 'Jan Svartvik']",,,"The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #AUTHOR_TAG ) .","['In what follows we will discuss the format of the grammar codes in some detail as they are the focus of the current paper, however, the reader should bear in mind that they represent only one comparatively constrained field of an LDOCE entry and therefore, a small proportion of the overall restructuring task.', 'Figure 4 illustrates the grammar code field for the third word sense of the verb believe as it appears in the published dictionary, on the typesetting tape and after restructuring.', 'Figure 4 LDOCE provides considerably more syntactic information than a traditional dictionary.', 'The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #AUTHOR_TAG ) .', 'A grammar code describes a particular pattern of behaviour of a word.', 'Patterns are descriptive, and are used to convey a range of information: eg.', 'distinctions between count and mass nouns (dog vs. desire), predicative, postpositive and attributive adjectives (asleep vs. elect vs. jokular), noun complementation (fondness, fact) and, most importantly, verb complementation and valency.']",0,"['In what follows we will discuss the format of the grammar codes in some detail as they are the focus of the current paper, however, the reader should bear in mind that they represent only one comparatively constrained field of an LDOCE entry and therefore, a small proportion of the overall restructuring task.', 'Figure 4 LDOCE provides considerably more syntactic information than a traditional dictionary.', 'The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #AUTHOR_TAG ) .', 'A grammar code describes a particular pattern of behaviour of a word.']"
CC773,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,syntactic argumentation and the structure of english,"['D M Perlmutter', 'S Soames']",,"Syntactic Argumentation and the Structure of English (SASE) presents the major theoretical developments in generative syntax and the empirical arguments motivating them. Beautifully and lucidly written, it is an invaluable resource for working linguists as well as a pedagogical tool of unequaled depth and breadth. The chief focus of the book is syntactic argumentation. Beginning with the fundamentals of generative syntax, it proceeds by a series of gradually unfolding arguments to analyses of some of the most sophisticated proposals. It includes a wide variety of problems that guide the reader in constructing arguments deciding between alternative analyses of syntactic constructions and alternative theoretical formulations. Someone who has worked through the problems and arguments in this book will be able to apply the skills in argumentation it develops to novel issues in syntax. While teaching syntactic argumentation, SASE covers the major empirical results of generative syntax. Its contents include: Transformations in single-clause sentences; Complementation and multi-clause transformations; Universal principles governing rule interaction: the cycle and strict cyclicity; Movement rules; Ross' constraints; Pronominal reference and anaphora. SASE is an important book for several different audiences: for students, it is an introduction to syntax that teaches argumentation as well as a wide range of empirical results in the field; for linguists, it is a sourcebook of classical analyses and arguments, with some new arguments bearing on classical issues; and, for scholars, teachers, and students in related fields, it is a comprehensive guide to the major empirical and theoretical developments in generative syntax. SASE contains enough material for a two-semester or three-quarler sequence in syntax. Because it assumes no previous background, it can be used as the main text in an introduction to syntax. Since it covers a wide range of material not available in other texts, it is also suitable for intermediate and advanced syntax courses and as a supplementary source in more specialized courses and courses in other disciplines. A storehouse of classical and original arguments, SASE will prove to be of lasting value to the teacher, the student, and researchers in both linguistics and related fields.","Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed ( #AUTHOR_TAG:460 ff . )","['This small experiment demonstrates a number of points.', 'Firstly, it seems reasonable to conclude that the assignment of individual codes to verbs is on the whole relatively accurate in LDOCE.', 'Of the 139 verbs tested, we only found code omissions in 10 cases.', 'Secondly though, when we consider the interaction between the assignments of codes and word sense classification, LDOCE appears less reliable.', 'This is the primary source of error in the case of the Object Raising rule.', 'Thirdly, it seems clear that the Object Raising rule is straining the limits of what can be reliably extracted from the LDOCE coding system.', 'Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed ( #AUTHOR_TAG:460 ff . )', 'However, only two of these criteria are explicit in the coding system.']",3,"['Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed ( #AUTHOR_TAG:460 ff . )']"
