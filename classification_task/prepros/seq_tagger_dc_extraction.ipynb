{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import pandas as pd\n",
    "from peft import PeftModel, LoraConfig, TaskType, get_peft_model\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import re\n",
    "\n",
    "SPECIAL_TOKEN = ['#AUTHOR_TAG', '#TAUTHOR_TAG']\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "DATASET = 'fine_cite'\n",
    "SEGMENT = 'token_scibert'\n",
    "SEGMENT_TYPE = 'sentence' if 'sentence' in SEGMENT else 'token'\n",
    "MODEL_NAME =  'scibert' #\"allenai/scibert_scivocab_uncased\"  # \"McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp\",\"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\"\n",
    "MODE = 'total'\n",
    "MODEL_PATH = f'/raid/deallab/CCE_Data/model_training/output/seq_tagger/fine_cite/{MODEL_NAME}/{SEGMENT}_{MODE}_2_1e-05_0.0/model_state_e_9.pt'\n",
    "\n",
    "# Define the argument values as Python variables\n",
    "num_labels = 4 if MODE == 'scopes' else 2\n",
    "batch_size = 2\n",
    "data_parallel = True\n",
    "\n",
    "with open(f'/home/deallab/lasse/CCE/postprocessing/output/finecite_{MODE}_weights.json') as weights_file:\n",
    "    weights = json.load(weights_file)\n",
    "    \n",
    "INPUT_DIR = f'/raid/deallab/CCE_Data/model_evaluation/classification_task/data/acl_arc/'\n",
    "#INPUT_DIR = f'/raid/deallab/CCE_Data/model_training/data/seq_tagger/fine_cite/sentence_majo_99__07-01-02/'\n",
    "OUTPUT_DIR = f'/raid/deallab/CCE_Data/model_evaluation/classification_task/data/acl_arc/finecite/'\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'd_nc', f'{SEGMENT}_{MODE}'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'd_c', f'{SEGMENT}_{MODE}'), exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if MODEL_NAME == 'scibert':\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': SPECIAL_TOKEN})\n",
    "    config = AutoConfig.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "    LMmodel = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\", config=config, torch_dtype=torch.bfloat16)\n",
    "    LMmodel.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SeqTagger(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        model,\n",
    "        num_labels,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_config = model.config\n",
    "        self.model = torch.nn.DataParallel(model) if data_parallel else model\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.classifier = torch.nn.Linear(self.model_config.hidden_size, num_labels, dtype=torch.bfloat16)\n",
    "        self.loss_fn = CrossEntropyLoss(weight=torch.BFloat16Tensor(weights[SEGMENT]).to(device))\n",
    "\n",
    "    \n",
    "    def predict(self, input_ids, attention_mask):\n",
    "        ids = input_ids.to(device, dtype = torch.long)\n",
    "        mask = attention_mask.to(device, dtype = torch.long)\n",
    "        output = self.model(input_ids= ids, attention_mask = mask)\n",
    "        if SEGMENT_TYPE == 'sentence':\n",
    "            cls_output_state = output[\"last_hidden_state\"][ids == self.tokenizer.cls_token_id]\n",
    "        if SEGMENT_TYPE == 'token':\n",
    "            cls_output_state = output['last_hidden_state'][attention_mask == 1]\n",
    "        logits = self.classifier(cls_output_state)\n",
    "        return logits\n",
    "\n",
    "model = SeqTagger(model=LMmodel, tokenizer=tokenizer, num_labels=num_labels)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper\n",
    "def replace_authortags(sent):\n",
    "    regex1 = r'((?:\\( *)?(?:(?:(?:(?:[A-Z][A-Za-z\\'\\`-]+)(?:,? (?:(?:and |& )?(?:[A-Z][A-Za-z\\'\\`-]+)|(?:et al.?)))* ?(?:, *\\(? *(?:19|20)[0-9][0-9](?:, p.? [0-9]+)?| *\\\\((?:19|20)[0-9][0-9](?:, p.? [0-9]+)? *\\)?\\\\)))(?: |;|,|and)*)+)?#AUTHOR_TAG(?:(?: |;|,|and)*(?:(?:(?:[A-Z][A-Za-z\\'\\`-]+)(?:,? (?:(?:and |& )?(?:[A-Z][A-Za-z\\'\\`-]+)|(?:et al.?)))* ?(?:, *\\(? *(?:19|20)[0-9][0-9](?:, p.? [0-9]+)?| *\\\\((?:19|20)[0-9][0-9](?:, p.? [0-9]+)? *\\)?\\\\)))(?: |;|,|and)*)+)?(?: *\\))?)'\n",
    "    regex2 = r'((?:\\( *)?(?:(?:[A-Z][a-z\\'\\`-]+)(?:,? (?:(?:and |& )?(?:[A-Z][a-z\\'\\`-]+)|(?:et al.?)))* ?(?:, *\\(? *(?:19|20)[0-9][0-9](?:, p.? [0-9]+)?| *\\\\((?:19|20)[0-9][0-9](?:, p.? [0-9]+)? *\\)?\\\\))(?: |;|,|and)*)+(?: *\\))?|(?:(?:[A-Z][a-z\\'\\`-]+)(?:,? (?:(?:and |& )?(?:[A-Z][a-z\\'\\`-]+)|(?:et al.?)))* *\\( *(?:19|20)[0-9][0-9](?:, p.? [0-9]+)? *\\)?))'\n",
    "    tauthro_tags = [match[0] for match in re.findall(regex1, sent)]\n",
    "    sent = re.sub(regex1, '#TAUTHOR_TAG ', sent)\n",
    "    author_tags = [match[0] for match in re.findall(regex2, sent)]\n",
    "    sent =  re.sub(regex2, '#AUTHOR_TAG ', sent)\n",
    "    assert len(re.findall(r'#T?AUTHOR_TAG',sent)) == len(tauthro_tags) + len(author_tags), f'{sent}, {tauthro_tags}, {author_tags}'\n",
    "    return sent, tauthro_tags, author_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cite_sent_id(par_list):\n",
    "    cite_sent_id = -1\n",
    "    for i, sent in enumerate(par_list):\n",
    "        if re.search(r'#AUTHOR_TAG', sent):\n",
    "            cite_sent_id = i\n",
    "            break\n",
    "    return cite_sent_id\n",
    "\n",
    "def build_contiguous_context(segments, labels, start_id):\n",
    "    context=[segments[start_id]]\n",
    "    prev_id = start_id -1\n",
    "    next_id = start_id +1\n",
    "    while prev_id >=0:\n",
    "        if labels[prev_id] != 0:\n",
    "            context.insert(0,segments[prev_id])\n",
    "            prev_id -= 1\n",
    "        else: break\n",
    "    while next_id < len(segments):\n",
    "        if labels[next_id] != 0:\n",
    "            context.append(segments[next_id])\n",
    "            next_id += 1\n",
    "        else: break\n",
    "    return context\n",
    "    \n",
    "def build_sentence_context(row):\n",
    "    par_list = eval(row['cite_context_paragraph'])\n",
    "    cite_sent_id = find_cite_sent_id(par_list)\n",
    "    if cite_sent_id == -1:\n",
    "        print('no author tag')\n",
    "        return [],[]\n",
    "    \n",
    "    # prepare for tokinization\n",
    "    inputs = [replace_authortags(sent)[0] for sent in par_list]\n",
    "    inputs = [seg.replace(tokenizer.cls_token, 'cls_token') for seg in inputs] # replace [CLS] / <s> token in text\n",
    "    inputs = f' {tokenizer.cls_token} '.join(inputs)\n",
    "    input_ids = tokenizer.encode(inputs)\n",
    "    \n",
    "    # ignore sentences over the 512 token limit, delete them but make sure the cls token are still there\n",
    "    if len(input_ids) > 512:\n",
    "        cls_token_list = []\n",
    "        while len(input_ids) + len(cls_token_list) > 512:\n",
    "            item = input_ids.pop()\n",
    "            if item == tokenizer.cls_token_id:\n",
    "                cls_token_list.insert(0, item)\n",
    "        input_ids.extend(cls_token_list)\n",
    "    assert len(input_ids) <= 512, f'the input ids are longer than the max amount of token: {len(input_ids), len(input_ids) > 512}'\n",
    "    \n",
    "    input_mask = torch.tensor([[1] * len(input_ids) + [0] * (512 - len(input_ids))])\n",
    "    input_ids = torch.tensor([input_ids + [tokenizer.pad_token_id] * (512 - len(input_ids))])\n",
    "    model.eval()\n",
    "    output = model.predict(input_ids, input_mask)\n",
    "    preds = output.argmax(-1)\n",
    "    if preds[cite_sent_id] == 0:\n",
    "        preds[cite_sent_id] = 1\n",
    "        \n",
    "    context_c = build_contiguous_context(par_list, preds, cite_sent_id)\n",
    "    context_nc = [sent for i, sent in enumerate(par_list) if preds[i] != 0]\n",
    "    return context_nc, context_c\n",
    "    \n",
    "    \n",
    "# build context for token segmentation\n",
    "def build_token_context(row):\n",
    "    par_list = eval(row['cite_context_paragraph'])\n",
    "    cite_sent_id = find_cite_sent_id(par_list)\n",
    "    if cite_sent_id == -1:\n",
    "        print('no author tag')\n",
    "        return [],[]\n",
    "    par_string = ' '.join(par_list)\n",
    "    inputs, tauthor_token, author_token = replace_authortags(par_string)      \n",
    "    input_ids = tokenizer.encode(inputs, add_special_tokens=False)\n",
    "    if len(input_ids) > 510:\n",
    "        input_ids = input_ids[:510]\n",
    "        deleted_token = ' '.join(tokenizer.convert_ids_to_tokens(input_ids[510:]))\n",
    "        print(deleted_token)\n",
    "        if re.search(r'#T?AUTHOR_TAG', deleted_token):\n",
    "            print('deleted author tag')\n",
    "            for _ in re.findall(r'#AUTHOR_TAG', deleted_token):\n",
    "                author_token.pop()\n",
    "            for _ in re.findall(r'#AUTHOR_TAG', deleted_token):\n",
    "                tauthor_token.pop()\n",
    "    #add special token to end and start\n",
    "    input_ids = [tokenizer.cls_token_id] + input_ids + [tokenizer.sep_token_id]\n",
    "    #check input length < 512 token \n",
    "    assert len(input_ids) <= 512, f'the input ids are longer than the max amount of token: {len(input_ids), len(input_ids) > 512}'\n",
    "    \n",
    "    input_mask_tensor = torch.tensor([[1] * len(input_ids) + [0] * (512 - len(input_ids))])\n",
    "    input_ids_tensor = torch.tensor([input_ids + [tokenizer.pad_token_id] * (512 - len(input_ids))])\n",
    "    \n",
    "    model.eval()\n",
    "    output = model.predict(input_ids_tensor, input_mask_tensor)\n",
    "    preds = output.argmax(-1)\n",
    "    assert len(preds) == len(input_ids), f'the len of preds is {len(preds)} and the len of the input ids is {len(input_ids)}'\n",
    "    \n",
    "    input_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    author_tag_ids = [i for i, token in enumerate(input_tokens) if token=='#AUTHOR_TAG']\n",
    "    tauthor_tag_ids = [i for i, token in enumerate(input_tokens) if token=='#TAUTHOR_TAG']\n",
    "    assert len(author_tag_ids) <= len(author_token) and len(tauthor_tag_ids) <= len(tauthor_token), f'{input_tokens}, {tauthor_token}, {author_token}, {len(tauthor_tag_ids)}, {len(author_tag_ids)}'\n",
    "    for i, id in enumerate(author_tag_ids):\n",
    "        input_tokens[id] = author_token[i]\n",
    "    for i, id in enumerate(tauthor_tag_ids):\n",
    "        input_tokens[id] = tauthor_token[i]\n",
    "    if tauthor_tag_ids:\n",
    "        context_c = build_contiguous_context(input_tokens, preds, tauthor_tag_ids[0])\n",
    "    else: context_c = []\n",
    "    context_nc = [sent for i, sent in enumerate(input_tokens) if preds[i] != 0]\n",
    "    return context_nc, context_c\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "no author tag\n",
      "no author tag\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['train', 'test']:\n",
    "    data_df = pd.read_csv(INPUT_DIR + f\"/{dataset}_raw.txt\", sep=\"\\t\", engine=\"python\", dtype=object)\n",
    "    res_df_nc = pd.DataFrame(columns=['CC','label'])\n",
    "    res_df_c = pd.DataFrame(columns=['CC','label'])\n",
    "    for idx, row in data_df.iterrows():\n",
    "        label = row['citation_class_label']\n",
    "        if SEGMENT_TYPE == 'sentence':\n",
    "            try: \n",
    "                context_nc, context_c = build_sentence_context(row)\n",
    "                if context_nc:\n",
    "                    res_df_nc.loc[len(res_df_nc)] = [context_nc, label]\n",
    "                if context_c:\n",
    "                    res_df_c.loc[len(res_df_c)] = [context_c, label]\n",
    "            except: \n",
    "                print('there was a issue with the context')\n",
    "                continue\n",
    "        elif SEGMENT_TYPE == 'token':\n",
    "            # try: \n",
    "            context_nc, context_c = build_token_context(row)\n",
    "            if context_nc:\n",
    "                res_df_nc.loc[len(res_df_nc)] = [context_nc, label]\n",
    "            if context_c:\n",
    "                res_df_c.loc[len(res_df_c)] = [context_c, label]\n",
    "            # except: \n",
    "            #     print('there was a issue with the context')\n",
    "            #     continue\n",
    "    res_df_nc.to_csv(os.path.join(OUTPUT_DIR, 'd_nc', f'{SEGMENT}_{MODE}', f'{dataset}.csv'), index=False)\n",
    "    res_df_c.to_csv(os.path.join(OUTPUT_DIR, 'd_c', f'{SEGMENT}_{MODE}', f'{dataset}.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lasse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
