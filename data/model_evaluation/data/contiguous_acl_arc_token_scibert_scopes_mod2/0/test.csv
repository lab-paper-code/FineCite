token_context,word_context,seg_context,sent_cotext,label
"['##e and polgu ~ re, 1991 ), lfs (Iordanskaja et al., 1992), and joyce (Rambow and Korelsky, 1992).', 'the framework was originally developed for the realization of deep - syntactic structures in nlg ( #AUTHOR_TAG )']","['framework represents a generalization of several predecessor nlg systems based on meaning - text theory : fog ( kittredge and polgu ~ re, 1991 ), lfs (Iordanskaja et al., 1992), and joyce (Rambow and Korelsky, 1992).', 'the framework was originally developed for the realization of deep - syntactic structures in nlg ( #AUTHOR_TAG )']","['##e and polgu ~ re, 1991 ), lfs (Iordanskaja et al., 1992), and joyce (Rambow and Korelsky, 1992).', 'the framework was originally developed for the realization of deep - syntactic structures in nlg ( #AUTHOR_TAG )']","['framework represents a generalization of several predecessor nlg systems based on meaning - text theory : fog ( kittredge and polgu ~ re, 1991 ), lfs (Iordanskaja et al., 1992), and joyce (Rambow and Korelsky, 1992).', 'the framework was originally developed for the realization of deep - syntactic structures in nlg ( #AUTHOR_TAG )']",0
['##ii notation used internally in the framework ( #AUTHOR_TAG )'],['ascii notation used internally in the framework ( #AUTHOR_TAG )'],"['from a meteorological application, meteocogent ( Kittredge and Lavoie , 1998 ), represented using the standard graphical notation and also the realpro ascii notation used internally in the framework ( #AUTHOR_TAG )']","['first of these characteristics makes a dependency tree structure a very useful representation for mt and multilingual nlg, since it gives linguists a representation that allows them to abstract over numerous crosslinguistic divergences due to language specific ordering ( polgu ~ re, 1991 in the implemented applications, the dsyntss are the pivotal representations involved in most transformations, as this is also often the case in practice in linguistic - based mt (Hutchins and Somers, 1997).', 'figure 2 illustrates a dsynts from a meteorological application, meteocogent ( Kittredge and Lavoie , 1998 ), represented using the standard graphical notation and also the realpro ascii notation used internally in the framework ( #AUTHOR_TAG )']",2
"['- o ii a failli pleuvoir.', 'more details on how the structural divergences described in ( #AUTHOR_TAG ) can be accounted for using our formalism can be found in ( Nasr et al. , 1998 )']","['- o ii a failli pleuvoir.', 'more details on how the structural divergences described in ( #AUTHOR_TAG ) can be accounted for using our formalism can be found in ( Nasr et al. , 1998 )']","['- o ii a failli pleuvoir.', 'more details on how the structural divergences described in ( #AUTHOR_TAG ) can be accounted for using our formalism can be found in ( Nasr et al. , 1998 )']","['- o ii a failli pleuvoir.', 'more details on how the structural divergences described in ( #AUTHOR_TAG ) can be accounted for using our formalism can be found in ( Nasr et al. , 1998 )']",0
"['taken in systems such as ariane ( Vauquois and Boitet , 1985 ), fog ( Kittredge and Polguere , 1991 ), joyce ( rambow and #AUTHOR_TAG ), and lfs ( Iordanskaja et al. , 1992 ).', 'although it adopts the general principles found in the abovementioned systems, the approach presented in this paper is more']","['taken in systems such as ariane ( Vauquois and Boitet , 1985 ), fog ( Kittredge and Polguere , 1991 ), joyce ( rambow and #AUTHOR_TAG ), and lfs ( Iordanskaja et al. , 1992 ).', 'although it adopts the general principles found in the abovementioned systems, the approach presented in this paper is more practical, and we believe, would eventually integrate better with emerging statistics - based approaches to mt']","['taken in systems such as ariane ( Vauquois and Boitet , 1985 ), fog ( Kittredge and Polguere , 1991 ), joyce ( rambow and #AUTHOR_TAG ), and lfs ( Iordanskaja et al. , 1992 ).', 'although it adopts the general principles found in the abovementioned systems, the approach presented in this paper is more']","['this paper we present a linguistically motivated framework for uniform lexicostructural processing.', 'it has been used for transformations of conceptual and syntactic structures during generation in monolingual and multilingual natural language generation ( nlg ) and for transfer in machine translation ( mt ).', 'our work extends directions taken in systems such as ariane ( Vauquois and Boitet , 1985 ), fog ( Kittredge and Polguere , 1991 ), joyce ( rambow and #AUTHOR_TAG ), and lfs ( Iordanskaja et al. , 1992 ).', 'although it adopts the general principles found in the abovementioned systems, the approach presented in this paper is more practical, and we believe, would eventually integrate better with emerging statistics - based approaches to mt']",2
"['framework represents a generalization of several predecessor nlg systems based on meaning - text theory : fog ( Kittredge and Polguere , 1991 ), lfs ( Iordanskaja et al. , 1992 ), and joyce ( rambow and #AUTHOR_TAG ).', 'the framework was originally developed for the realization of deep - syntactic structures in nlg']","['framework represents a generalization of several predecessor nlg systems based on meaning - text theory : fog ( Kittredge and Polguere , 1991 ), lfs ( Iordanskaja et al. , 1992 ), and joyce ( rambow and #AUTHOR_TAG ).', 'the framework was originally developed for the realization of deep - syntactic structures in nlg']","['framework represents a generalization of several predecessor nlg systems based on meaning - text theory : fog ( Kittredge and Polguere , 1991 ), lfs ( Iordanskaja et al. , 1992 ), and joyce ( rambow and #AUTHOR_TAG ).', 'the framework was originally developed for the realization of deep - syntactic structures in nlg']","['framework represents a generalization of several predecessor nlg systems based on meaning - text theory : fog ( Kittredge and Polguere , 1991 ), lfs ( Iordanskaja et al. , 1992 ), and joyce ( rambow and #AUTHOR_TAG ).', 'the framework was originally developed for the realization of deep - syntactic structures in nlg']",2
"['1 : example text shown in standard and asr format ation which is not available in asr output is sentence boundary information.', 'however, knowledge of sentence boundaries is required by many nlp technologies.', ""part of speech taggers typically require input in the format of a single sentence per line ( for example brill's tagger ( #AUTHOR_TAG ) ) and parsers generally aim to produce a tree spanning each sentence""]","['1 : example text shown in standard and asr format ation which is not available in asr output is sentence boundary information.', 'however, knowledge of sentence boundaries is required by many nlp technologies.', ""part of speech taggers typically require input in the format of a single sentence per line ( for example brill's tagger ( #AUTHOR_TAG ) ) and parsers generally aim to produce a tree spanning each sentence""]","['1 : example text shown in standard and asr format ation which is not available in asr output is sentence boundary information.', 'however, knowledge of sentence boundaries is required by many nlp technologies.', ""part of speech taggers typically require input in the format of a single sentence per line ( for example brill's tagger ( #AUTHOR_TAG ) ) and parsers generally aim to produce a tree spanning each sentence""]","['1 : example text shown in standard and asr format ation which is not available in asr output is sentence boundary information.', 'however, knowledge of sentence boundaries is required by many nlp technologies.', ""part of speech taggers typically require input in the format of a single sentence per line ( for example brill's tagger ( #AUTHOR_TAG ) ) and parsers generally aim to produce a tree spanning each sentence""]",0
"['both languages, we resolved coreference by using swizzle, our implementation of a bilingual coreference resolver.', 'swizzle is a multilingual enhancement of cocktail ( #AUTHOR_TAG ), a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information.', 'when cocktail was applied separately on the english and the ro - manian texts, coreferring links were identified for each english and romanian document respectively.', 'when aligned referential expressions corefer with non - aligned anaphors, swizzle derived new heuris - tics for coreference.', 'our experiments show that swizzleoutperform']","['both languages, we resolved coreference by using swizzle, our implementation of a bilingual coreference resolver.', 'swizzle is a multilingual enhancement of cocktail ( #AUTHOR_TAG ), a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information.', 'when cocktail was applied separately on the english and the ro - manian texts, coreferring links were identified for each english and romanian document respectively.', 'when aligned referential expressions corefer with non - aligned anaphors, swizzle derived new heuris - tics for coreference.', 'our experiments show that swizzleoutperformed cocktailon both english and romanian test documents']","['both languages, we resolved coreference by using swizzle, our implementation of a bilingual coreference resolver.', 'swizzle is a multilingual enhancement of cocktail ( #AUTHOR_TAG ), a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information.', 'when cocktail was applied separately on the english and the ro - manian texts, coreferring links were identified for each english and romanian document respectively.', 'when aligned referential expressions corefer with non - aligned anaphors, swizzle derived new heuris - tics for coreference.', 'our experiments show that swizzleoutperformed cocktailon both english and romanian test documents']","['both languages, we resolved coreference by using swizzle, our implementation of a bilingual coreference resolver.', 'swizzle is a multilingual enhancement of cocktail ( #AUTHOR_TAG ), a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information.', 'when cocktail was applied separately on the english and the ro - manian texts, coreferring links were identified for each english and romanian document respectively.', 'when aligned referential expressions corefer with non - aligned anaphors, swizzle derived new heuris - tics for coreference.', 'our experiments show that swizzleoutperformed cocktailon both english and romanian test documents']",2
"[', some of the best - performing and most robust coreference resolution systems employ knowledge - based techniques.', 'traditionally, these techniques have combined extensive syntactic, se - mantic, and discourse knowledge.', 'the acquisition of such knowledge is time - consuming, difficult, and error - prone.', 'nevertheless, recent results show that knowledge - poor methods perform with amazing accuracy ( cfxxx ( #AUTHOR_TAG ), ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ).', 'for example, cogniac (Baldwin, 1997), a system based on seven ordered heuristics, generates high - precision resolution ( over 90 % ) for some cases of pronominal reference.', 'for this research, we used a coreference resolution sys - tem ( (Harabagiu and Malorano, 1999) ) that imple - ment']","[', some of the best - performing and most robust coreference resolution systems employ knowledge - based techniques.', 'traditionally, these techniques have combined extensive syntactic, se - mantic, and discourse knowledge.', 'the acquisition of such knowledge is time - consuming, difficult, and error - prone.', 'nevertheless, recent results show that knowledge - poor methods perform with amazing accuracy ( cfxxx ( #AUTHOR_TAG ), ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ).', 'for example, cogniac (Baldwin, 1997), a system based on seven ordered heuristics, generates high - precision resolution ( over 90 % ) for some cases of pronominal reference.', 'for this research, we used a coreference resolution sys - tem ( (Harabagiu and Malorano, 1999) ) that imple - ments different sets of heuristics corresponding to various forms of coreference.', 'this system, called cocktail, resolves coreference by exploiting several textual cohesion constraints']","[', some of the best - performing and most robust coreference resolution systems employ knowledge - based techniques.', 'traditionally, these techniques have combined extensive syntactic, se - mantic, and discourse knowledge.', 'the acquisition of such knowledge is time - consuming, difficult, and error - prone.', 'nevertheless, recent results show that knowledge - poor methods perform with amazing accuracy ( cfxxx ( #AUTHOR_TAG ), ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ).', 'for example, cogniac (Baldwin, 1997), a system based on seven ordered heuristics, generates high - precision resolution ( over 90 % ) for some cases of pronominal reference.', 'for this research, we used a coreference resolution sys - tem ( (Harabagiu and Malorano, 1999) ) that imple - ments different sets of heuristics corresponding to various forms of coreference.', 'this system, called cocktail, resolves coreference by exploiting several textual cohesion constraints (']","[', some of the best - performing and most robust coreference resolution systems employ knowledge - based techniques.', 'traditionally, these techniques have combined extensive syntactic, se - mantic, and discourse knowledge.', 'the acquisition of such knowledge is time - consuming, difficult, and error - prone.', 'nevertheless, recent results show that knowledge - poor methods perform with amazing accuracy ( cfxxx ( #AUTHOR_TAG ), ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ).', 'for example, cogniac (Baldwin, 1997), a system based on seven ordered heuristics, generates high - precision resolution ( over 90 % ) for some cases of pronominal reference.', 'for this research, we used a coreference resolution sys - tem ( (Harabagiu and Malorano, 1999) ) that imple - ments different sets of heuristics corresponding to various forms of coreference.', 'this system, called cocktail, resolves coreference by exploiting several textual cohesion constraints ( e. g. term repetition ) combined with lexical and textual coherence cues ( e. g. subjects of communication verbs are more likely to refer to the last person mentioned in the text ).', 'these constraints are implemented as a set of heuristics ordered by their priority.', 'moreover, the cocktailframework uniformly addresses the prob - lem of interaction between different forms of coref - erence, thus making the extension to multilingual coreference very natural']",0
"['involve only derivational morphology - linking verbs with their nominalizations.', 'on other occasions, coercions are obtained as paths of meronyms ( e. g. is - part re - lations ) and hypernyms ( e. g. is - a relations ).', 'consistency checks implemented for this class of coref - erence are conservative : either the adjuncts must be identical or the adjunct of the referent must be less specific than the antecedent.', 'table 1 lists the top performing heuristics of cocktailfor pronominal and nominal coreference.', 'examples of the heuristics operation on the muc data are presented presented in table 2.', 'details of the top performing heuristics of cocktail were reported in ( #AUTHOR_TAG )']","['third class of heuristics resolves coreference by coercing nominals.', 'sometimes coercions involve only derivational morphology - linking verbs with their nominalizations.', 'on other occasions, coercions are obtained as paths of meronyms ( e. g. is - part re - lations ) and hypernyms ( e. g. is - a relations ).', 'consistency checks implemented for this class of coref - erence are conservative : either the adjuncts must be identical or the adjunct of the referent must be less specific than the antecedent.', 'table 1 lists the top performing heuristics of cocktailfor pronominal and nominal coreference.', 'examples of the heuristics operation on the muc data are presented presented in table 2.', 'details of the top performing heuristics of cocktail were reported in ( #AUTHOR_TAG )']","['third class of heuristics resolves coreference by coercing nominals.', 'sometimes coercions involve only derivational morphology - linking verbs with their nominalizations.', 'on other occasions, coercions are obtained as paths of meronyms ( e. g. is - part re - lations ) and hypernyms ( e. g. is - a relations ).', 'consistency checks implemented for this class of coref - erence are conservative : either the adjuncts must be identical or the adjunct of the referent must be less specific than the antecedent.', 'table 1 lists the top performing heuristics of cocktailfor pronominal and nominal coreference.', 'examples of the heuristics operation on the muc data are presented presented in table 2.', 'details of the top performing heuristics of cocktail were reported in ( #AUTHOR_TAG )']","['third class of heuristics resolves coreference by coercing nominals.', 'sometimes coercions involve only derivational morphology - linking verbs with their nominalizations.', 'on other occasions, coercions are obtained as paths of meronyms ( e. g. is - part re - lations ) and hypernyms ( e. g. is - a relations ).', 'consistency checks implemented for this class of coref - erence are conservative : either the adjuncts must be identical or the adjunct of the referent must be less specific than the antecedent.', 'table 1 lists the top performing heuristics of cocktailfor pronominal and nominal coreference.', 'examples of the heuristics operation on the muc data are presented presented in table 2.', 'details of the top performing heuristics of cocktail were reported in ( #AUTHOR_TAG )']",0
"[', some of the best - performing and most robust coreference resolution systems employ knowledge - based techniques.', 'traditionally, these techniques have combined extensive syntactic, semantic, and discourse knowledge.', 'the acquisition of such knowledge is time - consuming, difficult, and error - prone.', 'nevertheless, recent results show that knowledge - poor methods perform with amazing accuracy ( cf.', '(Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997) ).', 'for example, cogniac (Baldwin, 1997), a system based on seven ordered heuristics, generates high - precision resolution ( over 90 % ) for some cases of pronominal reference.', 'for this research, we used a coreference resolution system ( ( #AUTHOR_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference.', 'this system, called cocktail, resolves coreference by exploiting several textual cohesion constraints (']","[', some of the best - performing and most robust coreference resolution systems employ knowledge - based techniques.', 'traditionally, these techniques have combined extensive syntactic, semantic, and discourse knowledge.', 'the acquisition of such knowledge is time - consuming, difficult, and error - prone.', 'nevertheless, recent results show that knowledge - poor methods perform with amazing accuracy ( cf.', '(Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997) ).', 'for example, cogniac (Baldwin, 1997), a system based on seven ordered heuristics, generates high - precision resolution ( over 90 % ) for some cases of pronominal reference.', 'for this research, we used a coreference resolution system ( ( #AUTHOR_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference.', 'this system, called cocktail, resolves coreference by exploiting several textual cohesion constraints']","[', some of the best - performing and most robust coreference resolution systems employ knowledge - based techniques.', 'traditionally, these techniques have combined extensive syntactic, semantic, and discourse knowledge.', 'the acquisition of such knowledge is time - consuming, difficult, and error - prone.', 'nevertheless, recent results show that knowledge - poor methods perform with amazing accuracy ( cf.', '(Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997) ).', 'for example, cogniac (Baldwin, 1997), a system based on seven ordered heuristics, generates high - precision resolution ( over 90 % ) for some cases of pronominal reference.', 'for this research, we used a coreference resolution system ( ( #AUTHOR_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference.', 'this system, called cocktail, resolves coreference by exploiting several textual cohesion constraints (']","[', some of the best - performing and most robust coreference resolution systems employ knowledge - based techniques.', 'traditionally, these techniques have combined extensive syntactic, semantic, and discourse knowledge.', 'the acquisition of such knowledge is time - consuming, difficult, and error - prone.', 'nevertheless, recent results show that knowledge - poor methods perform with amazing accuracy ( cf.', '(Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997) ).', 'for example, cogniac (Baldwin, 1997), a system based on seven ordered heuristics, generates high - precision resolution ( over 90 % ) for some cases of pronominal reference.', 'for this research, we used a coreference resolution system ( ( #AUTHOR_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference.', 'this system, called cocktail, resolves coreference by exploiting several textual cohesion constraints ( e. g. term repetition ) combined with lexical and textual coherence cues ( e. g.', 'subjects of communication verbs are more likely to refer to the last person mentioned in the text ).', 'these constraints are implemented as a set of heuristics ordered by their priority.', 'moreover, the cocktail framework uniformly addresses the problem of interaction between different forms of coreference, thus making the extension to multilingual coreference very natural']",5
"['.', 'each component will return a confidence measure of the reliability of its prediction, c. f. ( #AUTHOR_TAG ).', 'the results from each component are evaluated to determine the final category of the word']","['deal with these issues we propose a multicomponent architecture where individual components specialize in identifying one particular type of unknown word.', 'for example, the misspelling identifier will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc.', 'each component will return a confidence measure of the reliability of its prediction, c. f. ( #AUTHOR_TAG ).', 'the results from each component are evaluated to determine the final category of the word']","['deal with these issues we propose a multicomponent architecture where individual components specialize in identifying one particular type of unknown word.', 'for example, the misspelling identifier will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc.', 'each component will return a confidence measure of the reliability of its prediction, c. f. ( #AUTHOR_TAG ).', 'the results from each component are evaluated to determine the final category of the word']","['deal with these issues we propose a multicomponent architecture where individual components specialize in identifying one particular type of unknown word.', 'for example, the misspelling identifier will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc.', 'each component will return a confidence measure of the reliability of its prediction, c. f. ( #AUTHOR_TAG ).', 'the results from each component are evaluated to determine the final category of the word']",4
"['first feature represents the part of speech of the word.', 'we use an in - house statistical tagger ( based on ( #AUTHOR_TAG ) ) to tag the text in which the unknown word occurs.', 'the tag set used is a simplified version of the tags used in the machinereadable version of the oxford advanced learners dictionary ( oa']","['first feature represents the part of speech of the word.', 'we use an in - house statistical tagger ( based on ( #AUTHOR_TAG ) ) to tag the text in which the unknown word occurs.', 'the tag set used is a simplified version of the tags used in the machinereadable version of the oxford advanced learners dictionary ( oald ).', 'the tag set contains just one tag to identify nouns']","['first feature represents the part of speech of the word.', 'we use an in - house statistical tagger ( based on ( #AUTHOR_TAG ) ) to tag the text in which the unknown word occurs.', 'the tag set used is a simplified version of the tags used in the machinereadable version of the oxford advanced learners dictionary ( oa']","['first feature represents the part of speech of the word.', 'we use an in - house statistical tagger ( based on ( #AUTHOR_TAG ) ) to tag the text in which the unknown word occurs.', 'the tag set used is a simplified version of the tags used in the machinereadable version of the oxford advanced learners dictionary ( oald ).', 'the tag set contains just one tag to identify nouns']",5
"['that is more similar in goal to that outlined in this paper is vosse ( #AUTHOR_TAG ).', 'vosse uses a simple algorithm to identify three classes of unknown words : misspellings, neologisms, and names.', 'capitalization is his sole means of identifying names.', 'however, capitalization information is not available in closed captions.', 'hence, his system would be ineffective on the closed caption domain with which we are working.', '(Granger, 1983) uses expectations generated by scripts to anmyze unknown words.', 'the drawback of his system is that it lacks portability since it incorporates scripts that make use of world knowledge of the situation being described ; in this case, naval ship - to - shore messages']","['that is more similar in goal to that outlined in this paper is vosse ( #AUTHOR_TAG ).', 'vosse uses a simple algorithm to identify three classes of unknown words : misspellings, neologisms, and names.', 'capitalization is his sole means of identifying names.', 'however, capitalization information is not available in closed captions.', 'hence, his system would be ineffective on the closed caption domain with which we are working.', '(Granger, 1983) uses expectations generated by scripts to anmyze unknown words.', 'the drawback of his system is that it lacks portability since it incorporates scripts that make use of world knowledge of the situation being described ; in this case, naval ship - to - shore messages']","['that is more similar in goal to that outlined in this paper is vosse ( #AUTHOR_TAG ).', 'vosse uses a simple algorithm to identify three classes of unknown words : misspellings, neologisms, and names.', 'capitalization is his sole means of identifying names.', 'however, capitalization information is not available in closed captions.', 'hence, his system would be ineffective on the closed caption domain with which we are working.', '(Granger, 1983) uses expectations generated by scripts to anmyze unknown words.', 'the drawback of his system is that it lacks portability since it incorporates scripts that make use of world knowledge of the situation being described ; in this case, naval ship - to - shore messages']","['that is more similar in goal to that outlined in this paper is vosse ( #AUTHOR_TAG ).', 'vosse uses a simple algorithm to identify three classes of unknown words : misspellings, neologisms, and names.', 'capitalization is his sole means of identifying names.', 'however, capitalization information is not available in closed captions.', 'hence, his system would be ineffective on the closed caption domain with which we are working.', '(Granger, 1983) uses expectations generated by scripts to anmyze unknown words.', 'the drawback of his system is that it lacks portability since it incorporates scripts that make use of world knowledge of the situation being described ; in this case, naval ship - to - shore messages']",1
['frequency : ( #AUTHOR_TAG )'],"['frequency : ( #AUTHOR_TAG ) differentiates between misspellings and neologisms ( new words ) in terms of their frequency.', 'his algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms.', 'our corpus frequency variable specifies the frequency of each unknown word in a 2. 6 million word corpus of business news closed captions']",['frequency : ( #AUTHOR_TAG )'],"['frequency : ( #AUTHOR_TAG ) differentiates between misspellings and neologisms ( new words ) in terms of their frequency.', 'his algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms.', 'our corpus frequency variable specifies the frequency of each unknown word in a 2. 6 million word corpus of business news closed captions']",5
"['research reported here is the first that we know of to automatically analyze a corpus of logs from a spoken dialogue system for the purpose of learning to predict problematic situations.', 'our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #AUTHOR_TAG ).', 'however, that work was based on a much']","['research reported here is the first that we know of to automatically analyze a corpus of logs from a spoken dialogue system for the purpose of learning to predict problematic situations.', 'our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #AUTHOR_TAG ).', 'however, that work was based on a much']","['research reported here is the first that we know of to automatically analyze a corpus of logs from a spoken dialogue system for the purpose of learning to predict problematic situations.', 'our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #AUTHOR_TAG ).', 'however, that work was based on a much smaller set of experimental dialogues where the notion of a good or bad dialogue was automatically approximated rather than being labelled']","['research reported here is the first that we know of to automatically analyze a corpus of logs from a spoken dialogue system for the purpose of learning to predict problematic situations.', 'our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #AUTHOR_TAG ).', 'however, that work was based on a much smaller set of experimental dialogues where the notion of a good or bad dialogue was automatically approximated rather than being labelled by humans.', 'in addition, because that work was based on features synthesized over the entire dialogues, the hypotheses that were learned could not be used for prediction during runtime']",2
"[').', 'the use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( #AUTHOR_TAG )']","['dm features also include running tallies for the number of reprompts ( num - reprompts ), number of confirmation prompts ( num. confirms ), and number of subdialogue prompts ( num - subdials ), that had been played up to each point in the dimogue, as well as running percentages ( percent - reprompts, percentconfirms, percent - subdials ).', 'the use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( #AUTHOR_TAG )']","['reprompts ), number of confirmation prompts ( num. confirms ), and number of subdialogue prompts ( num - subdials ), that had been played up to each point in the dimogue, as well as running percentages ( percent - reprompts, percentconfirms, percent - subdials ).', 'the use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( #AUTHOR_TAG )']","['dm features also include running tallies for the number of reprompts ( num - reprompts ), number of confirmation prompts ( num. confirms ), and number of subdialogue prompts ( num - subdials ), that had been played up to each point in the dimogue, as well as running percentages ( percent - reprompts, percentconfirms, percent - subdials ).', 'the use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( #AUTHOR_TAG )']",4
"['f ( • ) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and z θ ( x ) def = y u ( x, y ) is a normalizer.', 'we use the same set of binary features as in previous work on this dataset ( #AUTHOR_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ).', 'specifically, let v = { v 1,..., v 17744 } be the set of word types with count ≥ 4 in the full 2000 - document corpus.', 'define f h ( x, y ) to be y if v h appears at least once in x, and 0']","['f ( • ) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and z θ ( x ) def = y u ( x, y ) is a normalizer.', 'we use the same set of binary features as in previous work on this dataset ( #AUTHOR_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ).', 'specifically, let v = { v 1,..., v 17744 } be the set of word types with count ≥ 4 in the full 2000 - document corpus.', 'define f h ( x, y ) to be y if v h appears at least once in x, and 0 otherwise.', 'thus θ ∈ r 17744, and positive weights in θ favor class label y = + 1 and equally discourage y = −1, while negative weights do the opposite.', 'this standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales.', 'future work should consider more complex features and how they are signaled by rationales, as discussed in section 3. 2']","['f ( • ) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and z θ ( x ) def = y u ( x, y ) is a normalizer.', 'we use the same set of binary features as in previous work on this dataset ( #AUTHOR_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ).', 'specifically, let v = { v 1,..., v 17744 } be the set of word types with count ≥ 4 in the full 2000 - document corpus.', 'define f h ( x, y ) to be y if v h appears at least once in x, and 0 otherwise.', 'thus θ ∈ r 17744, and positive weights in θ favor class label y = + 1 and equally discour']","['f ( • ) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and z θ ( x ) def = y u ( x, y ) is a normalizer.', 'we use the same set of binary features as in previous work on this dataset ( #AUTHOR_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ).', 'specifically, let v = { v 1,..., v 17744 } be the set of word types with count ≥ 4 in the full 2000 - document corpus.', 'define f h ( x, y ) to be y if v h appears at least once in x, and 0 otherwise.', 'thus θ ∈ r 17744, and positive weights in θ favor class label y = + 1 and equally discourage y = −1, while negative weights do the opposite.', 'this standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales.', 'future work should consider more complex features and how they are signaled by rationales, as discussed in section 3. 2']",5
"['sentence representation we chose for this work is rather simple, and was intended primarily to demonstrate the efficacy of our approach.', ""in future work we plan to experiment with richer representations, e. g. including long - range n - grams ( Rosenfeld , 1996 ), class n - grams ( #AUTHOR_TAG ), grammatical features ( Amaya and Benedy , 2001 ), etc '""]","['sentence representation we chose for this work is rather simple, and was intended primarily to demonstrate the efficacy of our approach.', ""in future work we plan to experiment with richer representations, e. g. including long - range n - grams ( Rosenfeld , 1996 ), class n - grams ( #AUTHOR_TAG ), grammatical features ( Amaya and Benedy , 2001 ), etc '""]","['sentence representation we chose for this work is rather simple, and was intended primarily to demonstrate the efficacy of our approach.', ""in future work we plan to experiment with richer representations, e. g. including long - range n - grams ( Rosenfeld , 1996 ), class n - grams ( #AUTHOR_TAG ), grammatical features ( Amaya and Benedy , 2001 ), etc '""]","['sentence representation we chose for this work is rather simple, and was intended primarily to demonstrate the efficacy of our approach.', ""in future work we plan to experiment with richer representations, e. g. including long - range n - grams ( Rosenfeld , 1996 ), class n - grams ( #AUTHOR_TAG ), grammatical features ( Amaya and Benedy , 2001 ), etc '""]",3
"['.', '[UNK] both cases essentially linear classifiers were used as features.', 'as these are computationally very efficient, the authors could use a variant of gibbs sampling for generating negative samples.', 'unfortunately, as shown in ( #AUTHOR_TAG ), with the represetation of sentences that we use, linear classifiers can not discriminate real sentences from sentences sampled from a trigram, which is the model we use as a baseline, so here we resort to a non - linear large - margin classifier ( see section 3']","['data.', '[UNK] both cases essentially linear classifiers were used as features.', 'as these are computationally very efficient, the authors could use a variant of gibbs sampling for generating negative samples.', 'unfortunately, as shown in ( #AUTHOR_TAG ), with the represetation of sentences that we use, linear classifiers can not discriminate real sentences from sentences sampled from a trigram, which is the model we use as a baseline, so here we resort to a non - linear large - margin classifier ( see section 3']","['', '[UNK] both cases essentially linear classifiers were used as features.', 'as these are computationally very efficient, the authors could use a variant of gibbs sampling for generating negative samples.', 'unfortunately, as shown in ( #AUTHOR_TAG ), with the represetation of sentences that we use, linear classifiers can not discriminate real sentences from sentences sampled from a trigram, which is the model we use as a baseline, so here we resort to a non - linear large - margin classifier ( see section 3']","['- supervised boosting was presented as a general method for density estimation, and was not tested in the context of language modeling.', 'rather, welling at al. demonstrated its effectiveness in modeling hand - written digits and on synthetic data.', '[UNK] both cases essentially linear classifiers were used as features.', 'as these are computationally very efficient, the authors could use a variant of gibbs sampling for generating negative samples.', 'unfortunately, as shown in ( #AUTHOR_TAG ), with the represetation of sentences that we use, linear classifiers can not discriminate real sentences from sentences sampled from a trigram, which is the model we use as a baseline, so here we resort to a non - linear large - margin classifier ( see section 3 for details ).', 'while large - margin classifiers consistently out - perform other learning algorithms in many nlp tasks, their non - linear variations are also notoriously slow when it comes to computing their decision function - taking time that can be linear in the size of their training data.', 'this means that mcmc techniques like gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications.', 'for this reason we use a different sampling scheme which we refer to as rejection sampling.', ""this allows us to sample from the true model distribution while requiring a drastically smaller number of classifications, as long as the current model isn't too far removed from the baseline""]",4
"['our features we used large - margin classifiers trained using the online algorithm described in (Crammer et al., 2006).', 'the code for the classifier was generously provided by daisuke okanohara.', 'this code was extensively optimized to take advantage of the very sparse sentence representation described above.', 'as shown in ( #AUTHOR_TAG ), using this representation, a linear classifier can not distinguish sentences sampled from a trigram and real sentences.', 'therefore, we used a 3rd order polynomial kernel, which was found to give good results.', 'no special effort was otherwise made in order to optimize the parameters of the classifiers']","['our features we used large - margin classifiers trained using the online algorithm described in (Crammer et al., 2006).', 'the code for the classifier was generously provided by daisuke okanohara.', 'this code was extensively optimized to take advantage of the very sparse sentence representation described above.', 'as shown in ( #AUTHOR_TAG ), using this representation, a linear classifier can not distinguish sentences sampled from a trigram and real sentences.', 'therefore, we used a 3rd order polynomial kernel, which was found to give good results.', 'no special effort was otherwise made in order to optimize the parameters of the classifiers']","['our features we used large - margin classifiers trained using the online algorithm described in (Crammer et al., 2006).', 'the code for the classifier was generously provided by daisuke okanohara.', 'this code was extensively optimized to take advantage of the very sparse sentence representation described above.', 'as shown in ( #AUTHOR_TAG ), using this representation, a linear classifier can not distinguish sentences sampled from a trigram and real sentences.', 'therefore, we used a 3rd order polynomial kernel, which was found to give good results.', 'no special effort was otherwise made in order to optimize the parameters of the classifiers']","['our features we used large - margin classifiers trained using the online algorithm described in (Crammer et al., 2006).', 'the code for the classifier was generously provided by daisuke okanohara.', 'this code was extensively optimized to take advantage of the very sparse sentence representation described above.', 'as shown in ( #AUTHOR_TAG ), using this representation, a linear classifier can not distinguish sentences sampled from a trigram and real sentences.', 'therefore, we used a 3rd order polynomial kernel, which was found to give good results.', 'no special effort was otherwise made in order to optimize the parameters of the classifiers']",4
"['language model : for p0 we used a trigram with modified kneser - ney smoothing [ chen and #AUTHOR_TAG ], which is still considered one of the best smoothing methods for n - gram language models.', '2. sentence representation : each sentence was represented as the collection of unigrams, bigrams and trigrams']","['language model : for p0 we used a trigram with modified kneser - ney smoothing [ chen and #AUTHOR_TAG ], which is still considered one of the best smoothing methods for n - gram language models.', '2. sentence representation : each sentence was represented as the collection of unigrams, bigrams and trigrams']","['language model : for p0 we used a trigram with modified kneser - ney smoothing [ chen and #AUTHOR_TAG ], which is still considered one of the best smoothing methods for n - gram language models.', '2. sentence representation : each sentence was represented as the collection of unigrams, bigrams and trigrams it contained.', 'a coordinate was reserved for each such n - gram which appeared in the data, whether real or sampled.', ""the value of then'th coordinate in the vector representation of interestingly, in practice both methods result in near identical rejection probabilities, within a precision of 0. 0001."", 'this indicates that satisfying the constraint ( 18 ) for the new feature is more important, in terms of perplexity, than preserving the constraints of the previous features, insofar as those get violated']","['language model : for p0 we used a trigram with modified kneser - ney smoothing [ chen and #AUTHOR_TAG ], which is still considered one of the best smoothing methods for n - gram language models.', '2. sentence representation : each sentence was represented as the collection of unigrams, bigrams and trigrams it contained.', 'a coordinate was reserved for each such n - gram which appeared in the data, whether real or sampled.', ""the value of then'th coordinate in the vector representation of interestingly, in practice both methods result in near identical rejection probabilities, within a precision of 0. 0001."", 'this indicates that satisfying the constraint ( 18 ) for the new feature is more important, in terms of perplexity, than preserving the constraints of the previous features, insofar as those get violated']",5
"['features can be easily obtained by modifying the tat extraction algorithm described in ( #AUTHOR_TAG ).', 'when a tat is extracted from a word - aligned, source - parsed parallel sentence, we just record the contextual features and the features of the sub - trees.', 'then we use the toolkit implemented by Zhang (2004) to train mers models for the ambiguous source syntactic trees separately.', 'we set the iteration number to 100 and gaussian prior to 1']","['features can be easily obtained by modifying the tat extraction algorithm described in ( #AUTHOR_TAG ).', 'when a tat is extracted from a word - aligned, source - parsed parallel sentence, we just record the contextual features and the features of the sub - trees.', 'then we use the toolkit implemented by Zhang (2004) to train mers models for the ambiguous source syntactic trees separately.', 'we set the iteration number to 100 and gaussian prior to 1']","['features can be easily obtained by modifying the tat extraction algorithm described in ( #AUTHOR_TAG ).', 'when a tat is extracted from a word - aligned, source - parsed parallel sentence, we just record the contextual features and the features of the sub - trees.', 'then we use the toolkit implemented by Zhang (2004) to train mers models for the ambiguous source syntactic trees separately.', 'we set the iteration number to 100 and gaussian prior to 1']","['features can be easily obtained by modifying the tat extraction algorithm described in ( #AUTHOR_TAG ).', 'when a tat is extracted from a word - aligned, source - parsed parallel sentence, we just record the contextual features and the features of the sub - trees.', 'then we use the toolkit implemented by Zhang (2004) to train mers models for the ambiguous source syntactic trees separately.', 'we set the iteration number to 100 and gaussian prior to 1']",2
"['other higher - order features, such as those used in parse reranking ( #AUTHOR_TAG ; Huang , 2008 ) and history - based parsing ( nivre and mc Donald , 2008 ).', 'we']","['other higher - order features, such as those used in parse reranking ( #AUTHOR_TAG ; Huang , 2008 ) and history - based parsing ( nivre and mc Donald , 2008 ).', 'we']","['propagation improves non - projective dependency parsing with features that would make exact inference intractable.', 'for projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, we are interested in extending these ideas to phrase - structure and lattice parsing, and in trying other higher - order features, such as those used in parse reranking ( #AUTHOR_TAG ; Huang , 2008 ) and history - based parsing ( nivre and mc Donald , 2008 ).', 'we']","['propagation improves non - projective dependency parsing with features that would make exact inference intractable.', 'for projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, we are interested in extending these ideas to phrase - structure and lattice parsing, and in trying other higher - order features, such as those used in parse reranking ( #AUTHOR_TAG ; Huang , 2008 ) and history - based parsing ( nivre and mc Donald , 2008 ).', 'we could also introduce new variables, e. g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links m ij ( not constrained by tree / ptree ) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993; buch - Kromann, 2006)']",3
"['other higher - order features, such as those used in parse reranking ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) and history - based parsing ( nivre and mc Donald , 2008 ).', 'we']","['other higher - order features, such as those used in parse reranking ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) and history - based parsing ( nivre and mc Donald , 2008 ).', 'we']","['propagation improves non - projective dependency parsing with features that would make exact inference intractable.', 'for projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, we are interested in extending these ideas to phrase - structure and lattice parsing, and in trying other higher - order features, such as those used in parse reranking ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) and history - based parsing ( nivre and mc Donald , 2008 ).', 'we']","['propagation improves non - projective dependency parsing with features that would make exact inference intractable.', 'for projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, we are interested in extending these ideas to phrase - structure and lattice parsing, and in trying other higher - order features, such as those used in parse reranking ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) and history - based parsing ( nivre and mc Donald , 2008 ).', 'we could also introduce new variables, e. g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links m ij ( not constrained by tree / ptree ) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993; buch - Kromann, 2006)']",3
"['other higher - order features, such as those used in parse reranking ( Charniak and Johnson , 2005 ; Huang , 2008 ) and history - based parsing ( #AUTHOR_TAG ).', 'we']","['other higher - order features, such as those used in parse reranking ( Charniak and Johnson , 2005 ; Huang , 2008 ) and history - based parsing ( #AUTHOR_TAG ).', 'we']","['propagation improves non - projective dependency parsing with features that would make exact inference intractable.', 'for projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, we are interested in extending these ideas to phrase - structure and lattice parsing, and in trying other higher - order features, such as those used in parse reranking ( Charniak and Johnson , 2005 ; Huang , 2008 ) and history - based parsing ( #AUTHOR_TAG ).', 'we']","['propagation improves non - projective dependency parsing with features that would make exact inference intractable.', 'for projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, we are interested in extending these ideas to phrase - structure and lattice parsing, and in trying other higher - order features, such as those used in parse reranking ( Charniak and Johnson , 2005 ; Huang , 2008 ) and history - based parsing ( #AUTHOR_TAG ).', 'we could also introduce new variables, e. g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links m ij ( not constrained by tree / ptree ) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993; buch - Kromann, 2006)']",3
"['., nonterminal refinements ( #AUTHOR_TAG ), or secondary links mid ( not constrained by tree / ptree ) that augment the parse with representations of control, binding,']","['also introduce new variables, e. g., nonterminal refinements ( #AUTHOR_TAG ), or secondary links mid ( not constrained by tree / ptree ) that augment the parse with representations of control, binding, etc. ( Sleator and Temperley , 1993 ;  buch - Kromann , 2006 )']","['., nonterminal refinements ( #AUTHOR_TAG ), or secondary links mid ( not constrained by tree / ptree ) that augment the parse with representations of control, binding,']","['propagation improves non - projective dependency parsing with features that would make exact inference intractable.', 'for projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, we are interested in extending these ideas to phrase - structure and lattice parsing, and in trying other higher - order features, such as those used in parse reranking (Charniak and Johnson, 2005;Huang, 2008) and history - based parsing ( nivre and mc Donald, 2008).', 'we could also introduce new variables, e. g., nonterminal refinements ( #AUTHOR_TAG ), or secondary links mid ( not constrained by tree / ptree ) that augment the parse with representations of control, binding, etc. ( Sleator and Temperley , 1993 ;  buch - Kromann , 2006 )']",3
"['instead propose a novel approach that extracts rules from packed forests ( section 3 ), which compactly encodes many more alternatives than kbest lists.', 'experiments ( section 5 ) show that forestbased extraction improves bleu score by over 1 point on a state - of - the - art tree - to - string system ( Liu et al. , 2006 ; #AUTHOR_TAG ), which is also 0. 5 points better than ( and twice as fast as ) extracting on 30 - best parses.', 'when combined with our previous orthogonal work on forest - based decoding (Mi et al., 2008), the forest - forest approach achieves a 2. 5 bleu points improvement over the baseline, and even outperforms the hierarchical system of hiero, one of the best - performing systems to date']","['instead propose a novel approach that extracts rules from packed forests ( section 3 ), which compactly encodes many more alternatives than kbest lists.', 'experiments ( section 5 ) show that forestbased extraction improves bleu score by over 1 point on a state - of - the - art tree - to - string system ( Liu et al. , 2006 ; #AUTHOR_TAG ), which is also 0. 5 points better than ( and twice as fast as ) extracting on 30 - best parses.', 'when combined with our previous orthogonal work on forest - based decoding (Mi et al., 2008), the forest - forest approach achieves a 2. 5 bleu points improvement over the baseline, and even outperforms the hierarchical system of hiero, one of the best - performing systems to date']","['instead propose a novel approach that extracts rules from packed forests ( section 3 ), which compactly encodes many more alternatives than kbest lists.', 'experiments ( section 5 ) show that forestbased extraction improves bleu score by over 1 point on a state - of - the - art tree - to - string system ( Liu et al. , 2006 ; #AUTHOR_TAG ), which is also 0. 5 points better than ( and twice as fast as ) extracting on 30 - best parses.', 'when combined with our previous orthogonal work on forest - based decoding (Mi et al., 2008), the forest - forest approach achieves a 2. 5 bleu points improvement over the baseline, and even outperforms the hierarchical system of hiero, one of the best - performing systems to date']","['instead propose a novel approach that extracts rules from packed forests ( section 3 ), which compactly encodes many more alternatives than kbest lists.', 'experiments ( section 5 ) show that forestbased extraction improves bleu score by over 1 point on a state - of - the - art tree - to - string system ( Liu et al. , 2006 ; #AUTHOR_TAG ), which is also 0. 5 points better than ( and twice as fast as ) extracting on 30 - best parses.', 'when combined with our previous orthogonal work on forest - based decoding (Mi et al., 2008), the forest - forest approach achieves a 2. 5 bleu points improvement over the baseline, and even outperforms the hierarchical system of hiero, one of the best - performing systems to date']",2
"['with integrated language models (Huang and Chiang, 2007).', 'the first direct application of parse forest in translation is our previous work ( #AUTHOR_TAG ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ).', 'this work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding']","['with integrated language models (Huang and Chiang, 2007).', 'the first direct application of parse forest in translation is our previous work ( #AUTHOR_TAG ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ).', 'this work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding']","['with integrated language models (Huang and Chiang, 2007).', 'the first direct application of parse forest in translation is our previous work ( #AUTHOR_TAG ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ).', 'this work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding']","['forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007).', 'the first direct application of parse forest in translation is our previous work ( #AUTHOR_TAG ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ).', 'this work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding']",2
"['answering ( narayanan and harabagiu 2004 ), information extraction ( #AUTHOR_TAG ), and machine translation ( boas 2002 ).', 'with the efforts of many researchers ( carreras and']","['answering ( narayanan and harabagiu 2004 ), information extraction ( #AUTHOR_TAG ), and machine translation ( boas 2002 ).', 'with the efforts of many researchers ( carreras and']","['answering ( narayanan and harabagiu 2004 ), information extraction ( #AUTHOR_TAG ), and machine translation ( boas 2002 ).', 'with the efforts of many researchers ( carreras and']","['role labeling ( srl ) was first defined in Gildea and Jurafsky (2002).', 'the purpose of srl task is to identify and classify the semantic roles of each predicate in a sentence.', 'the semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'typical tags include agent, patient, source, etc. and some adjuncts such as temporal, manner, extent, etc.', 'since the arguments can provide useful semantic information, the srl is crucial to many natural language processing tasks, such as question and answering ( narayanan and harabagiu 2004 ), information extraction ( #AUTHOR_TAG ), and machine translation ( boas 2002 ).', 'with the efforts of many researchers ( carreras and marquez 2004, moschitti 2004, pradhan et al 2005, zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made srl task progress fast']",0
"['prove that our method is effective, we also make a comparison between the performances of our system and #AUTHOR_TAG, Xue ( 2008 ).', 'Xue (2008) is the best srl system until now and it has the same data setting with ours.', 'the results are presented in table 6 we have to point out that all the three systems are based on gold standard parsing.', 'from the table 6, we can find that our system is better than both of the related systems.', 'our system has outperformed Xue (2008) with a relative error reduction rate of 9. 8 %']","['prove that our method is effective, we also make a comparison between the performances of our system and #AUTHOR_TAG, Xue ( 2008 ).', 'Xue (2008) is the best srl system until now and it has the same data setting with ours.', 'the results are presented in table 6 we have to point out that all the three systems are based on gold standard parsing.', 'from the table 6, we can find that our system is better than both of the related systems.', 'our system has outperformed Xue (2008) with a relative error reduction rate of 9. 8 %']","['prove that our method is effective, we also make a comparison between the performances of our system and #AUTHOR_TAG, Xue ( 2008 ).', 'Xue (2008) is the best srl system until now and it has the same data setting with ours.', 'the results are presented in table 6 we have to point out that all the three systems are based on gold standard parsing.', 'from the table 6, we can find that our system is better than both of the related systems.', 'our system has outperformed Xue (2008) with a relative error reduction rate of 9. 8 %']","['prove that our method is effective, we also make a comparison between the performances of our system and #AUTHOR_TAG, Xue ( 2008 ).', 'Xue (2008) is the best srl system until now and it has the same data setting with ours.', 'the results are presented in table 6 we have to point out that all the three systems are based on gold standard parsing.', 'from the table 6, we can find that our system is better than both of the related systems.', 'our system has outperformed Xue (2008) with a relative error reduction rate of 9. 8 %']",1
"['##s such as temporal, manner, extent, etc.', 'since the arguments can provide useful semantic information, the srl is crucial to many natural language processing tasks, such as question and answering ( #AUTHOR_TAG ), information extraction ( sur']","['role labeling ( srl ) was first defined in Gildea and Jurafsky (2002).', 'the purpose of srl task is to identify and classify the semantic roles of each predicate in a sentence.', 'the semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'typical tags include agent, patient, source, etc. and some adjuncts such as temporal, manner, extent, etc.', 'since the arguments can provide useful semantic information, the srl is crucial to many natural language processing tasks, such as question and answering ( #AUTHOR_TAG ), information extraction ( surdeanu et al. 2003 ),']","['role labeling ( srl ) was first defined in Gildea and Jurafsky (2002).', 'the purpose of srl task is to identify and classify the semantic roles of each predicate in a sentence.', 'the semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'typical tags include agent, patient, source, etc. and some adjuncts such as temporal, manner, extent, etc.', 'since the arguments can provide useful semantic information, the srl is crucial to many natural language processing tasks, such as question and answering ( #AUTHOR_TAG ), information extraction ( sur']","['role labeling ( srl ) was first defined in Gildea and Jurafsky (2002).', 'the purpose of srl task is to identify and classify the semantic roles of each predicate in a sentence.', 'the semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'typical tags include agent, patient, source, etc. and some adjuncts such as temporal, manner, extent, etc.', 'since the arguments can provide useful semantic information, the srl is crucial to many natural language processing tasks, such as question and answering ( #AUTHOR_TAG ), information extraction ( surdeanu et al. 2003 ), and machine translation ( boas 2002 ).', 'with the efforts of many researchers ( carreras and marquez 2004, moschitti 2004, pradhan et al 2005, zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made srl task progress fast']",0
['words are extracted from the semantic knowledge - base of contemporary chinese ( #AUTHOR_TAG )'],['words are extracted from the semantic knowledge - base of contemporary chinese ( #AUTHOR_TAG )'],"['##cat ( semantic category ) of predicate, sem - cat of first word, semcat of head word, semcat of last word, semcat of predicate + semcat of first word, semcat of predicate + semcat of last word, predicate + semcat of head word, semcat of predicate + head word.', 'the semantic categories of verbs and other words are extracted from the semantic knowledge - base of contemporary chinese ( #AUTHOR_TAG )']","['##cat ( semantic category ) of predicate, sem - cat of first word, semcat of head word, semcat of last word, semcat of predicate + semcat of first word, semcat of predicate + semcat of last word, predicate + semcat of head word, semcat of predicate + head word.', 'the semantic categories of verbs and other words are extracted from the semantic knowledge - base of contemporary chinese ( #AUTHOR_TAG )']",5
"['made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', '#AUTHOR_TAG did very encouraging work on the feature']","['made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', '#AUTHOR_TAG did very encouraging work on the feature']","[', the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky (2004), and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', '#AUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky (2004), and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', '#AUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']",0
"['collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( #AUTHOR_TAG ) was built, Xue and Palmer ( 2005 ) and Xue ( 2008 ) have']","['collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( #AUTHOR_TAG ) was built, Xue and Palmer ( 2005 ) and Xue ( 2008 ) have']","[', the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky (2004), and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( #AUTHOR_TAG ) was built, Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky (2004), and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( #AUTHOR_TAG ) was built, Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']",0
"['of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( #AUTHOR_TAG, xue']","['of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( #AUTHOR_TAG, xue']","[', the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky (2004), and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( #AUTHOR_TAG, xue 2008 ) reassured these findings']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky (2004), and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( #AUTHOR_TAG, xue 2008 ) reassured these findings']",4
"['has built a semantic role classifier exploiting the interdependence of semantic roles.', 'it has turned the single point classification problem']","['has built a semantic role classifier exploiting the interdependence of semantic roles.', 'it has turned the single point classification problem']","['has built a semantic role classifier exploiting the interdependence of semantic roles.', 'it has turned the single point classification problem']","['has built a semantic role classifier exploiting the interdependence of semantic roles.', 'it has turned the single point classification problem into the sequence labeling problem with the introduction of semantic context features.', 'se - mantic context features indicates the features ex - tracted from the arguments around the current one.', 'we can use window size to represent the scope of the context.', 'window size [ - m, n ] means that, in the sequence that all the arguments has constructed, the features of previous m and following n argu - ments will be utilized for the classification of cur - rent semantic role.', 'there are two kinds of argu - ment sequences in Jiang et al. (2005), and we only test the linear sequence.', 'take the sentence in fig - ure 1 as an example.', 'the linear sequence of the arguments in this sentence is : _ _ _ _ ( until then ), _ _ _ _ ( the insurance company ), _ ( has ), _ _ _ _ _ ( for the sanxia project ), _ _ _ _ ( in - surance services ).', 'for the argument _ ( has ), if the semantic context window size is [ - 1, 2 ], the seman - tic context features e. g. headword, phrase type and etc. of _ _ _ _ ( the insurance company ), _ _ _ _ _ ( for the sanxia project ) and _ _ _ _ ( insurance services ) will be utilized to serve the classification task of _ ( has )']",5
"['we make discriminations of arguments and adjuncts, the analysis is still coarse - grained.', '#AUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement.', 'however, the impact of this idea is limited due to that the amount of the research target, arg2, is few in propbank.', 'what if we']","['we make discriminations of arguments and adjuncts, the analysis is still coarse - grained.', '#AUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement.', 'however, the impact of this idea is limited due to that the amount of the research target, arg2, is few in propbank.', 'what if we']","['we make discriminations of arguments and adjuncts, the analysis is still coarse - grained.', '#AUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement.', 'however, the impact of this idea is limited due to that the amount of the research target, arg2, is few in propbank.', 'what if']","['we make discriminations of arguments and adjuncts, the analysis is still coarse - grained.', '#AUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement.', 'however, the impact of this idea is limited due to that the amount of the research target, arg2, is few in propbank.', 'what if we could extend the idea of hierarchical architecture to the single semantic role level?', 'would that help the improvement of src']",1
"[', the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as #AUTHOR_TAG, Xue and Palmer ( 2005 ) and Xue ( 2008 ).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as #AUTHOR_TAG, Xue and Palmer ( 2005 ) and Xue ( 2008 ).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a']","[', the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as #AUTHOR_TAG, Xue and Palmer ( 2005 ) and Xue ( 2008 ).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as #AUTHOR_TAG, Xue and Palmer ( 2005 ) and Xue ( 2008 ).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']",0
"['prove that our method is effective, we also make a comparison between the performances of our system and Xue and Palmer ( 2005 ), #AUTHOR_TAG.', 'Xue (2008) is the best srl system until now and it has the same data setting with ours.', 'the results are presented in table 6 we have to point out that all the three systems are based on gold standard parsing.', 'from the table 6, we can find that our system is better than both of the related systems.', 'our system has outperformed Xue (2008) with a relative error reduction rate of 9. 8 %']","['prove that our method is effective, we also make a comparison between the performances of our system and Xue and Palmer ( 2005 ), #AUTHOR_TAG.', 'Xue (2008) is the best srl system until now and it has the same data setting with ours.', 'the results are presented in table 6 we have to point out that all the three systems are based on gold standard parsing.', 'from the table 6, we can find that our system is better than both of the related systems.', 'our system has outperformed Xue (2008) with a relative error reduction rate of 9. 8 %']","['prove that our method is effective, we also make a comparison between the performances of our system and Xue and Palmer ( 2005 ), #AUTHOR_TAG.', 'Xue (2008) is the best srl system until now and it has the same data setting with ours.', 'the results are presented in table 6 we have to point out that all the three systems are based on gold standard parsing.', 'from the table 6, we can find that our system is better than both of the related systems.', 'our system has outperformed Xue (2008) with a relative error reduction rate of 9. 8 %']","['prove that our method is effective, we also make a comparison between the performances of our system and Xue and Palmer ( 2005 ), #AUTHOR_TAG.', 'Xue (2008) is the best srl system until now and it has the same data setting with ours.', 'the results are presented in table 6 we have to point out that all the three systems are based on gold standard parsing.', 'from the table 6, we can find that our system is better than both of the related systems.', 'our system has outperformed Xue (2008) with a relative error reduction rate of 9. 8 %']",1
"['. g.', 'phrase type, are limited.', 'be found in figure 2, which is similar with that in #AUTHOR_TAG']","['e. g.', 'predicate may be still useful because that the information, provided by the high - level description of selfdescriptive features, e. g.', 'phrase type, are limited.', 'be found in figure 2, which is similar with that in #AUTHOR_TAG']","['. g.', 'predicate may be still useful because that the information, provided by the high - level description of selfdescriptive features, e. g.', 'phrase type, are limited.', 'be found in figure 2, which is similar with that in #AUTHOR_TAG']","['semantic role classifiers always did the classification problem in one - step.', 'however, in this paper, we did src in two steps.', 'the architectures of hierarchical semantic role classifiers can 2 extra features e. g.', 'predicate may be still useful because that the information, provided by the high - level description of selfdescriptive features, e. g.', 'phrase type, are limited.', 'be found in figure 2, which is similar with that in #AUTHOR_TAG']",1
"['pos, predicate + head word, predicate + phrase type, path to ba and bei, verb class 3, verb class + head word, verb class + phrase type, from #AUTHOR_TAG']","['pos, predicate + head word, predicate + phrase type, path to ba and bei, verb class 3, verb class + head word, verb class + phrase type, from #AUTHOR_TAG']","['pos, predicate + head word, predicate + phrase type, path to ba and bei, verb class 3, verb class + head word, verb class + phrase type, from #AUTHOR_TAG']","[', subcat frame, phrase type, first word, last word, subcat frame +, predicate, path, head word and its pos, predicate + head word, predicate + phrase type, path to ba and bei, verb class 3, verb class + head word, verb class + phrase type, from #AUTHOR_TAG']",5
"['##anu et al. 2003 ), and machine translation ( #AUTHOR_TAG ).', 'with the efforts of many researchers ( carreras and']","['answering ( narayanan and harabagiu 2004 ), information extraction ( surdeanu et al. 2003 ), and machine translation ( #AUTHOR_TAG ).', 'with the efforts of many researchers ( carreras and marquez 2004,']","['##anu et al. 2003 ), and machine translation ( #AUTHOR_TAG ).', 'with the efforts of many researchers ( carreras and']","['role labeling ( srl ) was first defined in Gildea and Jurafsky (2002).', 'the purpose of srl task is to identify and classify the semantic roles of each predicate in a sentence.', 'the semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'typical tags include agent, patient, source, etc. and some adjuncts such as temporal, manner, extent, etc.', 'since the arguments can provide useful semantic information, the srl is crucial to many natural language processing tasks, such as question and answering ( narayanan and harabagiu 2004 ), information extraction ( surdeanu et al. 2003 ), and machine translation ( #AUTHOR_TAG ).', 'with the efforts of many researchers ( carreras and marquez 2004, moschitti 2004, pradhan et al 2005, zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made srl task progress fast']",0
"['##8 files ( from chtb _ 081 to chtb _ 899. fid )', 'are used as the training set.', 'the development set includes 40 files, from chtb _ 041. fid to chtb _ 080. fid.', 'the test set includes 72 files, which are chtb _ 001 to chtb _ 041, and chtb _ 900 to chtb _ 931.', 'we use the same data setting with #AUTHOR_TAG, however a bit different from Xue and Palmer ( 2005 )']","['use chinese propbank 1. 0 ( ldc number : ldc2005t23 ) in our experiments.', 'propbank 1. 0 includes the annotations for files chtb _ 001. fid', 'to chtb _ 931. fid,', 'or the first 250k words of the chinese treebank 5. 1.', 'for the experiments, the data of propbank is divided into three parts.', '648 files ( from chtb _ 081 to chtb _ 899. fid )', 'are used as the training set.', 'the development set includes 40 files, from chtb _ 041. fid to chtb _ 080. fid.', 'the test set includes 72 files, which are chtb _ 001 to chtb _ 041, and chtb _ 900 to chtb _ 931.', 'we use the same data setting with #AUTHOR_TAG, however a bit different from Xue and Palmer ( 2005 )']","['##8 files ( from chtb _ 081 to chtb _ 899. fid )', 'are used as the training set.', 'the development set includes 40 files, from chtb _ 041. fid to chtb _ 080. fid.', 'the test set includes 72 files, which are chtb _ 001 to chtb _ 041, and chtb _ 900 to chtb _ 931.', 'we use the same data setting with #AUTHOR_TAG, however a bit different from Xue and Palmer ( 2005 )']","['use chinese propbank 1. 0 ( ldc number : ldc2005t23 ) in our experiments.', 'propbank 1. 0 includes the annotations for files chtb _ 001. fid', 'to chtb _ 931. fid,', 'or the first 250k words of the chinese treebank 5. 1.', 'for the experiments, the data of propbank is divided into three parts.', '648 files ( from chtb _ 081 to chtb _ 899. fid )', 'are used as the training set.', 'the development set includes 40 files, from chtb _ 041. fid to chtb _ 080. fid.', 'the test set includes 72 files, which are chtb _ 001 to chtb _ 041, and chtb _ 900 to chtb _ 931.', 'we use the same data setting with #AUTHOR_TAG, however a bit different from Xue and Palmer ( 2005 )']",5
"[', the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky ( 2004 ), Xue and Palmer ( 2005 ) and #AUTHOR_TAG.', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky ( 2004 ), Xue and Palmer ( 2005 ) and #AUTHOR_TAG.', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a']","[', the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky ( 2004 ), Xue and Palmer ( 2005 ) and #AUTHOR_TAG.', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky ( 2004 ), Xue and Palmer ( 2005 ) and #AUTHOR_TAG.', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']",0
"['of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue and palmer 2005, #AUTHOR_TAG ) reassured these findings']","['of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue and palmer 2005, #AUTHOR_TAG ) reassured these findings']","[', the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky (2004), and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue and palmer 2005, #AUTHOR_TAG ) reassured these findings']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky (2004), and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue and palmer 2005, #AUTHOR_TAG ) reassured these findings']",0
"['collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', '#AUTHOR_TAG']","['collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', '#AUTHOR_TAG']","[', the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky (2004), and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', '#AUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky (2004), and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', '#AUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']",0
"[', the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky ( 2004 ), #AUTHOR_TAG and Xue ( 2008 ).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky ( 2004 ), #AUTHOR_TAG and Xue ( 2008 ).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a']","[', the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky ( 2004 ), #AUTHOR_TAG and Xue ( 2008 ).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky ( 2004 ), #AUTHOR_TAG and Xue ( 2008 ).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']",0
"['##8 files ( from chtb _ 081 to chtb _ 899. fid )', 'are used as the training set.', 'the development set includes 40 files, from chtb _ 041. fid to chtb _ 080. fid.', 'the test set includes 72 files, which are chtb _ 001 to chtb _ 041, and chtb _ 900 to chtb _ 931.', 'we use the same data setting with Xue ( 2008 ), however a bit different from #AUTHOR_TAG']","['use chinese propbank 1. 0 ( ldc number : ldc2005t23 ) in our experiments.', 'propbank 1. 0 includes the annotations for files chtb _ 001. fid', 'to chtb _ 931. fid,', 'or the first 250k words of the chinese treebank 5. 1.', 'for the experiments, the data of propbank is divided into three parts.', '648 files ( from chtb _ 081 to chtb _ 899. fid )', 'are used as the training set.', 'the development set includes 40 files, from chtb _ 041. fid to chtb _ 080. fid.', 'the test set includes 72 files, which are chtb _ 001 to chtb _ 041, and chtb _ 900 to chtb _ 931.', 'we use the same data setting with Xue ( 2008 ), however a bit different from #AUTHOR_TAG']","['##8 files ( from chtb _ 081 to chtb _ 899. fid )', 'are used as the training set.', 'the development set includes 40 files, from chtb _ 041. fid to chtb _ 080. fid.', 'the test set includes 72 files, which are chtb _ 001 to chtb _ 041, and chtb _ 900 to chtb _ 931.', 'we use the same data setting with Xue ( 2008 ), however a bit different from #AUTHOR_TAG']","['use chinese propbank 1. 0 ( ldc number : ldc2005t23 ) in our experiments.', 'propbank 1. 0 includes the annotations for files chtb _ 001. fid', 'to chtb _ 931. fid,', 'or the first 250k words of the chinese treebank 5. 1.', 'for the experiments, the data of propbank is divided into three parts.', '648 files ( from chtb _ 081 to chtb _ 899. fid )', 'are used as the training set.', 'the development set includes 40 files, from chtb _ 041. fid to chtb _ 080. fid.', 'the test set includes 72 files, which are chtb _ 001 to chtb _ 041, and chtb _ 900 to chtb _ 931.', 'we use the same data setting with Xue ( 2008 ), however a bit different from #AUTHOR_TAG']",1
['candidate feature templates include : voice from #AUTHOR_TAG'],['candidate feature templates include : voice from #AUTHOR_TAG'],['candidate feature templates include : voice from #AUTHOR_TAG'],['candidate feature templates include : voice from #AUTHOR_TAG'],5
"['chinese propbank has labeled the predicateargument structures of sentences from the chinese treebank ( #AUTHOR_TAG ).', 'it is constituted of two parts.', 'one is the labeled data, which indicates the positions of the predicates and']","['chinese propbank has labeled the predicateargument structures of sentences from the chinese treebank ( #AUTHOR_TAG ).', 'it is constituted of two parts.', 'one is the labeled data, which indicates the positions of the predicates and']","['chinese propbank has labeled the predicateargument structures of sentences from the chinese treebank ( #AUTHOR_TAG ).', 'it is constituted of two parts.', 'one is the labeled data, which indicates the positions of the predicates and']","['chinese propbank has labeled the predicateargument structures of sentences from the chinese treebank ( #AUTHOR_TAG ).', 'it is constituted of two parts.', 'one is the labeled data, which indicates the positions of the predicates and its arguments in the chinese treebank.', 'the other is a dictionary which lists the frames of all the labeled predicates.', 'figure 1 is an example from the propbank 1.', 'we put the word - by - word translation and the translation of the whole sentence below the example.', 'it is quite a complex sentence, as there are many semantic roles in it.', 'in this sentence, all the semantic roles of the verb [UNK] [UNK] ( provide ) are presented in the syntactic tree.', 'we can separate the semantic roles into two groups']",5
"['role labeling ( srl ) was first defined in #AUTHOR_TAG.', 'the purpose of srl task is to identify and classify the semantic roles of each predicate in a sentence.', 'the semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'typical tags include agent, patient, source, etc. and some']","['role labeling ( srl ) was first defined in #AUTHOR_TAG.', 'the purpose of srl task is to identify and classify the semantic roles of each predicate in a sentence.', 'the semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'typical tags include agent, patient, source, etc. and some adjuncts such as temporal, manner, extent, etc.', 'since the arguments can provide useful semantic information, the srl is crucial to many natural language processing tasks, such as question']","['role labeling ( srl ) was first defined in #AUTHOR_TAG.', 'the purpose of srl task is to identify and classify the semantic roles of each predicate in a sentence.', 'the semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'typical tags include agent, patient, source, etc. and some adjuncts such as temporal, manner, extent, etc.', 'since the arguments can provide useful semantic information, the srl is crucial to many natural language processing tasks, such as question']","['role labeling ( srl ) was first defined in #AUTHOR_TAG.', 'the purpose of srl task is to identify and classify the semantic roles of each predicate in a sentence.', 'the semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'typical tags include agent, patient, source, etc. and some adjuncts such as temporal, manner, extent, etc.', 'since the arguments can provide useful semantic information, the srl is crucial to many natural language processing tasks, such as question and answering ( narayanan and harabagiu 2004 ), information extraction ( surdeanu et al. 2003 ), and machine translation ( boas 2002 ).', 'with the efforts of many researchers ( carreras and marquez 2004, moschitti 2004, pradhan et al 2005, zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made srl task progress fast']",0
"['( #AUTHOR_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ).', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow']","['( #AUTHOR_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ).', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow )']","['instance ( #AUTHOR_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ).', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow']","['most used feature for the web people search task, however, are nes.', 'Ravin (1999) introduced a rule - based approach that tackles both variation and ambiguity analysing the structure of names.', 'in most recent research, nes ( person, location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #AUTHOR_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ).', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow )']",0
"['( Blume , 2005 ; #AUTHOR_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ).', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow']","['( Blume , 2005 ; #AUTHOR_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ).', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow )']","['instance ( Blume , 2005 ; #AUTHOR_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ).', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow']","['most used feature for the web people search task, however, are nes.', 'Ravin (1999) introduced a rule - based approach that tackles both variation and ambiguity analysing the structure of names.', 'in most recent research, nes ( person, location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; #AUTHOR_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ).', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow )']",0
"['( Blume , 2005 ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; #AUTHOR_TAG ).', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow']","['( Blume , 2005 ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; #AUTHOR_TAG ).', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow )']","['instance ( Blume , 2005 ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; #AUTHOR_TAG ).', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow']","['most used feature for the web people search task, however, are nes.', 'Ravin (1999) introduced a rule - based approach that tackles both variation and ambiguity analysing the structure of names.', 'in most recent research, nes ( person, location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; #AUTHOR_TAG ).', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow )']",0
"['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure ( #AUTHOR_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure ( #AUTHOR_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure ( #AUTHOR_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure ( #AUTHOR_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers ( Cucerzan , 2007 ; #AUTHOR_TAG ) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers ( Cucerzan , 2007 ; #AUTHOR_TAG ) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers ( Cucerzan , 2007 ; #AUTHOR_TAG ) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers ( Cucerzan , 2007 ; #AUTHOR_TAG ) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) (Agirre and Edmonds, 2006) and cross - document coreference ( cdc ) (Bagga and Baldwin, 1998).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task web people search on its own ( Artiles et al. , 2005 ; #AUTHOR_TAG )']","['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) (Agirre and Edmonds, 2006) and cross - document coreference ( cdc ) (Bagga and Baldwin, 1998).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task web people search on its own ( Artiles et al. , 2005 ; #AUTHOR_TAG )']","['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) (Agirre and Edmonds, 2006) and cross - document coreference ( cdc ) (Bagga and Baldwin, 1998).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task web people search on its own ( Artiles et al. , 2005 ; #AUTHOR_TAG )']","['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) (Agirre and Edmonds, 2006) and cross - document coreference ( cdc ) (Bagga and Baldwin, 1998).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task web people search on its own ( Artiles et al. , 2005 ; #AUTHOR_TAG )']",0
"['##the weps - 1 corpus includes data from the web03 testbed ( #AUTHOR_TAG ) which follows similar annotation guidelines, although the number of document per ambiguous name is more variable']","['##the weps - 1 corpus includes data from the web03 testbed ( #AUTHOR_TAG ) which follows similar annotation guidelines, although the number of document per ambiguous name is more variable']","['##the weps - 1 corpus includes data from the web03 testbed ( #AUTHOR_TAG ) which follows similar annotation guidelines, although the number of document per ambiguous name is more variable']","['##the weps - 1 corpus includes data from the web03 testbed ( #AUTHOR_TAG ) which follows similar annotation guidelines, although the number of document per ambiguous name is more variable']",5
"['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( Bagga and Baldwin , 1998 ; #AUTHOR_TAG ).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( Bagga and Baldwin , 1998 ; #AUTHOR_TAG ).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( Bagga and Baldwin , 1998 ; #AUTHOR_TAG ).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( Bagga and Baldwin , 1998 ; #AUTHOR_TAG ).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) (Agirre and Edmonds, 2006) and cross - document coreference ( cdc ) (Bagga and Baldwin, 1998).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task web people search on its own ( #AUTHOR_TAG ; Artiles et al. , 2007 )']","['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) (Agirre and Edmonds, 2006) and cross - document coreference ( cdc ) (Bagga and Baldwin, 1998).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task web people search on its own ( #AUTHOR_TAG ; Artiles et al. , 2007 )']","['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) (Agirre and Edmonds, 2006) and cross - document coreference ( cdc ) (Bagga and Baldwin, 1998).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task web people search on its own ( #AUTHOR_TAG ; Artiles et al. , 2007 )']","['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) (Agirre and Edmonds, 2006) and cross - document coreference ( cdc ) (Bagga and Baldwin, 1998).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task web people search on its own ( #AUTHOR_TAG ; Artiles et al. , 2007 )']",0
"['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( #AUTHOR_TAG ; Popescu and Magnini , 2007 ) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( #AUTHOR_TAG ; Popescu and Magnini , 2007 ) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( #AUTHOR_TAG ; Popescu and Magnini , 2007 ) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( #AUTHOR_TAG ; Popescu and Magnini , 2007 ) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #AUTHOR_TAG ).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #AUTHOR_TAG ).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #AUTHOR_TAG ).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #AUTHOR_TAG ).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['study of the query log of the alltheweb and altavista search sites gives an idea of the relevance of the people search task : 11 - 17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #AUTHOR_TAG ).', 'according to the data available from 1990 u. s. census bureau, only 90, 000 different names are shared by 100 million people (Artiles et al., 2005).', 'as the amount of information in the www grows, more of these people are mentioned in different web pages.', 'therefore, a query for a common name in the web will usually produce a list of results where different people are mentioned']","['study of the query log of the alltheweb and altavista search sites gives an idea of the relevance of the people search task : 11 - 17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #AUTHOR_TAG ).', 'according to the data available from 1990 u. s. census bureau, only 90, 000 different names are shared by 100 million people (Artiles et al., 2005).', 'as the amount of information in the www grows, more of these people are mentioned in different web pages.', 'therefore, a query for a common name in the web will usually produce a list of results where different people are mentioned']","['study of the query log of the alltheweb and altavista search sites gives an idea of the relevance of the people search task : 11 - 17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #AUTHOR_TAG ).', 'according to the data available from 1990 u. s. census bureau, only 90, 000 different names are shared by 100 million people (Artiles et al., 2005).', 'as the amount of information in the www grows, more of these people are mentioned in different web pages.', 'therefore, a query for a common name in the web will usually produce a list of results where different people are mentioned']","['study of the query log of the alltheweb and altavista search sites gives an idea of the relevance of the people search task : 11 - 17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #AUTHOR_TAG ).', 'according to the data available from 1990 u. s. census bureau, only 90, 000 different names are shared by 100 million people (Artiles et al., 2005).', 'as the amount of information in the www grows, more of these people are mentioned in different web pages.', 'therefore, a query for a common name in the web will usually produce a list of results where different people are mentioned']",0
"['instance (Blume, 2005; Chen and Martin, 2007; Popescu and Magnini, 2007; Kalashnikov et al., 2007) -.', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', '#AUTHOR_TAG compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow']","['instance (Blume, 2005; Chen and Martin, 2007; Popescu and Magnini, 2007; Kalashnikov et al., 2007) -.', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', '#AUTHOR_TAG compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow )']","['instance (Blume, 2005; Chen and Martin, 2007; Popescu and Magnini, 2007; Kalashnikov et al., 2007) -.', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', '#AUTHOR_TAG compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow']","['most used feature for the web people search task, however, are nes.', 'Ravin (1999) introduced a rule - based approach that tackles both variation and ambiguity analysing the structure of names.', 'in most recent research, nes ( person, location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance (Blume, 2005; Chen and Martin, 2007; Popescu and Magnini, 2007; Kalashnikov et al., 2007) -.', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', '#AUTHOR_TAG compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow )']",0
"['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #AUTHOR_TAG ; Gooi and Allan , 2004 ).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #AUTHOR_TAG ; Gooi and Allan , 2004 ).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #AUTHOR_TAG ; Gooi and Allan , 2004 ).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #AUTHOR_TAG ; Gooi and Allan , 2004 ).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['7 is a rule based english analyser that includes many functionalities ( pos tagger, stemmer, chunker, named entity ( ne ) tagger, dependency analyser, parser, etc ).', 'it provides a fine grained ne recognition covering 100 different ne types ( #AUTHOR_TAG ).', 'given the sparseness of most of these fine - grained ne types, we have merged them in coarser groups : event, facility, location, person, organisation, product, periodx, timex and numex']","['7 is a rule based english analyser that includes many functionalities ( pos tagger, stemmer, chunker, named entity ( ne ) tagger, dependency analyser, parser, etc ).', 'it provides a fine grained ne recognition covering 100 different ne types ( #AUTHOR_TAG ).', 'given the sparseness of most of these fine - grained ne types, we have merged them in coarser groups : event, facility, location, person, organisation, product, periodx, timex and numex']","['7 is a rule based english analyser that includes many functionalities ( pos tagger, stemmer, chunker, named entity ( ne ) tagger, dependency analyser, parser, etc ).', 'it provides a fine grained ne recognition covering 100 different ne types ( #AUTHOR_TAG ).', 'given the sparseness of most of these fine - grained ne types, we have merged them in coarser groups : event, facility, location, person, organisation, product, periodx, timex and numex']","['7 is a rule based english analyser that includes many functionalities ( pos tagger, stemmer, chunker, named entity ( ne ) tagger, dependency analyser, parser, etc ).', 'it provides a fine grained ne recognition covering 100 different ne types ( #AUTHOR_TAG ).', 'given the sparseness of most of these fine - grained ne types, we have merged them in coarser groups : event, facility, location, person, organisation, product, periodx, timex and numex']",5
"['study of the query log of the alltheweb and altavista search sites gives an idea of the relevance of the people search task : 11 - 17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names (Spink et al., 2004).', 'according to the data available from 1990 u. s. census bureau, only 90, 000 different names are shared by 100 million people ( #AUTHOR_TAG ).', 'as the amount of information in the www grows, more of these people are mentioned in different web pages.', 'therefore, a query for a common name in the web will usually produce a list of results where different people are mentioned']","['study of the query log of the alltheweb and altavista search sites gives an idea of the relevance of the people search task : 11 - 17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names (Spink et al., 2004).', 'according to the data available from 1990 u. s. census bureau, only 90, 000 different names are shared by 100 million people ( #AUTHOR_TAG ).', 'as the amount of information in the www grows, more of these people are mentioned in different web pages.', 'therefore, a query for a common name in the web will usually produce a list of results where different people are mentioned']","['study of the query log of the alltheweb and altavista search sites gives an idea of the relevance of the people search task : 11 - 17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names (Spink et al., 2004).', 'according to the data available from 1990 u. s. census bureau, only 90, 000 different names are shared by 100 million people ( #AUTHOR_TAG ).', 'as the amount of information in the www grows, more of these people are mentioned in different web pages.', 'therefore, a query for a common name in the web will usually produce a list of results where different people are mentioned']","['study of the query log of the alltheweb and altavista search sites gives an idea of the relevance of the people search task : 11 - 17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names (Spink et al., 2004).', 'according to the data available from 1990 u. s. census bureau, only 90, 000 different names are shared by 100 million people ( #AUTHOR_TAG ).', 'as the amount of information in the www grows, more of these people are mentioned in different web pages.', 'therefore, a query for a common name in the web will usually produce a list of results where different people are mentioned']",0
"['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers ( #AUTHOR_TAG ; Nguyen and Cao , 2008 ) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers ( #AUTHOR_TAG ; Nguyen and Cao , 2008 ) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers ( #AUTHOR_TAG ; Nguyen and Cao , 2008 ) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers ( #AUTHOR_TAG ; Nguyen and Cao , 2008 ) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['the task of finding information about other people sharing her name.', 'the web people search task, as defined in the first weps evaluation campaign ( #AUTHOR_TAG ), consists of grouping search results for a given name according to the different people that share it']","['is interested in.', 'the user might refine the original query with additional terms, but this risks excluding relevant documents in the process.', 'in some cases, the existence of a predominant person ( such as a celebrity or a historical figure ) makes it likely to dominate the ranking of search results, complicating the task of finding information about other people sharing her name.', 'the web people search task, as defined in the first weps evaluation campaign ( #AUTHOR_TAG ), consists of grouping search results for a given name according to the different people that share it']","['situation leaves to the user the task of finding the pages relevant to the particular person he is interested in.', 'the user might refine the original query with additional terms, but this risks excluding relevant documents in the process.', 'in some cases, the existence of a predominant person ( such as a celebrity or a historical figure ) makes it likely to dominate the ranking of search results, complicating the task of finding information about other people sharing her name.', 'the web people search task, as defined in the first weps evaluation campaign ( #AUTHOR_TAG ), consists of grouping search results for a given name according to the different people that share it']","['situation leaves to the user the task of finding the pages relevant to the particular person he is interested in.', 'the user might refine the original query with additional terms, but this risks excluding relevant documents in the process.', 'in some cases, the existence of a predominant person ( such as a celebrity or a historical figure ) makes it likely to dominate the ranking of search results, complicating the task of finding information about other people sharing her name.', 'the web people search task, as defined in the first weps evaluation campaign ( #AUTHOR_TAG ), consists of grouping search results for a given name according to the different people that share it']",0
"['have used the testbeds from weps - 1 ( #AUTHOR_TAG, 2007 ) 2 and weps - 2 (Artiles et al., 2009) evaluation campaigns 3']","['have used the testbeds from weps - 1 ( #AUTHOR_TAG, 2007 ) 2 and weps - 2 (Artiles et al., 2009) evaluation campaigns 3']","['have used the testbeds from weps - 1 ( #AUTHOR_TAG, 2007 ) 2 and weps - 2 (Artiles et al., 2009) evaluation campaigns 3']","['have used the testbeds from weps - 1 ( #AUTHOR_TAG, 2007 ) 2 and weps - 2 (Artiles et al., 2009) evaluation campaigns 3']",5
"['( Blume , 2005 ; Chen and Martin , 2007 ; #AUTHOR_TAG ; Kalashnikov et al. , 2007 ).', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow']","['( Blume , 2005 ; Chen and Martin , 2007 ; #AUTHOR_TAG ; Kalashnikov et al. , 2007 ).', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow )']","['instance ( Blume , 2005 ; Chen and Martin , 2007 ; #AUTHOR_TAG ; Kalashnikov et al. , 2007 ).', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow']","['most used feature for the web people search task, however, are nes.', 'Ravin (1999) introduced a rule - based approach that tackles both variation and ambiguity analysing the structure of names.', 'in most recent research, nes ( person, location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; #AUTHOR_TAG ; Kalashnikov et al. , 2007 ).', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow )']",0
"['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) ( Agirre and Edmonds , 2006 ) and cross - document coreference ( cdc ) ( #AUTHOR_TAG ).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task - web people search - on its own (Artiles et al., 2005;Artiles et al., 2007)']","['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) ( Agirre and Edmonds , 2006 ) and cross - document coreference ( cdc ) ( #AUTHOR_TAG ).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task - web people search - on its own (Artiles et al., 2005;Artiles et al., 2007)']","['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) ( Agirre and Edmonds , 2006 ) and cross - document coreference ( cdc ) ( #AUTHOR_TAG ).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task - web people search - on its own (Artiles et al., 2005;Artiles et al., 2007)']","['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) ( Agirre and Edmonds , 2006 ) and cross - document coreference ( cdc ) ( #AUTHOR_TAG ).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task - web people search - on its own (Artiles et al., 2005;Artiles et al., 2007)']",0
"['the 16 teams that submitted results for the first weps campaign, 10 of them 1 used nes in their document representation.', 'this makes nes the second most common type of feature ; only the bow feature was more popular.', 'other features used by the systems include noun phrases (Chen and Martin, 2007), word n - grams (Popescu and Magnini, 2007), emails and urls ( del valle - Agudo et al., 2007), etc.', 'in 2009, the second weps campaign showed similar trends regarding the use of ne features ( #AUTHOR_TAG ).', 'due to the complexity of systems, the results of the weps evaluation do not provide a direct answer regarding the advantages of using nes over other computationally lighter features such as bow or word n - grams.', 'but the weps campaigns did provide a useful, standardised resource to perform the type of studies that were not possible before.', 'in the next section we describe this dataset and how it has been adapted for our purposes']","['the 16 teams that submitted results for the first weps campaign, 10 of them 1 used nes in their document representation.', 'this makes nes the second most common type of feature ; only the bow feature was more popular.', 'other features used by the systems include noun phrases (Chen and Martin, 2007), word n - grams (Popescu and Magnini, 2007), emails and urls ( del valle - Agudo et al., 2007), etc.', 'in 2009, the second weps campaign showed similar trends regarding the use of ne features ( #AUTHOR_TAG ).', 'due to the complexity of systems, the results of the weps evaluation do not provide a direct answer regarding the advantages of using nes over other computationally lighter features such as bow or word n - grams.', 'but the weps campaigns did provide a useful, standardised resource to perform the type of studies that were not possible before.', 'in the next section we describe this dataset and how it has been adapted for our purposes']","['the 16 teams that submitted results for the first weps campaign, 10 of them 1 used nes in their document representation.', 'this makes nes the second most common type of feature ; only the bow feature was more popular.', 'other features used by the systems include noun phrases (Chen and Martin, 2007), word n - grams (Popescu and Magnini, 2007), emails and urls ( del valle - Agudo et al., 2007), etc.', 'in 2009, the second weps campaign showed similar trends regarding the use of ne features ( #AUTHOR_TAG ).', 'due to the complexity of systems, the results of the weps evaluation do not provide a direct answer regarding the advantages of using nes over other computationally lighter features such as bow or word n - grams.', 'but the weps campaigns did provide a useful, standardised resource to perform the type of studies that were not possible before.', 'in the next section we describe this dataset and how it has been adapted for our purposes']","['the 16 teams that submitted results for the first weps campaign, 10 of them 1 used nes in their document representation.', 'this makes nes the second most common type of feature ; only the bow feature was more popular.', 'other features used by the systems include noun phrases (Chen and Martin, 2007), word n - grams (Popescu and Magnini, 2007), emails and urls ( del valle - Agudo et al., 2007), etc.', 'in 2009, the second weps campaign showed similar trends regarding the use of ne features ( #AUTHOR_TAG ).', 'due to the complexity of systems, the results of the weps evaluation do not provide a direct answer regarding the advantages of using nes over other computationally lighter features such as bow or word n - grams.', 'but the weps campaigns did provide a useful, standardised resource to perform the type of studies that were not possible before.', 'in the next section we describe this dataset and how it has been adapted for our purposes']",0
"['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( #AUTHOR_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( #AUTHOR_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( #AUTHOR_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( #AUTHOR_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['our current unsupervised approach ( Erk , 2007 ; #AUTHOR_TAG ).', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially when the data is sparse, requires also further investigation']","['our current unsupervised approach ( Erk , 2007 ; #AUTHOR_TAG ).', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially when the data is sparse, requires also further investigation']","['our current unsupervised approach ( Erk , 2007 ; #AUTHOR_TAG ).', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially when the data is sparse, requires also further investigation']","['addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring sps for verb classification.', 'considerable research has been done on sp acquisition most of which has involved collecting argument headwords from data and generalizing to word - net classes.', 'Brockmann and Lapata ( 2003 ) have showed that wordnet - based approaches do not always outperform simple frequency - based models, and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( Erk , 2007 ; #AUTHOR_TAG ).', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially when the data is sparse, requires also further investigation']",3
"['our current unsupervised approach ( #AUTHOR_TAG ; Bergsma et al. , 2008 ).', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially when the data is sparse, requires also further investigation']","['our current unsupervised approach ( #AUTHOR_TAG ; Bergsma et al. , 2008 ).', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially when the data is sparse, requires also further investigation']","['our current unsupervised approach ( #AUTHOR_TAG ; Bergsma et al. , 2008 ).', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially when the data is sparse, requires also further investigation']","['addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring sps for verb classification.', 'considerable research has been done on sp acquisition most of which has involved collecting argument headwords from data and generalizing to word - net classes.', 'Brockmann and Lapata ( 2003 ) have showed that wordnet - based approaches do not always outperform simple frequency - based models, and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #AUTHOR_TAG ; Bergsma et al. , 2008 ).', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially when the data is sparse, requires also further investigation']",3
"['affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case']","['affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case']","['conjecture based on our analysis that the em training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case']","['conjecture based on our analysis that the em training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case']",3
"['affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches ( #AUTHOR_TAG ; Huang , 2008 ) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case']","['affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches ( #AUTHOR_TAG ; Huang , 2008 ) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case']","['conjecture based on our analysis that the em training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches ( #AUTHOR_TAG ; Huang , 2008 ) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case']","['conjecture based on our analysis that the em training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches ( #AUTHOR_TAG ; Huang , 2008 ) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case']",3
"['affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches (Charniak and Johnson, 2005;Huang, 2008) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations ( #AUTHOR_TAG ), although training would be much slower compared to using generative models, as in our case']","['affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches (Charniak and Johnson, 2005;Huang, 2008) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations ( #AUTHOR_TAG ), although training would be much slower compared to using generative models, as in our case']","['conjecture based on our analysis that the em training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches (Charniak and Johnson, 2005;Huang, 2008) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations ( #AUTHOR_TAG ), although training would be much slower compared to using generative models, as in our case']","['conjecture based on our analysis that the em training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches (Charniak and Johnson, 2005;Huang, 2008) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations ( #AUTHOR_TAG ), although training would be much slower compared to using generative models, as in our case']",3
"['Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; #AUTHOR_TAG ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']","['Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; #AUTHOR_TAG ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']","['Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; #AUTHOR_TAG ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']","['future work, there are many research line that may be followed : i ) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet - based features ( Basili et al. , 2005 a ; Basili et al. , 2005 b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; #AUTHOR_TAG ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']",3
"['Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; #AUTHOR_TAG ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']","['Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; #AUTHOR_TAG ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']","['Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; #AUTHOR_TAG ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']","['future work, there are many research line that may be followed : i ) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet - based features ( Basili et al. , 2005 a ; Basili et al. , 2005 b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; #AUTHOR_TAG ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']",3
"['Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; #AUTHOR_TAG )']","['Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; #AUTHOR_TAG )']","['Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; #AUTHOR_TAG )']","['future work, there are many research line that may be followed : i ) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet - based features ( Basili et al. , 2005 a ; Basili et al. , 2005 b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; #AUTHOR_TAG )']",3
"['be followed : i ) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet - based features ( #AUTHOR_TAG a']","['be followed : i ) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet - based features ( #AUTHOR_TAG a']","['future work, there are many research line that may be followed : i ) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet - based features ( #AUTHOR_TAG a']","['future work, there are many research line that may be followed : i ) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet - based features ( #AUTHOR_TAG a ; Basili et al. , 2005 b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']",3
"['range of classifiers used in various nlp tasks.', 'to speed up classifiers used in a real - time application, we can build fstries incrementally by using feature vectors generated from user inputs.', 'when we run our classifiers on resource - tight environments such as cell - phones, we can use a random feature mixing technique ( #AUTHOR_TAG ) or a memory - efficient trie implementation based on a succinct data structure ( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage']","['range of classifiers used in various nlp tasks.', 'to speed up classifiers used in a real - time application, we can build fstries incrementally by using feature vectors generated from user inputs.', 'when we run our classifiers on resource - tight environments such as cell - phones, we can use a random feature mixing technique ( #AUTHOR_TAG ) or a memory - efficient trie implementation based on a succinct data structure ( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage']","['range of classifiers used in various nlp tasks.', 'to speed up classifiers used in a real - time application, we can build fstries incrementally by using feature vectors generated from user inputs.', 'when we run our classifiers on resource - tight environments such as cell - phones, we can use a random feature mixing technique ( #AUTHOR_TAG ) or a memory - efficient trie implementation based on a succinct data structure ( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage']","['plan to apply our method to wider range of classifiers used in various nlp tasks.', 'to speed up classifiers used in a real - time application, we can build fstries incrementally by using feature vectors generated from user inputs.', 'when we run our classifiers on resource - tight environments such as cell - phones, we can use a random feature mixing technique ( #AUTHOR_TAG ) or a memory - efficient trie implementation based on a succinct data structure ( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage']",3
"['reordering models we describe follow our previous work using function word models for translation ( #AUTHOR_TAG ; Setiawan et al. , 2009 ).', 'the core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'to make this']","['reordering models we describe follow our previous work using function word models for translation ( #AUTHOR_TAG ; Setiawan et al. , 2009 ).', 'the core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'to make this']","['reordering models we describe follow our previous work using function word models for translation ( #AUTHOR_TAG ; Setiawan et al. , 2009 ).', 'the core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'to make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word ( which functions as an anchor ) using two kinds of information : 1 ) the relative ordering of the phrases with respect to the function word anchor ; and 2 ) the span of the phrases.', 'this section provides a high level overview of our reordering model, which attempts to leverage this information']","['reordering models we describe follow our previous work using function word models for translation ( #AUTHOR_TAG ; Setiawan et al. , 2009 ).', 'the core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'to make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word ( which functions as an anchor ) using two kinds of information : 1 ) the relative ordering of the phrases with respect to the function word anchor ; and 2 ) the span of the phrases.', 'this section provides a high level overview of our reordering model, which attempts to leverage this information']",2
"['as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words.', 'with respect to the focus on function words, our reordering model is closely related to the ualign system ( #AUTHOR_TAG ).', 'however, ualign uses deep syntactic analysis and hand - crafted heuristics in its model']","['as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words.', 'with respect to the focus on function words, our reordering model is closely related to the ualign system ( #AUTHOR_TAG ).', 'however, ualign uses deep syntactic analysis and hand - crafted heuristics in its model']","['reordering model is closely related to the model proposed by Zhang and Gildea (2005 ; 2007a ), with respect to conditioning the reordering predictions on lexical items.', 'these related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words.', 'with respect to the focus on function words, our reordering model is closely related to the ualign system ( #AUTHOR_TAG ).', 'however, ualign uses deep syntactic analysis and hand - crafted heuristics in its model']","['reordering model is closely related to the model proposed by Zhang and Gildea (2005 ; 2007a ), with respect to conditioning the reordering predictions on lexical items.', 'these related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words.', 'with respect to the focus on function words, our reordering model is closely related to the ualign system ( #AUTHOR_TAG ).', 'however, ualign uses deep syntactic analysis and hand - crafted heuristics in its model']",1
"['. the reordering of the neighboring phrases of a function word, we employ the orientation model introduced by #AUTHOR_TAG.', 'formally, this model takes the form of probability distribution p ori ( o ( l i, s→t ), o ( r i, s→t ) | y i, s→t ), which conditions the reordering on the lexical identity of the function word alignment ( but independent of the lexical identity of its neighboring phrases']","['model o ( li, s a t ), o ( ri, s a t ), i. e. the reordering of the neighboring phrases of a function word, we employ the orientation model introduced by #AUTHOR_TAG.', 'formally, this model takes the form of probability distribution p ori ( o ( l i, s→t ), o ( r i, s→t ) | y i, s→t ), which conditions the reordering on the lexical identity of the function word alignment ( but independent of the lexical identity of its neighboring phrases ).', 'in particular, o maps the reordering into one of the following four orientation values ( borrowed from Nagata et al. (2006) ) with respect to the function word : monotone adjacent ( ma ), monotone gap ( mg ), reverse adjacent ( ra ) and reverse gap ( rg ).', 'the monotone / reverse distinction indicates whether the projected order follows the original order, while the adjacent / gap distinction indicates whether the pro - this heuristic is commonly used in learning phrase pairs from parallel text.', 'the maximality ensures the uniqueness of l and r']","['. the reordering of the neighboring phrases of a function word, we employ the orientation model introduced by #AUTHOR_TAG.', 'formally, this model takes the form of probability distribution p ori ( o ( l i, s→t ), o ( r i, s→t ) | y i, s→t ), which conditions the reordering on the lexical identity of the function word alignment ( but independent of the lexical identity of its neighboring phrases ).', 'in particular, o maps the reordering into one of the following four orientation values ( borrow']","['model o ( li, s a t ), o ( ri, s a t ), i. e. the reordering of the neighboring phrases of a function word, we employ the orientation model introduced by #AUTHOR_TAG.', 'formally, this model takes the form of probability distribution p ori ( o ( l i, s→t ), o ( r i, s→t ) | y i, s→t ), which conditions the reordering on the lexical identity of the function word alignment ( but independent of the lexical identity of its neighboring phrases ).', 'in particular, o maps the reordering into one of the following four orientation values ( borrowed from Nagata et al. (2006) ) with respect to the function word : monotone adjacent ( ma ), monotone gap ( mg ), reverse adjacent ( ra ) and reverse gap ( rg ).', 'the monotone / reverse distinction indicates whether the projected order follows the original order, while the adjacent / gap distinction indicates whether the pro - this heuristic is commonly used in learning phrase pairs from parallel text.', 'the maximality ensures the uniqueness of l and r']",5
"['reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ).', 'the core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'to make this']","['reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ).', 'the core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'to make this']","['reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ).', 'the core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'to make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word ( which functions as an anchor ) using two kinds of information : 1 ) the relative ordering of the phrases with respect to the function word anchor ; and 2 ) the span of the phrases.', 'this section provides a high level overview of our reordering model, which attempts to leverage this information']","['reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ).', 'the core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'to make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word ( which functions as an anchor ) using two kinds of information : 1 ) the relative ordering of the phrases with respect to the function word anchor ; and 2 ) the span of the phrases.', 'this section provides a high level overview of our reordering model, which attempts to leverage this information']",2
"['. whether li, s a t and ri, s a t extend beyond the neighboring function word phrase pairs, we utilize the pairwise dominance model of #AUTHOR_TAG.', 'taking']","[', s a t ), d ( fwi + 1, s a t ), i. e. whether li, s a t and ri, s a t extend beyond the neighboring function word phrase pairs, we utilize the pairwise dominance model of #AUTHOR_TAG.', 'taking d ( f w i−1, s→t ) as a case in point, this model takes the']","['. whether li, s a t and ri, s a t extend beyond the neighboring function word phrase pairs, we utilize the pairwise dominance model of #AUTHOR_TAG.', 'taking']","['model d ( fwi a 1, s a t ), d ( fwi + 1, s a t ), i. e. whether li, s a t and ri, s a t extend beyond the neighboring function word phrase pairs, we utilize the pairwise dominance model of #AUTHOR_TAG.', 'taking d ( f w i−1, s→t ) as a case in point, this model takes the']",5
"['our previous work ( #AUTHOR_TAG ), we started an initial investigation on conversation entailment.', 'we have']","['our previous work ( #AUTHOR_TAG ), we started an initial investigation on conversation entailment.', 'we have']","['our previous work ( #AUTHOR_TAG ), we started an initial investigation on conversation entailment.', 'we have collected']","['our previous work ( #AUTHOR_TAG ), we started an initial investigation on conversation entailment.', 'we have collected a dataset of 875 instances.', 'each instance consists of a conversation segment and a hypothesis ( as described in section 1 ).', 'the hypotheses are statements about conversation participants and are further categorized into four types : about their profile information, their beliefs and opinions, their desires, and their communicative intentions.', 'we developed an approach that is motivated by previous work on textual entailment.', 'we use clauses in the logic - based approaches as the underlying representation of our system.', 'based on this representation, we apply a two stage entailment process similar to mac Cartney et al. (2006) developed for textual entailment : an alignment stage followed by an entailment stage']",2
"['the implicit modeling of argument consistency, we follow the same approach as in our previous work ( #AUTHOR_TAG ) and trained a logistic regression model to predict verb alignment based on the features in table 1']","['the implicit modeling of argument consistency, we follow the same approach as in our previous work ( #AUTHOR_TAG ) and trained a logistic regression model to predict verb alignment based on the features in table 1']","['the implicit modeling of argument consistency, we follow the same approach as in our previous work ( #AUTHOR_TAG ) and trained a logistic regression model to predict verb alignment based on the features in table 1']","['the implicit modeling of argument consistency, we follow the same approach as in our previous work ( #AUTHOR_TAG ) and trained a logistic regression model to predict verb alignment based on the features in table 1']",2
"['that in our original work ( #AUTHOR_TAG ), only development data were used to show some initial observations.', 'here we trained our mod - els on the development data and results shown are from the testing data']","['that in our original work ( #AUTHOR_TAG ), only development data were used to show some initial observations.', 'here we trained our mod - els on the development data and results shown are from the testing data']","['that in our original work ( #AUTHOR_TAG ), only development data were used to show some initial observations.', 'here we trained our mod - els on the development data and results shown are from the testing data']","['that in our original work ( #AUTHOR_TAG ), only development data were used to show some initial observations.', 'here we trained our mod - els on the development data and results shown are from the testing data']",1
"['string representation of paths is used to capture both the subject consistency and the object consistency.', 'since they are non - numerical features, and the variability of their values can be extremely large, so we applied an instance - based classification model ( e. g., k - nearest neighbor ) to determine alignments between verb terms.', 'we measure the distance between two path features by their minimal string edit distance, and then simply use the euclidean distance to measure the closeness between any two verbs.', 'again this model is trained from our development data described in Zhang and Chai (2009).', 'figure 3 shows an example of alignment between the conversation terms and hypothesis terms in example 2. note that in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act.', 'this alignment is obtained by following the same set of rules learned from the development dataset as in ( #AUTHOR_TAG )']","['string representation of paths is used to capture both the subject consistency and the object consistency.', 'since they are non - numerical features, and the variability of their values can be extremely large, so we applied an instance - based classification model ( e. g., k - nearest neighbor ) to determine alignments between verb terms.', 'we measure the distance between two path features by their minimal string edit distance, and then simply use the euclidean distance to measure the closeness between any two verbs.', 'again this model is trained from our development data described in Zhang and Chai (2009).', 'figure 3 shows an example of alignment between the conversation terms and hypothesis terms in example 2. note that in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act.', 'this alignment is obtained by following the same set of rules learned from the development dataset as in ( #AUTHOR_TAG )']","['string representation of paths is used to capture both the subject consistency and the object consistency.', 'since they are non - numerical features, and the variability of their values can be extremely large, so we applied an instance - based classification model ( e. g., k - nearest neighbor ) to determine alignments between verb terms.', 'we measure the distance between two path features by their minimal string edit distance, and then simply use the euclidean distance to measure the closeness between any two verbs.', 'again this model is trained from our development data described in Zhang and Chai (2009).', 'figure 3 shows an example of alignment between the conversation terms and hypothesis terms in example 2. note that in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act.', 'this alignment is obtained by following the same set of rules learned from the development dataset as in ( #AUTHOR_TAG )']","['string representation of paths is used to capture both the subject consistency and the object consistency.', 'since they are non - numerical features, and the variability of their values can be extremely large, so we applied an instance - based classification model ( e. g., k - nearest neighbor ) to determine alignments between verb terms.', 'we measure the distance between two path features by their minimal string edit distance, and then simply use the euclidean distance to measure the closeness between any two verbs.', 'again this model is trained from our development data described in Zhang and Chai (2009).', 'figure 3 shows an example of alignment between the conversation terms and hypothesis terms in example 2. note that in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act.', 'this alignment is obtained by following the same set of rules learned from the development dataset as in ( #AUTHOR_TAG )']",5
"['address this limitation, our previous work ( #AUTHOR_TAG ) has initiated an investigation on the problem of conversation entailment.', 'the problem was formulated as follows : given a conversation discourse d and a hypothesis h concerning its participant, the goal was to identify whether d entails h.', 'for instance, as in example 1, the first hypothesis can be entailed from the conversation segment while the second hypothesis cannot.', '']","['address this limitation, our previous work ( #AUTHOR_TAG ) has initiated an investigation on the problem of conversation entailment.', 'the problem was formulated as follows : given a conversation discourse d and a hypothesis h concerning its participant, the goal was to identify whether d entails h.', 'for instance, as in example 1, the first hypothesis can be entailed from the conversation segment while the second hypothesis cannot.', '']","['address this limitation, our previous work ( #AUTHOR_TAG ) has initiated an investigation on the problem of conversation entailment.', 'the problem was formulated as follows : given a conversation discourse d and a hypothesis h concerning its participant, the goal was to identify whether d entails h.', 'for instance, as in example 1, the first hypothesis can be entailed from the conversation segment while the second hypothesis cannot.', '']","['address this limitation, our previous work ( #AUTHOR_TAG ) has initiated an investigation on the problem of conversation entailment.', 'the problem was formulated as follows : given a conversation discourse d and a hypothesis h concerning its participant, the goal was to identify whether d entails h.', 'for instance, as in example 1, the first hypothesis can be entailed from the conversation segment while the second hypothesis cannot.', 'while our previous work has provided some interesting preliminary observations, it mostly focused on data collection and initial experiments and analysis using a small set of development data.', 'it is not clear whether the previous results are generally applicable, how different components in the entailment framework interact with each other, and how different representations may influence the entailment outcome']",2
"['our previous work ( #AUTHOR_TAG ), conversation entailment is formulated as the following : given a conversation segment d which is represented by a set of clauses d = d1 a §... a § dm, and a hypothesis']","['our previous work ( #AUTHOR_TAG ), conversation entailment is formulated as the following : given a conversation segment d which is represented by a set of clauses d = d1 a §... a § dm, and a hypothesis']","['our previous work ( #AUTHOR_TAG ), conversation entailment is formulated as the following : given a conversation segment d which is represented by a set of clauses d = d1 a §... a § dm, and a hypothesis h represented by another set of clauses h = h1 a §... a § hn, the prediction on whether d entails h is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1... dm as follows.', 'this is based on a simple as - sumption that whether a clause is entailed from a conversation segment is conditionally independent from other clauses']","['our previous work ( #AUTHOR_TAG ), conversation entailment is formulated as the following : given a conversation segment d which is represented by a set of clauses d = d1 a §... a § dm, and a hypothesis h represented by another set of clauses h = h1 a §... a § hn, the prediction on whether d entails h is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1... dm as follows.', 'this is based on a simple as - sumption that whether a clause is entailed from a conversation segment is conditionally independent from other clauses']",2
"['by the kernel we used.', 'we use the structures previously used by #AUTHOR_TAG, and propose one new structure.', 'although we experiment']","['by the kernel we used.', 'we use the structures previously used by #AUTHOR_TAG, and propose one new structure.', 'although we experimented']","['a linear learning machine ( support vector machines ) for our classification task.', 'now we present the "" discrete "" structures followed by the kernel we used.', 'we use the structures previously used by #AUTHOR_TAG, and propose one new structure.', 'although we experimented']","['learning machines are one of the most popular machines used for classification problems.', 'the objective of a typical classification problem is to learn a function that separates the data into different classes.', 'the data is usually in the form of features extracted from abstract objects like strings, trees, etc.', 'a drawback of learning by using complex functions is that complex functions do not generalize well and thus tend to over - fit.', 'the research community therefore prefers linear classifiers over other complex classifiers.', 'but more often than not, the data is not linearly separable.', 'it can be made linearly separable by increasing the dimensionality of data but then learning suffers from the curse of dimensionality and classification becomes computationally intractable.', 'this is where kernels come to the rescue.', 'the well - known kernel trick aids us in finding similarity between feature vectors in a high dimensional space without having to write down the expanded feature space.', 'the essence of kernel methods is that they compare two feature vectors in high dimensional space by using a dot product that is a function of the dot product of feature vectors in the lower dimensional space.', 'moreover, convolution kernels ( first introduced by Haussler (1999) ) can be used to compare abstract objects instead of feature vectors.', 'this is because these kernels involve a recursive calculation over the "" parts "" of a discrete structure.', 'this calculation is usually made computationally efficient using dynamic programming techniques.', 'therefore, convolution kernels alleviate the need of feature extraction ( which usually requires domain knowledge, results in extraction of incomplete information and introduces noise in the data ).', 'therefore, we use convolution kernels with a linear learning machine ( support vector machines ) for our classification task.', 'now we present the "" discrete "" structures followed by the kernel we used.', 'we use the structures previously used by #AUTHOR_TAG, and propose one new structure.', 'although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task.', 'all the structures and their combinations are derived from a variation of the underlying structures, phrase structure trees ( pst ) and dependency trees ( dt ).', 'for all trees we first extract their path enclosed tree, which is the smallest common subtree that contains the two target entities (Moschitti, 2004).', 'we use the stanford parser (Klein and Manning, 2003) to get the basic psts and dts.', 'following are the structures that we refer to in our experiments and results section : pet : this refers to the smallest common phrase structure tree that contains the two target entities.', 'dependency words ( dw ) tree : this is the smallest common dependency tree that contains the two target entities.', 'in figure 1, since the target entities are at the leftmost and rightmost']",5
"['this paper, we have introduced the novel tasks of social event detection and classification.', 'we show that data sampling techniques play a crucial role for the task of relation detection.', 'through oversampling we achieve an increase in f1 - measure of 22. 2 % absolute over a baseline system.', 'our experiments show that as a result of how language expresses the relevant information, dependency - based structures are best suited for encoding this information.', 'furthermore, because of the complexity of the task, a combination of phrase based structures and dependency - based structures perform the best.', 'this revalidates the observation of #AUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task.', 'we also introduced a new sequence structure ( sqgrw ) which plays a role in achieving the best accuracy for both, social event detection and social event classification tasks']","['this paper, we have introduced the novel tasks of social event detection and classification.', 'we show that data sampling techniques play a crucial role for the task of relation detection.', 'through oversampling we achieve an increase in f1 - measure of 22. 2 % absolute over a baseline system.', 'our experiments show that as a result of how language expresses the relevant information, dependency - based structures are best suited for encoding this information.', 'furthermore, because of the complexity of the task, a combination of phrase based structures and dependency - based structures perform the best.', 'this revalidates the observation of #AUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task.', 'we also introduced a new sequence structure ( sqgrw ) which plays a role in achieving the best accuracy for both, social event detection and social event classification tasks']","['this paper, we have introduced the novel tasks of social event detection and classification.', 'we show that data sampling techniques play a crucial role for the task of relation detection.', 'through oversampling we achieve an increase in f1 - measure of 22. 2 % absolute over a baseline system.', 'our experiments show that as a result of how language expresses the relevant information, dependency - based structures are best suited for encoding this information.', 'furthermore, because of the complexity of the task, a combination of phrase based structures and dependency - based structures perform the best.', 'this revalidates the observation of #AUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task.', 'we also introduced a new sequence structure ( sqgrw ) which plays a role in achieving the best accuracy for both, social event detection and social event classification tasks']","['this paper, we have introduced the novel tasks of social event detection and classification.', 'we show that data sampling techniques play a crucial role for the task of relation detection.', 'through oversampling we achieve an increase in f1 - measure of 22. 2 % absolute over a baseline system.', 'our experiments show that as a result of how language expresses the relevant information, dependency - based structures are best suited for encoding this information.', 'furthermore, because of the complexity of the task, a combination of phrase based structures and dependency - based structures perform the best.', 'this revalidates the observation of #AUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task.', 'we also introduced a new sequence structure ( sqgrw ) which plays a role in achieving the best accuracy for both, social event detection and social event classification tasks']",1
"[', i. e. randomly removing examples belonging to the negative class until its size matches the positive class.', 'table 2 shows a large gain in f1 - measure of 9. 72 % absolute over the baseline system ( table 1 ).', 'we found that worst performing kernel with under - sampling is sk1 with an f1 - measure of 39. 2 % which is better than the best performance without undersampling.', 'these results make it clear that doing under - sampling greatly improves the performance of the classifier, despite the fact that we are using less training data ( fewer negative examples ).', 'this is as expected because we are evaluating on f1 - measure and the classifier is optimizing for accuracy.', 'absolute.', 'as in the baseline system, a combination of structures performs best.', 'as in the undersampled system, when the data is balanced, sqgrw ( sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes ) achieves the best recall.', 'here, the pet and gr kernel perform similar : this is different from the results of ( #AUTHOR_TAG ) where gr performed much worse']","['the discussion.', 'we now turn to experiments involving sampling.', 'table 2 presents results for under - sampling, i. e. randomly removing examples belonging to the negative class until its size matches the positive class.', 'table 2 shows a large gain in f1 - measure of 9. 72 % absolute over the baseline system ( table 1 ).', 'we found that worst performing kernel with under - sampling is sk1 with an f1 - measure of 39. 2 % which is better than the best performance without undersampling.', 'these results make it clear that doing under - sampling greatly improves the performance of the classifier, despite the fact that we are using less training data ( fewer negative examples ).', 'this is as expected because we are evaluating on f1 - measure and the classifier is optimizing for accuracy.', 'absolute.', 'as in the baseline system, a combination of structures performs best.', 'as in the undersampled system, when the data is balanced, sqgrw ( sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes ) achieves the best recall.', 'here, the pet and gr kernel perform similar : this is different from the results of ( #AUTHOR_TAG ) where gr performed much worse']","[', i. e. randomly removing examples belonging to the negative class until its size matches the positive class.', 'table 2 shows a large gain in f1 - measure of 9. 72 % absolute over the baseline system ( table 1 ).', 'we found that worst performing kernel with under - sampling is sk1 with an f1 - measure of 39. 2 % which is better than the best performance without undersampling.', 'these results make it clear that doing under - sampling greatly improves the performance of the classifier, despite the fact that we are using less training data ( fewer negative examples ).', 'this is as expected because we are evaluating on f1 - measure and the classifier is optimizing for accuracy.', 'absolute.', 'as in the baseline system, a combination of structures performs best.', 'as in the undersampled system, when the data is balanced, sqgrw ( sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes ) achieves the best recall.', 'here, the pet and gr kernel perform similar : this is different from the results of ( #AUTHOR_TAG ) where gr performed much worse']","['event detection is the task of detecting if any social event exists between a pair of entities in a sentence.', 'we formulate the problem as a binary classification task by labeling an example that does not have a social event as class - 1 and by labeling an example that either has an inr or cog social event as class 1.', 'first we present results for our baseline system.', 'our baseline system uses various structures and their combinations but without any data balancing.', '1 presents results for our baseline system.', 'grammatical relation tree structure ( gr ), a structure derived from dependency tree by replacing the words by their grammatical relations achieves the best precision.', 'this is probably because the clas - sifier learns that if both the arguments of a predicate contain target entities then it is a social event.', 'among kernels for single structures, the path enclosed tree for psts ( pet ) achieves the best recall.', 'furthermore, a combination of structures derived from psts and dts performs best.', 'the sequence kernels, perform much worse than sqgrw ( f1 - measure as low as 0. 45 ).', 'since it is the same case for all subsequent experiments, we omit them from the discussion.', 'we now turn to experiments involving sampling.', 'table 2 presents results for under - sampling, i. e. randomly removing examples belonging to the negative class until its size matches the positive class.', 'table 2 shows a large gain in f1 - measure of 9. 72 % absolute over the baseline system ( table 1 ).', 'we found that worst performing kernel with under - sampling is sk1 with an f1 - measure of 39. 2 % which is better than the best performance without undersampling.', 'these results make it clear that doing under - sampling greatly improves the performance of the classifier, despite the fact that we are using less training data ( fewer negative examples ).', 'this is as expected because we are evaluating on f1 - measure and the classifier is optimizing for accuracy.', 'absolute.', 'as in the baseline system, a combination of structures performs best.', 'as in the undersampled system, when the data is balanced, sqgrw ( sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes ) achieves the best recall.', 'here, the pet and gr kernel perform similar : this is different from the results of ( #AUTHOR_TAG ) where gr performed much worse than pet for ace data.', 'this exemplifies the difference in the nature of our event annotations from that of ace relations.', 'since the average distance between target entities in the surface word order is higher for our events, the phrase structure trees are bigger']",1
"['results also confirm the insights gained by #AUTHOR_TAG,']","['results also confirm the insights gained by #AUTHOR_TAG,']","['results also confirm the insights gained by #AUTHOR_TAG,']","['results also confirm the insights gained by #AUTHOR_TAG, who observed that in crossdomain polarity analysis adding more training data is not always beneficial.', 'apparently even the smallest training dataset ( cameras ) contain enough feature instances to learn a model which performs well on the testing data']",1
"['designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #AUTHOR_TAG ), perform in comparison to our approach.', 'since three of the features we employed in our crf - based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'we observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.', 'although our data is user -']","['designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #AUTHOR_TAG ), perform in comparison to our approach.', 'since three of the features we employed in our crf - based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'we observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.', 'although our data is user - generated from web 2. 0 communities, a manual']","[', given that the opinion target vocabularies are substantially different.', 'for future work, we might investigate how machine learning algorithms, which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #AUTHOR_TAG ), perform in comparison to our approach.', 'since three of the features we employed in our crf - based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'we observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.', 'although our data is user -']","['this paper, we have shown how a crf - based approach for opinion target extraction performs in a single - and cross - domain setting.', 'we have presented a comparative evaluation of our approach on datasets from four different domains.', 'in the single - domain setting, our crf - based approach outperforms a supervised baseline on all four datasets.', 'our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction.', 'our crf - based approach also yields promising results in the crossdomain setting.', 'the features we employ scale well across domains, given that the opinion target vocabularies are substantially different.', 'for future work, we might investigate how machine learning algorithms, which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #AUTHOR_TAG ), perform in comparison to our approach.', 'since three of the features we employed in our crf - based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'we observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.', 'although our data is user - generated from web 2. 0 communities, a manual inspection has shown that the documents were of relatively high textual quality.', 'it is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user - generated discourse']",3
"['designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ), perform in comparison to our approach.', 'since three of the features we employed in our crf - based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'we observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.', 'although our data is user -']","['designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ), perform in comparison to our approach.', 'since three of the features we employed in our crf - based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'we observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.', 'although our data is user - generated from web 2. 0 communities, a manual']","[', given that the opinion target vocabularies are substantially different.', 'for future work, we might investigate how machine learning algorithms, which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ), perform in comparison to our approach.', 'since three of the features we employed in our crf - based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'we observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.', 'although our data is user -']","['this paper, we have shown how a crf - based approach for opinion target extraction performs in a single - and cross - domain setting.', 'we have presented a comparative evaluation of our approach on datasets from four different domains.', 'in the single - domain setting, our crf - based approach outperforms a supervised baseline on all four datasets.', 'our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction.', 'our crf - based approach also yields promising results in the crossdomain setting.', 'the features we employ scale well across domains, given that the opinion target vocabularies are substantially different.', 'for future work, we might investigate how machine learning algorithms, which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ), perform in comparison to our approach.', 'since three of the features we employed in our crf - based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'we observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.', 'although our data is user - generated from web 2. 0 communities, a manual inspection has shown that the documents were of relatively high textual quality.', 'it is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user - generated discourse']",3
"['implementation of the transition - based dependency parsing frame - work ( #AUTHOR_TAG ) using an arc - eager transi - tion strategy and are trained using the percep - tron algorithm as in Zhang and Clark (2008) with a beam size of 8. beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - of - speech tags of the first four words on the buffer and of the top two words on the stack ; the word identities of the first two words on the buffer and of the top word on the stack ; the word iden - tity of the syntactic head of the top word on the stack ( if available ) ; dependency arc label iden - tities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer ( if available ).', 'all feature conjunc - tions are included']","['implementation of the transition - based dependency parsing frame - work ( #AUTHOR_TAG ) using an arc - eager transi - tion strategy and are trained using the percep - tron algorithm as in Zhang and Clark (2008) with a beam size of 8. beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - of - speech tags of the first four words on the buffer and of the top two words on the stack ; the word identities of the first two words on the buffer and of the top word on the stack ; the word iden - tity of the syntactic head of the top word on the stack ( if available ) ; dependency arc label iden - tities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer ( if available ).', 'all feature conjunc - tions are included']","['implementation of the transition - based dependency parsing frame - work ( #AUTHOR_TAG ) using an arc - eager transi - tion strategy and are trained using the percep - tron algorithm as in Zhang and Clark (2008) with a beam size of 8. beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - of - speech tags of the first four words on the buffer and of the top two words on the stack ; the word identities of the first two words on the buffer and of the top word on the stack ; the word iden - tity of the syntactic head of the top word on the stack ( if available ) ; dependency arc label iden - tities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer ( if available ).', 'all feature conjunc - tions are included']","['implementation of the transition - based dependency parsing frame - work ( #AUTHOR_TAG ) using an arc - eager transi - tion strategy and are trained using the percep - tron algorithm as in Zhang and Clark (2008) with a beam size of 8. beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - of - speech tags of the first four words on the buffer and of the top two words on the stack ; the word identities of the first two words on the buffer and of the top word on the stack ; the word iden - tity of the syntactic head of the top word on the stack ( if available ) ; dependency arc label iden - tities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer ( if available ).', 'all feature conjunc - tions are included']",5
"['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( mann and mc Callum , 2010 ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; #AUTHOR_TAG ).', 'the work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( mann and mc Callum , 2010 ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; #AUTHOR_TAG ).', 'the work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( mann and mc Callum , 2010 ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; #AUTHOR_TAG ).', 'the work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( mann and mc Callum , 2010 ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; #AUTHOR_TAG ).', 'the work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in section 5.', 'in these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', 'for our setting this would mean using weak application specific signals to improve dependency parsing.', 'though we explore such ideas in our experiments, in particular for semi - supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training']",0
"['graph - based : an implementation of graphbased parsing algorithms with an arc - factored parameterization ( mc Donald et al., 2005).', 'we use the non - projective k - best mst algorithm to generate k - best lists ( #AUTHOR_TAG ), where k = 8 for the experiments in this paper.', 'the graphbased parser features used in the experiments in this paper are defined']","['graph - based : an implementation of graphbased parsing algorithms with an arc - factored parameterization ( mc Donald et al., 2005).', 'we use the non - projective k - best mst algorithm to generate k - best lists ( #AUTHOR_TAG ), where k = 8 for the experiments in this paper.', 'the graphbased parser features used in the experiments in this paper are defined']","['graph - based : an implementation of graphbased parsing algorithms with an arc - factored parameterization ( mc Donald et al., 2005).', 'we use the non - projective k - best mst algorithm to generate k - best lists ( #AUTHOR_TAG ), where k = 8 for the experiments in this paper.', 'the graphbased parser features used in the experiments in this paper are defined']","['graph - based : an implementation of graphbased parsing algorithms with an arc - factored parameterization ( mc Donald et al., 2005).', 'we use the non - projective k - best mst algorithm to generate k - best lists ( #AUTHOR_TAG ), where k = 8 for the experiments in this paper.', 'the graphbased parser features used in the experiments in this paper are defined over a word, w i at position i ; the head of this word w ρ ( i ) where ρ ( i ) provides the index of the head word ; and partof - speech tags of these words t i.', 'we use the following set of features similar to mc Donald et al. ( 2005)']",5
"['application of the augmented - loss framework is to improve parser domain portability in the presence of partially labeled data.', 'consider, for example, the case of questions.', '#AUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the penntreebank.', 'table 2 we consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer.', 'the question should be posed so that the resulting answer produces a partially labeled dependency tree.', 'root - f1 scores from table 2 suggest that one simple question is "" what is the main verb of this sentence? ""', 'for sentences that are questions.', 'in most cases this task is straight - forward and will result in a single dependency, that from the root to the main verb of the sentence.', 'we feel this is a realistic partial labeled training setting where it would be possible to quickly collect a significant amount of data']","['application of the augmented - loss framework is to improve parser domain portability in the presence of partially labeled data.', 'consider, for example, the case of questions.', '#AUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the penntreebank.', 'table 2 we consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer.', 'the question should be posed so that the resulting answer produces a partially labeled dependency tree.', 'root - f1 scores from table 2 suggest that one simple question is "" what is the main verb of this sentence? ""', 'for sentences that are questions.', 'in most cases this task is straight - forward and will result in a single dependency, that from the root to the main verb of the sentence.', 'we feel this is a realistic partial labeled training setting where it would be possible to quickly collect a significant amount of data']","['application of the augmented - loss framework is to improve parser domain portability in the presence of partially labeled data.', 'consider, for example, the case of questions.', '#AUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the penntreebank.', 'table 2 we consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer.', 'the question should be posed so that the resulting answer produces a partially labeled dependency tree.', 'root - f1 scores from table 2 suggest that one simple question is "" what is the main verb of this sentence? ""', 'for sentences that are questions.', 'in most cases this task is straight - forward and will result in a single dependency, that from the root to the main verb of the sentence.', 'we feel this is a realistic partial labeled training setting where it would be possible to quickly collect a significant amount of data']","['application of the augmented - loss framework is to improve parser domain portability in the presence of partially labeled data.', 'consider, for example, the case of questions.', '#AUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the penntreebank.', 'table 2 we consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer.', 'the question should be posed so that the resulting answer produces a partially labeled dependency tree.', 'root - f1 scores from table 2 suggest that one simple question is "" what is the main verb of this sentence? ""', 'for sentences that are questions.', 'in most cases this task is straight - forward and will result in a single dependency, that from the root to the main verb of the sentence.', 'we feel this is a realistic partial labeled training setting where it would be possible to quickly collect a significant amount of data']",1
['rules are similar to those from #AUTHOR_TAG'],['##our rules are similar to those from #AUTHOR_TAG'],['##our rules are similar to those from #AUTHOR_TAG'],['##our rules are similar to those from #AUTHOR_TAG'],1
"['the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( Wang et al. , 2007 ), sentiment analysis ( #AUTHOR_TAG ), mt reordering ( Xu et al. , 2009 ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help']","['the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( Wang et al. , 2007 ), sentiment analysis ( #AUTHOR_TAG ), mt reordering ( Xu et al. , 2009 ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( Wang et al. , 2007 ), sentiment analysis ( #AUTHOR_TAG ), mt reordering ( Xu et al. , 2009 ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( Wang et al. , 2007 ), sentiment analysis ( #AUTHOR_TAG ), mt reordering ( Xu et al. , 2009 ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']",0
"['implementation of graph - based parsing algorithms with an arc - factored parameterization ( #AUTHOR_TAG ).', 'we use the non - projective k - best mst algorithm to generate k - best lists (Hall, 2007), where k = 8 for the experiments in this paper.', 'the graph - based parser features used in the experiments in this paper are defined']","['implementation of graph - based parsing algorithms with an arc - factored parameterization ( #AUTHOR_TAG ).', 'we use the non - projective k - best mst algorithm to generate k - best lists (Hall, 2007), where k = 8 for the experiments in this paper.', 'the graph - based parser features used in the experiments in this paper are defined']","['implementation of graph - based parsing algorithms with an arc - factored parameterization ( #AUTHOR_TAG ).', 'we use the non - projective k - best mst algorithm to generate k - best lists (Hall, 2007), where k = 8 for the experiments in this paper.', 'the graph - based parser features used in the experiments in this paper are defined']","['implementation of graph - based parsing algorithms with an arc - factored parameterization ( #AUTHOR_TAG ).', 'we use the non - projective k - best mst algorithm to generate k - best lists (Hall, 2007), where k = 8 for the experiments in this paper.', 'the graph - based parser features used in the experiments in this paper are defined over a word, wi at po - sition i ; the head of this word w _ ( i ) where _ ( i ) provides the index of the head word ; and part - of - speech tags of these words ti.', 'we use the following set of features similar to mc Donald et al. (2005)']",5
"['it is similar in vein to self - training ( #AUTHOR_TAG ), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', 'Liang et al. (2006) presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in']","['it is similar in vein to self - training ( #AUTHOR_TAG ), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', 'Liang et al. (2006) presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in']","['it is similar in vein to self - training ( #AUTHOR_TAG ), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', 'Liang et al. (2006) presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in']","['recent study by also investigates the task of training parsers to improve mt reordering.', 'in that work, a parser is used to first parse a set of manually reordered sentences to produce k - best lists.', 'the parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'the method is called targeted self - training as it is similar in vein to self - training ( #AUTHOR_TAG ), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', 'Liang et al. (2006) presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in their work, they are interested in learning a parameterization over translation phrases ( including the underlying wordalignment ) which optimizes the bleu score.', 'their goal is considerably different ; they want to incorporate additional features into their model and define an objective function which allows them to do so ; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic ( parsing ) objectives']",1
"['our experiments we work with a set of english - japanese reordering rules 1 and gold reorderings based on human generated correct reordering of an aligned target sentences.', 'we use a reordering score based on the reordering penalty from the meteor scoring metric.', 'though we could have used a further downstream measure like bleu, meteor has also been shown to directly correlate with translation quality ( #AUTHOR_TAG ) and is simpler to measure.', 'results for three augmented - loss schedules are shown : 0. 5 where for every two treebank updates we make one augmented - loss update, 1 is a 1 - to - 1 mix, and 2 is where we make twice as many augmented - loss updates as treebank updates']","['our experiments we work with a set of english - japanese reordering rules 1 and gold reorderings based on human generated correct reordering of an aligned target sentences.', 'we use a reordering score based on the reordering penalty from the meteor scoring metric.', 'though we could have used a further downstream measure like bleu, meteor has also been shown to directly correlate with translation quality ( #AUTHOR_TAG ) and is simpler to measure.', 'results for three augmented - loss schedules are shown : 0. 5 where for every two treebank updates we make one augmented - loss update, 1 is a 1 - to - 1 mix, and 2 is where we make twice as many augmented - loss updates as treebank updates']","['our experiments we work with a set of english - japanese reordering rules 1 and gold reorderings based on human generated correct reordering of an aligned target sentences.', 'we use a reordering score based on the reordering penalty from the meteor scoring metric.', 'though we could have used a further downstream measure like bleu, meteor has also been shown to directly correlate with translation quality ( #AUTHOR_TAG ) and is simpler to measure.', 'results for three augmented - loss schedules are shown : 0. 5 where for every two treebank updates we make one augmented - loss update, 1 is a 1 - to - 1 mix, and 2 is where we make twice as many augmented - loss updates as treebank updates']","['our experiments we work with a set of english - japanese reordering rules 1 and gold reorderings based on human generated correct reordering of an aligned target sentences.', 'we use a reordering score based on the reordering penalty from the meteor scoring metric.', 'though we could have used a further downstream measure like bleu, meteor has also been shown to directly correlate with translation quality ( #AUTHOR_TAG ) and is simpler to measure.', 'results for three augmented - loss schedules are shown : 0. 5 where for every two treebank updates we make one augmented - loss update, 1 is a 1 - to - 1 mix, and 2 is where we make twice as many augmented - loss updates as treebank updates']",4
"['the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( Wang et al. , 2007 ), sentiment analysis ( Nakagawa et al. , 2010 ), mt reordering ( #AUTHOR_TAG ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help']","['the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( Wang et al. , 2007 ), sentiment analysis ( Nakagawa et al. , 2010 ), mt reordering ( #AUTHOR_TAG ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( Wang et al. , 2007 ), sentiment analysis ( Nakagawa et al. , 2010 ), mt reordering ( #AUTHOR_TAG ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( Wang et al. , 2007 ), sentiment analysis ( Nakagawa et al. , 2010 ), mt reordering ( #AUTHOR_TAG ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']",0
"['transition - based : an implementation of the transition - based dependency parsing framework ( Nivre , 2008 ) using an arc - eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG with a beam size of 8. beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - ofspeech tags of the first four words on the buffer and of the top two words on the stack ; the word']","['transition - based : an implementation of the transition - based dependency parsing framework ( Nivre , 2008 ) using an arc - eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG with a beam size of 8. beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - ofspeech tags of the first four words on the buffer and of the top two words on the stack ; the word']","['transition - based : an implementation of the transition - based dependency parsing framework ( Nivre , 2008 ) using an arc - eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG with a beam size of 8. beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - ofspeech tags of the first four words on the buffer and of the top two words on the stack ; the word identities of the first two words on the buffer and of the top word on the stack ; the word identity of the syntactic head of the top word on the stack ( if available ) ; dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer ( if available ).', 'all feature conjunctions are included']","['transition - based : an implementation of the transition - based dependency parsing framework ( Nivre , 2008 ) using an arc - eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG with a beam size of 8. beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - ofspeech tags of the first four words on the buffer and of the top two words on the stack ; the word identities of the first two words on the buffer and of the top word on the stack ; the word identity of the syntactic head of the top word on the stack ( if available ) ; dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer ( if available ).', 'all feature conjunctions are included']",5
"['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #AUTHOR_TAG ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #AUTHOR_TAG ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #AUTHOR_TAG ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #AUTHOR_TAG ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']",0
"['terms of treebank data, the primary training corpus is the penn wall street journal treebank ( ptb ) ( #AUTHOR_TAG ).', 'we also make use of the brown corpus, and the question treebank ( qtb )']","['terms of treebank data, the primary training corpus is the penn wall street journal treebank ( ptb ) ( #AUTHOR_TAG ).', 'we also make use of the brown corpus, and the question treebank ( qtb )']","['terms of treebank data, the primary training corpus is the penn wall street journal treebank ( ptb ) ( #AUTHOR_TAG ).', 'we also make use of the brown corpus, and the question treebank ( qtb )']","['terms of treebank data, the primary training corpus is the penn wall street journal treebank ( ptb ) ( #AUTHOR_TAG ).', 'we also make use of the brown corpus, and the question treebank ( qtb )']",5
"['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; #AUTHOR_TAG ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; #AUTHOR_TAG ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; #AUTHOR_TAG ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; #AUTHOR_TAG ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']",0
"['work that is most similar to ours is that of #AUTHOR_TAG, who introduced the constraint driven learning algorithm ( codl ).', 'their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled']","['work that is most similar to ours is that of #AUTHOR_TAG, who introduced the constraint driven learning algorithm ( codl ).', 'their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled']","['work that is most similar to ours is that of #AUTHOR_TAG, who introduced the constraint driven learning algorithm ( codl ).', 'their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled data ( what we call extrinsic datasets ).', 'for each unlabeled example, they use the current model along with their set of constraints to select a set of k automatically labeled examples which best meet the constraints.', 'these induced examples are then added to their training set and, after processing each unlabeled dataset, they perform full model optimization with the concatenation of training data and newly generated training items.', 'the augmented - loss algorithm can be viewed as an online version of this algorithm which performs model updates based on the augmented - loss functions directly ( rather than adding a set of examples to the training set ).', 'unlike the codl approach, we do not perform complete optimization on each iteration over the unlabeled dataset ; rather, we incorporate the updates in our online learning algorithm.', 'as mentioned earlier, codl is one example of learning algorithms that use weak supervision, others include mann and mc - Callum (2010) and Ganchev et al. (2010).', 'again, these works are typically interested in using the extrinsic metric - or, in general, extrinsic information - to optimize the intrinsic metric in the absence of']","['work that is most similar to ours is that of #AUTHOR_TAG, who introduced the constraint driven learning algorithm ( codl ).', 'their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled data ( what we call extrinsic datasets ).', 'for each unlabeled example, they use the current model along with their set of constraints to select a set of k automatically labeled examples which best meet the constraints.', 'these induced examples are then added to their training set and, after processing each unlabeled dataset, they perform full model optimization with the concatenation of training data and newly generated training items.', 'the augmented - loss algorithm can be viewed as an online version of this algorithm which performs model updates based on the augmented - loss functions directly ( rather than adding a set of examples to the training set ).', 'unlike the codl approach, we do not perform complete optimization on each iteration over the unlabeled dataset ; rather, we incorporate the updates in our online learning algorithm.', 'as mentioned earlier, codl is one example of learning algorithms that use weak supervision, others include mann and mc - Callum (2010) and Ganchev et al. (2010).', 'again, these works are typically interested in using the extrinsic metric - or, in general, extrinsic information - to optimize the intrinsic metric in the absence of any labeled intrinsic data.', 'our goal is to optimize both simultaneously']",1
"['recent study by also investigates the task of training parsers to improve mt reordering.', 'in that work, a parser is used to first parse a set of manually reordered sentences to produce k - best lists.', 'the parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'the method is called targeted self - training as it is similar in vein to self - training ( mc Closky et al., 2006), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', '#AUTHOR_TAG presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in']","['recent study by also investigates the task of training parsers to improve mt reordering.', 'in that work, a parser is used to first parse a set of manually reordered sentences to produce k - best lists.', 'the parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'the method is called targeted self - training as it is similar in vein to self - training ( mc Closky et al., 2006), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', '#AUTHOR_TAG presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in']","['recent study by also investigates the task of training parsers to improve mt reordering.', 'in that work, a parser is used to first parse a set of manually reordered sentences to produce k - best lists.', 'the parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'the method is called targeted self - training as it is similar in vein to self - training ( mc Closky et al., 2006), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', '#AUTHOR_TAG presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in']","['recent study by also investigates the task of training parsers to improve mt reordering.', 'in that work, a parser is used to first parse a set of manually reordered sentences to produce k - best lists.', 'the parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'the method is called targeted self - training as it is similar in vein to self - training ( mc Closky et al., 2006), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', '#AUTHOR_TAG presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in their work, they are interested in learning a parameterization over translation phrases ( including the underlying wordalignment ) which optimizes the bleu score.', 'their goal is considerably different ; they want to incorporate additional features into their model and define an objective function which allows them to do so ; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic ( parsing ) objectives']",1
"['recent study by #AUTHOR_TAG also investigates the task of training parsers to improve mt reordering.', 'in that work, a parser is used to first parse a set of manually reordered sentences to produce k - best lists.', 'the parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'the method is called targeted self - training as it is similar in vein to self - training ( mc Closky et al., 2006), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', 'Liang et al. (2006) presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in']","['recent study by #AUTHOR_TAG also investigates the task of training parsers to improve mt reordering.', 'in that work, a parser is used to first parse a set of manually reordered sentences to produce k - best lists.', 'the parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'the method is called targeted self - training as it is similar in vein to self - training ( mc Closky et al., 2006), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', 'Liang et al. (2006) presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in']","['recent study by #AUTHOR_TAG also investigates the task of training parsers to improve mt reordering.', 'in that work, a parser is used to first parse a set of manually reordered sentences to produce k - best lists.', 'the parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'the method is called targeted self - training as it is similar in vein to self - training ( mc Closky et al., 2006), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', 'Liang et al. (2006) presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in']","['recent study by #AUTHOR_TAG also investigates the task of training parsers to improve mt reordering.', 'in that work, a parser is used to first parse a set of manually reordered sentences to produce k - best lists.', 'the parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'the method is called targeted self - training as it is similar in vein to self - training ( mc Closky et al., 2006), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', 'Liang et al. (2006) presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in their work, they are interested in learning a parameterization over translation phrases ( including the underlying wordalignment ) which optimizes the bleu score.', 'their goal is considerably different ; they want to incorporate additional features into their model and define an objective function which allows them to do so ; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic ( parsing ) objectives']",1
"['like information extraction ( #AUTHOR_TAG ) and textual entailment ( Berant et al. , 2010 )']","['like information extraction ( #AUTHOR_TAG ) and textual entailment ( Berant et al. , 2010 )']","['like information extraction ( #AUTHOR_TAG ) and textual entailment ( Berant et al. , 2010 )']","['augmented - loss framework can be used to incorporate multiple treebank - based loss functions as well.', 'labeled attachment score is used as our base model loss function.', 'in this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average ( labeled ) arc - length score : _ _ _ ( _, _ ) ( i _ _ ) for each word of the sentence we compute the dis - tance between the words position i and the posi - tion of the words head _ i.', 'the arc - length score is the summed length of all those with correct head assignments ( _ ( _ i, _ i ) is 1 if the predicted head and the correct head match, 0 otherwise ).', 'the score is normalized by the summed arc lengths for the sentence.', 'the labeled version of this score requires that the labels of the arc are also correct.', 'optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( mc Donald and Nivre, 2007) and these dependencies are typically the most meaningful for down - stream tasks, e. g., main verb dependencies for tasks like information extraction ( #AUTHOR_TAG ) and textual entailment ( Berant et al. , 2010 )']",0
"['to the standard perceptron proof, e. g., #AUTHOR_TAG, by inserting in loss - separ']","['to the standard perceptron proof, e. g., #AUTHOR_TAG, by inserting in loss - separability']","['to the standard perceptron proof, e. g., #AUTHOR_TAG, by inserting in loss - separ']","['', 'identical to the standard perceptron proof, e. g., #AUTHOR_TAG, by inserting in loss - separability for normal separability']",0
"['augmented - loss framework can be used to incorporate multiple treebank - based loss functions as well.', 'labeled attachment score is used as our base model loss function.', 'in this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average ( labeled ) arc - length score : _ _ _ ( _, _ ) ( i _ _ ) for each word of the sentence we compute the dis - tance between the words position i and the posi - tion of the words head _ i.', 'the arc - length score is the summed length of all those with correct head assignments ( _ ( _ i, _ i ) is 1 if the predicted head and the correct head match, 0 otherwise ).', 'the score is normalized by the summed arc lengths for the sentence.', 'the labeled version of this score requires that the labels of the arc are also correct.', 'optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #AUTHOR_TAG ) and these dependencies are typically the most meaningful for downstream tasks, e. g., main verb dependencies for']","['augmented - loss framework can be used to incorporate multiple treebank - based loss functions as well.', 'labeled attachment score is used as our base model loss function.', 'in this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average ( labeled ) arc - length score : _ _ _ ( _, _ ) ( i _ _ ) for each word of the sentence we compute the dis - tance between the words position i and the posi - tion of the words head _ i.', 'the arc - length score is the summed length of all those with correct head assignments ( _ ( _ i, _ i ) is 1 if the predicted head and the correct head match, 0 otherwise ).', 'the score is normalized by the summed arc lengths for the sentence.', 'the labeled version of this score requires that the labels of the arc are also correct.', 'optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #AUTHOR_TAG ) and these dependencies are typically the most meaningful for downstream tasks, e. g., main verb dependencies for']","['augmented - loss framework can be used to incorporate multiple treebank - based loss functions as well.', 'labeled attachment score is used as our base model loss function.', 'in this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average ( labeled ) arc - length score : _ _ _ ( _, _ ) ( i _ _ ) for each word of the sentence we compute the dis - tance between the words position i and the posi - tion of the words head _ i.', 'the arc - length score is the summed length of all those with correct head assignments ( _ ( _ i, _ i ) is 1 if the predicted head and the correct head match, 0 otherwise ).', 'the score is normalized by the summed arc lengths for the sentence.', 'the labeled version of this score requires that the labels of the arc are also correct.', 'optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #AUTHOR_TAG ) and these dependencies are typically the most meaningful for downstream tasks, e. g., main verb dependencies for']","['augmented - loss framework can be used to incorporate multiple treebank - based loss functions as well.', 'labeled attachment score is used as our base model loss function.', 'in this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average ( labeled ) arc - length score : _ _ _ ( _, _ ) ( i _ _ ) for each word of the sentence we compute the dis - tance between the words position i and the posi - tion of the words head _ i.', 'the arc - length score is the summed length of all those with correct head assignments ( _ ( _ i, _ i ) is 1 if the predicted head and the correct head match, 0 otherwise ).', 'the score is normalized by the summed arc lengths for the sentence.', 'the labeled version of this score requires that the labels of the arc are also correct.', 'optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #AUTHOR_TAG ) and these dependencies are typically the most meaningful for downstream tasks, e. g., main verb dependencies for tasks like information extraction (Yates and Etzioni, 2009) and textual entailment (Berant et al., 2010)']",4
"['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']",0
"['obvious approach to this problem is to employ parser reranking ( #AUTHOR_TAG ).', 'in such a setting, an auxiliary reranker is added in a pipeline following the parser.', 'the standard setting involves training the base parser and applying it to a development set ( this is often done in a cross - validated jack - knife training framework ).', 'the reranker can then be trained to optimize for the downstream or extrinsic objective.', 'while this will bias the reranker towards the target task, it is limited by the oracle performance of the original base parser']","['obvious approach to this problem is to employ parser reranking ( #AUTHOR_TAG ).', 'in such a setting, an auxiliary reranker is added in a pipeline following the parser.', 'the standard setting involves training the base parser and applying it to a development set ( this is often done in a cross - validated jack - knife training framework ).', 'the reranker can then be trained to optimize for the downstream or extrinsic objective.', 'while this will bias the reranker towards the target task, it is limited by the oracle performance of the original base parser']","['obvious approach to this problem is to employ parser reranking ( #AUTHOR_TAG ).', 'in such a setting, an auxiliary reranker is added in a pipeline following the parser.', 'the standard setting involves training the base parser and applying it to a development set ( this is often done in a cross - validated jack - knife training framework ).', 'the reranker can then be trained to optimize for the downstream or extrinsic objective.', 'while this will bias the reranker towards the target task, it is limited by the oracle performance of the original base parser']","['obvious approach to this problem is to employ parser reranking ( #AUTHOR_TAG ).', 'in such a setting, an auxiliary reranker is added in a pipeline following the parser.', 'the standard setting involves training the base parser and applying it to a development set ( this is often done in a cross - validated jack - knife training framework ).', 'the reranker can then be trained to optimize for the downstream or extrinsic objective.', 'while this will bias the reranker towards the target task, it is limited by the oracle performance of the original base parser']",0
"['augmented - loss framework can be used to incorporate multiple treebank - based loss functions as well.', 'labeled attachment score is used as our base model loss function.', 'in this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average ( labeled ) arc - length score : _ _ _ ( _, _ ) ( i _ _ ) for each word of the sentence we compute the dis - tance between the words position i and the posi - tion of the words head _ i.', 'the arc - length score is the summed length of all those with correct head assignments ( _ ( _ i, _ i ) is 1 if the predicted head and the correct head match, 0 otherwise ).', 'the score is normalized by the summed arc lengths for the sentence.', 'the labeled version of this score requires that the labels of the arc are also correct.', 'optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( mc Donald and Nivre, 2007) and these dependencies are typically the most meaningful for downstream tasks, e. g., main verb dependencies for tasks like information extraction ( Yates and Etzioni , 2009 ) and textual entailment ( #AUTHOR_TAG )']","['augmented - loss framework can be used to incorporate multiple treebank - based loss functions as well.', 'labeled attachment score is used as our base model loss function.', 'in this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average ( labeled ) arc - length score : _ _ _ ( _, _ ) ( i _ _ ) for each word of the sentence we compute the dis - tance between the words position i and the posi - tion of the words head _ i.', 'the arc - length score is the summed length of all those with correct head assignments ( _ ( _ i, _ i ) is 1 if the predicted head and the correct head match, 0 otherwise ).', 'the score is normalized by the summed arc lengths for the sentence.', 'the labeled version of this score requires that the labels of the arc are also correct.', 'optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( mc Donald and Nivre, 2007) and these dependencies are typically the most meaningful for downstream tasks, e. g., main verb dependencies for tasks like information extraction ( Yates and Etzioni , 2009 ) and textual entailment ( #AUTHOR_TAG )']","['augmented - loss framework can be used to incorporate multiple treebank - based loss functions as well.', 'labeled attachment score is used as our base model loss function.', 'in this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average ( labeled ) arc - length score : _ _ _ ( _, _ ) ( i _ _ ) for each word of the sentence we compute the dis - tance between the words position i and the posi - tion of the words head _ i.', 'the arc - length score is the summed length of all those with correct head assignments ( _ ( _ i, _ i ) is 1 if the predicted head and the correct head match, 0 otherwise ).', 'the score is normalized by the summed arc lengths for the sentence.', 'the labeled version of this score requires that the labels of the arc are also correct.', 'optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( mc Donald and Nivre, 2007) and these dependencies are typically the most meaningful for downstream tasks, e. g., main verb dependencies for tasks like information extraction ( Yates and Etzioni , 2009 ) and textual entailment ( #AUTHOR_TAG )']","['augmented - loss framework can be used to incorporate multiple treebank - based loss functions as well.', 'labeled attachment score is used as our base model loss function.', 'in this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average ( labeled ) arc - length score : _ _ _ ( _, _ ) ( i _ _ ) for each word of the sentence we compute the dis - tance between the words position i and the posi - tion of the words head _ i.', 'the arc - length score is the summed length of all those with correct head assignments ( _ ( _ i, _ i ) is 1 if the predicted head and the correct head match, 0 otherwise ).', 'the score is normalized by the summed arc lengths for the sentence.', 'the labeled version of this score requires that the labels of the arc are also correct.', 'optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( mc Donald and Nivre, 2007) and these dependencies are typically the most meaningful for downstream tasks, e. g., main verb dependencies for tasks like information extraction ( Yates and Etzioni , 2009 ) and textual entailment ( #AUTHOR_TAG )']",0
"['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( mann and mc Callum , 2010 ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #AUTHOR_TAG ; Chang et al. , 2010 ).', 'the work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( mann and mc Callum , 2010 ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #AUTHOR_TAG ; Chang et al. , 2010 ).', 'the work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( mann and mc Callum , 2010 ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #AUTHOR_TAG ; Chang et al. , 2010 ).', 'the work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( mann and mc Callum , 2010 ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #AUTHOR_TAG ; Chang et al. , 2010 ).', 'the work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in section 5.', 'in these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', 'for our setting this would mean using weak application specific signals to improve dependency parsing.', 'though we explore such ideas in our experiments, in particular for semi - supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training']",1
"['implementation of the transition - based dependency parsing framework ( #AUTHOR_TAG ) using an arc - eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark ( 2008 ) with a beam size of 8.', 'beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - of - speech tags of the first four words on the buffer and of the top two words on the stack ; the word']","['implementation of the transition - based dependency parsing framework ( #AUTHOR_TAG ) using an arc - eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark ( 2008 ) with a beam size of 8.', 'beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - of - speech tags of the first four words on the buffer and of the top two words on the stack ; the word']","['implementation of the transition - based dependency parsing framework ( #AUTHOR_TAG ) using an arc - eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark ( 2008 ) with a beam size of 8.', 'beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - of - speech tags of the first four words on the buffer and of the top two words on the stack ; the word identities of the first two words on the buffer and of the top word on the stack ; the word identity of the syntactic head of the top word on the stack ( if available ) ; dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer ( if available ).', 'all feature conjunctions are included']","['implementation of the transition - based dependency parsing framework ( #AUTHOR_TAG ) using an arc - eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark ( 2008 ) with a beam size of 8.', 'beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - of - speech tags of the first four words on the buffer and of the top two words on the stack ; the word identities of the first two words on the buffer and of the top word on the stack ; the word identity of the syntactic head of the top word on the stack ( if available ) ; dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer ( if available ).', 'all feature conjunctions are included']",5
"['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( #AUTHOR_TAG ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ).', 'the work of']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( #AUTHOR_TAG ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ).', 'the work of']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( #AUTHOR_TAG ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ).', 'the work of']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( #AUTHOR_TAG ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ).', 'the work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in section 5.', 'in these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', 'for our setting this would mean using weak application specific signals to improve dependency parsing.', 'though we explore such ideas in our experiments, in particular for semi - supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training']",0
"['in a new augmented - loss for each one.', 'augmented - loss learning is then applied to target a downstream task using the loss functions to measure gains.', 'we show empirical results for two extrinsic loss - functions ( optimizing for the downstream task ) : machine translation and domain adaptation ; and for one intrinsic loss - function : an arclength parsing score.', 'for some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( uas ) and labeled attachment score ( las ) ( #AUTHOR_TAG )']","['in a new augmented - loss for each one.', 'augmented - loss learning is then applied to target a downstream task using the loss functions to measure gains.', 'we show empirical results for two extrinsic loss - functions ( optimizing for the downstream task ) : machine translation and domain adaptation ; and for one intrinsic loss - function : an arclength parsing score.', 'for some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( uas ) and labeled attachment score ( las ) ( #AUTHOR_TAG )']","['the next section, we present a set of scoring functions that can be used in the inline reranker loss framework, resulting in a new augmented - loss for each one.', 'augmented - loss learning is then applied to target a downstream task using the loss functions to measure gains.', 'we show empirical results for two extrinsic loss - functions ( optimizing for the downstream task ) : machine translation and domain adaptation ; and for one intrinsic loss - function : an arclength parsing score.', 'for some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( uas ) and labeled attachment score ( las ) ( #AUTHOR_TAG )']","['the next section, we present a set of scoring functions that can be used in the inline reranker loss framework, resulting in a new augmented - loss for each one.', 'augmented - loss learning is then applied to target a downstream task using the loss functions to measure gains.', 'we show empirical results for two extrinsic loss - functions ( optimizing for the downstream task ) : machine translation and domain adaptation ; and for one intrinsic loss - function : an arclength parsing score.', 'for some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( uas ) and labeled attachment score ( las ) ( #AUTHOR_TAG )']",5
"['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #AUTHOR_TAG ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #AUTHOR_TAG ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #AUTHOR_TAG ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #AUTHOR_TAG ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']",0
"['the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( #AUTHOR_TAG ), sentiment analysis ( Nakagawa et al. , 2010 ), mt reordering ( Xu et al. , 2009 ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help']","['the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( #AUTHOR_TAG ), sentiment analysis ( Nakagawa et al. , 2010 ), mt reordering ( Xu et al. , 2009 ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( #AUTHOR_TAG ), sentiment analysis ( Nakagawa et al. , 2010 ), mt reordering ( Xu et al. , 2009 ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( #AUTHOR_TAG ), sentiment analysis ( Nakagawa et al. , 2010 ), mt reordering ( Xu et al. , 2009 ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']",0
['and data used in our experiments are based on the work of #AUTHOR_TAG'],['and data used in our experiments are based on the work of #AUTHOR_TAG'],['and data used in our experiments are based on the work of #AUTHOR_TAG'],['and data used in our experiments are based on the work of #AUTHOR_TAG'],5
"['this paper, inspired by knn - svm ( #AUTHOR_TAG ), we propose a local training method, which trains sentence - wise weights instead of a single weight, to address the above two problems.', 'compared with global training methods, such as mert, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing.', 'this online fashion has an advantage in that it can adapt the weights for each of the test sentences, by dynamically tuning the weights on translation examples which are similar to these test sentences.', 'similar to the method of development set automatical selection, the local training method may also suffer the problem of efficiency.', 'to put it into practice, we propose incremental training methods which avoid retraining and iterative decoding on a development set']","['this paper, inspired by knn - svm ( #AUTHOR_TAG ), we propose a local training method, which trains sentence - wise weights instead of a single weight, to address the above two problems.', 'compared with global training methods, such as mert, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing.', 'this online fashion has an advantage in that it can adapt the weights for each of the test sentences, by dynamically tuning the weights on translation examples which are similar to these test sentences.', 'similar to the method of development set automatical selection, the local training method may also suffer the problem of efficiency.', 'to put it into practice, we propose incremental training methods which avoid retraining and iterative decoding on a development set']","['this paper, inspired by knn - svm ( #AUTHOR_TAG ), we propose a local training method, which trains sentence - wise weights instead of a single weight, to address the above two problems.', 'compared with global training methods, such as mert, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing.', 'this online fashion has an advantage in that it can adapt the weights for each of the test sentences, by dynamically tuning the weights on translation examples which are similar to these test sentences.', 'similar to the method of development set automatical selection, the local training method may also suffer the problem of efficiency.', 'to put it into practice, we propose incremental training methods which avoid retraining and iterative decoding on a development set']","['this paper, inspired by knn - svm ( #AUTHOR_TAG ), we propose a local training method, which trains sentence - wise weights instead of a single weight, to address the above two problems.', 'compared with global training methods, such as mert, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing.', 'this online fashion has an advantage in that it can adapt the weights for each of the test sentences, by dynamically tuning the weights on translation examples which are similar to these test sentences.', 'similar to the method of development set automatical selection, the local training method may also suffer the problem of efficiency.', 'to put it into practice, we propose incremental training methods which avoid retraining and iterative decoding on a development set']",4
"['to the existence of l2 norm in objective function ( 5 ), the optimization algorithm mert can not be applied for this question since the exact line search routine does not hold here.', 'motivated by (#AUTHOR_TAG, 2003 ; Smith and Eisner, 2006), we approximate the error in ( 5 ) by the expected loss, and then derive the following function : x 2']","['to the existence of l2 norm in objective function ( 5 ), the optimization algorithm mert can not be applied for this question since the exact line search routine does not hold here.', ""motivated by (#AUTHOR_TAG, 2003 ; Smith and Eisner, 2006), we approximate the error in ( 5 ) by the expected loss, and then derive the following function : x 2iiw−wbii2 + a j = 1 systems nist02 nist05 nist06 nist08 moses 30. 39 26. 31 25. 34 19. 07 moses hier 33. 68 26. 94 26. 28 18. 65 in - hiero 31. 24 27. 07 26. 32 19. 03 table 1 : the performance comparison of the baseline inhiero vs moses and moses hier. with exp [ αw · h ( fj, e ) ] pα ( e | fj ; w ) = ( 7 ) ee'ec ; exp [ αw · h ( fj, e') ], where α > 0 is a real number valued smoother."", 'one can see that, in the extreme case, for α — * oc, ( 6 ) converges to ( 5 )']","['to the existence of l2 norm in objective function ( 5 ), the optimization algorithm mert can not be applied for this question since the exact line search routine does not hold here.', 'motivated by (#AUTHOR_TAG, 2003 ; Smith and Eisner, 2006), we approximate the error in ( 5 ) by the expected loss, and then derive the following function : x 2']","['to the existence of l2 norm in objective function ( 5 ), the optimization algorithm mert can not be applied for this question since the exact line search routine does not hold here.', ""motivated by (#AUTHOR_TAG, 2003 ; Smith and Eisner, 2006), we approximate the error in ( 5 ) by the expected loss, and then derive the following function : x 2iiw−wbii2 + a j = 1 systems nist02 nist05 nist06 nist08 moses 30. 39 26. 31 25. 34 19. 07 moses hier 33. 68 26. 94 26. 28 18. 65 in - hiero 31. 24 27. 07 26. 32 19. 03 table 1 : the performance comparison of the baseline inhiero vs moses and moses hier. with exp [ αw · h ( fj, e ) ] pα ( e | fj ; w ) = ( 7 ) ee'ec ; exp [ αw · h ( fj, e') ], where α > 0 is a real number valued smoother."", 'one can see that, in the extreme case, for α — * oc, ( 6 ) converges to ( 5 )']",4
"['introduced the log - linear model for statistical machine translation ( smt ), in which translation is considered as the following optimization problem']","['introduced the log - linear model for statistical machine translation ( smt ), in which translation is considered as the following optimization problem']","['introduced the log - linear model for statistical machine translation ( smt ), in which translation is considered as the following optimization problem']","['introduced the log - linear model for statistical machine translation ( smt ), in which translation is considered as the following optimization problem']",0
"['method resorts to some translation examples, which is similar as example - based translation or translation memory ( #AUTHOR_TAG ; He et al. , 2010 ; Ma et al. , 2011 ).', 'instead of using translation examples to construct translation rules for enlarging the decoding space, we employed them to discriminatively learn local']","['method resorts to some translation examples, which is similar as example - based translation or translation memory ( #AUTHOR_TAG ; He et al. , 2010 ; Ma et al. , 2011 ).', 'instead of using translation examples to construct translation rules for enlarging the decoding space, we employed them to discriminatively learn local weights']","['method resorts to some translation examples, which is similar as example - based translation or translation memory ( #AUTHOR_TAG ; He et al. , 2010 ; Ma et al. , 2011 ).', 'instead of using translation examples to construct translation rules for enlarging the decoding space, we employed them to discriminatively learn local weights']","['method resorts to some translation examples, which is similar as example - based translation or translation memory ( #AUTHOR_TAG ; He et al. , 2010 ; Ma et al. , 2011 ).', 'instead of using translation examples to construct translation rules for enlarging the decoding space, we employed them to discriminatively learn local weights']",1
"['2 depicts that testing each sentence with local training method takes 2. 9 seconds, which is comparable to the testing time 2. 0 seconds with global training method 4.', 'this shows that the local method is efficient.', 'further, compared to the retrieval, the local training is not the bottleneck.', 'actually, if we use lsh technique ( #AUTHOR_TAG ) in retrieval process, the local method can be easily scaled to a larger training data']","['2 depicts that testing each sentence with local training method takes 2. 9 seconds, which is comparable to the testing time 2. 0 seconds with global training method 4.', 'this shows that the local method is efficient.', 'further, compared to the retrieval, the local training is not the bottleneck.', 'actually, if we use lsh technique ( #AUTHOR_TAG ) in retrieval process, the local method can be easily scaled to a larger training data']","['2 depicts that testing each sentence with local training method takes 2. 9 seconds, which is comparable to the testing time 2. 0 seconds with global training method 4.', 'this shows that the local method is efficient.', 'further, compared to the retrieval, the local training is not the bottleneck.', 'actually, if we use lsh technique ( #AUTHOR_TAG ) in retrieval process, the local method can be easily scaled to a larger training data']","['2 depicts that testing each sentence with local training method takes 2. 9 seconds, which is comparable to the testing time 2. 0 seconds with global training method 4.', 'this shows that the local method is efficient.', 'further, compared to the retrieval, the local training is not the bottleneck.', 'actually, if we use lsh technique ( #AUTHOR_TAG ) in retrieval process, the local method can be easily scaled to a larger training data']",3
"['##ques to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '(Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #AUTHOR_TAG ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimiza - tion objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative tech - niques to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '(Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #AUTHOR_TAG ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimiza - tion objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative tech - niques to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '(Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #AUTHOR_TAG ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimiza - tion objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative tech - niques to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '(Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #AUTHOR_TAG ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimiza - tion objectives by introducing a margin - based and ranking - based indirect loss functions']",1
"['local training method ( #AUTHOR_TAG ) is widely employed in computer vision ( Zhang et al. , 2006 ; Cheng et al. , 2010 ).', 'compared with the global training method which tries to fit a single weight on the training data, the local one learns weights based on the local neighborhood information for each test example.', 'it is superior to the global one when the data sets are not evenly distributed (Bottou and Vapnik, 1992;Zhang et al., 2006)']","['local training method ( #AUTHOR_TAG ) is widely employed in computer vision ( Zhang et al. , 2006 ; Cheng et al. , 2010 ).', 'compared with the global training method which tries to fit a single weight on the training data, the local one learns weights based on the local neighborhood information for each test example.', 'it is superior to the global one when the data sets are not evenly distributed (Bottou and Vapnik, 1992;Zhang et al., 2006)']","['local training method ( #AUTHOR_TAG ) is widely employed in computer vision ( Zhang et al. , 2006 ; Cheng et al. , 2010 ).', 'compared with the global training method which tries to fit a single weight on the training data, the local one learns weights based on the local neighborhood information for each test example.', 'it is superior to the global one when the data sets are not evenly distributed (Bottou and Vapnik, 1992;Zhang et al., 2006)']","['local training method ( #AUTHOR_TAG ) is widely employed in computer vision ( Zhang et al. , 2006 ; Cheng et al. , 2010 ).', 'compared with the global training method which tries to fit a single weight on the training data, the local one learns weights based on the local neighborhood information for each test example.', 'it is superior to the global one when the data sets are not evenly distributed (Bottou and Vapnik, 1992;Zhang et al., 2006)']",0
"['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; #AUTHOR_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', '']","['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; #AUTHOR_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', '']","['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; #AUTHOR_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', 'all these training methods follow the same pipeline : they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'we call them a global training method.', 'one of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'however, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline']","['f and e ( e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; #AUTHOR_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', 'all these training methods follow the same pipeline : they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'we call them a global training method.', 'one of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'however, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline']",0
"['##aword corpus using the srilm toolkits ( Stolcke , 2002 ) with modified kneser - ney smoothing ( #AUTHOR_TAG ).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a.']","['run giza + + (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits ( Stolcke , 2002 ) with modified kneser - ney smoothing ( #AUTHOR_TAG ).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a. pl', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling (Koehn, 2004)']","['run giza + + (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits ( Stolcke , 2002 ) with modified kneser - ney smoothing ( #AUTHOR_TAG ).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a.']","['run giza + + (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits ( Stolcke , 2002 ) with modified kneser - ney smoothing ( #AUTHOR_TAG ).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a. pl', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling (Koehn, 2004)']",5
"['##aword corpus using the srilm toolkits ( #AUTHOR_TAG ) with modified kneser - ney smoothing ( Chen and Goodman , 1998 ).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a.']","['run giza + + (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits ( #AUTHOR_TAG ) with modified kneser - ney smoothing ( Chen and Goodman , 1998 ).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a. pl', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling (Koehn, 2004)']","['run giza + + (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits ( #AUTHOR_TAG ) with modified kneser - ney smoothing ( Chen and Goodman , 1998 ).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a.']","['run giza + + (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits ( #AUTHOR_TAG ) with modified kneser - ney smoothing ( Chen and Goodman , 1998 ).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a. pl', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling (Koehn, 2004)']",5
"['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #AUTHOR_TAG ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', '']","['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #AUTHOR_TAG ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', '']","['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #AUTHOR_TAG ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', 'all these training methods follow the same pipeline : they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'we call them a global training method.', 'one of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'however, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline']","['f and e ( e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #AUTHOR_TAG ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', 'all these training methods follow the same pipeline : they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'we call them a global training method.', 'one of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'however, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline']",0
"['with retraining mode, incremental training can improve the training efficiency.', 'in the field of machine learning research, incremental training has been employed in the work ( #AUTHOR_TAG ; Shilton et al. , 2005 ), but there is little work for tuning parameters of statistical machine translation.', 'the biggest difficulty lies in that the fea - ture vector of a given training example, i. e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable.', 'in this section, we will investigate the incremental trainingmethodsinsmtscenario']","['with retraining mode, incremental training can improve the training efficiency.', 'in the field of machine learning research, incremental training has been employed in the work ( #AUTHOR_TAG ; Shilton et al. , 2005 ), but there is little work for tuning parameters of statistical machine translation.', 'the biggest difficulty lies in that the fea - ture vector of a given training example, i. e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable.', 'in this section, we will investigate the incremental trainingmethodsinsmtscenario']","['with retraining mode, incremental training can improve the training efficiency.', 'in the field of machine learning research, incremental training has been employed in the work ( #AUTHOR_TAG ; Shilton et al. , 2005 ), but there is little work for tuning parameters of statistical machine translation.', 'the biggest difficulty lies in that the fea - ture vector of a given training example, i. e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable.', 'in this section, we will investigate the incremental trainingmethodsinsmtscenario']","['with retraining mode, incremental training can improve the training efficiency.', 'in the field of machine learning research, incremental training has been employed in the work ( #AUTHOR_TAG ; Shilton et al. , 2005 ), but there is little work for tuning parameters of statistical machine translation.', 'the biggest difficulty lies in that the fea - ture vector of a given training example, i. e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable.', 'in this section, we will investigate the incremental trainingmethodsinsmtscenario']",0
"['works have proposed discriminative tech - niques to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '( Och , 2003 ; Moore and Quirk , 2008 ; #AUTHOR_TAG ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative tech - niques to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '( Och , 2003 ; Moore and Quirk , 2008 ; #AUTHOR_TAG ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative tech - niques to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '( Och , 2003 ; Moore and Quirk , 2008 ; #AUTHOR_TAG ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative tech - niques to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '( Och , 2003 ; Moore and Quirk , 2008 ; #AUTHOR_TAG ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']",1
"['works have proposed discriminative tech - niques to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '( Och , 2003 ; #AUTHOR_TAG ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative tech - niques to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '( Och , 2003 ; #AUTHOR_TAG ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative tech - niques to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '( Och , 2003 ; #AUTHOR_TAG ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative tech - niques to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '( Och , 2003 ; #AUTHOR_TAG ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']",1
"['', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling ( #AUTHOR_TAG )']","['run giza + + (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits (Stolcke, 2002) with modified kneser - ney smoothing (Chen and Goodman, 1998).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a. pl', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling ( #AUTHOR_TAG )']","['', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling ( #AUTHOR_TAG )']","['run giza + + (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits (Stolcke, 2002) with modified kneser - ney smoothing (Chen and Goodman, 1998).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a. pl', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling ( #AUTHOR_TAG )']",5
"['via tuning as ranking (#AUTHOR_TAG, 2011 ).', 'since score of e11 is greater than that of e12, ( 1, 0 ) corresponds to a possitive example denoted as ” • ”, and ( −1, 0 ) corresponds to a negative example denoted as ” * ”.', 'since the transformed classification problem is not linearly separable, there does not exist a single weight which can obtain e11 and e21 as translation results meanwhile.', 'however, one can obtain e11 and e21 with weights : ( 1, 1 ) and ( −1, 1 ),']","['b ). the nonlinearly separable classification problem transformed from ( a ) via tuning as ranking (#AUTHOR_TAG, 2011 ).', 'since score of e11 is greater than that of e12, ( 1, 0 ) corresponds to a possitive example denoted as ” • ”, and ( −1, 0 ) corresponds to a negative example denoted as ” * ”.', 'since the transformed classification problem is not linearly separable, there does not exist a single weight which can obtain e11 and e21 as translation results meanwhile.', 'however, one can obtain e11 and e21 with weights : ( 1, 1 ) and ( −1, 1 ), respectively']","['via tuning as ranking (#AUTHOR_TAG, 2011 ).', 'since score of e11 is greater than that of e12, ( 1, 0 ) corresponds to a possitive example denoted as ” • ”, and ( −1, 0 ) corresponds to a negative example denoted as ” * ”.', 'since the transformed classification problem is not linearly separable, there does not exist a single weight which can obtain e11 and e21 as translation results meanwhile.', 'however, one can obtain e11 and e21 with weights : ( 1, 1 ) and ( −1, 1 ), respectively']","['b ). the nonlinearly separable classification problem transformed from ( a ) via tuning as ranking (#AUTHOR_TAG, 2011 ).', 'since score of e11 is greater than that of e12, ( 1, 0 ) corresponds to a possitive example denoted as ” • ”, and ( −1, 0 ) corresponds to a negative example denoted as ” * ”.', 'since the transformed classification problem is not linearly separable, there does not exist a single weight which can obtain e11 and e21 as translation results meanwhile.', 'however, one can obtain e11 and e21 with weights : ( 1, 1 ) and ( −1, 1 ), respectively']",0
"['works have proposed discriminative techniques to train log - linear model for smt.', '( #AUTHOR_TAG ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for mt.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative techniques to train log - linear model for smt.', '( #AUTHOR_TAG ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for mt.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative techniques to train log - linear model for smt.', '( #AUTHOR_TAG ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for mt.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative techniques to train log - linear model for smt.', '( #AUTHOR_TAG ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for mt.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']",1
"['metric we consider here is derived from an example - based machine translation.', 'to retrieve translation examples for a test sentence, ( #AUTHOR_TAG ) defined a metric based on the combination of edit distance and tf - idf ( manning and sch [UNK] utze, 1999 ) as follows']","['metric we consider here is derived from an example - based machine translation.', 'to retrieve translation examples for a test sentence, ( #AUTHOR_TAG ) defined a metric based on the combination of edit distance and tf - idf ( manning and sch [UNK] utze, 1999 ) as follows']","['metric we consider here is derived from an example - based machine translation.', 'to retrieve translation examples for a test sentence, ( #AUTHOR_TAG ) defined a metric based on the combination of edit distance and tf - idf ( manning and sch [UNK] utze, 1999 ) as follows']","['metric we consider here is derived from an example - based machine translation.', 'to retrieve translation examples for a test sentence, ( #AUTHOR_TAG ) defined a metric based on the combination of edit distance and tf - idf ( manning and sch [UNK] utze, 1999 ) as follows']",5
"['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( #AUTHOR_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', '']","['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( #AUTHOR_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', '']","['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( #AUTHOR_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', 'all these training methods follow the same pipeline : they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'we call them a global training method.', 'one of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'however, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline']","['f and e ( e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( #AUTHOR_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', 'all these training methods follow the same pipeline : they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'we call them a global training method.', 'one of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'however, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline']",0
"['k j = 1, in the rest of this paper.', 'our goal is to find an optimal weight, denoted by w i, which is a local weight and used for decoding the sentence t i.', 'unlike the global method which performs tuning on the whole development set dev + d i as in algorithm 1, w i can be incrementally learned by optimizing on d i based on w b.', 'we employ the idea of ultraconservative update ( #AUTHOR_TAG ; Crammer et al. , 2006 ) to propose two incremental methods for local training in algorithm 2 as follows']","['k j = 1, in the rest of this paper.', 'our goal is to find an optimal weight, denoted by w i, which is a local weight and used for decoding the sentence t i.', 'unlike the global method which performs tuning on the whole development set dev + d i as in algorithm 1, w i can be incrementally learned by optimizing on d i based on w b.', 'we employ the idea of ultraconservative update ( #AUTHOR_TAG ; Crammer et al. , 2006 ) to propose two incremental methods for local training in algorithm 2 as follows']","['k j = 1, in the rest of this paper.', 'our goal is to find an optimal weight, denoted by w i, which is a local weight and used for decoding the sentence t i.', 'unlike the global method which performs tuning on the whole development set dev + d i as in algorithm 1, w i can be incrementally learned by optimizing on d i based on w b.', 'we employ the idea of ultraconservative update ( #AUTHOR_TAG ; Crammer et al. , 2006 ) to propose two incremental methods for local training in algorithm 2 as follows']","['the notations in algorithm 2, w b is the baseline weight, d i = { f i j, c i j, r i j } k j = 1 denotes training examples for t i.', 'for the sake of brevity, we will drop the index i, d i = { f j, c j, r j } k j = 1, in the rest of this paper.', 'our goal is to find an optimal weight, denoted by w i, which is a local weight and used for decoding the sentence t i.', 'unlike the global method which performs tuning on the whole development set dev + d i as in algorithm 1, w i can be incrementally learned by optimizing on d i based on w b.', 'we employ the idea of ultraconservative update ( #AUTHOR_TAG ; Crammer et al. , 2006 ) to propose two incremental methods for local training in algorithm 2 as follows']",5
"['works have proposed discriminative techniques to train log - linear model for smt.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative techniques to train log - linear model for smt.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative techniques to train log - linear model for smt.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative techniques to train log - linear model for smt.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']",1
"['use an in - house developed hierarchical phrase - based translation ( #AUTHOR_TAG ) as our baseline system, and we denote it as in - hiero.', 'to obtain satisfactory baseline performance, we tune in - hiero system for 5 times using mert, and then se - (Koehn et al., 2007).', 'both of these systems are with default setting.', 'all three systems are trained by mert with 100 best candidates']","['use an in - house developed hierarchical phrase - based translation ( #AUTHOR_TAG ) as our baseline system, and we denote it as in - hiero.', 'to obtain satisfactory baseline performance, we tune in - hiero system for 5 times using mert, and then se - (Koehn et al., 2007).', 'both of these systems are with default setting.', 'all three systems are trained by mert with 100 best candidates']","['use an in - house developed hierarchical phrase - based translation ( #AUTHOR_TAG ) as our baseline system, and we denote it as in - hiero.', 'to obtain satisfactory baseline performance, we tune in - hiero system for 5 times using mert, and then se - (Koehn et al., 2007).', 'both of these systems are with default setting.', 'all three systems are trained by mert with 100 best candidates']","['use an in - house developed hierarchical phrase - based translation ( #AUTHOR_TAG ) as our baseline system, and we denote it as in - hiero.', 'to obtain satisfactory baseline performance, we tune in - hiero system for 5 times using mert, and then se - (Koehn et al., 2007).', 'both of these systems are with default setting.', 'all three systems are trained by mert with 100 best candidates']",5
"['run giza + + ( #AUTHOR_TAG ) on the training corpus in both directions ( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english']","['run giza + + ( #AUTHOR_TAG ) on the training corpus in both directions ( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits (Stolcke, 2002) with modified kneser - ney smoothing (Chen and Goodman, 1998).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a. pl', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling (Koehn, 2004)']","['run giza + + ( #AUTHOR_TAG ) on the training corpus in both directions ( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits (Stolcke, 2002) with modified kneser - ney smoothing (Chen and Goodman, 1998).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a.']","['run giza + + ( #AUTHOR_TAG ) on the training corpus in both directions ( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits (Stolcke, 2002) with modified kneser - ney smoothing (Chen and Goodman, 1998).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a. pl', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling (Koehn, 2004)']",5
"['works have proposed discriminative techniques to train log - linear model for smt.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #AUTHOR_TAG ) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative techniques to train log - linear model for smt.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #AUTHOR_TAG ) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative techniques to train log - linear model for smt.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #AUTHOR_TAG ) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative techniques to train log - linear model for smt.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #AUTHOR_TAG ) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']",1
"['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', '']","['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', '']","['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', 'all these training methods follow the same pipeline : they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'we call them a global training method.', 'one of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'however, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline']","['f and e ( e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', 'all these training methods follow the same pipeline : they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'we call them a global training method.', 'one of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'however, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline']",0
"[', our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception.', 'although evaluated on different data sets, this result is consistent with results from previous work ( Gatt and Belz , 2008 ; #AUTHOR_TAG ).', ""however, what is more interesting here is that while graph - based approaches perform well when the agent has perfect knowledge of the environment, as its human partner, these approaches literally fall apart with close to 40 % performance degradation when applied to the situation where the agent's representation of the shared world is problematic and full of mistakes""]","[', our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception.', 'although evaluated on different data sets, this result is consistent with results from previous work ( Gatt and Belz , 2008 ; #AUTHOR_TAG ).', ""however, what is more interesting here is that while graph - based approaches perform well when the agent has perfect knowledge of the environment, as its human partner, these approaches literally fall apart with close to 40 % performance degradation when applied to the situation where the agent's representation of the shared world is problematic and full of mistakes""]","[', our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception.', 'although evaluated on different data sets, this result is consistent with results from previous work ( Gatt and Belz , 2008 ; #AUTHOR_TAG ).', ""however, what is more interesting here is that while graph - based approaches perform well when the agent has perfect knowledge of the environment, as its human partner, these approaches literally fall apart with close to 40 % performance degradation when applied to the situation where the agent's representation of the shared world is problematic and full of mistakes""]","[', our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception.', 'although evaluated on different data sets, this result is consistent with results from previous work ( Gatt and Belz , 2008 ; #AUTHOR_TAG ).', ""however, what is more interesting here is that while graph - based approaches perform well when the agent has perfect knowledge of the environment, as its human partner, these approaches literally fall apart with close to 40 % performance degradation when applied to the situation where the agent's representation of the shared world is problematic and full of mistakes""]",1
"['lower level visual features (Harnad, 1990).', 'previous work has developed various approaches for grounded semantics mainly for the reference resolution task, i. e., identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG ).', 'for the referring expression generation task here, we also need a lexicon with grounded semantics']","['lower level visual features (Harnad, 1990).', 'previous work has developed various approaches for grounded semantics mainly for the reference resolution task, i. e., identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG ).', 'for the referring expression generation task here, we also need a lexicon with grounded semantics']","['lower level visual features (Harnad, 1990).', 'previous work has developed various approaches for grounded semantics mainly for the reference resolution task, i. e., identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG ).', 'for the referring expression generation task here, we also need a lexicon with grounded semantics']","['semantics provides a bridge to connect symbolic labels or words with lower level visual features (Harnad, 1990).', 'previous work has developed various approaches for grounded semantics mainly for the reference resolution task, i. e., identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG ).', 'for the referring expression generation task here, we also need a lexicon with grounded semantics']",2
"['in our previous work ( #AUTHOR_TAG ).', 'in that work, the main focus is on reference resolution : given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.', 'since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation ( reg ) becomes an equally important problem in situated dialogue.', 'robots have much lower perceptual capabilities of the environment']","['in our previous work ( #AUTHOR_TAG ).', 'in that work, the main focus is on reference resolution : given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.', 'since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation ( reg ) becomes an equally important problem in situated dialogue.', 'robots have much lower perceptual capabilities of the environment']","['this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #AUTHOR_TAG ).', 'in that work, the main focus is on reference resolution : given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.', 'since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation ( reg ) becomes an equally important problem in situated dialogue.', 'robots have much lower perceptual capabilities of the environment']","['this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #AUTHOR_TAG ).', 'in that work, the main focus is on reference resolution : given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.', 'since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation ( reg ) becomes an equally important problem in situated dialogue.', 'robots have much lower perceptual capabilities of the environment than humans.', 'how can a robot effectively generate referential descriptions about the environment so that its human partner can understand which objects are being referred to']",2
"['models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, #AUTHOR_TAG showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of']","['others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) ).', 'Griffiths et al. (2007) helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, #AUTHOR_TAG showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of']","['models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, #AUTHOR_TAG showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) ).', 'Griffiths et al. (2007) helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, #AUTHOR_TAG showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['raw perceptual in - formation and using high - level representations in - stead.', 'some works abstract perception via the usage of symbolic logic representations ( #AUTHOR_TAG ; Chen and Mooney , 2011 ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ),']","['raw perceptual in - formation and using high - level representations in - stead.', 'some works abstract perception via the usage of symbolic logic representations ( #AUTHOR_TAG ; Chen and Mooney , 2011 ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ),']","['approaches to multimodal research have succeeded by abstracting away raw perceptual in - formation and using high - level representations in - stead.', 'some works abstract perception via the usage of symbolic logic representations ( #AUTHOR_TAG ; Chen and Mooney , 2011 ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ),']","['approaches to multimodal research have succeeded by abstracting away raw perceptual in - formation and using high - level representations in - stead.', 'some works abstract perception via the usage of symbolic logic representations ( #AUTHOR_TAG ; Chen and Mooney , 2011 ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter cat - egory, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) )']",0
"['norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model ( #AUTHOR_TAG ) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '']","['norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model ( #AUTHOR_TAG ) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model ( #AUTHOR_TAG ) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model ( #AUTHOR_TAG ) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']",0
"['a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. ( 2013 ) show that visual attribute classifiers, which have been immensely successful in object recognition ( #AUTHOR_TAG ), act as excellent substitutes for feature', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. ( 2013 ) show that visual attribute classifiers, which have been immensely successful in object recognition ( #AUTHOR_TAG ), act as excellent substitutes for feature', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. ( 2013 ) show that visual attribute classifiers, which have been immensely successful in object recognition ( #AUTHOR_TAG ), act as excellent substitutes for feature', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. ( 2013 ) show that visual attribute classifiers, which have been immensely successful in object recognition ( #AUTHOR_TAG ), act as excellent substitutes for feature', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']",0
"['( Eisenstein et al. , 2010 ; #AUTHOR_TAG ; Roller et al. , 2012 )']","['( Eisenstein et al. , 2010 ; #AUTHOR_TAG ; Roller et al. , 2012 )']","['Ordonez et al. , 2011 ), text illustration ( Joshi et al. , 2006 ), or automatic location identification of twitter users ( Eisenstein et al. , 2010 ; #AUTHOR_TAG ; Roller et al. , 2012 )']","['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010 a ; Ordonez et al. , 2011 ), text illustration ( Joshi et al. , 2006 ), or automatic location identification of twitter users ( Eisenstein et al. , 2010 ; #AUTHOR_TAG ; Roller et al. , 2012 )']",0
"['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #AUTHOR_TAG ; Steyvers , 2010 ; Feng and Lapata , 2010 b']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #AUTHOR_TAG ; Steyvers , 2010 ; Feng and Lapata , 2010 b']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #AUTHOR_TAG ; Steyvers , 2010 ; Feng and Lapata , 2010 b']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #AUTHOR_TAG ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']",0
"['is, we simply take the original mlda model of #AUTHOR_TAG ( 2009 ) and generalize it in the same way']","['is, we simply take the original mlda model of #AUTHOR_TAG ( 2009 ) and generalize it in the same way']","['is, we simply take the original mlda model of #AUTHOR_TAG ( 2009 ) and generalize it in the same way']","['is, we simply take the original mlda model of #AUTHOR_TAG ( 2009 ) and generalize it in the same way they generalize lda.', 'at first glance, it seems that the inference task should become more difficult as the number of modalities increases and observed tuples become sparser, but the task remains roughly the same difficulty, as all of the observed elements of a tuple are conditionally independent given the topic assignment zi']",2
"['sentences are mapped to logical structure meaning ( #AUTHOR_TAG ).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'some efforts have']","['sentences are mapped to logical structure meaning ( #AUTHOR_TAG ).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'some efforts have']","['sentences are mapped to logical structure meaning ( #AUTHOR_TAG ).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'some efforts']","['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning ( #AUTHOR_TAG ).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010 a ; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of twitter users (Eisenstein et al., 2010;Wing and Baldridge, 2011;Roller et al., 2012)']",0
"['extend lda to allow for the inference of document and topic distributions in a multimodal corpus.', 'in their model, a document consists of a set of ( word, feature ) pairs, 4 rather than just words, and documents are still modeled as mixtures of shared topics.', 'topics consist of multinomial distributions over words, β k, but are extended to also include multinomial distributions over features, ψ k.', 'the generative process is amend']","['extend lda to allow for the inference of document and topic distributions in a multimodal corpus.', 'in their model, a document consists of a set of ( word, feature ) pairs, 4 rather than just words, and documents are still modeled as mixtures of shared topics.', 'topics consist of multinomial distributions over words, β k, but are extended to also include multinomial distributions over features, ψ k.', 'the generative process is amended']","['extend lda to allow for the inference of document and topic distributions in a multimodal corpus.', 'in their model, a document consists of a set of ( word, feature ) pairs, 4 rather than just words, and documents are still modeled as mixtures of shared topics.', 'topics consist of multinomial distributions over words, β k, but are extended to also include multinomial distributions over features, ψ k.', 'the generative process is amended']","['extend lda to allow for the inference of document and topic distributions in a multimodal corpus.', 'in their model, a document consists of a set of ( word, feature ) pairs, 4 rather than just words, and documents are still modeled as mixtures of shared topics.', 'topics consist of multinomial distributions over words, β k, but are extended to also include multinomial distributions over features, ψ k.', 'the generative process is amended to include these feature distributions']",0
"[', we compute a simple bag of visual words ( bovw ) model for our images using surf keypoints (Bay et al., 2008).', 'surf is a method for selecting points - of - interest within an image.', 'it is faster and more forgiving than the commonly known sift algorithm.', 'we compute surf keypoints for every image in our data set using sim - plecv 3 and randomly sample 1 % of the keypoints.', 'the keypoints are clustered into 5, 000 visual codewords ( centroids ) using k - means clustering ( #AUTHOR_TAG ), and images are then quantized over the 5, 000 codewords.', 'all images for a given word are summed together to provide an average representation for the word.', 'we refer to this representation as the surf modality']","[', we compute a simple bag of visual words ( bovw ) model for our images using surf keypoints (Bay et al., 2008).', 'surf is a method for selecting points - of - interest within an image.', 'it is faster and more forgiving than the commonly known sift algorithm.', 'we compute surf keypoints for every image in our data set using sim - plecv 3 and randomly sample 1 % of the keypoints.', 'the keypoints are clustered into 5, 000 visual codewords ( centroids ) using k - means clustering ( #AUTHOR_TAG ), and images are then quantized over the 5, 000 codewords.', 'all images for a given word are summed together to provide an average representation for the word.', 'we refer to this representation as the surf modality']","[', we compute a simple bag of visual words ( bovw ) model for our images using surf keypoints (Bay et al., 2008).', 'surf is a method for selecting points - of - interest within an image.', 'it is faster and more forgiving than the commonly known sift algorithm.', 'we compute surf keypoints for every image in our data set using sim - plecv 3 and randomly sample 1 % of the keypoints.', 'the keypoints are clustered into 5, 000 visual codewords ( centroids ) using k - means clustering ( #AUTHOR_TAG ), and images are then quantized over the 5, 000 codewords.', 'all images for a given word are summed together to provide an average representation for the word.', 'we refer to this representation as the surf modality']","[', we compute a simple bag of visual words ( bovw ) model for our images using surf keypoints (Bay et al., 2008).', 'surf is a method for selecting points - of - interest within an image.', 'it is faster and more forgiving than the commonly known sift algorithm.', 'we compute surf keypoints for every image in our data set using sim - plecv 3 and randomly sample 1 % of the keypoints.', 'the keypoints are clustered into 5, 000 visual codewords ( centroids ) using k - means clustering ( #AUTHOR_TAG ), and images are then quantized over the 5, 000 codewords.', 'all images for a given word are summed together to provide an average representation for the word.', 'we refer to this representation as the surf modality']",5
"['t - test ).', 'this result is consistent with other works using this model with these features ( Andrews et al. , 2009 ; #AUTHOR_TAG )']","['1 shows our results for each of our selected models with our compositionality evaluation.', 'the 2d models employing feature norms and association norms do significantly better than the text - only model ( two - tailed t - test ).', 'this result is consistent with other works using this model with these features ( Andrews et al. , 2009 ; #AUTHOR_TAG )']","['1 shows our results for each of our selected models with our compositionality evaluation.', 'the 2d models employing feature norms and association norms do significantly better than the text - only model ( two - tailed t - test ).', 'this result is consistent with other works using this model with these features ( Andrews et al. , 2009 ; #AUTHOR_TAG )']","['1 shows our results for each of our selected models with our compositionality evaluation.', 'the 2d models employing feature norms and association norms do significantly better than the text - only model ( two - tailed t - test ).', 'this result is consistent with other works using this model with these features ( Andrews et al. , 2009 ; #AUTHOR_TAG )']",1
"['a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( Mathe et al. , 2008 ; #AUTHOR_TAG )']","['a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( Mathe et al. , 2008 ; #AUTHOR_TAG )']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( Mathe et al. , 2008 ; #AUTHOR_TAG )']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( Mathe et al. , 2008 ; #AUTHOR_TAG )']",0
"['others choose to employ concepts elicited from psycholin - guistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. ( 2004 ) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., #AUTHOR_TAG )']","['others choose to employ concepts elicited from psycholin - guistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. ( 2004 ) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., #AUTHOR_TAG )']","['others choose to employ concepts elicited from psycholin - guistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. ( 2004 ) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., #AUTHOR_TAG )']","['approaches to multimodal research have succeeded by abstracting away raw perceptual in - formation and using high - level representations in - stead.', 'some works abstract perception via the us - age of symbolic logic representations (Chen et al., 2010; Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholin - guistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. ( 2004 ) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., #AUTHOR_TAG )']",0
"['sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions ( #AUTHOR_TAG ) or robot commands ( Tellex et al. , 2011 ; Matuszek et al. , 2012 ).', 'some efforts have']","['sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions ( #AUTHOR_TAG ) or robot commands ( Tellex et al. , 2011 ; Matuszek et al. , 2012 ).', 'some efforts have']","['sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions ( #AUTHOR_TAG ) or robot commands ( Tellex et al. , 2011 ; Matuszek et al. , 2012 ).', 'some efforts']","['language grounding problem has come in many different flavors with just as many different ap - proaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions ( #AUTHOR_TAG ) or robot commands ( Tellex et al. , 2011 ; Matuszek et al. , 2012 ).', 'some efforts have tackled tasks such as automatic image caption generation ( feng and la - pata, 2010a ; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identifica - tion of twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012)']",0
"['a ; #AUTHOR_TAG ), text illustration ( Joshi et al. , 2006 ), or automatic location identification of twitter']","['a ; #AUTHOR_TAG ), text illustration ( Joshi et al. , 2006 ), or automatic location identification of twitter']","['a ; #AUTHOR_TAG ), text illustration ( Joshi et al. , 2006 ), or automatic location identification of twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )']","['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010 a ; #AUTHOR_TAG ), text illustration ( Joshi et al. , 2006 ), or automatic location identification of twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )']",0
"['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; #AUTHOR_TAG ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words ""']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; #AUTHOR_TAG ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words ""']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; #AUTHOR_TAG ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; #AUTHOR_TAG ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']",0
"['order to evaluate our algorithms, we first need to generate multimodal corpora for each of our non - textual modalities.', ""we use the same method as #AUTHOR_TAG for generating our multimodal corpora : for each word token in the text corpus, a feature is selected stochastically from the word's feature distribution, creating a word - feature pair."", 'words without grounded features are all given the same placeholder feature, also resulting in a word - feature pair. 5', 'that is, for the feature norm modal - ity, we generate ( word, feature norm ) pairs']","['order to evaluate our algorithms, we first need to generate multimodal corpora for each of our non - textual modalities.', ""we use the same method as #AUTHOR_TAG for generating our multimodal corpora : for each word token in the text corpus, a feature is selected stochastically from the word's feature distribution, creating a word - feature pair."", 'words without grounded features are all given the same placeholder feature, also resulting in a word - feature pair. 5', 'that is, for the feature norm modal - ity, we generate ( word, feature norm ) pairs ; for the surf modality, we generate ( word, codeword ) pairs, etc.', 'the resulting stochastically generated corpus is used in its corresponding experiments']","['order to evaluate our algorithms, we first need to generate multimodal corpora for each of our non - textual modalities.', ""we use the same method as #AUTHOR_TAG for generating our multimodal corpora : for each word token in the text corpus, a feature is selected stochastically from the word's feature distribution, creating a word - feature pair."", 'words without grounded features are all given the same placeholder feature, also resulting in a word - feature pair. 5', 'that is, for the feature norm modal - ity, we generate ( word, feature norm ) pairs ; for the surf modality, we generate ( word, codeword ) pairs, etc.', 'the resulting stochastically generated corpus is used in its corresponding experiments']","['order to evaluate our algorithms, we first need to generate multimodal corpora for each of our non - textual modalities.', ""we use the same method as #AUTHOR_TAG for generating our multimodal corpora : for each word token in the text corpus, a feature is selected stochastically from the word's feature distribution, creating a word - feature pair."", 'words without grounded features are all given the same placeholder feature, also resulting in a word - feature pair. 5', 'that is, for the feature norm modal - ity, we generate ( word, feature norm ) pairs ; for the surf modality, we generate ( word, codeword ) pairs, etc.', 'the resulting stochastically generated corpus is used in its corresponding experiments']",5
"['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; #AUTHOR_TAG ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  az']","['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; #AUTHOR_TAG ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']","['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; #AUTHOR_TAG ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  az']","['language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; #AUTHOR_TAG ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']",0
"['experiments are based on the multimodal extension of latent dirichlet allocation developed by #AUTHOR_TAG.', 'previously lda has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012).', 'it has also been shown to be useful in joint inference of text with visual']","['experiments are based on the multimodal extension of latent dirichlet allocation developed by #AUTHOR_TAG.', 'previously lda has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012).', 'it has also been shown to be useful in joint inference of text with visual']","['experiments are based on the multimodal extension of latent dirichlet allocation developed by #AUTHOR_TAG.', 'previously lda has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012).', 'it has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013).', 'these multimodal lda models ( hereafter, mlda ) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).', 'however, prior work using mlda is limited to two modalities at a time.', 'in this section, we describe bimodal mlda and define two methods for extending it to three or more modalities']","['experiments are based on the multimodal extension of latent dirichlet allocation developed by #AUTHOR_TAG.', 'previously lda has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012).', 'it has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013).', 'these multimodal lda models ( hereafter, mlda ) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).', 'however, prior work using mlda is limited to two modalities at a time.', 'in this section, we describe bimodal mlda and define two methods for extending it to three or more modalities']",5
"['see that the image modalities are much more useful than they are in compositionality prediction.', 'the surf modality does extremely well in particular, but the gist features also provide statistically significant improvements over the text - only model.', 'since the surf and gist image features tend to capture object - likeness and scene - likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them.', 'this seems to provide additional evidence of #AUTHOR_TAG']","['see that the image modalities are much more useful than they are in compositionality prediction.', 'the surf modality does extremely well in particular, but the gist features also provide statistically significant improvements over the text - only model.', 'since the surf and gist image features tend to capture object - likeness and scene - likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them.', 'this seems to provide additional evidence of #AUTHOR_TAG']","['see that the image modalities are much more useful than they are in compositionality prediction.', 'the surf modality does extremely well in particular, but the gist features also provide statistically significant improvements over the text - only model.', 'since the surf and gist image features tend to capture object - likeness and scene - likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them.', ""this seems to provide additional evidence of #AUTHOR_TAG b )'s suggestion that something like a distributional hypothesis of images is plausible""]","['see that the image modalities are much more useful than they are in compositionality prediction.', 'the surf modality does extremely well in particular, but the gist features also provide statistically significant improvements over the text - only model.', 'since the surf and gist image features tend to capture object - likeness and scene - likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them.', ""this seems to provide additional evidence of #AUTHOR_TAG b )'s suggestion that something like a distributional hypothesis of images is plausible""]",1
"['also compute gist vectors (Oliva and Torralba, 2001) for every image using leargist (Douze et al., 2009).', 'unlike surf descriptors, gist produces a single vector representation for an image.', 'the vector does not find points of interest in the image, but rather attempts to provide a representation for the overall "" gist "" of the whole image.', 'it is frequently used in tasks like scene identification, and #AUTHOR_TAG shows that distance in gist space correlates well with semantic distance in wordnet.', 'after computing the gist vectors, each textual word is represented as the centroid gist vector of all its images, forming the gist modality']","['also compute gist vectors (Oliva and Torralba, 2001) for every image using leargist (Douze et al., 2009).', 'unlike surf descriptors, gist produces a single vector representation for an image.', 'the vector does not find points of interest in the image, but rather attempts to provide a representation for the overall "" gist "" of the whole image.', 'it is frequently used in tasks like scene identification, and #AUTHOR_TAG shows that distance in gist space correlates well with semantic distance in wordnet.', 'after computing the gist vectors, each textual word is represented as the centroid gist vector of all its images, forming the gist modality']","['also compute gist vectors (Oliva and Torralba, 2001) for every image using leargist (Douze et al., 2009).', 'unlike surf descriptors, gist produces a single vector representation for an image.', 'the vector does not find points of interest in the image, but rather attempts to provide a representation for the overall "" gist "" of the whole image.', 'it is frequently used in tasks like scene identification, and #AUTHOR_TAG shows that distance in gist space correlates well with semantic distance in wordnet.', 'after computing the gist vectors, each textual word is represented as the centroid gist vector of all its images, forming the gist modality']","['also compute gist vectors (Oliva and Torralba, 2001) for every image using leargist (Douze et al., 2009).', 'unlike surf descriptors, gist produces a single vector representation for an image.', 'the vector does not find points of interest in the image, but rather attempts to provide a representation for the overall "" gist "" of the whole image.', 'it is frequently used in tasks like scene identification, and #AUTHOR_TAG shows that distance in gist space correlates well with semantic distance in wordnet.', 'after computing the gist vectors, each textual word is represented as the centroid gist vector of all its images, forming the gist modality']",4
"['norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '#AUTHOR_TAG a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '#AUTHOR_TAG a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '#AUTHOR_TAG a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '#AUTHOR_TAG a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']",0
['tasks such as automatic image caption generation ( #AUTHOR_TAG'],['tasks such as automatic image caption generation ( #AUTHOR_TAG'],['tasks such as automatic image caption generation ( #AUTHOR_TAG'],"['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'some efforts have tackled tasks such as automatic image caption generation ( #AUTHOR_TAG a ; Ordonez et al. , 2011 ), text illustration ( Joshi et al. , 2006 ), or automatic location identification of twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )']",0
"['our text modality, we use dewac, a large german web corpus created by the wacky group ( #AUTHOR_TAG ) containing approximately 1. 7 b word tokens.', 'we filtered the corpus by : removing words with unprintable characters or encoding troubles']","['our text modality, we use dewac, a large german web corpus created by the wacky group ( #AUTHOR_TAG ) containing approximately 1. 7 b word tokens.', 'we filtered the corpus by : removing words with unprintable characters or encoding troubles ; removing all stopwords ; removing word types with a total frequency of less than 500 ; and removing documents with a length shorter than 100.', 'the resulting corpus has 1, 038, 883 documents consisting of 75, 678 word types and 466m word tokens']","['our text modality, we use dewac, a large german web corpus created by the wacky group ( #AUTHOR_TAG ) containing approximately 1. 7 b word tokens.', 'we filtered the corpus by : removing words with unprintable characters or encoding troubles ; removing all stopwords ; removing word types with a total frequency of less than 500 ; and removing documents with a length shorter than 100.', 'the resulting corpus has 1, 038, 883 documents consisting of 75, 678 word types and 466m word tokens']","['our text modality, we use dewac, a large german web corpus created by the wacky group ( #AUTHOR_TAG ) containing approximately 1. 7 b word tokens.', 'we filtered the corpus by : removing words with unprintable characters or encoding troubles ; removing all stopwords ; removing word types with a total frequency of less than 500 ; and removing documents with a length shorter than 100.', 'the resulting corpus has 1, 038, 883 documents consisting of 75, 678 word types and 466m word tokens']",5
"['greatly from efforts to unify the two modalities.', 'to name a few examples, Rohrbach et al. ( 2010 ) and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos']","['computer vision community has also benefited greatly from efforts to unify the two modalities.', 'to name a few examples, Rohrbach et al. ( 2010 ) and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos']","['computer vision community has also benefited greatly from efforts to unify the two modalities.', 'to name a few examples, Rohrbach et al. ( 2010 ) and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos']","['computer vision community has also benefited greatly from efforts to unify the two modalities.', 'to name a few examples, Rohrbach et al. ( 2010 ) and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos']",0
"['models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', '#AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal']","['others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) ).', 'Griffiths et al. (2007) helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', '#AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal']","['models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', '#AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on canonical correlation analysis,']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) ).', 'Griffiths et al. (2007) helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', '#AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality.', 'following #AUTHOR_TAG, we measure association norm prediction as an average of percentile ranks.', 'for all possible pairs of words in our vocabulary, we compute the negative symmetric kl divergence between the two words.', 'we then compute the percentile ranks of similarity for each word pair, e. g., "" cat "" is more similar to "" dog "" than 97. 3 % of the rest of the vocabulary.', 'we report the weighted mean percentile ranks for all cue - association pairs, i. e., if a cue - association is given more than once, it is counted more than once']","['it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality.', 'following #AUTHOR_TAG, we measure association norm prediction as an average of percentile ranks.', 'for all possible pairs of words in our vocabulary, we compute the negative symmetric kl divergence between the two words.', 'we then compute the percentile ranks of similarity for each word pair, e. g., "" cat "" is more similar to "" dog "" than 97. 3 % of the rest of the vocabulary.', 'we report the weighted mean percentile ranks for all cue - association pairs, i. e., if a cue - association is given more than once, it is counted more than once']","['it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality.', 'following #AUTHOR_TAG, we measure association norm prediction as an average of percentile ranks.', 'for all possible pairs of words in our vocabulary, we compute the negative symmetric kl divergence between the two words.', 'we then compute the percentile ranks of similarity for each word pair, e. g., "" cat "" is more similar to "" dog "" than 97. 3 % of the rest of the vocabulary.', 'we report the weighted mean percentile ranks for all cue - association pairs, i. e., if a cue - association is given more than once, it is counted more than once']","[', we also evaluate using the association norms data set described in section 3. since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality.', 'following #AUTHOR_TAG, we measure association norm prediction as an average of percentile ranks.', 'for all possible pairs of words in our vocabulary, we compute the negative symmetric kl divergence between the two words.', 'we then compute the percentile ranks of similarity for each word pair, e. g., "" cat "" is more similar to "" dog "" than 97. 3 % of the rest of the vocabulary.', 'we report the weighted mean percentile ranks for all cue - association pairs, i. e., if a cue - association is given more than once, it is counted more than once']",5
"['compatibility with the multimodal latent dirichlet allocation model of #AUTHOR_TAG.', 'we found both fea - ture sets were directly compatible with multimodal lda and provided significant gains in their ability to predict association norms over traditional text - only lda.', 'surf features also provided significant gains over text - only lda in predicting the compositionality of noun compounds']","['compatibility with the multimodal latent dirichlet allocation model of #AUTHOR_TAG.', 'we found both fea - ture sets were directly compatible with multimodal lda and provided significant gains in their ability to predict association norms over traditional text - only lda.', 'surf features also provided significant gains over text - only lda in predicting the compositionality of noun compounds']","['compatibility with the multimodal latent dirichlet allocation model of #AUTHOR_TAG.', 'we found both fea - ture sets were directly compatible with multimodal lda and provided significant gains in their ability to predict association norms over traditional text - only lda.', 'surf features also provided significant gains over text - only lda in predicting the compositionality of noun compounds']","['this paper, we evaluated the role of low - level image features, surf and gist, for their compatibility with the multimodal latent dirichlet allocation model of #AUTHOR_TAG.', 'we found both fea - ture sets were directly compatible with multimodal lda and provided significant gains in their ability to predict association norms over traditional text - only lda.', 'surf features also provided significant gains over text - only lda in predicting the compositionality of noun compounds']",5
"['also compute gist vectors ( #AUTHOR_TAG ) for every image using leargist ( Douze et al. , 2009 ).', 'unlike surf descriptors, gist produces a single vector representation for an image.', 'the vector does not find points of interest in the image, but rather attempts to provide a representation for the overall "" gist "" of the whole image.', 'it is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in gist space correlates well with semantic distance in wordnet.', 'after computing the gist vectors, each textual word is represented as the centroid gist vector of all its images, forming the gist modality']","['also compute gist vectors ( #AUTHOR_TAG ) for every image using leargist ( Douze et al. , 2009 ).', 'unlike surf descriptors, gist produces a single vector representation for an image.', 'the vector does not find points of interest in the image, but rather attempts to provide a representation for the overall "" gist "" of the whole image.', 'it is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in gist space correlates well with semantic distance in wordnet.', 'after computing the gist vectors, each textual word is represented as the centroid gist vector of all its images, forming the gist modality']","['also compute gist vectors ( #AUTHOR_TAG ) for every image using leargist ( Douze et al. , 2009 ).', 'unlike surf descriptors, gist produces a single vector representation for an image.', 'the vector does not find points of interest in the image, but rather attempts to provide a representation for the overall "" gist "" of the whole image.', 'it is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in gist space correlates well with semantic distance in wordnet.', 'after computing the gist vectors, each textual word is represented as the centroid gist vector of all its images, forming the gist modality']","['also compute gist vectors ( #AUTHOR_TAG ) for every image using leargist ( Douze et al. , 2009 ).', 'unlike surf descriptors, gist produces a single vector representation for an image.', 'the vector does not find points of interest in the image, but rather attempts to provide a representation for the overall "" gist "" of the whole image.', 'it is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in gist space correlates well with semantic distance in wordnet.', 'after computing the gist vectors, each textual word is represented as the centroid gist vector of all its images, forming the gist modality']",5
"['( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #AUTHOR_TAG )']","['( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #AUTHOR_TAG )']","['Ordonez et al. , 2011 ), text illustration ( Joshi et al. , 2006 ), or automatic location identification of twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #AUTHOR_TAG )']","['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010 a ; Ordonez et al. , 2011 ), text illustration ( Joshi et al. , 2006 ), or automatic location identification of twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #AUTHOR_TAG )']",0
"['emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) ).', '#AUTHOR_TAG helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperform']","['others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) ).', '#AUTHOR_TAG helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis ( Deerwester et al. , 1990 ) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of']","['others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) ).', '#AUTHOR_TAG helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperform']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) ).', '#AUTHOR_TAG helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis ( Deerwester et al. , 1990 ) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['.', 'to name a few examples, Rohrbach et al. ( 2010 ) and #AUTHOR_TAG show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']","['computer vision community has also benefited greatly from efforts to unify the two modalities.', 'to name a few examples, Rohrbach et al. ( 2010 ) and #AUTHOR_TAG show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']","['computer vision community has also benefited greatly from efforts to unify the two modalities.', 'to name a few examples, Rohrbach et al. ( 2010 ) and #AUTHOR_TAG show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']","['computer vision community has also benefited greatly from efforts to unify the two modalities.', 'to name a few examples, Rohrbach et al. ( 2010 ) and #AUTHOR_TAG show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']",0
"['##abian german ) is our new data set of german noun - to - imagenet synset mappings.', 'imagenet is a large - scale and widely used image database, built on top of wordnet, which maps words into groups of images, called synsets ( #AUTHOR_TAG ).', 'multiple synsets exist for each meaning of a word.', 'for example, im - agenet contains two different synsets for the word mouse : one contains images of the animal, while the other contains images of the computer peripheral.', 'this bildernetle data set provides mappings from german noun types to images of the nouns via imagenet']","['##ernetle ( "" little imagenet "" in swabian german ) is our new data set of german noun - to - imagenet synset mappings.', 'imagenet is a large - scale and widely used image database, built on top of wordnet, which maps words into groups of images, called synsets ( #AUTHOR_TAG ).', 'multiple synsets exist for each meaning of a word.', 'for example, im - agenet contains two different synsets for the word mouse : one contains images of the animal, while the other contains images of the computer peripheral.', 'this bildernetle data set provides mappings from german noun types to images of the nouns via imagenet']","['##abian german ) is our new data set of german noun - to - imagenet synset mappings.', 'imagenet is a large - scale and widely used image database, built on top of wordnet, which maps words into groups of images, called synsets ( #AUTHOR_TAG ).', 'multiple synsets exist for each meaning of a word.', 'for example, im - agenet contains two different synsets for the word mouse : one contains images of the animal, while the other contains images of the computer peripheral.', 'this bildernetle data set provides mappings from german noun types to images of the nouns via imagenet']","['##ernetle ( "" little imagenet "" in swabian german ) is our new data set of german noun - to - imagenet synset mappings.', 'imagenet is a large - scale and widely used image database, built on top of wordnet, which maps words into groups of images, called synsets ( #AUTHOR_TAG ).', 'multiple synsets exist for each meaning of a word.', 'for example, im - agenet contains two different synsets for the word mouse : one contains images of the animal, while the other contains images of the computer peripheral.', 'this bildernetle data set provides mappings from german noun types to images of the nouns via imagenet']",5
"['multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', '#AUTHOR_TAG introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']","['helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', '#AUTHOR_TAG introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']","['helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', '#AUTHOR_TAG introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']","['helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', '#AUTHOR_TAG introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, #AUTHOR_TAG show that visual attribute classifiers, which have been immensely successful in object recognition ( Farhadi et al. , 2009 ), act as excellent substitutes for feature', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, #AUTHOR_TAG show that visual attribute classifiers, which have been immensely successful in object recognition ( Farhadi et al. , 2009 ), act as excellent substitutes for feature', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, #AUTHOR_TAG show that visual attribute classifiers, which have been immensely successful in object recognition ( Farhadi et al. , 2009 ), act as excellent substitutes for feature', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, #AUTHOR_TAG show that visual attribute classifiers, which have been immensely successful in object recognition ( Farhadi et al. , 2009 ), act as excellent substitutes for feature', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']",0
['the other parameters are selected as the best from #AUTHOR_TAG'],['the other parameters are selected as the best from #AUTHOR_TAG'],['the other parameters are selected as the best from #AUTHOR_TAG'],"['all settings, we fix all dirichlet priors at 0. 1, use a learning rate 0. 7, and use minibatch sizes of 1024 documents.', 'we do not optimize these hyperparameters or vary them over time.', 'the high dirichlet priors are chosen to prevent sparsity in topic distributions, while the other parameters are selected as the best from #AUTHOR_TAG']",5
"['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words ""']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words ""']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']",0
"['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  az']","['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']","['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  az']","['language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']",0
"['experiments are based on the multimodal extension of latent dirichlet allocation developed by Andrews et al. (2009).', 'previously lda has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #AUTHOR_TAG ).', 'it has also been shown to be useful in joint inference of text with visual']","['experiments are based on the multimodal extension of latent dirichlet allocation developed by Andrews et al. (2009).', 'previously lda has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #AUTHOR_TAG ).', 'it has also been shown to be useful in joint inference of text with visual']","['experiments are based on the multimodal extension of latent dirichlet allocation developed by Andrews et al. (2009).', 'previously lda has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #AUTHOR_TAG ).', 'it has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013).', 'these multimodal lda models ( hereafter, mlda ) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).', 'however, prior work using mlda is limited to two modalities at a time.', 'in this section, we describe bimodal mlda and define two methods for extending it to three or more modalities']","['experiments are based on the multimodal extension of latent dirichlet allocation developed by Andrews et al. (2009).', 'previously lda has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #AUTHOR_TAG ).', 'it has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013).', 'these multimodal lda models ( hereafter, mlda ) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).', 'however, prior work using mlda is limited to two modalities at a time.', 'in this section, we describe bimodal mlda and define two methods for extending it to three or more modalities']",0
"['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010 b']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010 b']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010 b']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']",0
"['a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #AUTHOR_TAG ; Regneri et al. , 2013 )']","['a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #AUTHOR_TAG ; Regneri et al. , 2013 )']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #AUTHOR_TAG ; Regneri et al. , 2013 )']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #AUTHOR_TAG ; Regneri et al. , 2013 )']",0
"['norms ( an ) is a collection of association norms collected by schulte im #AUTHOR_TAG.', 'in association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.', 'with enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.', 'after removing responses given only once in the entire study, the data set contains a total of 95, 214 cue - response pairs for 1, 012 nouns and 5, 716 response types']","['norms ( an ) is a collection of association norms collected by schulte im #AUTHOR_TAG.', 'in association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.', 'with enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.', 'after removing responses given only once in the entire study, the data set contains a total of 95, 214 cue - response pairs for 1, 012 nouns and 5, 716 response types']","['norms ( an ) is a collection of association norms collected by schulte im #AUTHOR_TAG.', 'in association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.', 'with enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.', 'after removing responses given only once in the entire study, the data set contains a total of 95, 214 cue - response pairs for 1, 012 nouns and 5, 716 response types']","['norms ( an ) is a collection of association norms collected by schulte im #AUTHOR_TAG.', 'in association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.', 'with enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.', 'after removing responses given only once in the entire study, the data set contains a total of 95, 214 cue - response pairs for 1, 012 nouns and 5, 716 response types']",5
"['dirichlet allocation ( #AUTHOR_TAG ), or lda, is an unsupervised bayesian probabilistic model of text documents.', 'it assumes that all documents are probabilistically generated from a shared set of k common topics, where each topic is a multinomial distribution over the vocabulary ( notated as β ), and documents are modeled as mixtures of these shared topics ( notated as θ ).', 'lda assumes every document in the corpus is generated using the fol - lowing generative process']","['dirichlet allocation ( #AUTHOR_TAG ), or lda, is an unsupervised bayesian probabilistic model of text documents.', 'it assumes that all documents are probabilistically generated from a shared set of k common topics, where each topic is a multinomial distribution over the vocabulary ( notated as β ), and documents are modeled as mixtures of these shared topics ( notated as θ ).', 'lda assumes every document in the corpus is generated using the fol - lowing generative process']","['dirichlet allocation ( #AUTHOR_TAG ), or lda, is an unsupervised bayesian probabilistic model of text documents.', 'it assumes that all documents are probabilistically generated from a shared set of k common topics, where each topic is a multinomial distribution over the vocabulary ( notated as β ), and documents are modeled as mixtures of these shared topics ( notated as θ ).', 'lda assumes every document in the corpus is generated using the fol - lowing generative process']","['dirichlet allocation ( #AUTHOR_TAG ), or lda, is an unsupervised bayesian probabilistic model of text documents.', 'it assumes that all documents are probabilistically generated from a shared set of k common topics, where each topic is a multinomial distribution over the vocabulary ( notated as β ), and documents are modeled as mixtures of these shared topics ( notated as θ ).', 'lda assumes every document in the corpus is generated using the fol - lowing generative process']",0
"['raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #AUTHOR_TAG ; Artzi and Zettlemoyer , 2013 ),']","['raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #AUTHOR_TAG ; Artzi and Zettlemoyer , 2013 ),']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #AUTHOR_TAG ; Artzi and Zettlemoyer , 2013 ),']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #AUTHOR_TAG ; Artzi and Zettlemoyer , 2013 ), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) ).', 'Griffiths et al. (2007) helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['.', 'to name a few examples, #AUTHOR_TAG and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']","['computer vision community has also benefited greatly from efforts to unify the two modalities.', 'to name a few examples, #AUTHOR_TAG and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']","['computer vision community has also benefited greatly from efforts to unify the two modalities.', 'to name a few examples, #AUTHOR_TAG and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']","['computer vision community has also benefited greatly from efforts to unify the two modalities.', 'to name a few examples, #AUTHOR_TAG and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']",0
"['raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #AUTHOR_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ),']","['raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #AUTHOR_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ),']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #AUTHOR_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ),']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #AUTHOR_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) ).', 'Griffiths et al. (2007) helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', '#AUTHOR_TAG furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperform']","['others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) ).', 'Griffiths et al. (2007) helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', '#AUTHOR_TAG furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of']","['latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', '#AUTHOR_TAG furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperform']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) ).', 'Griffiths et al. (2007) helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', '#AUTHOR_TAG furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; #AUTHOR_TAG ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words ""']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; #AUTHOR_TAG ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words ""']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; #AUTHOR_TAG ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; #AUTHOR_TAG ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']",0
"['obtained using visual classifiers ( #AUTHOR_TAG ).', 'these']","['obtained using visual classifiers ( #AUTHOR_TAG ).', 'these']","['experiments are based on the multimodal extension of latent dirichlet allocation developed by Andrews et al. (2009).', 'previously lda has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012).', 'it has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #AUTHOR_TAG ).', 'these multimodal lda models ( hereafter, mlda ) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).', 'however, prior work using mlda is limited to two modalities at a time.', 'in this section, we describe bimodal mlda and define two methods for extending it to three or more modalities']","['experiments are based on the multimodal extension of latent dirichlet allocation developed by Andrews et al. (2009).', 'previously lda has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012).', 'it has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #AUTHOR_TAG ).', 'these multimodal lda models ( hereafter, mlda ) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).', 'however, prior work using mlda is limited to two modalities at a time.', 'in this section, we describe bimodal mlda and define two methods for extending it to three or more modalities']",0
"['Ordonez et al. , 2011 ), text illustration ( #AUTHOR_TAG ), or']","['Ordonez et al. , 2011 ), text illustration ( #AUTHOR_TAG ), or']","['Ordonez et al. , 2011 ), text illustration ( #AUTHOR_TAG ), or automatic location identification of twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )']","['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010 a ; Ordonez et al. , 2011 ), text illustration ( #AUTHOR_TAG ), or automatic location identification of twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )']",0
"['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions ( Chen and Mooney , 2011 ) or robot commands ( Tellex et al. , 2011 ; #AUTHOR_TAG ).', 'some efforts have']","['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions ( Chen and Mooney , 2011 ) or robot commands ( Tellex et al. , 2011 ; #AUTHOR_TAG ).', 'some efforts have']","['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions ( Chen and Mooney , 2011 ) or robot commands ( Tellex et al. , 2011 ; #AUTHOR_TAG ).', 'some efforts']","['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions ( Chen and Mooney , 2011 ) or robot commands ( Tellex et al. , 2011 ; #AUTHOR_TAG ).', 'some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010 a ; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of twitter users (Eisenstein et al., 2010;Wing and Baldridge, 2011;Roller et al., 2012)']",0
"['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ; #AUTHOR_TAG )']","['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ; #AUTHOR_TAG )']","['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ; #AUTHOR_TAG )']","['language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ; #AUTHOR_TAG )']",0
"['solve these scaling issues, we implement online variational bayesian inference ( Hoffman et al. , 2010 ; #AUTHOR_TAG ) for our models.', 'in variational bayesian inference ( vbi ), one approximates the true posterior using simpler distributions with free variables.', 'the free variables are then optimized in an em - like algorithm to minimize difference between the true and approximate posteriors.', 'online vbi differs from normal vbi by using randomly sampled minibatches in each em step rather than the entire data set.', 'online vbi easily scales and quickly converges in all of our experiments.', 'a listing of the inference algorithm may be found in the supplementary materials and the source code is available as open source']","['solve these scaling issues, we implement online variational bayesian inference ( Hoffman et al. , 2010 ; #AUTHOR_TAG ) for our models.', 'in variational bayesian inference ( vbi ), one approximates the true posterior using simpler distributions with free variables.', 'the free variables are then optimized in an em - like algorithm to minimize difference between the true and approximate posteriors.', 'online vbi differs from normal vbi by using randomly sampled minibatches in each em step rather than the entire data set.', 'online vbi easily scales and quickly converges in all of our experiments.', 'a listing of the inference algorithm may be found in the supplementary materials and the source code is available as open source']","['solve these scaling issues, we implement online variational bayesian inference ( Hoffman et al. , 2010 ; #AUTHOR_TAG ) for our models.', 'in variational bayesian inference ( vbi ), one approximates the true posterior using simpler distributions with free variables.', 'the free variables are then optimized in an em - like algorithm to minimize difference between the true and approximate posteriors.', 'online vbi differs from normal vbi by using randomly sampled minibatches in each em step rather than the entire data set.', 'online vbi easily scales and quickly converges in all of our experiments.', 'a listing of the inference algorithm may be found in the supplementary materials and the source code is available as open source']","['solve these scaling issues, we implement online variational bayesian inference ( Hoffman et al. , 2010 ; #AUTHOR_TAG ) for our models.', 'in variational bayesian inference ( vbi ), one approximates the true posterior using simpler distributions with free variables.', 'the free variables are then optimized in an em - like algorithm to minimize difference between the true and approximate posteriors.', 'online vbi differs from normal vbi by using randomly sampled minibatches in each em step rather than the entire data set.', 'online vbi easily scales and quickly converges in all of our experiments.', 'a listing of the inference algorithm may be found in the supplementary materials and the source code is available as open source']",5
"['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAG a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words ""']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAG a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words ""']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAG a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAG a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']",0
"['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words ""']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words ""']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']",0
"['t - test ).', 'this result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 )']","['1 shows our results for each of our selected models with our compositionality evaluation.', 'the 2d models employing feature norms and association norms do significantly better than the text - only model ( two - tailed t - test ).', 'this result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 )']","['1 shows our results for each of our selected models with our compositionality evaluation.', 'the 2d models employing feature norms and association norms do significantly better than the text - only model ( two - tailed t - test ).', 'this result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 )']","['1 shows our results for each of our selected models with our compositionality evaluation.', 'the 2d models employing feature norms and association norms do significantly better than the text - only model ( two - tailed t - test ).', 'this result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 )']",1
"['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  az']","['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']","['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  az']","['language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']",0
"['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  az']","['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']","['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  az']","['language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']",0
"['latent semantic analysis ( #AUTHOR_TAG ) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperform']","['others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) ).', 'Griffiths et al. ( 2007 ) helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis ( #AUTHOR_TAG ) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of']","['latent semantic analysis ( #AUTHOR_TAG ) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperform']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) ).', 'Griffiths et al. ( 2007 ) helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis ( #AUTHOR_TAG ) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['solve these scaling issues, we implement online variational bayesian inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models.', 'in variational bayesian inference ( vbi ), one approximates the true posterior using simpler distributions with free variables.', 'the free variables are then optimized in an em - like algorithm to minimize difference between the true and approximate posteriors.', 'online vbi differs from normal vbi by using randomly sampled minibatches in each em step rather than the entire data set.', 'online vbi easily scales and quickly converges in all of our experiments.', 'a listing of the inference algorithm may be found in the supplementary materials and the source code is available as open source']","['solve these scaling issues, we implement online variational bayesian inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models.', 'in variational bayesian inference ( vbi ), one approximates the true posterior using simpler distributions with free variables.', 'the free variables are then optimized in an em - like algorithm to minimize difference between the true and approximate posteriors.', 'online vbi differs from normal vbi by using randomly sampled minibatches in each em step rather than the entire data set.', 'online vbi easily scales and quickly converges in all of our experiments.', 'a listing of the inference algorithm may be found in the supplementary materials and the source code is available as open source']","['solve these scaling issues, we implement online variational bayesian inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models.', 'in variational bayesian inference ( vbi ), one approximates the true posterior using simpler distributions with free variables.', 'the free variables are then optimized in an em - like algorithm to minimize difference between the true and approximate posteriors.', 'online vbi differs from normal vbi by using randomly sampled minibatches in each em step rather than the entire data set.', 'online vbi easily scales and quickly converges in all of our experiments.', 'a listing of the inference algorithm may be found in the supplementary materials and the source code is available as open source']","['solve these scaling issues, we implement online variational bayesian inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models.', 'in variational bayesian inference ( vbi ), one approximates the true posterior using simpler distributions with free variables.', 'the free variables are then optimized in an em - like algorithm to minimize difference between the true and approximate posteriors.', 'online vbi differs from normal vbi by using randomly sampled minibatches in each em step rather than the entire data set.', 'online vbi easily scales and quickly converges in all of our experiments.', 'a listing of the inference algorithm may be found in the supplementary materials and the source code is available as open source']",5
"['norms.', 'the first work to do this with topic models is #AUTHOR_TAG b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '']","['norms.', 'the first work to do this with topic models is #AUTHOR_TAG b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is #AUTHOR_TAG b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is #AUTHOR_TAG b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']",0
"['this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.', 'the model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of latent dirichlet allocation.', 'this model has previously been shown to provide excellent performance on multiple tasks, including prediction of association norms, word substitution errors, semantic inferences, and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ).', 'while prior work has used the model only with feature norms and visual attributes, we show that low - level image features are directly compatible with the model and provide improved representations of word meaning.', 'we also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features.', 'finally, we describe two ways to extend the model by incorporating three or more modalities.', 'we find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning.', 'we release both our code and data to the community for future research.']","['this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.', 'the model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of latent dirichlet allocation.', 'this model has previously been shown to provide excellent performance on multiple tasks, including prediction of association norms, word substitution errors, semantic inferences, and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ).', 'while prior work has used the model only with feature norms and visual attributes, we show that low - level image features are directly compatible with the model and provide improved representations of word meaning.', 'we also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features.', 'finally, we describe two ways to extend the model by incorporating three or more modalities.', 'we find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning.', 'we release both our code and data to the community for future research.']","['this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.', 'the model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of latent dirichlet allocation.', 'this model has previously been shown to provide excellent performance on multiple tasks, including prediction of association norms, word substitution errors, semantic inferences, and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ).', 'while prior work has used the model only with feature norms and visual attributes, we show that low - level image features are directly compatible with the model and provide improved representations of word meaning.', 'we also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features.', 'finally, we describe two ways to extend the model by incorporating three or more modalities.', 'we find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning.', 'we release both our code and data to the community for future research.']","['this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.', 'the model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of latent dirichlet allocation.', 'this model has previously been shown to provide excellent performance on multiple tasks, including prediction of association norms, word substitution errors, semantic inferences, and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ).', 'while prior work has used the model only with feature norms and visual attributes, we show that low - level image features are directly compatible with the model and provide improved representations of word meaning.', 'we also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features.', 'finally, we describe two ways to extend the model by incorporating three or more modalities.', 'we find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning.', 'we release both our code and data to the community for future research.']",2
"['we made previously ( #AUTHOR_TAG a ) : since each post in a sequence is a reply to the preceding post, we could exploit their']","['we made previously ( #AUTHOR_TAG a ) : since each post in a sequence is a reply to the preceding post, we could exploit their']","['a stance sequence, with one stance la - bel for each post in the input post sequence.', 'this choice is motivated by an observation we made previously ( #AUTHOR_TAG a ) : since each post in a sequence is a reply to the preceding post, we could exploit their']","['p2, on the other hand, we recast sc as a se - quence labeling task.', 'in other words, we train a sc model that assumes as input a post sequence and outputs a stance sequence, with one stance la - bel for each post in the input post sequence.', 'this choice is motivated by an observation we made previously ( #AUTHOR_TAG a ) : since each post in a sequence is a reply to the preceding post, we could exploit their dependencies by determining their stance labels together.']",2
"['- semantic features.', 'while dependencybased features capture the syntactic dependencies, frame - semantic features encode the semantic representation of the concepts in a sentence.', 'following our previous work on stance classification ( #AUTHOR_TAG c ), we employ three types of features computed based on the frame - semantic parse of each sentence in a post obtained from semafor ( Das et al. , 2010 ).', 'frame - word interaction features encode whether two words appear in different elements of the same frame.', 'hence, each frame - word interaction feature consists of ( 1 ) the name of the frame f from which it is created,']","['- semantic features.', 'while dependencybased features capture the syntactic dependencies, frame - semantic features encode the semantic representation of the concepts in a sentence.', 'following our previous work on stance classification ( #AUTHOR_TAG c ), we employ three types of features computed based on the frame - semantic parse of each sentence in a post obtained from semafor ( Das et al. , 2010 ).', 'frame - word interaction features encode whether two words appear in different elements of the same frame.', 'hence, each frame - word interaction feature consists of ( 1 ) the name of the frame f from which it is created,']","['- semantic features.', 'while dependencybased features capture the syntactic dependencies, frame - semantic features encode the semantic representation of the concepts in a sentence.', 'following our previous work on stance classification ( #AUTHOR_TAG c ), we employ three types of features computed based on the frame - semantic parse of each sentence in a post obtained from semafor ( Das et al. , 2010 ).', 'frame - word interaction features encode whether two words appear in different elements of the same frame.', 'hence, each frame - word interaction feature consists of ( 1 ) the name of the frame f from which it is created,']","['- semantic features.', 'while dependencybased features capture the syntactic dependencies, frame - semantic features encode the semantic representation of the concepts in a sentence.', 'following our previous work on stance classification ( #AUTHOR_TAG c ), we employ three types of features computed based on the frame - semantic parse of each sentence in a post obtained from semafor ( Das et al. , 2010 ).', 'frame - word interaction features encode whether two words appear in different elements of the same frame.', 'hence, each frame - word interaction feature consists of ( 1 ) the name of the frame f from which it is created,']",2
"['follows our previous work #AUTHOR_TAG a ) comparing scratch translation to post - edit.', 'many research translation uis have been proposed including transtype (Langlais et al., 2000), caitra (Koehn, 2009 b ), thot ( ortiz - martinez and Casacuberta, 2014), transcenter (Denkowski et al., 2014 b ), and casmacat (Alabau et al., 2013).', 'however, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative']","['who compared scratch, post - edit, and simple interactive modes.', 'however, he used undergraduate, non - professional subjects, and did not consider re - tuning.', 'our experimental design with professional bilingual translators follows our previous work #AUTHOR_TAG a ) comparing scratch translation to post - edit.', 'many research translation uis have been proposed including transtype (Langlais et al., 2000), caitra (Koehn, 2009 b ), thot ( ortiz - martinez and Casacuberta, 2014), transcenter (Denkowski et al., 2014 b ), and casmacat (Alabau et al., 2013).', 'however, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative']","['process study most similar to ours is that of Koehn (2009 a ), who compared scratch, post - edit, and simple interactive modes.', 'however, he used undergraduate, non - professional subjects, and did not consider re - tuning.', 'our experimental design with professional bilingual translators follows our previous work #AUTHOR_TAG a ) comparing scratch translation to post - edit.', 'many research translation uis have been proposed including transtype (Langlais et al., 2000), caitra (Koehn, 2009 b ), thot ( ortiz - martinez and Casacuberta, 2014), transcenter (Denkowski et al., 2014 b ), and casmacat (Alabau et al., 2013).', 'however, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the hci literature']","['process study most similar to ours is that of Koehn (2009 a ), who compared scratch, post - edit, and simple interactive modes.', 'however, he used undergraduate, non - professional subjects, and did not consider re - tuning.', 'our experimental design with professional bilingual translators follows our previous work #AUTHOR_TAG a ) comparing scratch translation to post - edit.', 'many research translation uis have been proposed including transtype (Langlais et al., 2000), caitra (Koehn, 2009 b ), thot ( ortiz - martinez and Casacuberta, 2014), transcenter (Denkowski et al., 2014 b ), and casmacat (Alabau et al., 2013).', 'however, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the hci literature']",2
"[""1 shows the pearson's product correlation between each topical feature and candidate's power."", 'we obtain a highly significant ( p = 0. 002 ) negative correlation between topic shift tendency of a candidate ( pi ) and his / her power.', ""in other words, the variation in the topic shifting tendencies is significantly correlated with the candidates'recent poll standings."", 'candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often.', 'this is in line with our previous findings from ( #AUTHOR_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators.', 'it is also in line with the findings by Prabhakaran et al. (2013']","[""1 shows the pearson's product correlation between each topical feature and candidate's power."", 'we obtain a highly significant ( p = 0. 002 ) negative correlation between topic shift tendency of a candidate ( pi ) and his / her power.', ""in other words, the variation in the topic shifting tendencies is significantly correlated with the candidates'recent poll standings."", 'candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often.', 'this is in line with our previous findings from ( #AUTHOR_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators.', 'it is also in line with the findings by Prabhakaran et al. (2013']","[""1 shows the pearson's product correlation between each topical feature and candidate's power."", 'we obtain a highly significant ( p = 0. 002 ) negative correlation between topic shift tendency of a candidate ( pi ) and his / her power.', ""in other words, the variation in the topic shifting tendencies is significantly correlated with the candidates'recent poll standings."", 'candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often.', 'this is in line with our previous findings from ( #AUTHOR_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators.', 'it is also in line with the findings by']","[""1 shows the pearson's product correlation between each topical feature and candidate's power."", 'we obtain a highly significant ( p = 0. 002 ) negative correlation between topic shift tendency of a candidate ( pi ) and his / her power.', ""in other words, the variation in the topic shifting tendencies is significantly correlated with the candidates'recent poll standings."", 'candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often.', 'this is in line with our previous findings from ( #AUTHOR_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators.', 'it is also in line with the findings by Prabhakaran et al. (2013 a ) that candidates with higher power tend not to interrupt others.', 'on the other hand, we did not obtain any significant correlation for the features proposed by Nguyen et al. (2013)']",1
"['or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975;Prince, 1981;Gundel et al., 1993; lobner, 1998 ).', 'Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun ( different - head coreference ).', 'we follow our previous work ( #AUTHOR_TAG b ) and']","['or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975;Prince, 1981;Gundel et al., 1993; lobner, 1998 ).', 'Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun ( different - head coreference ).', 'we follow our previous work ( #AUTHOR_TAG b ) and']","['or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975;Prince, 1981;Gundel et al., 1993; lobner, 1998 ).', 'Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun ( different - head coreference ).', 'we follow our previous work ( #AUTHOR_TAG b ) and']","['or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975;Prince, 1981;Gundel et al., 1993; lobner, 1998 ).', 'Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun ( different - head coreference ).', 'we follow our previous work ( #AUTHOR_TAG b ) and restrict bridging to non - coreferential cases.', 'we also exclude comparative anaphora (Modjeska et al., 2003)']",2
"['in isnotes, 71 % of np antecedents occur in the same or up to two sentences prior to the anaphor.', 'initial experiments show that increasing the window size more than two sentences decreases the performance. 8', 'to deal with data imbalance, the svm light parameter is set according to the ratio between positive and negative instances in the training set. 9', 'to compare the learning - based approach to the rulebased system described in section 3 directly, we report the mlsystem rulefeats we provide mlsystem rulefeats with the same knowledge resources as the rule - based system.', 'all rules from the rule - based system are incorporated into mlsystem rulefeats as the features.', 'mlsystem rulefeats + atomfeats we augment mlsystem rulefeats with more features from our previous work ( Markert et al. , 2012 ; #AUTHOR_TAG a']","['in isnotes, 71 % of np antecedents occur in the same or up to two sentences prior to the anaphor.', 'initial experiments show that increasing the window size more than two sentences decreases the performance. 8', 'to deal with data imbalance, the svm light parameter is set according to the ratio between positive and negative instances in the training set. 9', 'to compare the learning - based approach to the rulebased system described in section 3 directly, we report the mlsystem rulefeats we provide mlsystem rulefeats with the same knowledge resources as the rule - based system.', 'all rules from the rule - based system are incorporated into mlsystem rulefeats as the features.', 'mlsystem rulefeats + atomfeats we augment mlsystem rulefeats with more features from our previous work ( Markert et al. , 2012 ; #AUTHOR_TAG a']","['in isnotes, 71 % of np antecedents occur in the same or up to two sentences prior to the anaphor.', 'initial experiments show that increasing the window size more than two sentences decreases the performance. 8', 'to deal with data imbalance, the svm light parameter is set according to the ratio between positive and negative instances in the training set. 9', 'to compare the learning - based approach to the rulebased system described in section 3 directly, we report the mlsystem rulefeats we provide mlsystem rulefeats with the same knowledge resources as the rule - based system.', 'all rules from the rule - based system are incorporated into mlsystem rulefeats as the features.', 'mlsystem rulefeats + atomfeats we augment mlsystem rulefeats with more features from our previous work ( Markert et al. , 2012 ; #AUTHOR_TAG a']","['in isnotes, 71 % of np antecedents occur in the same or up to two sentences prior to the anaphor.', 'initial experiments show that increasing the window size more than two sentences decreases the performance. 8', 'to deal with data imbalance, the svm light parameter is set according to the ratio between positive and negative instances in the training set. 9', 'to compare the learning - based approach to the rulebased system described in section 3 directly, we report the mlsystem rulefeats we provide mlsystem rulefeats with the same knowledge resources as the rule - based system.', 'all rules from the rule - based system are incorporated into mlsystem rulefeats as the features.', 'mlsystem rulefeats + atomfeats we augment mlsystem rulefeats with more features from our previous work ( Markert et al. , 2012 ; #AUTHOR_TAG a ; Hou et al. , 2013 b ) on bridging anaphora recognition and antecedent selection.', 'some of these features overlap with the atomic features used in the rule - based system']",2
"['probability model we use is generative and history - based.', 'generative models are expressed in terms of a stochastic process which generates both the phrase structure tree and the input sentence.', 'at each step, the process chooses a characteristic of the tree or predicts a word in the sentence.', 'this sequence of decisions is the derivation of the tree, which we will denote d1,..., dm.', ""because there is a one - to - one mapping from phrase structure trees to our derivations, the probability of a derivation p ( di,..., dm ) is equal to the joint probability of the derivation's tree and the input sentence."", 'the probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation.', 'in history - based models ( #AUTHOR_TAG ), the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1,..., d, _ 1, which is called the derivation history at step i.', 'this allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions']","['probability model we use is generative and history - based.', 'generative models are expressed in terms of a stochastic process which generates both the phrase structure tree and the input sentence.', 'at each step, the process chooses a characteristic of the tree or predicts a word in the sentence.', 'this sequence of decisions is the derivation of the tree, which we will denote d1,..., dm.', ""because there is a one - to - one mapping from phrase structure trees to our derivations, the probability of a derivation p ( di,..., dm ) is equal to the joint probability of the derivation's tree and the input sentence."", 'the probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation.', 'in history - based models ( #AUTHOR_TAG ), the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1,..., d, _ 1, which is called the derivation history at step i.', 'this allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions']","['probability model we use is generative and history - based.', 'generative models are expressed in terms of a stochastic process which generates both the phrase structure tree and the input sentence.', 'at each step, the process chooses a characteristic of the tree or predicts a word in the sentence.', 'this sequence of decisions is the derivation of the tree, which we will denote d1,..., dm.', ""because there is a one - to - one mapping from phrase structure trees to our derivations, the probability of a derivation p ( di,..., dm ) is equal to the joint probability of the derivation's tree and the input sentence."", 'the probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation.', 'in history - based models ( #AUTHOR_TAG ), the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1,..., d, _ 1, which is called the derivation history at step i.', 'this allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions']","['probability model we use is generative and history - based.', 'generative models are expressed in terms of a stochastic process which generates both the phrase structure tree and the input sentence.', 'at each step, the process chooses a characteristic of the tree or predicts a word in the sentence.', 'this sequence of decisions is the derivation of the tree, which we will denote d1,..., dm.', ""because there is a one - to - one mapping from phrase structure trees to our derivations, the probability of a derivation p ( di,..., dm ) is equal to the joint probability of the derivation's tree and the input sentence."", 'the probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation.', 'in history - based models ( #AUTHOR_TAG ), the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1,..., d, _ 1, which is called the derivation history at step i.', 'this allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions']",5
"['most important step in designing a statistical parser with a history - based probability model is choosing a method for estimating the parameters d, _ 1 ).', 'the main difficulty with this estimation is that the history d 1,..., di _ 1 is of unbounded length.', 'most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG )']","['most important step in designing a statistical parser with a history - based probability model is choosing a method for estimating the parameters d, _ 1 ).', 'the main difficulty with this estimation is that the history d 1,..., di _ 1 is of unbounded length.', 'most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG )']","['most important step in designing a statistical parser with a history - based probability model is choosing a method for estimating the parameters d, _ 1 ).', 'the main difficulty with this estimation is that the history d 1,..., di _ 1 is of unbounded length.', 'most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG )']","['most important step in designing a statistical parser with a history - based probability model is choosing a method for estimating the parameters d, _ 1 ).', 'the main difficulty with this estimation is that the history d 1,..., di _ 1 is of unbounded length.', 'most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG )']",1
['##a11 our results are computed with the evalb program following the now - standard criteria in ( #AUTHOR_TAG )'],['##a11 our results are computed with the evalb program following the now - standard criteria in ( #AUTHOR_TAG )'],['##a11 our results are computed with the evalb program following the now - standard criteria in ( #AUTHOR_TAG )'],['##a11 our results are computed with the evalb program following the now - standard criteria in ( #AUTHOR_TAG )'],5
"['statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2001 ) are based on a history - based probability model ( #AUTHOR_TAG ), where the probability of each decision in a parse is conditioned on the previous decisions in the parse.', 'a major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'previous approaches have used a hand - crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001).', 'in the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'we perform this induction using an artificial neural network architecture, called simple synchrony networks ( ssns ) (Lane and Henderson, 2001;Henderson, 2000).', 'because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation.', 'the resulting parser achieves performance far greater than previous approaches to neural network parsing (Ho and Chan, 1999;Costa et al., 2001), and only marginally below the current state - of - the - art for parsing the penn treebank']","['statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2001 ) are based on a history - based probability model ( #AUTHOR_TAG ), where the probability of each decision in a parse is conditioned on the previous decisions in the parse.', 'a major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'previous approaches have used a hand - crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001).', 'in the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'we perform this induction using an artificial neural network architecture, called simple synchrony networks ( ssns ) (Lane and Henderson, 2001;Henderson, 2000).', 'because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation.', 'the resulting parser achieves performance far greater than previous approaches to neural network parsing (Ho and Chan, 1999;Costa et al., 2001), and only marginally below the current state - of - the - art for parsing the penn treebank']","['statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2001 ) are based on a history - based probability model ( #AUTHOR_TAG ), where the probability of each decision in a parse is conditioned on the previous decisions in the parse.', 'a major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'previous approaches have used a hand - crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001).', 'in the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'we perform this induction using an artificial neural network architecture, called simple synchrony networks ( ssns ) (Lane and Henderson, 2001;Henderson, 2000).', 'because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation.', 'the resulting parser achieves performance far greater than previous approaches to neural network parsing (Ho and Chan, 1999;Costa et al., 2001), and only marginally below the current state - of - the - art for parsing the penn treebank']","['statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2001 ) are based on a history - based probability model ( #AUTHOR_TAG ), where the probability of each decision in a parse is conditioned on the previous decisions in the parse.', 'a major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'previous approaches have used a hand - crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001).', 'in the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'we perform this induction using an artificial neural network architecture, called simple synchrony networks ( ssns ) (Lane and Henderson, 2001;Henderson, 2000).', 'because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation.', 'the resulting parser achieves performance far greater than previous approaches to neural network parsing (Ho and Chan, 1999;Costa et al., 2001), and only marginally below the current state - of - the - art for parsing the penn treebank']",0
"['most important step in designing a statistical parser with a history - based probability model is choosing a method for estimating the parameters d, _ 1 ).', 'the main difficulty with this estimation is that the history d 1,..., di _ 1 is of unbounded length.', 'most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2000 )']","['most important step in designing a statistical parser with a history - based probability model is choosing a method for estimating the parameters d, _ 1 ).', 'the main difficulty with this estimation is that the history d 1,..., di _ 1 is of unbounded length.', 'most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2000 )']","['most important step in designing a statistical parser with a history - based probability model is choosing a method for estimating the parameters d, _ 1 ).', 'the main difficulty with this estimation is that the history d 1,..., di _ 1 is of unbounded length.', 'most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2000 )']","['most important step in designing a statistical parser with a history - based probability model is choosing a method for estimating the parameters d, _ 1 ).', 'the main difficulty with this estimation is that the history d 1,..., di _ 1 is of unbounded length.', 'most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2000 )']",1
"[""feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as collins'previous work on re - ranking using a finite set of features ( #AUTHOR_TAG )""]","[""feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as collins'previous work on re - ranking using a finite set of features ( #AUTHOR_TAG )""]","['a single node with a new label ( becoming [ s - vp... ] ).', 'these transforms are undone before any evaluation is performed on the output trees.', 'we do not believe these transforms have a major impact on performance, but we have not currently run tests without them.', 'feature sets, but then efficiency becomes a problem.', ""Collins and Duffy ( 2002 ) define a kernel over parse trees and apply it to re - ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as collins'previous work on re - ranking using a finite set of features ( #AUTHOR_TAG )""]","['probabilities are then assumed to be independent of all the infoimation about the history which is not captured by the chosen features.', 'the difficulty with this approach is that the choice of features can have a large impact on the performance of the system, but it is not feasible to search the space of possible feature sets by hand.', 'one alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded 2 we extended the left - corner parsing model in a few minor ways using grammar transforms.', 'we replace chomsky adjunction structures ( i. e.', 'structures of the form [ x [ x... ] [ y... ] ] ) with a special "" modifier "" link in the tree ( becoming [ x... [ mod y • requiring nodes which are popped from the stack to choose between attaching with a normal link or a modifier link.', 'we also compiled some frequent chains of non - branching nodes ( such as [ s [ vp... 1 ] ) into a single node with a new label ( becoming [ s - vp... ] ).', 'these transforms are undone before any evaluation is performed on the output trees.', 'we do not believe these transforms have a major impact on performance, but we have not currently run tests without them.', 'feature sets, but then efficiency becomes a problem.', ""Collins and Duffy ( 2002 ) define a kernel over parse trees and apply it to re - ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as collins'previous work on re - ranking using a finite set of features ( #AUTHOR_TAG )""]",0
"[""child, if any ), and top's most recent child ( which was top, _ 1, if any )."", 'for right - branching structures, the leftcorner ancestor is the parent, conditioning on which has been found to be beneficial ( #AUTHOR_TAG ), as has conditioning on the left']","['f is that we want the inductive bias to reflect structural locality.', 'for this reason, d ( top ) includes nodes which are structurally local to top,.', ""these nodes are the left - corner ancestor of top, ( which is below top, on the stack ), top's left - corner child ( its leftmost child, if any ), and top's most recent child ( which was top, _ 1, if any )."", 'for right - branching structures, the leftcorner ancestor is the parent, conditioning on which has been found to be beneficial ( #AUTHOR_TAG ), as has conditioning on the left - corner child ( Roark and Johnson , 1999 ).', 'because these inputs include the history features of both the left - corner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i 1, and thus ( by induction ) any information from the entire previous derivation history could in principle be stored in the history features.', 'thus this model is making']","[""child, if any ), and top's most recent child ( which was top, _ 1, if any )."", 'for right - branching structures, the leftcorner ancestor is the parent, conditioning on which has been found to be beneficial ( #AUTHOR_TAG ), as has conditioning on the left - corner child ( Roark and Johnson , 1999 ).', 'because these inputs include the history features of both the left - corner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i 1, and thus ( by induction ) any information from the entire previous derivation history could in principle be stored in the history features.', 'thus this model is making']","['principle we apply when designing d ( top, ) and f is that we want the inductive bias to reflect structural locality.', 'for this reason, d ( top ) includes nodes which are structurally local to top,.', ""these nodes are the left - corner ancestor of top, ( which is below top, on the stack ), top's left - corner child ( its leftmost child, if any ), and top's most recent child ( which was top, _ 1, if any )."", 'for right - branching structures, the leftcorner ancestor is the parent, conditioning on which has been found to be beneficial ( #AUTHOR_TAG ), as has conditioning on the left - corner child ( Roark and Johnson , 1999 ).', 'because these inputs include the history features of both the left - corner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i 1, and thus ( by induction ) any information from the entire previous derivation history could in principle be stored in the history features.', 'thus this model is making no a priori hard independence assumptions, just a priori soft biases']",0
"[').', 'these transforms are undone before any evaluation is performed on the output trees.', 'we do not believe these transforms have a major impact on performance, but we have not currently run tests without them.', 'feature sets, but then efficiency becomes a problem.', ""#AUTHOR_TAG define a kernel over parse trees and apply it to re - ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as collins'previous work on re - ranking using a finite set of features ( Collins , 2000 )""]","['a single node with a new label ( becoming [ s - vp... ] ).', 'these transforms are undone before any evaluation is performed on the output trees.', 'we do not believe these transforms have a major impact on performance, but we have not currently run tests without them.', 'feature sets, but then efficiency becomes a problem.', ""#AUTHOR_TAG define a kernel over parse trees and apply it to re - ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as collins'previous work on re - ranking using a finite set of features ( Collins , 2000 )""]","['a single node with a new label ( becoming [ s - vp... ] ).', 'these transforms are undone before any evaluation is performed on the output trees.', 'we do not believe these transforms have a major impact on performance, but we have not currently run tests without them.', 'feature sets, but then efficiency becomes a problem.', ""#AUTHOR_TAG define a kernel over parse trees and apply it to re - ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as collins'previous work on re - ranking using a finite set of features ( Collins , 2000 )""]","['probabilities are then assumed to be independent of all the infoimation about the history which is not captured by the chosen features.', 'the difficulty with this approach is that the choice of features can have a large impact on the performance of the system, but it is not feasible to search the space of possible feature sets by hand.', 'one alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded 2 we extended the left - corner parsing model in a few minor ways using grammar transforms.', 'we replace chomsky adjunction structures ( i. e.', 'structures of the form [ x [ x... ] [ y... ] ] ) with a special "" modifier "" link in the tree ( becoming [ x... [ mod y • requiring nodes which are popped from the stack to choose between attaching with a normal link or a modifier link.', 'we also compiled some frequent chains of non - branching nodes ( such as [ s [ vp... 1 ] ) into a single node with a new label ( becoming [ s - vp... ] ).', 'these transforms are undone before any evaluation is performed on the output trees.', 'we do not believe these transforms have a major impact on performance, but we have not currently run tests without them.', 'feature sets, but then efficiency becomes a problem.', ""#AUTHOR_TAG define a kernel over parse trees and apply it to re - ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as collins'previous work on re - ranking using a finite set of features ( Collins , 2000 )""]",0
"['compatibility with other parsers, we did not use the hand - corrected tags which come with the corpus.', 'we used a publicly available tagger ( #AUTHOR_TAG ) to tag the words and then used these in the input to the system']","['compatibility with other parsers, we did not use the hand - corrected tags which come with the corpus.', 'we used a publicly available tagger ( #AUTHOR_TAG ) to tag the words and then used these in the input to the system']","['compatibility with other parsers, we did not use the hand - corrected tags which come with the corpus.', 'we used a publicly available tagger ( #AUTHOR_TAG ) to tag the words and then used these in the input to the system']","['these experiments the tags are included in the input to the system, but, for compatibility with other parsers, we did not use the hand - corrected tags which come with the corpus.', 'we used a publicly available tagger ( #AUTHOR_TAG ) to tag the words and then used these in the input to the system']",5
"['of table 1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ; Collins , 2000 ; Bod , 2001 ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexical']","['of table 1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ; Collins , 2000 ; Bod , 2001 ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexicalized model.', 'the ssn parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'it is also achieved without any explicit notion of lexical head']","['bottom panel of table 1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ; Collins , 2000 ; Bod , 2001 ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexicalized model.', 'the ssn parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'it is also achieved without any explicit notion of lexical head']","['bottom panel of table 1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ; Collins , 2000 ; Bod , 2001 ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexicalized model.', 'the ssn parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'it is also achieved without any explicit notion of lexical head']",1
"['statistical parsers ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2001 ) are based on a history - based probability model ( Black et al. , 1993 ), where the probability of each decision in a parse is conditioned on the previous decisions in the parse.', 'a major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'previous approaches have used a hand - crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001).', 'in the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'we perform this induction using an artificial neural network architecture, called simple synchrony networks ( ssns ) (Lane and Henderson, 2001;Henderson, 2000).', 'because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation.', 'the resulting parser achieves performance far greater than previous approaches to neural network parsing (Ho and Chan, 1999;Costa et al., 2001), and only marginally below the current state - of - the - art for parsing the penn treebank']","['statistical parsers ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2001 ) are based on a history - based probability model ( Black et al. , 1993 ), where the probability of each decision in a parse is conditioned on the previous decisions in the parse.', 'a major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'previous approaches have used a hand - crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001).', 'in the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'we perform this induction using an artificial neural network architecture, called simple synchrony networks ( ssns ) (Lane and Henderson, 2001;Henderson, 2000).', 'because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation.', 'the resulting parser achieves performance far greater than previous approaches to neural network parsing (Ho and Chan, 1999;Costa et al., 2001), and only marginally below the current state - of - the - art for parsing the penn treebank']","['statistical parsers ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2001 ) are based on a history - based probability model ( Black et al. , 1993 ), where the probability of each decision in a parse is conditioned on the previous decisions in the parse.', 'a major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'previous approaches have used a hand - crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001).', 'in the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'we perform this induction using an artificial neural network architecture, called simple synchrony networks ( ssns ) (Lane and Henderson, 2001;Henderson, 2000).', 'because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation.', 'the resulting parser achieves performance far greater than previous approaches to neural network parsing (Ho and Chan, 1999;Costa et al., 2001), and only marginally below the current state - of - the - art for parsing the penn treebank']","['statistical parsers ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2001 ) are based on a history - based probability model ( Black et al. , 1993 ), where the probability of each decision in a parse is conditioned on the previous decisions in the parse.', 'a major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'previous approaches have used a hand - crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001).', 'in the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'we perform this induction using an artificial neural network architecture, called simple synchrony networks ( ssns ) (Lane and Henderson, 2001;Henderson, 2000).', 'because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation.', 'the resulting parser achieves performance far greater than previous approaches to neural network parsing (Ho and Chan, 1999;Costa et al., 2001), and only marginally below the current state - of - the - art for parsing the penn treebank']",0
"['has conditioning on the left - corner child ( #AUTHOR_TAG ).', 'because these inputs include the history features of both the leftcorner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i - 1, and thus ( by induction ) any information from the entire previous derivation history could in']","['has conditioning on the left - corner child ( #AUTHOR_TAG ).', 'because these inputs include the history features of both the leftcorner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i - 1, and thus ( by induction ) any information from the entire previous derivation history could in']","['has conditioning on the left - corner child ( #AUTHOR_TAG ).', 'because these inputs include the history features of both the leftcorner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i - 1, and thus ( by induction ) any information from the entire previous derivation history could in']","['principle we apply when designing d ( top, ) and f is that we want the inductive bias to reflect structural locality.', 'for this reason, d ( top ) includes nodes which are structurally local to top,.', ""these nodes are the left - corner ancestor of top, ( which is below top, on the stack ), top's left - corner child ( its leftmost child, if any ), and top's most recent child ( which was top, _ 1, if any )."", 'for right - branching structures, the leftcorner ancestor is the parent, conditioning on which has been found to be beneficial ( Johnson , 1998 ), as has conditioning on the left - corner child ( #AUTHOR_TAG ).', 'because these inputs include the history features of both the leftcorner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i - 1, and thus ( by induction ) any information from the entire previous derivation history could in principle be stored in the history features.', 'thus this model is making no a priori hard independence assumptions, just a priori soft biases']",0
"['of table 1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexical']","['of table 1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexicalized model.', 'the ssn parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'it is also achieved without any explicit notion of lexical head']","['bottom panel of table 1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexicalized model.', 'the ssn parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'it is also achieved without any explicit notion of lexical head']","['bottom panel of table 1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexicalized model.', 'the ssn parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'it is also achieved without any explicit notion of lexical head']",1
"['of table 1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexical']","['of table 1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexicalized model.', 'the ssn parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'it is also achieved without any explicit notion of lexical head']","['bottom panel of table 1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexicalized model.', 'the ssn parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'it is also achieved without any explicit notion of lexical head']","['bottom panel of table 1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexicalized model.', 'the ssn parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'it is also achieved without any explicit notion of lexical head']",1
"['inducing a finite set of features for representing the derivation history.', 'the method is a form of multi - layered artificial neural network called simple synchrony networks (Lane and Henderson, 2001;Henderson, 2000).', 'the outputs of this network are probability estimates computed with a log - linear model ( also known as a maximum entropy model ), as is done in (Ratnaparkhi, 1999).', 'log - linear models have proved successful in a wide variety of applications, and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ).', 'the difference from previous approaches is in the nature of the input to the log - linear model.', 'we do not use handcrafted features, but instead we use a finite vector of real - valued features which are induced as part of the neural network training process.', 'these induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities.', 'in neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history features']","['inducing a finite set of features for representing the derivation history.', 'the method is a form of multi - layered artificial neural network called simple synchrony networks (Lane and Henderson, 2001;Henderson, 2000).', 'the outputs of this network are probability estimates computed with a log - linear model ( also known as a maximum entropy model ), as is done in (Ratnaparkhi, 1999).', 'log - linear models have proved successful in a wide variety of applications, and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ).', 'the difference from previous approaches is in the nature of the input to the log - linear model.', 'we do not use handcrafted features, but instead we use a finite vector of real - valued features which are induced as part of the neural network training process.', 'these induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities.', 'in neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history features']","['this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'the method is a form of multi - layered artificial neural network called simple synchrony networks (Lane and Henderson, 2001;Henderson, 2000).', 'the outputs of this network are probability estimates computed with a log - linear model ( also known as a maximum entropy model ), as is done in (Ratnaparkhi, 1999).', 'log - linear models have proved successful in a wide variety of applications, and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ).', 'the difference from previous approaches is in the nature of the input to the log - linear model.', 'we do not use handcrafted features, but instead we use a finite vector of real - valued features which are induced as part of the neural network training process.', 'these induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities.', 'in neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history features']","['this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'the method is a form of multi - layered artificial neural network called simple synchrony networks (Lane and Henderson, 2001;Henderson, 2000).', 'the outputs of this network are probability estimates computed with a log - linear model ( also known as a maximum entropy model ), as is done in (Ratnaparkhi, 1999).', 'log - linear models have proved successful in a wide variety of applications, and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ).', 'the difference from previous approaches is in the nature of the input to the log - linear model.', 'we do not use handcrafted features, but instead we use a finite vector of real - valued features which are induced as part of the neural network training process.', 'these induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities.', 'in neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history features']",1
"['2. results of sl - dop and ls - dop on the wsj ( sentences 100 words ) note that there is an increase in accuracy for both sl - dop and ls - dop if the value of n increases from 1 to 12.', 'but while the accuracy of sl - dop decreases after n = 14 and converges to simplicity - dop, the accuracy of ls - dop continues to increase and converges to likelihood - dop.', 'the highest accuracy is obtained by sl - dop at 12 n 14 : an lp of 90. 8 % and an lr of 90. 7 %.', 'this is roughly an 11 % relative reduction in error rate over #AUTHOR_TAG and bods pcfg - reduction reported in table 1.', 'compared to the reranking technique in Collins (2000), who obtained an lp of 89. 9 % and an lr of 89. 6 %, our results show a 9 % relative error rate reduction.', 'while sl - dop and ls - dop have been compared before']","['2. results of sl - dop and ls - dop on the wsj ( sentences 100 words ) note that there is an increase in accuracy for both sl - dop and ls - dop if the value of n increases from 1 to 12.', 'but while the accuracy of sl - dop decreases after n = 14 and converges to simplicity - dop, the accuracy of ls - dop continues to increase and converges to likelihood - dop.', 'the highest accuracy is obtained by sl - dop at 12 n 14 : an lp of 90. 8 % and an lr of 90. 7 %.', 'this is roughly an 11 % relative reduction in error rate over #AUTHOR_TAG and bods pcfg - reduction reported in table 1.', 'compared to the reranking technique in Collins (2000), who obtained an lp of 89. 9 % and an lr of 89. 6 %, our results show a 9 % relative error rate reduction.', 'while sl - dop and ls - dop have been compared before']","['2. results of sl - dop and ls - dop on the wsj ( sentences 100 words ) note that there is an increase in accuracy for both sl - dop and ls - dop if the value of n increases from 1 to 12.', 'but while the accuracy of sl - dop decreases after n = 14 and converges to simplicity - dop, the accuracy of ls - dop continues to increase and converges to likelihood - dop.', 'the highest accuracy is obtained by sl - dop at 12 n 14 : an lp of 90. 8 % and an lr of 90. 7 %.', 'this is roughly an 11 % relative reduction in error rate over #AUTHOR_TAG and bods pcfg - reduction reported in table 1.', 'compared to the reranking technique in Collins (2000), who obtained an lp of 89. 9 % and an lr of 89. 6 %, our results show a 9 % relative error rate reduction.', 'while sl - dop and ls - dop have been compared before']","['2. results of sl - dop and ls - dop on the wsj ( sentences 100 words ) note that there is an increase in accuracy for both sl - dop and ls - dop if the value of n increases from 1 to 12.', 'but while the accuracy of sl - dop decreases after n = 14 and converges to simplicity - dop, the accuracy of ls - dop continues to increase and converges to likelihood - dop.', 'the highest accuracy is obtained by sl - dop at 12 n 14 : an lp of 90. 8 % and an lr of 90. 7 %.', 'this is roughly an 11 % relative reduction in error rate over #AUTHOR_TAG and bods pcfg - reduction reported in table 1.', 'compared to the reranking technique in Collins (2000), who obtained an lp of 89. 9 % and an lr of 89. 6 %, our results show a 9 % relative error rate reduction.', 'while sl - dop and ls - dop have been compared before']",1
"[""observed that another problem with dop1's subtree - estimation method is that it provides more probability to nodes with more"", 'subtrees, and therefore more probability to larger subtrees. as an alternative, Bonnema et al. (1999)', 'propose a subtree estimator which reduces the probability of a tree by a factor of two for each non - root non', '- terminal it contains. Bonnema et al. (1999) used an alternative technique which samples a', 'fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data. although bod', ""' s method obtains very competitive results on the wall"", 'street journal ( wsj ) task, the parsing time was reported to be over 200 seconds per sentence ( bod 2003 ). #AUTHOR_TAG showed how the perceptron algorithm can be used to efficiently', ""compute the best parse with dop1's subtrees, reporting a 5. 1 % relative reduction in error rate over the model""]","[""observed that another problem with dop1's subtree - estimation method is that it provides more probability to nodes with more"", 'subtrees, and therefore more probability to larger subtrees. as an alternative, Bonnema et al. (1999)', 'propose a subtree estimator which reduces the probability of a tree by a factor of two for each non - root non', '- terminal it contains. Bonnema et al. (1999) used an alternative technique which samples a', 'fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data. although bod', ""' s method obtains very competitive results on the wall"", 'street journal ( wsj ) task, the parsing time was reported to be over 200 seconds per sentence ( bod 2003 ). #AUTHOR_TAG showed how the perceptron algorithm can be used to efficiently', ""compute the best parse with dop1's subtrees, reporting a 5. 1 % relative reduction in error rate over the model in Bod (2001) on the wsj.""]","[""showed that dop1's subtree estimation method is statistically biased and inconsistent. Johnson (1998 a ) solved this problem by"", 'training the subtree probabilities by a maximum likelihood procedure based on expectation - maximization. this resulted in a statistically consistent model dubbed ml - dop. however, ml', '- dop suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from. cross - validation is needed to avoid this problem. but even with cross - validation,', ""ml - dop is outperformed by the much simpler dop1 model on both the atis and ovis treebanks ( bod 2000b ). Bod (2000 observed that another problem with dop1's subtree - estimation method is that it provides more probability to nodes with more"", 'subtrees, and therefore more probability to larger subtrees. as an alternative, Bonnema et al. (1999)', 'propose a subtree estimator which reduces the probability of a tree by a factor of two for each non - root non', '- terminal it contains. Bonnema et al. (1999) used an alternative technique which samples a', 'fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data. although bod', ""' s method obtains very competitive results on the wall"", 'street journal ( wsj ) task, the parsing time was reported to be over 200 seconds per sentence ( bod 2003 ). #AUTHOR_TAG showed how the perceptron algorithm can be used to efficiently', ""compute the best parse with dop1's subtrees, reporting a 5. 1 % relative reduction in error rate over the model in Bod (2001) on the wsj""]","['##bi n - best search ( bod 2001 ), or by restricting the set of subtrees ( sima', ""' an 1999 ; chappelier et al. 2002 ). Bod (1993) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the"", ""most probable parse. Sima'an (1995) goodman (, 1998 developed a polynomial time pcfg - reduction of"", ""dop1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar. while goodman's method does still not allow for an"", 'efficient computation of the most probable parse in dop1, it does efficiently compute the "" maximum constituents parse "", i. e. the parse tree which is most likely to have the largest number of correct constituents. Goodman (1996 b showed that dop1\'s subtree estimation method is statistically biased and inconsistent. Johnson (1998 a ) solved this problem by', 'training the subtree probabilities by a maximum likelihood procedure based on expectation - maximization. this resulted in a statistically consistent model dubbed ml - dop. however, ml', '- dop suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from. cross - validation is needed to avoid this problem. but even with cross - validation,', ""ml - dop is outperformed by the much simpler dop1 model on both the atis and ovis treebanks ( bod 2000b ). Bod (2000 observed that another problem with dop1's subtree - estimation method is that it provides more probability to nodes with more"", 'subtrees, and therefore more probability to larger subtrees. as an alternative, Bonnema et al. (1999)', 'propose a subtree estimator which reduces the probability of a tree by a factor of two for each non - root non', '- terminal it contains. Bonnema et al. (1999) used an alternative technique which samples a', 'fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data. although bod', ""' s method obtains very competitive results on the wall"", 'street journal ( wsj ) task, the parsing time was reported to be over 200 seconds per sentence ( bod 2003 ). #AUTHOR_TAG showed how the perceptron algorithm can be used to efficiently', ""compute the best parse with dop1's subtrees, reporting a 5. 1 % relative reduction in error rate over the model in Bod (2001) on the wsj. Collins ( 1999 ) furthermore showed"", ""how bonnema et al.'s ( 1999 ) and Goodman (2002) estimators can be incorporated in his pcfgreduction, but did not report any experiments with these"", 'reductions']",0
"['- reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to Charniak ( 2000 ) and Collins ( 2000 ), bonnema et al.'s estimator performs worse and is comparable to #AUTHOR_TAG""]","[""paper presents the first published results with goodman's pcfg - reductions of both bonnema et al.'s ( 1999 ) and Bod's (2001) estimators on the wsj."", 'we show that these pcfg - reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to Charniak ( 2000 ) and Collins ( 2000 ), bonnema et al.'s estimator performs worse and is comparable to #AUTHOR_TAG""]","[""paper presents the first published results with goodman's pcfg - reductions of both bonnema et al.'s ( 1999 ) and Bod's (2001) estimators on the wsj."", 'we show that these pcfg - reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to Charniak ( 2000 ) and Collins ( 2000 ), bonnema et al.'s estimator performs worse and is comparable to #AUTHOR_TAG""]","[""paper presents the first published results with goodman's pcfg - reductions of both bonnema et al.'s ( 1999 ) and Bod's (2001) estimators on the wsj."", 'we show that these pcfg - reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to Charniak ( 2000 ) and Collins ( 2000 ), bonnema et al.'s estimator performs worse and is comparable to #AUTHOR_TAG""]",0
"['2. results of sl - dop and ls - dop on the wsj ( sentences 100 words ) note that there is an increase in accuracy for both sl - dop and ls - dop if the value of n increases from 1 to 12.', 'but while the accuracy of sl - dop decreases after n = 14 and converges to simplicity - dop, the accuracy of ls - dop continues to increase and converges to likelihood - dop.', 'the highest accuracy is obtained by sl - dop at 12 n 14 : an lp of 90. 8 % and an lr of 90. 7 %.', ""this is roughly an 11 % relative reduction in error rate over Charniak (2000) and bod's pcfg - reduction reported in table 1."", 'compared to the reranking technique in #AUTHOR_TAG, who obtained an lp of 89. 9 % and an lr of 89. 6 %, our results show a 9 % relative error rate reduction.', 'while sl - dop and ls - dop have been compared before']","['2. results of sl - dop and ls - dop on the wsj ( sentences 100 words ) note that there is an increase in accuracy for both sl - dop and ls - dop if the value of n increases from 1 to 12.', 'but while the accuracy of sl - dop decreases after n = 14 and converges to simplicity - dop, the accuracy of ls - dop continues to increase and converges to likelihood - dop.', 'the highest accuracy is obtained by sl - dop at 12 n 14 : an lp of 90. 8 % and an lr of 90. 7 %.', ""this is roughly an 11 % relative reduction in error rate over Charniak (2000) and bod's pcfg - reduction reported in table 1."", 'compared to the reranking technique in #AUTHOR_TAG, who obtained an lp of 89. 9 % and an lr of 89. 6 %, our results show a 9 % relative error rate reduction.', 'while sl - dop and ls - dop have been compared before']","['2. results of sl - dop and ls - dop on the wsj ( sentences 100 words ) note that there is an increase in accuracy for both sl - dop and ls - dop if the value of n increases from 1 to 12.', 'but while the accuracy of sl - dop decreases after n = 14 and converges to simplicity - dop, the accuracy of ls - dop continues to increase and converges to likelihood - dop.', 'the highest accuracy is obtained by sl - dop at 12 n 14 : an lp of 90. 8 % and an lr of 90. 7 %.', ""this is roughly an 11 % relative reduction in error rate over Charniak (2000) and bod's pcfg - reduction reported in table 1."", 'compared to the reranking technique in #AUTHOR_TAG, who obtained an lp of 89. 9 % and an lr of 89. 6 %, our results show a 9 % relative error rate reduction.', 'while sl - dop and ls - dop have been compared before']","['2. results of sl - dop and ls - dop on the wsj ( sentences 100 words ) note that there is an increase in accuracy for both sl - dop and ls - dop if the value of n increases from 1 to 12.', 'but while the accuracy of sl - dop decreases after n = 14 and converges to simplicity - dop, the accuracy of ls - dop continues to increase and converges to likelihood - dop.', 'the highest accuracy is obtained by sl - dop at 12 n 14 : an lp of 90. 8 % and an lr of 90. 7 %.', ""this is roughly an 11 % relative reduction in error rate over Charniak (2000) and bod's pcfg - reduction reported in table 1."", 'compared to the reranking technique in #AUTHOR_TAG, who obtained an lp of 89. 9 % and an lr of 89. 6 %, our results show a 9 % relative error rate reduction.', 'while sl - dop and ls - dop have been compared before']",1
"['- reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to Charniak ( 2000 ) and #AUTHOR_TAG, bonnema et al.'s estimator performs worse and is comparable to Collins ( 1996 )""]","[""paper presents the first published results with goodman's pcfg - reductions of both bonnema et al.'s ( 1999 ) and Bod's (2001) estimators on the wsj."", 'we show that these pcfg - reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to Charniak ( 2000 ) and #AUTHOR_TAG, bonnema et al.'s estimator performs worse and is comparable to Collins ( 1996 )""]","[""paper presents the first published results with goodman's pcfg - reductions of both bonnema et al.'s ( 1999 ) and Bod's (2001) estimators on the wsj."", 'we show that these pcfg - reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to Charniak ( 2000 ) and #AUTHOR_TAG, bonnema et al.'s estimator performs worse and is comparable to Collins ( 1996 )""]","[""paper presents the first published results with goodman's pcfg - reductions of both bonnema et al.'s ( 1999 ) and Bod's (2001) estimators on the wsj."", 'we show that these pcfg - reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to Charniak ( 2000 ) and #AUTHOR_TAG, bonnema et al.'s estimator performs worse and is comparable to Collins ( 1996 )""]",0
"[""dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), #AUTHOR_TAG, Sima'an ( 2000 ) and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e."", 'most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will']","[""dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), #AUTHOR_TAG, Sima'an ( 2000 ) and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e."", 'most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will']","[""dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), #AUTHOR_TAG, Sima'an ( 2000 ) and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e."", 'most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will']","[""dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), #AUTHOR_TAG, Sima'an ( 2000 ) and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e."", 'most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will specifically mean by "" likelihood - dop "" the pcfg - reduction of Bod (2001) given in section 2. 2']",0
"['##us / evalb /', 'our first experimental goal was to compare the two pcfg - reductions in section 2. 2, which we will refer to resp.', 'as bod01 and bon99.', 'table 1 gives the results of these experiments and compares them with some other statistical parsers ( resp.', 'collins 1996, charniak 1997, collins 1999 and #AUTHOR_TAG ).', 'as to the processing time, the pcfg reduction parses each sentence 100 words ) in 3. 6 seconds average, while the parser in Bod (2001 bod (, 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', 'this corresponds to a speedup of over 60 times.', 'it should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here ( the difference is only 0. 2 % for sentences 100 words ).', 'this may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained,']","['http : / / www. cs. nyu. edu / cs / projects / proteus / evalb /', 'our first experimental goal was to compare the two pcfg - reductions in section 2. 2, which we will refer to resp.', 'as bod01 and bon99.', 'table 1 gives the results of these experiments and compares them with some other statistical parsers ( resp.', 'collins 1996, charniak 1997, collins 1999 and #AUTHOR_TAG ).', 'as to the processing time, the pcfg reduction parses each sentence 100 words ) in 3. 6 seconds average, while the parser in Bod (2001 bod (, 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', 'this corresponds to a speedup of over 60 times.', 'it should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here ( the difference is only 0. 2 % for sentences 100 words ).', 'this may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained,']","['http : / / www. cs. nyu. edu / cs / projects / proteus / evalb /', 'our first experimental goal was to compare the two pcfg - reductions in section 2. 2, which we will refer to resp.', 'as bod01 and bon99.', 'table 1 gives the results of these experiments and compares them with some other statistical parsers ( resp.', 'collins 1996, charniak 1997, collins 1999 and #AUTHOR_TAG ).', 'as to the processing time, the pcfg reduction parses each sentence 100 words ) in 3. 6 seconds average, while the parser in Bod (2001 bod (, 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', 'this corresponds to a speedup of over 60 times.', 'it should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here ( the difference is only 0. 2 % for sentences 100 words ).', 'this may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained,']","['http : / / www. cs. nyu. edu / cs / projects / proteus / evalb /', 'our first experimental goal was to compare the two pcfg - reductions in section 2. 2, which we will refer to resp.', 'as bod01 and bon99.', 'table 1 gives the results of these experiments and compares them with some other statistical parsers ( resp.', 'collins 1996, charniak 1997, collins 1999 and #AUTHOR_TAG ).', 'as to the processing time, the pcfg reduction parses each sentence 100 words ) in 3. 6 seconds average, while the parser in Bod (2001 bod (, 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', 'this corresponds to a speedup of over 60 times.', 'it should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here ( the difference is only 0. 2 % for sentences 100 words ).', 'this may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the pcfg - reduction.', 'in the following section we will see that our new definition of best parse tree also outperforms the best results obtained in Bod (2001)']",1
"['our experiments we used the standard division of the wsj ( #AUTHOR_TAG ), with sections 2 through 21 for training ( approx.', '40, 000 sentences ) and section 23 for testing ( 241']","['our experiments we used the standard division of the wsj ( #AUTHOR_TAG ), with sections 2 through 21 for training ( approx.', '40, 000 sentences ) and section 23 for testing ( 2416 sentences 100 words ) ; section 22 was used as development set.', 'as usual, all trees were stripped']","['our experiments we used the standard division of the wsj ( #AUTHOR_TAG ), with sections 2 through 21 for training ( approx.', '40, 000 sentences ) and section 23 for testing ( 2416 sentences 100 words ) ; section 22 was used as development set.', 'as usual, all trees were stripped off']","['our experiments we used the standard division of the wsj ( #AUTHOR_TAG ), with sections 2 through 21 for training ( approx.', '40, 000 sentences ) and section 23 for testing ( 2416 sentences 100 words ) ; section 22 was used as development set.', 'as usual, all trees were stripped off their semantic tags, co - reference information and quotation marks.', 'without loss of generality, all trees were converted to binary branching ( and were reconverted to n - ary trees after parsing ).', 'we employed the same unknown ( category ) word model as in Bod (2001), based on statistics on word - endings, hyphenation and capitalization in combination with good - turing ( bod 1998 : 85 - 87 ).', 'we used "" evalb "" 4 to compute the standard parseval scores for our results ( manning & schiitze 1999 ).', 'we focused on the labeled precision ( lp ) and labeled recall ( lr ) scores, as these are commonly used to rank parsing systems']",5
"['', 'our first experimental goal was to compare the two pcfg - reductions in section 2. 2, which we will refer to resp.', 'as bod01 and bon99.', 'table 1 gives the results of these experiments and compares them with some other statistical parsers ( resp.', '#AUTHOR_TAG, charniak 1997, collins 1999 and char']","['http : / / www. cs. nyu. edu / cs / projects / proteus / evalb /', 'our first experimental goal was to compare the two pcfg - reductions in section 2. 2, which we will refer to resp.', 'as bod01 and bon99.', 'table 1 gives the results of these experiments and compares them with some other statistical parsers ( resp.', '#AUTHOR_TAG, charniak 1997, collins 1999 and charniak 2000 ).', '( 1996 ).', 'as to the processing time, the pcfg reduction parses each sentence 100 words ) in 3. 6 seconds average, while the parser in Bod (2001 bod (, 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', 'this corresponds to a speedup of over 60 times.', 'it should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here ( the difference is only 0. 2 % for sentences 100 words ).', 'this may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the pcfg - reduction.', 'in the following section we will see that our new definition of best parse tree also outperforms the best results obtained in Bod (2001)']","['http : / / www. cs. nyu. edu / cs / projects / proteus / evalb /', 'our first experimental goal was to compare the two pcfg - reductions in section 2. 2, which we will refer to resp.', 'as bod01 and bon99.', 'table 1 gives the results of these experiments and compares them with some other statistical parsers ( resp.', '#AUTHOR_TAG, charniak 1997, collins 1999 and char']","['http : / / www. cs. nyu. edu / cs / projects / proteus / evalb /', 'our first experimental goal was to compare the two pcfg - reductions in section 2. 2, which we will refer to resp.', 'as bod01 and bon99.', 'table 1 gives the results of these experiments and compares them with some other statistical parsers ( resp.', '#AUTHOR_TAG, charniak 1997, collins 1999 and charniak 2000 ).', '( 1996 ).', 'as to the processing time, the pcfg reduction parses each sentence 100 words ) in 3. 6 seconds average, while the parser in Bod (2001 bod (, 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', 'this corresponds to a speedup of over 60 times.', 'it should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here ( the difference is only 0. 2 % for sentences 100 words ).', 'this may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the pcfg - reduction.', 'in the following section we will see that our new definition of best parse tree also outperforms the best results obtained in Bod (2001)']",1
"['- reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to #AUTHOR_TAG and Collins ( 2000 ), bonnema et al.'s estimator performs worse and is comparable to Collins ( 1996 )""]","[""paper presents the first published results with goodman's pcfg - reductions of both bonnema et al.'s ( 1999 ) and Bod's (2001) estimators on the wsj."", 'we show that these pcfg - reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to #AUTHOR_TAG and Collins ( 2000 ), bonnema et al.'s estimator performs worse and is comparable to Collins ( 1996 )""]","[""paper presents the first published results with goodman's pcfg - reductions of both bonnema et al.'s ( 1999 ) and Bod's (2001) estimators on the wsj."", 'we show that these pcfg - reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to #AUTHOR_TAG and Collins ( 2000 ), bonnema et al.'s estimator performs worse and is comparable to Collins ( 1996 )""]","[""paper presents the first published results with goodman's pcfg - reductions of both bonnema et al.'s ( 1999 ) and Bod's (2001) estimators on the wsj."", 'we show that these pcfg - reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to #AUTHOR_TAG and Collins ( 2000 ), bonnema et al.'s estimator performs worse and is comparable to Collins ( 1996 )""]",0
"['##k 2000 ; goodman 1998 ).', ""and #AUTHOR_TAG argues for ` ` keeping track of counts of arbitrary fragments within parse trees'', which has indeed been carried out in Collins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 )""]","['', 'collins 1999 ; charniak 2000 ; goodman 1998 ).', ""and #AUTHOR_TAG argues for ` ` keeping track of counts of arbitrary fragments within parse trees'', which has indeed been carried out in Collins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 )""]","['', 'collins 1999 ; charniak 2000 ; goodman 1998 ).', ""and #AUTHOR_TAG argues for ` ` keeping track of counts of arbitrary fragments within parse trees'', which has indeed been carried out in Collins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 )""]","['other innovation of dop was to take ( in principle ) all corpus fragments, of any size, rather than a small subset.', 'this innovation has not become generally adopted yet : many approaches still work either with local trees, i. e. single level rules with limited means of information percolation, or with restricted fragments, as in stochastic tree - adjoining grammar ( schabes 1992 ; chiang 2000 ) that do not include nonlexicalized fragments.', 'however, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'while the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head - words, later models showed the importance of including context from higher nodes in the tree ( charniak 1997 ; johnson 1998a ).', 'the importance of including nonheadwords has become uncontroversial ( e. g.', 'collins 1999 ; charniak 2000 ; goodman 1998 ).', ""and #AUTHOR_TAG argues for ` ` keeping track of counts of arbitrary fragments within parse trees'', which has indeed been carried out in Collins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 )""]",0
"[""dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), Bonnema et al. ( 1997 ), Sima'an ( 2000 ) and #AUTHOR_TAG, use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e. most probable ) tree as a candidate for the best tree of a sentence."", 'we will refer to these models as likelihood - dop models, but in this paper we will']","[""dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), Bonnema et al. ( 1997 ), Sima'an ( 2000 ) and #AUTHOR_TAG, use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e. most probable ) tree as a candidate for the best tree of a sentence."", 'we will refer to these models as likelihood - dop models, but in this paper we will']","[""dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), Bonnema et al. ( 1997 ), Sima'an ( 2000 ) and #AUTHOR_TAG, use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e. most probable ) tree as a candidate for the best tree of a sentence."", 'we will refer to these models as likelihood - dop models, but in this paper we will']","[""dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), Bonnema et al. ( 1997 ), Sima'an ( 2000 ) and #AUTHOR_TAG, use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e. most probable ) tree as a candidate for the best tree of a sentence."", 'we will refer to these models as likelihood - dop models, but in this paper we will specifically mean by "" likelihood - dop "" the pcfg - reduction of Bod (2001) given in section 2. 2']",0
"[""dop models, such as in Bod ( 1993 ), #AUTHOR_TAG, Bonnema et al. ( 1997 ), Sima'an ( 2000 ) and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e."", 'most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will']","[""dop models, such as in Bod ( 1993 ), #AUTHOR_TAG, Bonnema et al. ( 1997 ), Sima'an ( 2000 ) and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e."", 'most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will']","[""dop models, such as in Bod ( 1993 ), #AUTHOR_TAG, Bonnema et al. ( 1997 ), Sima'an ( 2000 ) and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e."", 'most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will']","[""dop models, such as in Bod ( 1993 ), #AUTHOR_TAG, Bonnema et al. ( 1997 ), Sima'an ( 2000 ) and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e."", 'most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will specifically mean by "" likelihood - dop "" the pcfg - reduction of Bod (2001) given in section 2. 2']",0
"['dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), Bonnema et al. ( 1997 ), #AUTHOR_TAG and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e. most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will']","['dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), Bonnema et al. ( 1997 ), #AUTHOR_TAG and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e. most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will']","['dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), Bonnema et al. ( 1997 ), #AUTHOR_TAG and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e. most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will']","['dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), Bonnema et al. ( 1997 ), #AUTHOR_TAG and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e. most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will specifically mean by "" likelihood - dop "" the pcfg - reduction of Bod (2001) given in section 2. 2']",0
"['range of sizes in computing the probability of a tree : everything from counts of single - level rules to counts of entire trees.', 'a disadvantage of this model is that an extremely large number of subtrees ( and derivations ) must be taken into account.', 'fortunately, there exists a compact pcfg - reduction of dop1 that generates the same trees with the same probabilities, as shown by #AUTHOR_TAG, 2002 ).', 'here we will only sketch this pc']","['range of sizes in computing the probability of a tree : everything from counts of single - level rules to counts of entire trees.', 'a disadvantage of this model is that an extremely large number of subtrees ( and derivations ) must be taken into account.', 'fortunately, there exists a compact pcfg - reduction of dop1 that generates the same trees with the same probabilities, as shown by #AUTHOR_TAG, 2002 ).', 'here we will only sketch this pcfg - reduction, which is heavily based on Goodman (2002)']","['dop1 considers counts of subtrees of a wide range of sizes in computing the probability of a tree : everything from counts of single - level rules to counts of entire trees.', 'a disadvantage of this model is that an extremely large number of subtrees ( and derivations ) must be taken into account.', 'fortunately, there exists a compact pcfg - reduction of dop1 that generates the same trees with the same probabilities, as shown by #AUTHOR_TAG, 2002 ).', 'here we will only sketch this pcfg - reduction, which is heavily based on Goodman (2002)']","['dop1 considers counts of subtrees of a wide range of sizes in computing the probability of a tree : everything from counts of single - level rules to counts of entire trees.', 'a disadvantage of this model is that an extremely large number of subtrees ( and derivations ) must be taken into account.', 'fortunately, there exists a compact pcfg - reduction of dop1 that generates the same trees with the same probabilities, as shown by #AUTHOR_TAG, 2002 ).', 'here we will only sketch this pcfg - reduction, which is heavily based on Goodman (2002)']",0
"['##bes 1992 ).', 'the dop model, on the other hand, was the first model ( to the best of our knowledge ) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar.', 'this approach has now gained wide usage, as exemplified by the work of #AUTHOR_TAG, 1999 ), Charniak ( 1996 , 1997 ), Johnson ( 1998 ), Chiang ( 2000 ), and many others']","['##egner 1992 ; pereira and schabes 1992 ).', 'the dop model, on the other hand, was the first model ( to the best of our knowledge ) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar.', 'this approach has now gained wide usage, as exemplified by the work of #AUTHOR_TAG, 1999 ), Charniak ( 1996 , 1997 ), Johnson ( 1998 ), Chiang ( 2000 ), and many others']","['##bes 1992 ).', 'the dop model, on the other hand, was the first model ( to the best of our knowledge ) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar.', 'this approach has now gained wide usage, as exemplified by the work of #AUTHOR_TAG, 1999 ), Charniak ( 1996 , 1997 ), Johnson ( 1998 ), Chiang ( 2000 ), and many others']","['##egner 1992 ; pereira and schabes 1992 ).', 'the dop model, on the other hand, was the first model ( to the best of our knowledge ) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar.', 'this approach has now gained wide usage, as exemplified by the work of #AUTHOR_TAG, 1999 ), Charniak ( 1996 , 1997 ), Johnson ( 1998 ), Chiang ( 2000 ), and many others']",4
"['collins 1999 ; #AUTHOR_TAG ; goodman 1998 ).', 'And Collins (2000) argues for "" keeping track of counts of arbitrary fragments within parse trees "", which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of ( all ) tree fragments as proposed in Bod (1992)']","['collins 1999 ; #AUTHOR_TAG ; goodman 1998 ).', 'And Collins (2000) argues for "" keeping track of counts of arbitrary fragments within parse trees "", which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of ( all ) tree fragments as proposed in Bod (1992)']","['. collins 1999 ; #AUTHOR_TAG ; goodman 1998 ).', 'And Collins (2000) argues for "" keeping track of counts of arbitrary fragments within parse trees "", which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of ( all ) tree fragments as proposed in Bod (1992)']","['other innovation of dop was to take ( in principle ) all corpus fragments, of any size, rather than a small subset.', 'this innovation has not become generally adopted yet : many approaches still work either with local trees, i. e. single level rules with limited means of information percolation, or with restricted fragments, as in stochastic tree - adjoining grammar ( schabes 1992 ; chiang 2000 ) that do not include non - lexicalized fragments.', 'however, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'while the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head - words, later models showed the importance of including context from higher nodes in the tree ( charniak 1997 ; johnson 1998a ).', 'the importance of including nonheadwords has become uncontroversial ( e. g. collins 1999 ; #AUTHOR_TAG ; goodman 1998 ).', 'And Collins (2000) argues for "" keeping track of counts of arbitrary fragments within parse trees "", which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of ( all ) tree fragments as proposed in Bod (1992)']",0
"['##k 2000 ; goodman 1998 ).', ""And Collins ( 2000 ) argues for ` ` keeping track of counts of arbitrary fragments within parse trees'', which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 )""]","['', 'collins 1999 ; charniak 2000 ; goodman 1998 ).', ""And Collins ( 2000 ) argues for ` ` keeping track of counts of arbitrary fragments within parse trees'', which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 )""]","['', 'collins 1999 ; charniak 2000 ; goodman 1998 ).', ""And Collins ( 2000 ) argues for ` ` keeping track of counts of arbitrary fragments within parse trees'', which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 )""]","['other innovation of dop was to take ( in principle ) all corpus fragments, of any size, rather than a small subset.', 'this innovation has not become generally adopted yet : many approaches still work either with local trees, i. e. single level rules with limited means of information percolation, or with restricted fragments, as in stochastic tree - adjoining grammar ( schabes 1992 ; chiang 2000 ) that do not include nonlexicalized fragments.', 'however, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'while the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head - words, later models showed the importance of including context from higher nodes in the tree ( charniak 1997 ; johnson 1998a ).', 'the importance of including nonheadwords has become uncontroversial ( e. g.', 'collins 1999 ; charniak 2000 ; goodman 1998 ).', ""And Collins ( 2000 ) argues for ` ` keeping track of counts of arbitrary fragments within parse trees'', which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 )""]",4
"['a1 ( #AUTHOR_TAG ).', ""goodman's main theorem is that this construction produces pcfg derivations isomorphic to dop derivations with equal probability."", 'this means that summing up over derivations of a tree in dop yields the same probability as summing over all the isomorphic derivations in the pcfg.', ""note that goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence : there may still be exponentially many derivations generating the same tree."", 'but goodman shows that with his pc']","['##man then shows by simple induction that subderivations headed by a with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1 / a.', 'and subderivations headed by a1 with external nonterminals only at the leaves, internal nonterminals elsewhere, have probability 1 / a1 ( #AUTHOR_TAG ).', ""goodman's main theorem is that this construction produces pcfg derivations isomorphic to dop derivations with equal probability."", 'this means that summing up over derivations of a tree in dop yields the same probability as summing over all the isomorphic derivations in the pcfg.', ""note that goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence : there may still be exponentially many derivations generating the same tree."", 'but goodman shows that with his pcfg - reduction he can efficiently compute the aforementioned maximum constituents parse.', ""moreover, goodman's pcfg reduction may also be used to estimate the most probable parse by viterbi n - best search which computes the n most likely derivations and then sums up the probabilities of the derivations producing the same tree."", ""While Bod (2001) needed to use a very large sample from the wsj subtrees to do this, goodman's method can do the same job with a more compact grammar""]","['##man then shows by simple induction that subderivations headed by a with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1 / a.', 'and subderivations headed by a1 with external nonterminals only at the leaves, internal nonterminals elsewhere, have probability 1 / a1 ( #AUTHOR_TAG ).', ""goodman's main theorem is that this construction produces pcfg derivations isomorphic to dop derivations with equal probability."", 'this means that summing up over derivations of a tree in dop yields the same probability as summing over all the isomorphic derivations in the pcfg.', ""note that goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence : there may still be exponentially many derivations generating the same tree."", 'but goodman shows that with his pc']","['##man then shows by simple induction that subderivations headed by a with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1 / a.', 'and subderivations headed by a1 with external nonterminals only at the leaves, internal nonterminals elsewhere, have probability 1 / a1 ( #AUTHOR_TAG ).', ""goodman's main theorem is that this construction produces pcfg derivations isomorphic to dop derivations with equal probability."", 'this means that summing up over derivations of a tree in dop yields the same probability as summing over all the isomorphic derivations in the pcfg.', ""note that goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence : there may still be exponentially many derivations generating the same tree."", 'but goodman shows that with his pcfg - reduction he can efficiently compute the aforementioned maximum constituents parse.', ""moreover, goodman's pcfg reduction may also be used to estimate the most probable parse by viterbi n - best search which computes the n most likely derivations and then sums up the probabilities of the derivations producing the same tree."", ""While Bod (2001) needed to use a very large sample from the wsj subtrees to do this, goodman's method can do the same job with a more compact grammar""]",0
"['et al. 2002 ).', ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", '#AUTHOR_TAG, 1998 ) developed a polynomial time pcfg - reduction of dop1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.', ""while goodman's method does""]","['showed how standard parsing techniques can be applied to dop1 by converting subtrees into rules.', ""however, the problem of computing the most probable parse turns out to be np - hard ( sima'an 1996 ), mainly because the same parse tree can be generated by exponentially many derivations."", ""many implementations of dop1 therefore estimate the most probable parse by monte carlo techniques ( bod 1998 ; chappelier & rajman 2000 ), or by viterbi n - best search ( bod 2001 ), or by restricting the set of subtrees ( sima'an 1999 ; chappelier et al. 2002 )."", ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", '#AUTHOR_TAG, 1998 ) developed a polynomial time pcfg - reduction of dop1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.', ""while goodman's method does""]","[""2001 ), or by restricting the set of subtrees ( sima'an 1999 ; chappelier et al. 2002 )."", ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", '#AUTHOR_TAG, 1998 ) developed a polynomial time pcfg - reduction of dop1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.', 'while goodman\'s method does still not allow for an efficient computation of the most probable parse in dop1, it does efficiently compute the "" maximum constituents parse "", i. e. the parse tree which is most likely to have the largest number of correct constituents.', '']","['instantiation of dop which has received considerable interest is the model known as dop1 2 ( bod 1992 ).', 'dop1 combines subtrees from a treebank by means of node - substitution and computes the probability of a tree from the normalized frequencies of the subtrees ( see section 2 for a full definition ).', 'Bod (1993) showed how standard parsing techniques can be applied to dop1 by converting subtrees into rules.', ""however, the problem of computing the most probable parse turns out to be np - hard ( sima'an 1996 ), mainly because the same parse tree can be generated by exponentially many derivations."", ""many implementations of dop1 therefore estimate the most probable parse by monte carlo techniques ( bod 1998 ; chappelier & rajman 2000 ), or by viterbi n - best search ( bod 2001 ), or by restricting the set of subtrees ( sima'an 1999 ; chappelier et al. 2002 )."", ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", '#AUTHOR_TAG, 1998 ) developed a polynomial time pcfg - reduction of dop1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.', 'while goodman\'s method does still not allow for an efficient computation of the most probable parse in dop1, it does efficiently compute the "" maximum constituents parse "", i. e. the parse tree which is most likely to have the largest number of correct constituents.', ""Johnson (1998 b showed that dop1's subtree estimation method is statistically biased and inconsistent."", 'Bod (2000 a ) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on expectation - maximization.', 'this resulted in a statistically consistent model dubbed ml - dop.', 'however, ml - dop suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.', 'cross - validation is needed to avoid this problem.', 'but even with cross - validation, ml - dop is outperformed by the much simpler dop1 model on both the atis and ovis treebanks ( bod 2000b ).', ""Bonnema et al. (1999) observed that another problem with dop1's subtree - estimation method is that it provides more probability to nodes with more subtrees, and therefore more probability to larger subtrees."", 'as an alternative, Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non - root non - terminal it contains.', 'Bod (2001) used an']",0
"['workshop on machine translation shared task on translation. 7', 'there are 82, 740 parallel sentences from news - commentary09. de - en and 1, 418, 115 parallel sentences from europarl - v4. de - en.', 'the monolingual data contains 9. 8 m sentences. 8', 'o build the baseline, the data was tokenized using the moses tokenizer and lowercased.', 'we use giza + + to generate alignments, by running 5 iterations of model 1, 5 iterations of the hmm model, and 4 iterations of model 4. we symmetrize using the "" grow - diag - final - and "" heuristic.', 'our moses systems use default settings.', 'the lm uses the monolingual data and is trained as a five - gram9 using the srilm - toolkit ( #AUTHOR_TAG ).', 'we run mert separately for each system.', 'the recaser used is the same for all systems.', 'it is the standard recaser supplied with moses, trained on all german training data.', 'the dev set is wmt - 2009 - a and the test set is wmt -']","['workshop on machine translation shared task on translation. 7', 'there are 82, 740 parallel sentences from news - commentary09. de - en and 1, 418, 115 parallel sentences from europarl - v4. de - en.', 'the monolingual data contains 9. 8 m sentences. 8', 'o build the baseline, the data was tokenized using the moses tokenizer and lowercased.', 'we use giza + + to generate alignments, by running 5 iterations of model 1, 5 iterations of the hmm model, and 4 iterations of model 4. we symmetrize using the "" grow - diag - final - and "" heuristic.', 'our moses systems use default settings.', 'the lm uses the monolingual data and is trained as a five - gram9 using the srilm - toolkit ( #AUTHOR_TAG ).', 'we run mert separately for each system.', 'the recaser used is the same for all systems.', 'it is the standard recaser supplied with moses, trained on all german training data.', 'the dev set is wmt - 2009 - a and the test set is wmt - 2009 - b, and we report end - to - end case sensitive bleu scores against the unmodified reference']","['evaluate our end - to - end system, we perform the well - studied task of news translation, using the moses smt package.', 'we use the english / german data released for the 2009 acl workshop on machine translation shared task on translation. 7', 'there are 82, 740 parallel sentences from news - commentary09. de - en and 1, 418, 115 parallel sentences from europarl - v4. de - en.', 'the monolingual data contains 9. 8 m sentences. 8', 'o build the baseline, the data was tokenized using the moses tokenizer and lowercased.', 'we use giza + + to generate alignments, by running 5 iterations of model 1, 5 iterations of the hmm model, and 4 iterations of model 4. we symmetrize using the "" grow - diag - final - and "" heuristic.', 'our moses systems use default settings.', 'the lm uses the monolingual data and is trained as a five - gram9 using the srilm - toolkit ( #AUTHOR_TAG ).', 'we run mert separately for each system.', 'the recaser used is the same for all systems.', 'it is the standard recaser supplied with moses, trained on all german training data.', 'the dev set is wmt - 2009 - a and the test set is wmt -']","['evaluate our end - to - end system, we perform the well - studied task of news translation, using the moses smt package.', 'we use the english / german data released for the 2009 acl workshop on machine translation shared task on translation. 7', 'there are 82, 740 parallel sentences from news - commentary09. de - en and 1, 418, 115 parallel sentences from europarl - v4. de - en.', 'the monolingual data contains 9. 8 m sentences. 8', 'o build the baseline, the data was tokenized using the moses tokenizer and lowercased.', 'we use giza + + to generate alignments, by running 5 iterations of model 1, 5 iterations of the hmm model, and 4 iterations of model 4. we symmetrize using the "" grow - diag - final - and "" heuristic.', 'our moses systems use default settings.', 'the lm uses the monolingual data and is trained as a five - gram9 using the srilm - toolkit ( #AUTHOR_TAG ).', 'we run mert separately for each system.', 'the recaser used is the same for all systems.', 'it is the standard recaser supplied with moses, trained on all german training data.', 'the dev set is wmt - 2009 - a and the test set is wmt - 2009 - b, and we report end - to - end case sensitive bleu scores against the unmodified reference sgml file.', 'the blind test set used is wmt - 2009 - blind ( all lines )']",5
"['have to be taken : i ) where to merge and ii ) how to merge.', 'following the work of #AUTHOR_TAG, we implement a linear - chain crf merging system using the following features : stemmed ( separated ) surface form, part - of - speech14 and frequencies from the training corpus for bigrams / merging of word and word + 1, word as true prefix, word + 1 as true suffix, plus frequency comparisons of these.', 'the crf is trained on the split monolingual data.', 'it only proposes merging decisions, merging itself uses a list extracted from the monolingual']","['have to be taken : i ) where to merge and ii ) how to merge.', 'following the work of #AUTHOR_TAG, we implement a linear - chain crf merging system using the following features : stemmed ( separated ) surface form, part - of - speech14 and frequencies from the training corpus for bigrams / merging of word and word + 1, word as true prefix, word + 1 as true suffix, plus frequency comparisons of these.', 'the crf is trained on the split monolingual data.', 'it only proposes merging decisions, merging itself uses a list extracted from the monolingual']","['translation, compound parts have to be resynthesized into compounds before inflection.', 'two decisions have to be taken : i ) where to merge and ii ) how to merge.', 'following the work of #AUTHOR_TAG, we implement a linear - chain crf merging system using the following features : stemmed ( separated ) surface form, part - of - speech14 and frequencies from the training corpus for bigrams / merging of word and word + 1, word as true prefix, word + 1 as true suffix, plus frequency comparisons of these.', 'the crf is trained on the split monolingual data.', 'it only proposes merging decisions, merging itself uses a list extracted from the monolingual data (Popovic et al., 2006)']","['translation, compound parts have to be resynthesized into compounds before inflection.', 'two decisions have to be taken : i ) where to merge and ii ) how to merge.', 'following the work of #AUTHOR_TAG, we implement a linear - chain crf merging system using the following features : stemmed ( separated ) surface form, part - of - speech14 and frequencies from the training corpus for bigrams / merging of word and word + 1, word as true prefix, word + 1 as true suffix, plus frequency comparisons of these.', 'the crf is trained on the split monolingual data.', 'it only proposes merging decisions, merging itself uses a list extracted from the monolingual data (Popovic et al., 2006)']",5
"['compound splitting, we follow #AUTHOR_TAG, using linguistic knowledge encoded in a rule - based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., po S-tags Stymne ( 2008) ) or are ( almost ) knowledge - free (']","['compound splitting, we follow #AUTHOR_TAG, using linguistic knowledge encoded in a rule - based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., po S-tags Stymne ( 2008) ) or are ( almost ) knowledge - free ( e. g., Koehn and Knight (2003) ).', 'compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging approach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow Stymne and Cancedda (2011), for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merging ) on one of our two test sets']","['compound splitting, we follow #AUTHOR_TAG, using linguistic knowledge encoded in a rule - based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., po S-tags Stymne ( 2008) ) or are ( almost ) knowledge - free (']","['compound splitting, we follow #AUTHOR_TAG, using linguistic knowledge encoded in a rule - based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., po S-tags Stymne ( 2008) ) or are ( almost ) knowledge - free ( e. g., Koehn and Knight (2003) ).', 'compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging approach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow Stymne and Cancedda (2011), for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merging ) on one of our two test sets']",5
"['use an additional target factor to obtain the coarse pos for each stem, applying a 7 - gram pos model.', '#AUTHOR_TAG showed that the use of a pos factor only results in negligible bleu improvements, but we need access to the pos in our inflection prediction models']","['use an additional target factor to obtain the coarse pos for each stem, applying a 7 - gram pos model.', '#AUTHOR_TAG showed that the use of a pos factor only results in negligible bleu improvements, but we need access to the pos in our inflection prediction models']","['use an additional target factor to obtain the coarse pos for each stem, applying a 7 - gram pos model.', '#AUTHOR_TAG showed that the use of a pos factor only results in negligible bleu improvements, but we need access to the pos in our inflection prediction models']","['use an additional target factor to obtain the coarse pos for each stem, applying a 7 - gram pos model.', '#AUTHOR_TAG showed that the use of a pos factor only results in negligible bleu improvements, but we need access to the pos in our inflection prediction models']",0
"['. g., Koehn and Knight (2003) ). compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging ap - proach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow #AUTHOR_TAG, for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merg - ing ) on one of our two test sets']","['compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge encoded in a rule - based morphological analyser and then selecting the best analysis based on the ge - ometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., po S-tags Stymne (2008) ) or are ( almost ) knowledge - free ( e. g., Koehn and Knight (2003) ). compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging ap - proach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow #AUTHOR_TAG, for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merg - ing ) on one of our two test sets']","['. g., Koehn and Knight (2003) ). compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging ap - proach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow #AUTHOR_TAG, for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merg - ing ) on one of our two test sets']","['compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge encoded in a rule - based morphological analyser and then selecting the best analysis based on the ge - ometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., po S-tags Stymne (2008) ) or are ( almost ) knowledge - free ( e. g., Koehn and Knight (2003) ). compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging ap - proach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow #AUTHOR_TAG, for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merg - ing ) on one of our two test sets']",5
"['to inflected forms.', '#AUTHOR_TAG improved on this by marking prepositions']","['to inflected forms.', '#AUTHOR_TAG improved on this by marking prepositions']","['to inflected forms.', '#AUTHOR_TAG improved on this by marking prepositions']","['a stem such as brother, toutanova et.', 'al\'s system might generate the "" stem and inflection "" corresponding to and his brother.', 'viewing and and his as inflection is problematic since a mapping from the english phrase and his brother to the arabic stem for brother is required.', 'the situation is worse if there are english words ( e. g., adjectives ) separating his and brother.', 'this required mapping is a significant problem for generalization.', 'we view this issue as a different sort of problem entirely, one of word - formation ( rather than inflection ).', 'we apply a "" split in preprocessing and resynthesize in postprocessing "" approach to these phenomena, combined with inflection prediction that is similar to that of toutanova et.', 'al.', 'the only work that we are aware of which deals with both issues is the work of de gispert and marino ( 2008 ), which deals with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', 'Koehn and Hoang (2007) introduced factored smt.', 'we use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an smt system for translating from stems to inflected forms.', '#AUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ).', 'both efforts were ineffective on large data sets.', 'Williams and Koehn (2011) used unification in an smt system to model some of the agreement phenomena that we model.', 'our crf framework allows us to use more complex context features']",1
"['., #AUTHOR_TAG ).', 'compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging approach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow Stymne and Cancedda (2011), for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merging ) on one of our two test sets']","[', #AUTHOR_TAG ).', 'compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging approach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow Stymne and Cancedda (2011), for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merging ) on one of our two test sets']","['., #AUTHOR_TAG ).', 'compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging approach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow Stymne and Cancedda (2011), for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merging ) on one of our two test sets']","['compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en - coded in a rule - based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., po S-tags Stymne ( 2008 ) ) or are ( almost ) knowledge - free ( e. g., #AUTHOR_TAG ).', 'compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging approach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow Stymne and Cancedda (2011), for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merging ) on one of our two test sets']",1
"['mark ( one of the most important markups in our system ).', 'both efforts were ineffective on large data sets.', '#AUTHOR_TAG used unification']","['mark ( one of the most important markups in our system ).', 'both efforts were ineffective on large data sets.', '#AUTHOR_TAG used unification']","['the case they mark ( one of the most important markups in our system ).', 'both efforts were ineffective on large data sets.', '#AUTHOR_TAG used unification']","['a stem such as brother, toutanova et. als system might generate the stem and inflection corresponding to and his brother.', 'viewing and and his as inflection is problematic since a map - ping from the english phrase and his brother to the arabic stem for brother is required.', 'the situation is worse if there are english words ( e. g., ad - jectives ) separating his and brother.', 'this required mapping is a significant problem for generaliza - tion.', 'we view this issue as a different sort of prob - lem entirely, one of word - formation ( rather than inflection ).', 'we apply a split in preprocessing and resynthesize in postprocessing approach to these phenomena, combined with inflection prediction that is similar to that of toutanova et. al.', 'the only work that we are aware of which deals with both issues is the work of de gispert and marin _ _ o ( 2008 ), which deals with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', 'Koehn and Hoang (2007) introduced factored smt.', 'we use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an smt system for translating from stems to in - flected forms.', 'Bojar and Kos (2010) improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ).', 'both efforts were ineffective on large data sets.', '#AUTHOR_TAG used unification in an smt system to model some of the agreement phenomena that we model.', 'our crf framework allows us to use more complex con - text features']",1
"['based on smor, a morphological analyzer / generator of german ( Schmid et al. , 2004 ) and the bitpar parser, which is a state - of - the - art parser of german ( #AUTHOR_TAG )']","['based on smor, a morphological analyzer / generator of german ( Schmid et al. , 2004 ) and the bitpar parser, which is a state - of - the - art parser of german ( #AUTHOR_TAG )']","['based on smor, a morphological analyzer / generator of german ( Schmid et al. , 2004 ) and the bitpar parser, which is a state - of - the - art parser of german ( #AUTHOR_TAG )']","['key linguistic knowledge sources that we use are morphological analysis and generation of german based on smor, a morphological analyzer / generator of german ( Schmid et al. , 2004 ) and the bitpar parser, which is a state - of - the - art parser of german ( #AUTHOR_TAG )']",5
"['prepare the training data by splitting compounds in two steps, following the technique of #AUTHOR_TAG.', 'first, possible split points are extracted using smor, and second, the best split points are selected using the geometric mean of word part frequencies.', 'training data is then stemmed as described in section 2. 3.', 'the formerly modifying words of the compound ( in our example the words to the left of the rightmost word ) do not have a stem markup assigned, except for two cases : i ) they are nouns themselves or ii ) they are particles separated from a verb.', 'in these cases, former modifiers are represented identically to']","['prepare the training data by splitting compounds in two steps, following the technique of #AUTHOR_TAG.', 'first, possible split points are extracted using smor, and second, the best split points are selected using the geometric mean of word part frequencies.', 'training data is then stemmed as described in section 2. 3.', 'the formerly modifying words of the compound ( in our example the words to the left of the rightmost word ) do not have a stem markup assigned, except for two cases : i ) they are nouns themselves or ii ) they are particles separated from a verb.', 'in these cases, former modifiers are represented identically to']","['prepare the training data by splitting compounds in two steps, following the technique of #AUTHOR_TAG.', 'first, possible split points are extracted using smor, and second, the best split points are selected using the geometric mean of word part frequencies.', 'training data is then stemmed as described in section 2. 3.', 'the formerly modifying words of the compound ( in our example the words to the left of the rightmost word ) do not have a stem markup assigned, except for two cases : i ) they are nouns themselves or ii ) they are particles separated from a verb.', 'in these cases, former modifiers are represented identically to']","['prepare the training data by splitting compounds in two steps, following the technique of #AUTHOR_TAG.', 'first, possible split points are extracted using smor, and second, the best split points are selected using the geometric mean of word part frequencies.', 'training data is then stemmed as described in section 2. 3.', 'the formerly modifying words of the compound ( in our example the words to the left of the rightmost word ) do not have a stem markup assigned, except for two cases : i ) they are nouns themselves or ii ) they are particles separated from a verb.', 'in these cases, former modifiers are represented identically to their individual occurring counterparts, which helps generalization']",5
"['shows that the feature - based approach is superior for english - to - german smt.', 'this is a striking result considering state - of - theart performance of german parsing is poor compared with the best performance on english parsing.', 'as parsing performance improves, the performance of linguistic - feature - based approaches will increase.', 'Virpioja et al. ( 2007 ), Badr et al. ( 2008 ), Luong et al. ( 2010 ), #AUTHOR_TAG, and others are primarily concerned with using morpheme segmentation in smt, which is a useful approach for dealing with issues of word - formation.', 'however, this does not deal directly with linguistic features marked by inflection.', 'in german these linguistic']","['shows that the feature - based approach is superior for english - to - german smt.', 'this is a striking result considering state - of - theart performance of german parsing is poor compared with the best performance on english parsing.', 'as parsing performance improves, the performance of linguistic - feature - based approaches will increase.', 'Virpioja et al. ( 2007 ), Badr et al. ( 2008 ), Luong et al. ( 2010 ), #AUTHOR_TAG, and others are primarily concerned with using morpheme segmentation in smt, which is a useful approach for dealing with issues of word - formation.', 'however, this does not deal directly with linguistic features marked by inflection.', 'in german these linguistic']","['shows that the feature - based approach is superior for english - to - german smt.', 'this is a striking result considering state - of - theart performance of german parsing is poor compared with the best performance on english parsing.', 'as parsing performance improves, the performance of linguistic - feature - based approaches will increase.', 'Virpioja et al. ( 2007 ), Badr et al. ( 2008 ), Luong et al. ( 2010 ), #AUTHOR_TAG, and others are primarily concerned with using morpheme segmentation in smt, which is a useful approach for dealing with issues of word - formation.', 'however, this does not deal directly with linguistic features marked by inflection.', 'in german these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features.', 'so it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are']","['have directly addressed the question as to whether inflection should be predicted using surface forms as the target of the prediction, or whether linguistic features should be predicted, along with the use of a subsequent generation step.', 'the direct prediction of surface forms is limited to those forms observed in the training data, which is a significant limitation.', 'however, it is reasonable to expect that the use of features ( and morphological generation ) could also be problematic as this requires the use of morphologically - aware syntactic parsers to annotate the training data with such features, and additionally depends on the coverage of morphological analysis and generation.', 'despite this, our research clearly shows that the feature - based approach is superior for english - to - german smt.', 'this is a striking result considering state - of - theart performance of german parsing is poor compared with the best performance on english parsing.', 'as parsing performance improves, the performance of linguistic - feature - based approaches will increase.', 'Virpioja et al. ( 2007 ), Badr et al. ( 2008 ), Luong et al. ( 2010 ), #AUTHOR_TAG, and others are primarily concerned with using morpheme segmentation in smt, which is a useful approach for dealing with issues of word - formation.', 'however, this does not deal directly with linguistic features marked by inflection.', 'in german these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features.', 'so it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing']",1
"['.', '#AUTHOR_TAG tried to solve the inflection prediction problem by simply building an smt system for translating from stems to inflected forms.', 'Bojar and Kos (2010) improved on this by marking prepositions']","['inflection is problematic since a mapping from the english phrase and his brother to the arabic stem for brother is required.', 'the situation is worse if there are english words ( e. g., adjectives ) separating his and brother.', 'this required mapping is a significant problem for generalization.', 'we view this issue as a different sort of problem entirely, one of word - formation ( rather than inflection ).', 'we apply a "" split in preprocessing and resynthesize in postprocessing "" approach to these phenomena, combined with inflection prediction that is similar to that of toutanova et.', 'al.', 'the only work that we are aware of which deals with both issues is the work of de gispert and marino ( 2008 ), which deals with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', 'Koehn and Hoang (2007) introduced factored smt.', 'we use more complex context features.', '#AUTHOR_TAG tried to solve the inflection prediction problem by simply building an smt system for translating from stems to inflected forms.', 'Bojar and Kos (2010) improved on this by marking prepositions']","['inflection is problematic since a mapping from the english phrase and his brother to the arabic stem for brother is required.', 'the situation is worse if there are english words ( e. g., adjectives ) separating his and brother.', 'this required mapping is a significant problem for generalization.', 'we view this issue as a different sort of problem entirely, one of word - formation ( rather than inflection ).', 'we apply a "" split in preprocessing and resynthesize in postprocessing "" approach to these phenomena, combined with inflection prediction that is similar to that of toutanova et.', 'al.', 'the only work that we are aware of which deals with both issues is the work of de gispert and marino ( 2008 ), which deals with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', 'Koehn and Hoang (2007) introduced factored smt.', 'we use more complex context features.', '#AUTHOR_TAG tried to solve the inflection prediction problem by simply building an smt system for translating from stems to inflected forms.', 'Bojar and Kos (2010) improved on this by marking prepositions']","['a stem such as brother, toutanova et.', 'al\'s system might generate the "" stem and inflection "" corresponding to and his brother.', 'viewing and and his as inflection is problematic since a mapping from the english phrase and his brother to the arabic stem for brother is required.', 'the situation is worse if there are english words ( e. g., adjectives ) separating his and brother.', 'this required mapping is a significant problem for generalization.', 'we view this issue as a different sort of problem entirely, one of word - formation ( rather than inflection ).', 'we apply a "" split in preprocessing and resynthesize in postprocessing "" approach to these phenomena, combined with inflection prediction that is similar to that of toutanova et.', 'al.', 'the only work that we are aware of which deals with both issues is the work of de gispert and marino ( 2008 ), which deals with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', 'Koehn and Hoang (2007) introduced factored smt.', 'we use more complex context features.', '#AUTHOR_TAG tried to solve the inflection prediction problem by simply building an smt system for translating from stems to inflected forms.', 'Bojar and Kos (2010) improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ).', 'both efforts were ineffective on large data sets.', 'Williams and Koehn (2011) used unification in an smt system to model some of the agreement phenomena that we model.', 'our crf framework allows us to use more complex context features']",1
"['.', 'we view this issue as a different sort of problem entirely, one of word - formation ( rather than inflection ).', 'we apply a split in preprocessing and resynthesize in postprocessing approach to these phenomena, combined with inflection prediction that is similar to that of toutanova et. al.', 'the only work that we are aware of which deals with both issues is the work of de gispert and marin _ _ o ( 2008 ), which deals with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', '#AUTHOR_TAG introduced factored smt.', 'we use more complex context']","['inflection is problematic since a mapping from the english phrase and his brother to the arabic stem for brother is required.', 'the situation is worse if there are english words ( e. g., adjectives ) separating his and brother.', 'this required mapping is a significant problem for generalization.', 'we view this issue as a different sort of problem entirely, one of word - formation ( rather than inflection ).', 'we apply a split in preprocessing and resynthesize in postprocessing approach to these phenomena, combined with inflection prediction that is similar to that of toutanova et. al.', 'the only work that we are aware of which deals with both issues is the work of de gispert and marin _ _ o ( 2008 ), which deals with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', '#AUTHOR_TAG introduced factored smt.', 'we use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an smt system for translating from']","['.', 'we view this issue as a different sort of problem entirely, one of word - formation ( rather than inflection ).', 'we apply a split in preprocessing and resynthesize in postprocessing approach to these phenomena, combined with inflection prediction that is similar to that of toutanova et. al.', 'the only work that we are aware of which deals with both issues is the work of de gispert and marin _ _ o ( 2008 ), which deals with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', '#AUTHOR_TAG introduced factored smt.', 'we use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an smt system for translating from']","['a stem such as brother, toutanova et. als system might generate the stem and inflection corresponding to and his brother.', 'viewing and and his as inflection is problematic since a mapping from the english phrase and his brother to the arabic stem for brother is required.', 'the situation is worse if there are english words ( e. g., adjectives ) separating his and brother.', 'this required mapping is a significant problem for generalization.', 'we view this issue as a different sort of problem entirely, one of word - formation ( rather than inflection ).', 'we apply a split in preprocessing and resynthesize in postprocessing approach to these phenomena, combined with inflection prediction that is similar to that of toutanova et. al.', 'the only work that we are aware of which deals with both issues is the work of de gispert and marin _ _ o ( 2008 ), which deals with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', '#AUTHOR_TAG introduced factored smt.', 'we use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an smt system for translating from stems to inflected forms.', 'Bojar and Kos (2010) improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ).', 'both efforts were ineffective on large data sets.', 'Williams and Koehn (2011) used unification in an smt system to model some of the agreement phenomena that we model.', 'our crf framework allows us to use more complex context features']",1
"['( i. e., feature functions on the aligned english ), such as those of Avramidis and Koehn ( 2008 ), #AUTHOR_TAG and others.', 'toutanova']","['( i. e., feature functions on the aligned english ), such as those of Avramidis and Koehn ( 2008 ), #AUTHOR_TAG and others.', 'toutanova et.', ""al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source""]","['previous work looks at the impact of using source side information ( i. e., feature functions on the aligned english ), such as those of Avramidis and Koehn ( 2008 ), #AUTHOR_TAG and others.', 'toutanova et.', ""al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information."", 'using additional source side information beyond the markup did not produce a gain in performance']","['previous work looks at the impact of using source side information ( i. e., feature functions on the aligned english ), such as those of Avramidis and Koehn ( 2008 ), #AUTHOR_TAG and others.', 'toutanova et.', ""al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information."", 'using additional source side information beyond the markup did not produce a gain in performance']",1
"['( i. e., feature functions on the aligned english ), such as those of #AUTHOR_TAG, Yeniterzi and Oflazer ( 2010 ) and others.', 'toutanova']","['( i. e., feature functions on the aligned english ), such as those of #AUTHOR_TAG, Yeniterzi and Oflazer ( 2010 ) and others.', 'toutanova et.', ""al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source""]","['previous work looks at the impact of using source side information ( i. e., feature functions on the aligned english ), such as those of #AUTHOR_TAG, Yeniterzi and Oflazer ( 2010 ) and others.', 'toutanova et.', ""al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information."", 'using additional source side information beyond the markup did not produce a gain in performance']","['previous work looks at the impact of using source side information ( i. e., feature functions on the aligned english ), such as those of #AUTHOR_TAG, Yeniterzi and Oflazer ( 2010 ) and others.', 'toutanova et.', ""al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information."", 'using additional source side information beyond the markup did not produce a gain in performance']",1
"['compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en - coded in a rule - based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., pos - tags #AUTHOR_TAG ) or are ( almost ) knowledge - free (']","['compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en - coded in a rule - based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., pos - tags #AUTHOR_TAG ) or are ( almost ) knowledge - free ( e. g., Koehn and Knight ( 2003 ) ).', 'compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging approach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow Stymne and Cancedda (2011), for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merging ) on one of our two test sets']","['compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en - coded in a rule - based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., pos - tags #AUTHOR_TAG ) or are ( almost ) knowledge - free (']","['compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en - coded in a rule - based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., pos - tags #AUTHOR_TAG ) or are ( almost ) knowledge - free ( e. g., Koehn and Knight ( 2003 ) ).', 'compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging approach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow Stymne and Cancedda (2011), for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merging ) on one of our two test sets']",1
"['approach to extract and classify social events builds on our previous work ( #AUTHOR_TAG ), which in turn builds on work from the relation extraction community ( Nguyen et al. , 2009 ).', 'therefore, the task of relation extraction is most closely related to the tasks']","['approach to extract and classify social events builds on our previous work ( #AUTHOR_TAG ), which in turn builds on work from the relation extraction community ( Nguyen et al. , 2009 ).', 'therefore, the task of relation extraction is most closely related to the tasks']","['approach to extract and classify social events builds on our previous work ( #AUTHOR_TAG ), which in turn builds on work from the relation extraction community ( Nguyen et al. , 2009 ).', 'therefore, the task of relation extraction is most closely related to the tasks']","['approach to extract and classify social events builds on our previous work ( #AUTHOR_TAG ), which in turn builds on work from the relation extraction community ( Nguyen et al. , 2009 ).', 'therefore, the task of relation extraction is most closely related to the tasks addressed in this paper.', 'researchers have used other notions of semantics in the literature such as latent semantic analysis (Plank and Moschitti, 2013) and relation - specific semantics (Zelenko et al., 2003;Culotta and Sorensen, 2004).', 'to the best of our knowledge, there is only one work that uses frame semantics for relation extraction (Harabagiu et al., 2005).', 'Harabagiu et al. (2005) propose a novel semantic kernel that incorporates frame parse information in the kernel computation that calculates similarity between two dependency trees.', 'they, however, do not propose data representations that are based on frame parses and the resulting arborescent structures, instead adding features to syntactic trees.', 'we believe the implicit feature space of kernels based on our data representation encode a richer and larger feature space than the one proposed by Harabagiu et al. (2005)']",2
"['##this view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in ( #AUTHOR_TAG ).', ""typed feature structures as normal form ir ~'~ eterms are merely syntactic objects""]","['##this view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in ( #AUTHOR_TAG ).', ""typed feature structures as normal form ir ~'~ eterms are merely syntactic objects""]","['##this view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in ( #AUTHOR_TAG ).', ""typed feature structures as normal form ir ~'~ eterms are merely syntactic objects""]","['##this view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in ( #AUTHOR_TAG ).', ""typed feature structures as normal form ir ~'~ eterms are merely syntactic objects""]",0
['a more detailed discussion of various aspects of the proposed parser can be found in ( #AUTHOR_TAG )'],['a more detailed discussion of various aspects of the proposed parser can be found in ( #AUTHOR_TAG )'],['a more detailed discussion of various aspects of the proposed parser can be found in ( #AUTHOR_TAG )'],['a more detailed discussion of various aspects of the proposed parser can be found in ( #AUTHOR_TAG )'],0
"[', the ale parser ( #AUTHOR_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom - up or topdown.', 'in the case of selective magic parsing we use so - called parse types which allow the user to specify how constraints in the grammar are to be interpreted.', 'a literal ( goal ) is considered a parse lype literal ( goal ) if it has as its single argument a typed feature structure of a type specified as a parse type.', '1° all types in the type hierarchy can be used as parse types.', 'this way parse type specification supports a flexible filtering component which allows us to experiment with the role of filtering.', 'however, in the remainder we will concentrate on a']","[', the ale parser ( #AUTHOR_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom - up or topdown.', 'in the case of selective magic parsing we use so - called parse types which allow the user to specify how constraints in the grammar are to be interpreted.', 'a literal ( goal ) is considered a parse lype literal ( goal ) if it has as its single argument a typed feature structure of a type specified as a parse type.', '1° all types in the type hierarchy can be used as parse types.', 'this way parse type specification supports a flexible filtering component which allows us to experiment with the role of filtering.', 'however, in the remainder we will concentrate on a']","[', the ale parser ( #AUTHOR_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom - up or topdown.', 'in the case of selective magic parsing we use so - called parse types which allow the user to specify how constraints in the grammar are to be interpreted.', 'a literal ( goal ) is considered a parse lype literal ( goal ) if it has as its single argument a typed feature structure of a type specified as a parse type.', '1° all types in the type hierarchy can be used as parse types.', 'this way parse type specification supports a flexible filtering component which allows us to experiment with the role of filtering.', 'however, in the remainder we will concentrate on a specific class of parse types : we assume the specification of type sign and']","['control strategies depends on a way to differentiate between types of constraints.', ""proceedings of eacl'99 example, the ale parser ( #AUTHOR_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom - up or topdown."", 'in the case of selective magic parsing we use so - called parse types which allow the user to specify how constraints in the grammar are to be interpreted.', 'a literal ( goal ) is considered a parse lype literal ( goal ) if it has as its single argument a typed feature structure of a type specified as a parse type.', '1° all types in the type hierarchy can be used as parse types.', 'this way parse type specification supports a flexible filtering component which allows us to experiment with the role of filtering.', 'however, in the remainder we will concentrate on a specific class of parse types : we assume the specification of type sign and its subtypes as parse types.', '11 this choice is based on the observation that the constraints on type sign and its sub - types play an important guiding role in the parsing process and are best interpreted bottom - up given the lexical orientation of i - ipsg.', ""the parsing process corresponding to such a parse type specification is represented schematically in figure 8. starting from the lexical entries, i. e., word word word figure 8 : schematic representation of the selective magic parsing process the : r ~'l definite clauses that specify the word objects in the grammar, phrases are built bottomup by matching the parse type literals of the definite clauses in the grammar against the edges in the table."", 'the non - parse type literals are processed according to the top - down control strategy 1°the notion of a parse type literal is closely related to that of a memo literal as in ( johnson and d Srre, 1995)']",0
"['others, ( ramakrishnan et al. 1992 ).', 'as shown in ( #AUTHOR_TAG )']","['others, ( ramakrishnan et al. 1992 ).', 'as shown in ( #AUTHOR_TAG )']","['others, ( ramakrishnan et al. 1992 ).', 'as shown in ( #AUTHOR_TAG )']","['is a compilation technique originally developed for goal - directed bottom - up processing of logic programs.', 'see, among others, ( ramakrishnan et al. 1992 ).', 'as shown in ( #AUTHOR_TAG ) a¢ the presented research was carried out at the university of tubingen, germany, as part of the sonderforschungsbereich 340']",0
"['; #AUTHOR_TAG )', '3 (Meurers and Minnen, 1997) propose a compilation of lexical rules into t ~ r / : definite clauses 2this view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in (Carpenter, 1992).', ""typed feature structures as normal form ir ~'~ e terms are merely syntactic objects""]","['; #AUTHOR_TAG )', '3 (Meurers and Minnen, 1997) propose a compilation of lexical rules into t ~ r / : definite clauses 2this view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in (Carpenter, 1992).', ""typed feature structures as normal form ir ~'~ e terms are merely syntactic objects""]","['; #AUTHOR_TAG )', '3 (Meurers and Minnen, 1997) propose a compilation of lexical rules into t ~ r / : definite clauses 2this view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in (Carpenter, 1992).', ""typed feature structures as normal form ir ~'~ e terms are merely syntactic objects""]","['feature grammars can be used as the basis for implementations of head - driven phrase structure grammar ( hpsg ; #AUTHOR_TAG )', '3 (Meurers and Minnen, 1997) propose a compilation of lexical rules into t ~ r / : definite clauses 2this view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in (Carpenter, 1992).', ""typed feature structures as normal form ir ~'~ e terms are merely syntactic objects""]",0
"['phrase structure grammar ( #AUTHOR_TAG ).', '3 ( Meurers and Minnen , 1997 ) propose a compilation of lexical rules into tit definite clauses which are used to restrict lexical entries.', '']","['can be used as the basis for implementations of head - driven phrase structure grammar ( #AUTHOR_TAG ).', '3 ( Meurers and Minnen , 1997 ) propose a compilation of lexical rules into tit definite clauses which are used to restrict lexical entries.', '']","['feature grammars can be used as the basis for implementations of head - driven phrase structure grammar ( #AUTHOR_TAG ).', '3 ( Meurers and Minnen , 1997 ) propose a compilation of lexical rules into tit definite clauses which are used to restrict lexical entries.', '']","['feature grammars can be used as the basis for implementations of head - driven phrase structure grammar ( #AUTHOR_TAG ).', '3 ( Meurers and Minnen , 1997 ) propose a compilation of lexical rules into tit definite clauses which are used to restrict lexical entries.', '( g Stz and Meurers, 1997 b ) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints. 4', 'because of space limitations we have to refrain from an example.', 'the controll grammar development system as described in ( g Stz and Meurers, 1997 b ) implements the above men - tioned techniques for compiling an hpsg theory into typed feature grammars']",0
"['others, ( #AUTHOR_TAG ).', 'as shown in (Minnen, 1996) magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.', 'in this paper we investigate the selective application of magic to typed feature grammars a type of constraint - logic grammar based on typed feature logic ( tgv :']","['others, ( #AUTHOR_TAG ).', 'as shown in (Minnen, 1996) magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.', 'in this paper we investigate the selective application of magic to typed feature grammars a type of constraint - logic grammar based on typed feature logic ( tgv : ;']","['others, ( #AUTHOR_TAG ).', 'as shown in (Minnen, 1996) magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.', 'in this paper we investigate the selective application of magic to typed feature grammars a type of constraint - logic grammar based on typed feature logic ( tgv :']","['is a compilation technique originally developed for goal - directed bottom - up processing of logic programs.', 'see, among others, ( #AUTHOR_TAG ).', 'as shown in (Minnen, 1996) magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.', 'in this paper we investigate the selective application of magic to typed feature grammars a type of constraint - logic grammar based on typed feature logic ( tgv : ; g Stz, 1995).', 'typed feature grammars can be used as the basis for implementations of head - driven phrase structure grammar ( hpsg ; Pollard and Sag, 1994) as discussed in ( g Stz and Meurers, 1997 a ) and (Meurers and Minnen, 1997).', 'typed feature grammar constraints that are inexpensive to resolve are dealt with using the top - down interpreter of the controll grammar development system ( g Stz and Meurers, 1997 b ) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation']",0
['development system as described in ( #AUTHOR_TAG b ) implements the above mentioned techniques for compil'],['development system as described in ( #AUTHOR_TAG b ) implements the above mentioned techniques for compiling an hpsg theory into typed feature grammars'],"['from an example.', 'the controll grammar development system as described in ( #AUTHOR_TAG b ) implements the above mentioned techniques for compiling an hpsg theory into typed feature grammars']","['##e (King, 1994) for a discussion of the appropriateness of t ~ - £ : for hpsg and a comparison with other feature logic approaches designed for hpsg.', 'append ( [ ~, [ ~, [ ~ ).', 'Meurers, 1997 b ) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', '4 because of space limitations we have to refrain from an example.', 'the controll grammar development system as described in ( #AUTHOR_TAG b ) implements the above mentioned techniques for compiling an hpsg theory into typed feature grammars']",0
"[', like for example, suspension, residuation, ( goal ) freezing, and blocking.', 'see also ( #AUTHOR_TAG ; Naish , 1986 )']","['##outining appears under many different guises, like for example, suspension, residuation, ( goal ) freezing, and blocking.', 'see also ( #AUTHOR_TAG ; Naish , 1986 )']","['##outining appears under many different guises, like for example, suspension, residuation, ( goal ) freezing, and blocking.', 'see also ( #AUTHOR_TAG ; Naish , 1986 )']","['##outining appears under many different guises, like for example, suspension, residuation, ( goal ) freezing, and blocking.', 'see also ( #AUTHOR_TAG ; Naish , 1986 )']",0
"['proposed parser is related to the so - called lemma table deduction system ( johnson and d Srre, 1995) which allows the user to specify whether top - down sub - computations are to be tabled.', ""in contrast to johnson and dsrre's deduction system, though, the selective magic parsing approach combines top - down and bottom - up control strategies."", 'as such it resembles the parser of the grammar development system attribute language engine ( ale ) of ( #AUTHOR_TAG ).', 'unlike the ale parser, though, the selective magic parser does not presuppose a phrase structure backbone and is more']","['proposed parser is related to the so - called lemma table deduction system ( johnson and d Srre, 1995) which allows the user to specify whether top - down sub - computations are to be tabled.', ""in contrast to johnson and dsrre's deduction system, though, the selective magic parsing approach combines top - down and bottom - up control strategies."", 'as such it resembles the parser of the grammar development system attribute language engine ( ale ) of ( #AUTHOR_TAG ).', 'unlike the ale parser, though, the selective magic parser does not presuppose a phrase structure backbone and is more']","['proposed parser is related to the so - called lemma table deduction system ( johnson and d Srre, 1995) which allows the user to specify whether top - down sub - computations are to be tabled.', ""in contrast to johnson and dsrre's deduction system, though, the selective magic parsing approach combines top - down and bottom - up control strategies."", 'as such it resembles the parser of the grammar development system attribute language engine ( ale ) of ( #AUTHOR_TAG ).', 'unlike the ale parser, though, the selective magic parser does not presuppose a phrase structure backbone and is more']","['proposed parser is related to the so - called lemma table deduction system ( johnson and d Srre, 1995) which allows the user to specify whether top - down sub - computations are to be tabled.', ""in contrast to johnson and dsrre's deduction system, though, the selective magic parsing approach combines top - down and bottom - up control strategies."", 'as such it resembles the parser of the grammar development system attribute language engine ( ale ) of ( #AUTHOR_TAG ).', 'unlike the ale parser, though, the selective magic parser does not presuppose a phrase structure backbone and is more flexible as to which sub - computations are tabled / filtered.', 'feature grammars on the basis of an example and introduce a dynamic bottom - up interpreter that can be used for gom - directed interpretation of magic - compiled typed feature grammars']",1
"['; Pollard and Sag , 1994 ) as discussed in ( #AUTHOR_TAG a ) and ( Meurers and Minnen , 1997 ).', 'typed feature grammar constraints that are inexpensive to resolve are dealt with using the top - down interpreter of the controll grammar development system (']","['; Pollard and Sag , 1994 ) as discussed in ( #AUTHOR_TAG a ) and ( Meurers and Minnen , 1997 ).', 'typed feature grammar constraints that are inexpensive to resolve are dealt with using the top - down interpreter of the controll grammar development system']","['; Pollard and Sag , 1994 ) as discussed in ( #AUTHOR_TAG a ) and ( Meurers and Minnen , 1997 ).', 'typed feature grammar constraints that are inexpensive to resolve are dealt with using the top - down interpreter of the controll grammar development system (']","['is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.', 'in this paper we investigate the selective application of magic to typed feature grammars a type of constraint - logic grammar based on typed feature logic ( tgv£ : ; g Stz, 1995).', 'typed feature grammars can be used as the basis for implementations of head - driven phrase structure grammar ( hpsg ; Pollard and Sag , 1994 ) as discussed in ( #AUTHOR_TAG a ) and ( Meurers and Minnen , 1997 ).', 'typed feature grammar constraints that are inexpensive to resolve are dealt with using the top - down interpreter of the controll grammar development system ( g Stz and Meurers, 1997 b ) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation']",2
['see ( #AUTHOR_TAG )'],['see ( #AUTHOR_TAG )'],['see ( #AUTHOR_TAG )'],"['see ( #AUTHOR_TAG ) for a discussion of the appropriateness of tig for hpsg and a comparison with other feature logic approaches designed for hpsg.', 'append ( [ ~, [ ~, [ ~ ).', 'Meurers, 1997 b ) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', '4 because of space limitations we have to refrain from an example.', 'the controll grammar development system as described in ( g Stz and Meurers, 1997 b ) implements the above mentioned techniques for compiling an hpsg theory into typed feature grammars']",0
"['., #AUTHOR_TAG ; watanabe 1995 ) to posi']","['are chosen to force a synchronized alignment ( for better or worse ) in order to simplify cases involving so - called head - switching.', 'this contrasts with one of the traditional approaches ( e. g., #AUTHOR_TAG ; watanabe 1995 ) to posing the translation problem, i. e., the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language']","['., #AUTHOR_TAG ; watanabe 1995 ) to posing the translation problem, i.']","['is worth emphasizing that we do not necessarily expect the dependency representations produced by the training method to be traditional dependency structures for the two languages.', 'instead, the aim is to produce bilingual ( i. e., synchronized, see below ) dependency representations that are appropriate to performing the translation task for a specific language pair or specific bilingual corpus.', 'for example, headwords in both languages are chosen to force a synchronized alignment ( for better or worse ) in order to simplify cases involving so - called head - switching.', 'this contrasts with one of the traditional approaches ( e. g., #AUTHOR_TAG ; watanabe 1995 ) to posing the translation problem, i. e., the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language']",1
"['the transducers produced by the training method described in this paper, the source and target positions are in the set - lcb - - 1, 0, 1 - rcb -, though we have also used handcoded transducers ( #AUTHOR_TAG ) and automatically trained transducers ( alshawi and douglas 2000 ) with a larger range of positions']","['the transducers produced by the training method described in this paper, the source and target positions are in the set - lcb - - 1, 0, 1 - rcb -, though we have also used handcoded transducers ( #AUTHOR_TAG ) and automatically trained transducers ( alshawi and douglas 2000 ) with a larger range of positions']","['the transducers produced by the training method described in this paper, the source and target positions are in the set - lcb - - 1, 0, 1 - rcb -, though we have also used handcoded transducers ( #AUTHOR_TAG ) and automatically trained transducers ( alshawi and douglas 2000 ) with a larger range of positions']","['the transducers produced by the training method described in this paper, the source and target positions are in the set - lcb - - 1, 0, 1 - rcb -, though we have also used handcoded transducers ( #AUTHOR_TAG ) and automatically trained transducers ( alshawi and douglas 2000 ) with a larger range of positions']",5
"['the same time, we believe our method has advantages over the approach developed initially at ibm ( #AUTHOR_TAG ; brown et al. 1993 ) for training translation systems automatically.', 'one advantage is that our method attempts to model the natural decomposition of sentences into phrases.', 'another is that the compilation of this decomposition into lexically anchored finite - state head transducers produces implementations that are much more efficient than those for the ibm model.', 'in particular, our search algorithm finds optimal transductions of test sentences in less than "" real time "" on a 300mhz processor, that is, the time to translate an utterance is less than the time taken to speak it, an important consideration for our speech translation application']","['the same time, we believe our method has advantages over the approach developed initially at ibm ( #AUTHOR_TAG ; brown et al. 1993 ) for training translation systems automatically.', 'one advantage is that our method attempts to model the natural decomposition of sentences into phrases.', 'another is that the compilation of this decomposition into lexically anchored finite - state head transducers produces implementations that are much more efficient than those for the ibm model.', 'in particular, our search algorithm finds optimal transductions of test sentences in less than "" real time "" on a 300mhz processor, that is, the time to translate an utterance is less than the time taken to speak it, an important consideration for our speech translation application']","['the same time, we believe our method has advantages over the approach developed initially at ibm ( #AUTHOR_TAG ; brown et al. 1993 ) for training translation systems automatically.', 'one advantage is that our method attempts to model the natural decomposition of sentences into phrases.', 'another is that the compilation of this decomposition into lexically anchored finite - state head transducers produces implementations that are much more efficient than those for the ibm model.', 'in particular, our search algorithm finds optimal transductions of test sentences in less than "" real time "" on a 300mhz processor, that is, the time to translate an utterance is less than the time taken to speak it, an important consideration for our speech translation application']","['the same time, we believe our method has advantages over the approach developed initially at ibm ( #AUTHOR_TAG ; brown et al. 1993 ) for training translation systems automatically.', 'one advantage is that our method attempts to model the natural decomposition of sentences into phrases.', 'another is that the compilation of this decomposition into lexically anchored finite - state head transducers produces implementations that are much more efficient than those for the ibm model.', 'in particular, our search algorithm finds optimal transductions of test sentences in less than "" real time "" on a 300mhz processor, that is, the time to translate an utterance is less than the time taken to speak it, an important consideration for our speech translation application']",1
"['to see which have effects that match the goal.', ""each matching plan's preconditions are checked ; if they are currently ( believed to be ) true, the planner then attempts to find all instantiations of the plan's body."", ""1 a° the body of a plan can be an action or sequence of actions, a goal or sequence 9 moore and paris also note that ` ` a generation system must maintain the kinds of information outlined by grosz and sidner'' ( #AUTHOR_TAG, 203 )."", '']","['to see which have effects that match the goal.', ""each matching plan's preconditions are checked ; if they are currently ( believed to be ) true, the planner then attempts to find all instantiations of the plan's body."", ""1 a° the body of a plan can be an action or sequence of actions, a goal or sequence 9 moore and paris also note that ` ` a generation system must maintain the kinds of information outlined by grosz and sidner'' ( #AUTHOR_TAG, 203 )."", '']","['##sson 1980 ).', 'the planner first checks all of its top - level plans to see which have effects that match the goal.', ""each matching plan's preconditions are checked ; if they are currently ( believed to be ) true, the planner then attempts to find all instantiations of the plan's body."", ""1 a° the body of a plan can be an action or sequence of actions, a goal or sequence 9 moore and paris also note that ` ` a generation system must maintain the kinds of information outlined by grosz and sidner'' ( #AUTHOR_TAG, 203 )."", '']","['##n constructs its plans using a hierarchical planning algorithm ( nilsson 1980 ).', 'the planner first checks all of its top - level plans to see which have effects that match the goal.', ""each matching plan's preconditions are checked ; if they are currently ( believed to be ) true, the planner then attempts to find all instantiations of the plan's body."", ""1 a° the body of a plan can be an action or sequence of actions, a goal or sequence 9 moore and paris also note that ` ` a generation system must maintain the kinds of information outlined by grosz and sidner'' ( #AUTHOR_TAG, 203 )."", ""their planner uses plan structures similar to igen's, except that the plan operators they use are generally instantiations of rhetorical relations drawn from rhetorical structure theory ( mann and thompson 1987 )."", 'in igen, the plans can involve any goals or actions that could be achieved via communication']",0
"['##t 1994 ; #AUTHOR_TAG ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( reiter 1994 )']","['1994 ; #AUTHOR_TAG ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( reiter 1994 )']","['##t 1994 ; #AUTHOR_TAG ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( reiter 1994 )']","['in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'the text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'the names given to the components vary ; they have been called "" strategic "" and "" tactical "" components ( e. g., mckeown 1985 ; thompson 1977 ; danlos 1987 ) 1, "" planning "" and "" realization "" ( e. g., mcdonald 1983 ; hovy 1988a ), or simply "" what to say "" versus "" how to say it "" ( e. g., danlos 1987 ; reithinger 1990 ).', 'the precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting ( as opposed to merely organizing ) the information to be expressed.', 'much ( if not most ) work in generation, though, continues to rely on this modular approach for its basic design.', ""for example, diogenes ( nirenburg et al. 1988 ), epicure ( dale 1989 ), spokesman ( meteer 1989 ), sibun's work on local organization of text ( sibun 1991 ), and comet ( fisher and mckeown 1990 ) all are organized this way."", 'mcdonald has even argued for extending the model to a large number of components ( mcdonald 1988 ), and several systems have indeed added an additional component between the planner and the linguistic component ( meteer 1994 ; panaget 1994 ; #AUTHOR_TAG ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( reiter 1994 )']",0
"['argued in favor of this approach, claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) ( #AUTHOR_TAG ).', 'while this']","['argued in favor of this approach, claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) ( #AUTHOR_TAG ).', 'while this']","['opposite approach is to simply ignore the limitations of a modular design and proceed as if there need be no interactions between the components.', 'whatever problems result will be handled as best they can, on a case - by - case basis.', 'this approach is the one taken ( implicitly or explicitly ) in the majority of generators.', 'in fact, reiter has even argued in favor of this approach, claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) ( #AUTHOR_TAG ).', 'while this']","['opposite approach is to simply ignore the limitations of a modular design and proceed as if there need be no interactions between the components.', 'whatever problems result will be handled as best they can, on a case - by - case basis.', 'this approach is the one taken ( implicitly or explicitly ) in the majority of generators.', 'in fact, reiter has even argued in favor of this approach, claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) ( #AUTHOR_TAG ).', 'while this certainly has appeal as a design methodology, it seems reckless to assume that problems will never appear.', ""certainly an approach to generation that does handle these interactions would be an improvement, as long as it didn't require abandoning modularity""]",0
"['), and several systems have indeed added an additional component between the planner and the linguistic component ( #AUTHOR_TAG ; panaget 1994 ; wanner 1994 ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( reiter 1994 )']","['), and several systems have indeed added an additional component between the planner and the linguistic component ( #AUTHOR_TAG ; panaget 1994 ; wanner 1994 ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( reiter 1994 )']","['), and several systems have indeed added an additional component between the planner and the linguistic component ( #AUTHOR_TAG ; panaget 1994 ; wanner 1994 ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( reiter 1994 )']","['in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'the text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'the names given to the components vary ; they have been called "" strategic "" and "" tactical "" components ( e. g., mckeown 1985 ; thompson 1977 ; danlos 1987 ) 1, "" planning "" and "" realization "" ( e. g., mcdonald 1983 ; hovy 1988a ), or simply "" what to say "" versus "" how to say it "" ( e. g., danlos 1987 ; reithinger 1990 ).', 'the precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting ( as opposed to merely organizing ) the information to be expressed.', 'much ( if not most ) work in generation, though, continues to rely on this modular approach for its basic design.', ""for example, diogenes ( nirenburg et al. 1988 ), epicure ( dale 1989 ), spokesman ( meteer 1989 ), sibun's work on local organization of text ( sibun 1991 ), and comet ( fisher and mckeown 1990 ) all are organized this way."", 'mcdonald has even argued for extending the model to a large number of components ( mcdonald 1988 ), and several systems have indeed added an additional component between the planner and the linguistic component ( #AUTHOR_TAG ; panaget 1994 ; wanner 1994 ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( reiter 1994 )']",0
"['. g., elhadad and robin 1992 ; penman 1989 ; #AUTHOR_TAG']","['used in some systems ( e. g., elhadad and robin 1992 ; penman 1989 ; #AUTHOR_TAG']","['. g., elhadad and robin 1992 ; penman 1989 ; #AUTHOR_TAG']","['point here is not just that igen can produce different lexical realizations for a particular concept.', 'if that were the only goal, we could dispense with the feedback mechanism and simply design some sort of discrimination network ( or similar device ) to test various features of the information being expressed.', 'the planner could supply whatever information is needed to drive the network.', 'something like this approach is in fact used in some systems ( e. g., elhadad and robin 1992 ; penman 1989 ; #AUTHOR_TAG a )']",0
"['##lt 1985 ; nogier 1989 ), allowing the linguistic component to interrogate the planner ( #AUTHOR_TAG ; sondheimer and nebel 1986 ), and']","[""appelt 1983 ), backtracking on failure ( appelt 1985 ; nogier 1989 ), allowing the linguistic component to interrogate the planner ( #AUTHOR_TAG ; sondheimer and nebel 1986 ), and hovy's notion of restrictive ( i. e., bottom - up ) planning ( hovy 1988a, 1988c )."", 'all of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'the text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.', '']","['##lt 1985 ; nogier 1989 ), allowing the linguistic component to interrogate the planner ( #AUTHOR_TAG ; sondheimer and nebel 1986 ), and']","['have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""these include devices such as interleaving the components ( mcdonald 1983 ; appelt 1983 ), backtracking on failure ( appelt 1985 ; nogier 1989 ), allowing the linguistic component to interrogate the planner ( #AUTHOR_TAG ; sondheimer and nebel 1986 ), and hovy's notion of restrictive ( i. e., bottom - up ) planning ( hovy 1988a, 1988c )."", 'all of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'the text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.', '']",0
"['#AUTHOR_TAG ), back']","['#AUTHOR_TAG ), backtracking on failure ( appelt 1985 ; nogier 1989 ), allowing the linguistic component to interrogate the planner ( mann 1983 ; sondheimer and nebel']","['#AUTHOR_TAG ), back']","['have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""these include devices such as interleaving the components ( mcdonald 1983 ; #AUTHOR_TAG ), backtracking on failure ( appelt 1985 ; nogier 1989 ), allowing the linguistic component to interrogate the planner ( mann 1983 ; sondheimer and nebel 1986 ), and hovy's notion of restrictive ( i. e., bottom - up ) planning ( hovy 1988a, 1988c )."", 'all of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'the text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.']",0
"[', in #AUTHOR_TAG and Delin et al. ( 1994 ) ; however, under this approach, all of the flexibility and simplicity of modular design is lost']","[', in #AUTHOR_TAG and Delin et al. ( 1994 ) ; however, under this approach, all of the flexibility and simplicity of modular design is lost']","[', in #AUTHOR_TAG and Delin et al. ( 1994 ) ; however, under this approach, all of the flexibility and simplicity of modular design is lost']","['possible response would be to abandon the separation ; the generator could be a single component that handles all of the work.', 'this approach has occasionally been taken, as in Kantrowitz and Bates ( 1992 ) and Danlos ( 1987 ) and, at least implicitly, in #AUTHOR_TAG and Delin et al. ( 1994 ) ; however, under this approach, all of the flexibility and simplicity of modular design is lost']",0
"[""##vy's notion of restrictive ( i. e., bottom - up ) planning ( #AUTHOR_TAG a, 1988c )."", 'all of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'the text planning component still has no way to detect and respond to unant']","[""appelt 1983 ), backtracking on failure ( appelt 1985 ; nogier 1989 ), allowing the linguistic component to interrogate the planner ( mann 1983 ; sondheimer and nebel 1986 ), and hovy's notion of restrictive ( i. e., bottom - up ) planning ( #AUTHOR_TAG a, 1988c )."", 'all of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'the text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.', '']","[""##vy's notion of restrictive ( i. e., bottom - up ) planning ( #AUTHOR_TAG a, 1988c )."", 'all of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'the text planning component still has no way to detect and respond to unant']","['have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""these include devices such as interleaving the components ( mcdonald 1983 ; appelt 1983 ), backtracking on failure ( appelt 1985 ; nogier 1989 ), allowing the linguistic component to interrogate the planner ( mann 1983 ; sondheimer and nebel 1986 ), and hovy's notion of restrictive ( i. e., bottom - up ) planning ( #AUTHOR_TAG a, 1988c )."", 'all of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'the text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.', '']",0
"['##vy has described another text planner that builds similar plans ( #AUTHOR_TAG b ).', 'this system, however, starts with a list of information to be expressed and merely arrang']","['##vy has described another text planner that builds similar plans ( #AUTHOR_TAG b ).', 'this system, however, starts with a list of information to be expressed and merely arranges it into a coherent pattern ; it is thus not a planner in the sense used here ( as hovy makes clear ).', '10 since text planning was not the primary focus of this work, igen is designed to simply assume that any false preconditions are unattainable.', ""igen's planner divides the""]","['##vy has described another text planner that builds similar plans ( #AUTHOR_TAG b ).', 'this system, however, starts with a list of information to be expressed and merely arrang']","['##vy has described another text planner that builds similar plans ( #AUTHOR_TAG b ).', 'this system, however, starts with a list of information to be expressed and merely arranges it into a coherent pattern ; it is thus not a planner in the sense used here ( as hovy makes clear ).', '10 since text planning was not the primary focus of this work, igen is designed to simply assume that any false preconditions are unattainable.', ""igen's planner divides the requirements of a plan into two parts : the preconditions, which are not planned for, and those in the plan body, which are."", 'this has no']",0
"['), and several systems have indeed added an additional component between the planner and the linguistic component ( meteer 1994 ; panaget 1994 ; wanner 1994 ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( #AUTHOR_TAG )']","['a large number of components ( mcdonald 1988 ), and several systems have indeed added an additional component between the planner and the linguistic component ( meteer 1994 ; panaget 1994 ; wanner 1994 ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( #AUTHOR_TAG )']","['), and several systems have indeed added an additional component between the planner and the linguistic component ( meteer 1994 ; panaget 1994 ; wanner 1994 ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( #AUTHOR_TAG )']","['in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'the text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'the names given to the components vary ; they have been called "" strategic "" and "" tactical "" components ( e. g., mckeown 1985 ; thompson 1977 ; danlos 1987 ) 1, "" planning "" and "" realization "" ( e. g., mcdonald 1983 ; hovy 1988a ), or simply "" what to say "" versus "" how to say it "" ( e. g., danlos 1987 ; reithinger 1990 ).', 'the precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting ( as opposed to merely organizing ) the information to be expressed.', 'much ( if not most ) work in generation, though, continues to rely on this modular approach for its basic design.', ""for example, diogenes ( nirenburg et al. 1988 ), epicure ( dale 1989 ), spokesman ( meteer 1989 ), sibun's work on local organization of text ( sibun 1991 ), and comet ( fisher and mckeown 1990 ) all are organized this way."", 'mcdonald has even argued for extending the model to a large number of components ( mcdonald 1988 ), and several systems have indeed added an additional component between the planner and the linguistic component ( meteer 1994 ; panaget 1994 ; wanner 1994 ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( #AUTHOR_TAG )']",0
"['. g., mcdonald 1983 ; #AUTHOR_TAG']","['to the components vary ; they have been called "" strategic "" and "" tactical "" components ( e. g., mckeown 1985 ; thompson 1977 ; danlos 1987 ) 1, "" planning "" and "" realization "" ( e. g., mcdonald 1983 ; #AUTHOR_TAG']","['. g., mcdonald 1983 ; #AUTHOR_TAG']","['in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'the text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'the names given to the components vary ; they have been called "" strategic "" and "" tactical "" components ( e. g., mckeown 1985 ; thompson 1977 ; danlos 1987 ) 1, "" planning "" and "" realization "" ( e. g., mcdonald 1983 ; #AUTHOR_TAG a ), or simply "" what to say "" versus "" how to say it "" ( e. g., danlos 1987 ; reithinger 1990 ).', 'the precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting ( as opposed to merely organizing ) the information to be expressed.', 'much ( if not most ) work in generation, though, continues to rely on this modular approach for its basic design.', ""for example, diogenes ( nirenburg et al. 1988 ), epicure ( dale 1989 ), spokesman ( meteer 1989 ), sibun's work on local organization of text ( sibun 1991 ), and comet ( fisher and mckeown 1990 ) all are organized this way."", 'mcdonald has even argued for extending the model to a large number of components ( mcdonald 1988 ), and several systems have indeed added an additional component between the planner and the linguistic component ( meteer 1994 ; panaget 1994 ; wanner 1994 ). reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( reiter 1994 )']",0
"['and articles on the topic include Lamarche and Retord ( 1996 ), de Groote and Retord ( 1996 ), and #AUTHOR_TAG.', 'still, at the risk of proceeding at a slightly slower pace, we aim nonetheless to include here enough details to make the present paper self - contained']","['and articles on the topic include Lamarche and Retord ( 1996 ), de Groote and Retord ( 1996 ), and #AUTHOR_TAG.', 'still, at the risk of proceeding at a slightly slower pace, we aim nonetheless to include here enough details to make the present paper self - contained']","['and articles on the topic include Lamarche and Retord ( 1996 ), de Groote and Retord ( 1996 ), and #AUTHOR_TAG.', 'still, at the risk of proceeding at a slightly slower pace, we aim nonetheless to include here enough details to make the present paper self - contained']","['and articles on the topic include Lamarche and Retord ( 1996 ), de Groote and Retord ( 1996 ), and #AUTHOR_TAG.', 'still, at the risk of proceeding at a slightly slower pace, we aim nonetheless to include here enough details to make the present paper self - contained']",0
"['the associated problem of spurious ambiguity is given in #AUTHOR_TAG but again, to our knowledge, there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction']","['the associated problem of spurious ambiguity is given in #AUTHOR_TAG but again, to our knowledge, there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction']","['the associated problem of spurious ambiguity is given in #AUTHOR_TAG but again, to our knowledge, there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction']","['17 ) by a result of Zielonka (1981), the lambek calculus is not axiomatizable by any finite set of combinatory schemata, so no such combinatory presentation can constitute the logic of concatenation in the sense of lambek calculus.', 'combinatory categorial grammar does not concern itself with the capture of all ( or only ) the concatenatively valid combinatory schemata, but rather with incrementality, for example, on a shiftreduce design.', 'an approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in #AUTHOR_TAG but again, to our knowledge, there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction']",0
"['approach to this problem consists in defining, within the cut - free atomic - id space, normal form derivations in which the succession of rule application is regulated ( #AUTHOR_TAG, hepple 1990, hend']","['approach to this problem consists in defining, within the cut - free atomic - id space, normal form derivations in which the succession of rule application is regulated ( #AUTHOR_TAG, hepple 1990,']","['approach to this problem consists in defining, within the cut - free atomic - id space, normal form derivations in which the succession of rule application is regulated ( #AUTHOR_TAG, hepple 1990, hend']","['approach to this problem consists in defining, within the cut - free atomic - id space, normal form derivations in which the succession of rule application is regulated ( #AUTHOR_TAG, hepple 1990, hendriks 1993 ).', 'each sequent has a distinguished category formula ( underlined ) on which rule applications are keyed : in the regulated calculus there is no spurious ambiguity, and provided there is no explicit or implicit antecedent product, i. e., provided. l is not needed, f ~ a is a theorem of the lambek calculus iff f ~ a is a theorem of the regulated calculus.', 'however, apart from the issue regarding. l, there is a general cause for dissatisfaction with this approach : it assumes the initial presence of the entire sequent to be proved, i. e., it is in principle nonincremental ; on the other hand, allowing incrementality on the basis of cut would reinstate with a vengeance the problem of spurious ambiguity, for then what are to be the cut formulas?', 'consequently, the sequent approach is ill - equipped to address the basic asymmetry of language - - the asymmetry of its processing in time - - - and has never been forwarded in a model of the kind of processing phenomena cited in the introduction']",0
"['classification using words is based on the observation that different das use distinctive word strings.', 'it is known that certain cue words and phrases ( #AUTHOR_TAG ) can serve as explicit indicators of discourse structure.', 'similarly, we find distinctive correlations between certain phrases and da types.', 'for example, 92. 4 % of the uh - huh\'s occur in backchannels, and 88. 4 % of the trigrams "" < start > do you "" occur in yes - no - questions.', 'to leverage this information source, without hand - coding knowledge about which words are indicative of which das, we will use statistical language models that model the full word sequences associated with each da type.', '5. 1. 1', 'classification from']","['classification using words is based on the observation that different das use distinctive word strings.', 'it is known that certain cue words and phrases ( #AUTHOR_TAG ) can serve as explicit indicators of discourse structure.', 'similarly, we find distinctive correlations between certain phrases and da types.', 'for example, 92. 4 % of the uh - huh\'s occur in backchannels, and 88. 4 % of the trigrams "" < start > do you "" occur in yes - no - questions.', 'to leverage this information source, without hand - coding knowledge about which words are indicative of which das, we will use statistical language models that model the full word sequences associated with each da type.', '5. 1. 1', 'classification from']","['classification using words is based on the observation that different das use distinctive word strings.', 'it is known that certain cue words and phrases ( #AUTHOR_TAG ) can serve as explicit indicators of discourse structure.', 'similarly, we find distinctive correlations between certain phrases and da types.', 'for example, 92. 4 % of the uh - huh\'s occur in backchannels, and 88. 4 % of the trigrams "" < start > do you "" occur in yes - no - questions.', 'to leverage this information source, without hand - coding knowledge about which words are indicative of which das, we will use statistical language models that model the full word sequences associated with each da type.', '5. 1. 1', 'classification from true words.', 'assuming that the true ( hand - transcribed ) words of utterances are given as evidence, we can compute word - based likelihoods p ( wiu ) in a straightforward way, by building a statistical language model for each of the 42 das.', 'all das of a particular type found in the training corpus were pooled, and a da - specific trigram model was estimated using standard techniques ( katz backoff [ katz 1987']","['classification using words is based on the observation that different das use distinctive word strings.', 'it is known that certain cue words and phrases ( #AUTHOR_TAG ) can serve as explicit indicators of discourse structure.', 'similarly, we find distinctive correlations between certain phrases and da types.', 'for example, 92. 4 % of the uh - huh\'s occur in backchannels, and 88. 4 % of the trigrams "" < start > do you "" occur in yes - no - questions.', 'to leverage this information source, without hand - coding knowledge about which words are indicative of which das, we will use statistical language models that model the full word sequences associated with each da type.', '5. 1. 1', 'classification from true words.', 'assuming that the true ( hand - transcribed ) words of utterances are given as evidence, we can compute word - based likelihoods p ( wiu ) in a straightforward way, by building a statistical language model for each of the 42 das.', 'all das of a particular type found in the training corpus were pooled, and a da - specific trigram model was estimated using standard techniques ( katz backoff [ katz 1987 ] with witten - bell discounting [ witten and bell 1991 ] )']",4
"['., of discourse grammars with da models ) and the ability to deal with noisy input ( e. g., from a speech recognizer ) in a principled way.', 'however, many other classifier architectures are applicable to the tasks discussed, in particular to da classification.', 'a nonprobabilistic approach for da labeling proposed by samuel, carberry, and vijay - Shanker ( 1998 ) is transformation - based learning ( #AUTHOR_TAG ).', 'finally it should be noted that there are other tasks with a mathematical structure similar to that of da tagging, such as shallow parsing for natural language processing ( munk 1999 ) and dna classification tasks ( ohler, harbeck, and niemann 1999 ), from which further']","['the work mentioned so far uses statistical models of various kinds.', 'as we have shown here, such models offer some fundamental advantages, such as modularity and composability ( e. g., of discourse grammars with da models ) and the ability to deal with noisy input ( e. g., from a speech recognizer ) in a principled way.', 'however, many other classifier architectures are applicable to the tasks discussed, in particular to da classification.', 'a nonprobabilistic approach for da labeling proposed by samuel, carberry, and vijay - Shanker ( 1998 ) is transformation - based learning ( #AUTHOR_TAG ).', 'finally it should be noted that there are other tasks with a mathematical structure similar to that of da tagging, such as shallow parsing for natural language processing ( munk 1999 ) and dna classification tasks ( ohler, harbeck, and niemann 1999 ), from which further']","['., of discourse grammars with da models ) and the ability to deal with noisy input ( e. g., from a speech recognizer ) in a principled way.', 'however, many other classifier architectures are applicable to the tasks discussed, in particular to da classification.', 'a nonprobabilistic approach for da labeling proposed by samuel, carberry, and vijay - Shanker ( 1998 ) is transformation - based learning ( #AUTHOR_TAG ).', 'finally it should be noted that there are other tasks with a mathematical structure similar to that of da tagging, such as shallow parsing for natural language processing ( munk 1999 ) and dna classification tasks ( ohler, harbeck, and niemann 1999 ), from which further techniques could be borrowed']","['the work mentioned so far uses statistical models of various kinds.', 'as we have shown here, such models offer some fundamental advantages, such as modularity and composability ( e. g., of discourse grammars with da models ) and the ability to deal with noisy input ( e. g., from a speech recognizer ) in a principled way.', 'however, many other classifier architectures are applicable to the tasks discussed, in particular to da classification.', 'a nonprobabilistic approach for da labeling proposed by samuel, carberry, and vijay - Shanker ( 1998 ) is transformation - based learning ( #AUTHOR_TAG ).', 'finally it should be noted that there are other tasks with a mathematical structure similar to that of da tagging, such as shallow parsing for natural language processing ( munk 1999 ) and dna classification tasks ( ohler, harbeck, and niemann 1999 ), from which further techniques could be borrowed']",1
"['( #AUTHOR_TAG ).', 'it maximizes the probability of getting the entire da sequence correct, but it does not necessarily find the da sequence that has the most da labels correct ( dermatas and ko']","['( #AUTHOR_TAG ).', 'it maximizes the probability of getting the entire da sequence correct, but it does not necessarily find the da sequence that has the most da labels correct ( dermatas and']",['( #AUTHOR_TAG )'],"['combination of likelihood and prior modeling, hmms, and viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( bahl, jelinek, and mercer 1983 ) and tagging ( #AUTHOR_TAG ).', 'it maximizes the probability of getting the entire da sequence correct, but it does not necessarily find the da sequence that has the most da labels correct ( dermatas and kokkinakis 1995 ).', 'to minimize the total number of utterance labeling errors, we need to maximize the probability of getting each da label correct individually, i. e., we need to maximize p ( uile ) for each i = 1..... n.', 'we can compute the per - utterance posterior da probabilities by summing']",1
"['combination of likelihood and prior modeling, hmms, and viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( bahl, jelinek, and mercer 1983 ) and tagging ( church 1988 ).', 'it maximizes the probability of getting the entire da sequence correct, but it does not necessarily find the da sequence that has the most da labels correct ( #AUTHOR_TAG ).', 'to minimize the total number of utterance labeling errors, we need to maximize the probability of getting each da label correct individually, i. e., we need to maximize p ( uile ) for each i = 1..... n.', 'we can compute the per - utterance posterior da probabilities by summing']","['combination of likelihood and prior modeling, hmms, and viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( bahl, jelinek, and mercer 1983 ) and tagging ( church 1988 ).', 'it maximizes the probability of getting the entire da sequence correct, but it does not necessarily find the da sequence that has the most da labels correct ( #AUTHOR_TAG ).', 'to minimize the total number of utterance labeling errors, we need to maximize the probability of getting each da label correct individually, i. e., we need to maximize p ( uile ) for each i = 1..... n.', 'we can compute the per - utterance posterior da probabilities by summing']","['combination of likelihood and prior modeling, hmms, and viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( bahl, jelinek, and mercer 1983 ) and tagging ( church 1988 ).', 'it maximizes the probability of getting the entire da sequence correct, but it does not necessarily find the da sequence that has the most da labels correct ( #AUTHOR_TAG ).', 'to minimize the total number of utterance labeling errors, we need to maximize the probability of getting each da label correct individually, i. e., we need to maximize p ( uile ) for each i = 1..... n.', 'we can compute the per - utterance posterior da probabilities by summing']","['combination of likelihood and prior modeling, hmms, and viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( bahl, jelinek, and mercer 1983 ) and tagging ( church 1988 ).', 'it maximizes the probability of getting the entire da sequence correct, but it does not necessarily find the da sequence that has the most da labels correct ( #AUTHOR_TAG ).', 'to minimize the total number of utterance labeling errors, we need to maximize the probability of getting each da label correct individually, i. e., we need to maximize p ( uile ) for each i = 1..... n.', 'we can compute the per - utterance posterior da probabilities by summing']",0
"[""equivalence is doing essentially the same job as pereira's pronoun abstraction schema in #AUTHOR_TAG."", 'it will identify a pronoun with any term of type e elsewhere in the qlf, relying on the binding conditions to prevent impossible associations']","[""equivalence is doing essentially the same job as pereira's pronoun abstraction schema in #AUTHOR_TAG."", 'it will identify a pronoun with any term of type e elsewhere in the qlf, relying on the binding conditions to prevent impossible associations']","[""equivalence is doing essentially the same job as pereira's pronoun abstraction schema in #AUTHOR_TAG."", 'it will identify a pronoun with any term of type e elsewhere in the qlf, relying on the binding conditions to prevent impossible associations']","[""equivalence is doing essentially the same job as pereira's pronoun abstraction schema in #AUTHOR_TAG."", 'it will identify a pronoun with any term of type e elsewhere in the qlf, relying on the binding conditions to prevent impossible associations']",1
"['is required is that qlfs are, as here, expressed in a typed higher - order logic, augmented with constructs representing the interpretation of context - dependent elements ( pronouns, ellipsis, focus, etc. ).', 'these constructs correspond as directly as possible to properties of the linguistic structure that express them and are, to as small an extent as possible, dependent on the requirements of contextual resolution ( unlike, say, the metavariables of standard qlfs [ #AUTHOR_TAG ], or the labels of udrs [ reyle 1996 ], which are motivated entirely by the mechanisms that operate on them after grammatical processing ).', 'syntactic properties relevant for binding constraints, parallelism, scope constraints, and']","['is required is that qlfs are, as here, expressed in a typed higher - order logic, augmented with constructs representing the interpretation of context - dependent elements ( pronouns, ellipsis, focus, etc. ).', 'these constructs correspond as directly as possible to properties of the linguistic structure that express them and are, to as small an extent as possible, dependent on the requirements of contextual resolution ( unlike, say, the metavariables of standard qlfs [ #AUTHOR_TAG ], or the labels of udrs [ reyle 1996 ], which are motivated entirely by the mechanisms that operate on them after grammatical processing ).', 'syntactic properties relevant for binding constraints, parallelism, scope constraints, and']","['is required is that qlfs are, as here, expressed in a typed higher - order logic, augmented with constructs representing the interpretation of context - dependent elements ( pronouns, ellipsis, focus, etc. ).', 'these constructs correspond as directly as possible to properties of the linguistic structure that express them and are, to as small an extent as possible, dependent on the requirements of contextual resolution ( unlike, say, the metavariables of standard qlfs [ #AUTHOR_TAG ], or the labels of udrs [ reyle 1996 ], which are motivated entirely by the mechanisms that operate on them after grammatical processing ).', 'syntactic properties relevant for binding constraints, parallelism, scope constraints, and']","['is required is that qlfs are, as here, expressed in a typed higher - order logic, augmented with constructs representing the interpretation of context - dependent elements ( pronouns, ellipsis, focus, etc. ).', 'these constructs correspond as directly as possible to properties of the linguistic structure that express them and are, to as small an extent as possible, dependent on the requirements of contextual resolution ( unlike, say, the metavariables of standard qlfs [ #AUTHOR_TAG ], or the labels of udrs [ reyle 1996 ], which are motivated entirely by the mechanisms that operate on them after grammatical processing ).', 'syntactic properties relevant for binding constraints, parallelism, scope constraints, and so on, are not directly represented at qlf ( again unlike standard qlfs ) but are assumed to be available as components of the linguistic context.', '']",0
"[""implemented in sri's core language engine ( cle )."", 'in the cle - qlf approach, as rationally reconstructed by #AUTHOR_TAG and Crouch and Putman ( 1994 ), the context - independent meaning of a sentence is given by one or more qlfs that are built directly from syntactic and semantic rules.', 'just as here, these qlfs repre - sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context']","[""implemented in sri's core language engine ( cle )."", 'in the cle - qlf approach, as rationally reconstructed by #AUTHOR_TAG and Crouch and Putman ( 1994 ), the context - independent meaning of a sentence is given by one or more qlfs that are built directly from syntactic and semantic rules.', 'just as here, these qlfs repre - sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context']","[""implemented in sri's core language engine ( cle )."", 'in the cle - qlf approach, as rationally reconstructed by #AUTHOR_TAG and Crouch and Putman ( 1994 ), the context - independent meaning of a sentence is given by one or more qlfs that are built directly from syntactic and semantic rules.', 'just as here, these qlfs repre - sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context']","[""starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi - logical form as described in Alshawi (1990, 1992 ), and implemented in sri's core language engine ( cle )."", 'in the cle - qlf approach, as rationally reconstructed by #AUTHOR_TAG and Crouch and Putman ( 1994 ), the context - independent meaning of a sentence is given by one or more qlfs that are built directly from syntactic and semantic rules.', 'just as here, these qlfs repre - sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context']",1
"['that this notion of the relationship between qlf and rqlf encourages one to adopt : it is that taken by #AUTHOR_TAG.', 'this describes the semantics of qlfs via a supervaluation over the semantics of the rqlfs that they subsume.', 'although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many qlfs subsumed rqlfs that are not actually permitted by the resolution rules : for example, those that can only arise via a violation of scoping or binding constraints.', 'the role of resolution rules ( for perfectly good presentational reasons ) is completely ignored by their treatment.', 'however, it is really the case that in giving the semantics of a qlf, one is interested only in the set of rqlfs that are obtainable from it under closure of the resolution rules.', 'ideally, therefore, we would like a formal reconstruction of resolution rules as well.', 'this is so, not just for reasons of formal hygiene in trying to make logical sense out of underspecified representations, but also because resolution rules and the knowledge they']","['third problem arises with the approach to the semantics of qlfs that this notion of the relationship between qlf and rqlf encourages one to adopt : it is that taken by #AUTHOR_TAG.', 'this describes the semantics of qlfs via a supervaluation over the semantics of the rqlfs that they subsume.', 'although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many qlfs subsumed rqlfs that are not actually permitted by the resolution rules : for example, those that can only arise via a violation of scoping or binding constraints.', 'the role of resolution rules ( for perfectly good presentational reasons ) is completely ignored by their treatment.', 'however, it is really the case that in giving the semantics of a qlf, one is interested only in the set of rqlfs that are obtainable from it under closure of the resolution rules.', 'ideally, therefore, we would like a formal reconstruction of resolution rules as well.', 'this is so, not just for reasons of formal hygiene in trying to make logical sense out of underspecified representations, but also because resolution rules and the knowledge they']","['that this notion of the relationship between qlf and rqlf encourages one to adopt : it is that taken by #AUTHOR_TAG.', 'this describes the semantics of qlfs via a supervaluation over the semantics of the rqlfs that they subsume.', 'although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many qlfs subsumed rqlfs that are not actually permitted by the resolution rules : for example, those that can only arise via a violation of scoping or binding constraints.', 'the role of resolution rules ( for perfectly good presentational reasons ) is completely ignored by their treatment.', 'however, it is really the case that in giving the semantics of a qlf, one is interested only in the set of rqlfs that are obtainable from it under closure of the resolution rules.', 'ideally, therefore, we would like a formal reconstruction of resolution rules as well.', 'this is so, not just for reasons of formal hygiene in trying to make logical sense out of underspecified representations, but also because resolution rules and the knowledge they express are an important object of study in their own right.', '']","['third problem arises with the approach to the semantics of qlfs that this notion of the relationship between qlf and rqlf encourages one to adopt : it is that taken by #AUTHOR_TAG.', 'this describes the semantics of qlfs via a supervaluation over the semantics of the rqlfs that they subsume.', 'although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many qlfs subsumed rqlfs that are not actually permitted by the resolution rules : for example, those that can only arise via a violation of scoping or binding constraints.', 'the role of resolution rules ( for perfectly good presentational reasons ) is completely ignored by their treatment.', 'however, it is really the case that in giving the semantics of a qlf, one is interested only in the set of rqlfs that are obtainable from it under closure of the resolution rules.', 'ideally, therefore, we would like a formal reconstruction of resolution rules as well.', 'this is so, not just for reasons of formal hygiene in trying to make logical sense out of underspecified representations, but also because resolution rules and the knowledge they express are an important object of study in their own right.', 'anyone who has built a wide - coverage system knows that the range of context - dependent phenomena encountered in real life is a lot wider than the preoccupations of many linguists might suggest.', 'in the cle, for example, contextual resolution forms a larger part of the system than do syntactic and semantic processing.', 'unfortunately, in the cle there is no formal theory of resolution rules, and thus no prospect of capturing their role in assigning a semantics to qlfs']",1
"['then go on to compare the current approach with that of some other theories with similar aims : the ` ` standard version of quasi - logical form implemented in the core language engine, as rationally reconstructed by #AUTHOR_TAG and Crouch and Pulman ( 1994 ) ; underspecified discourse representation theory ( reyle 1993 ) ; and the ` ` glue language approach of Dalrymple et al. ( 1996 )']","['then go on to compare the current approach with that of some other theories with similar aims : the ` ` standard version of quasi - logical form implemented in the core language engine, as rationally reconstructed by #AUTHOR_TAG and Crouch and Pulman ( 1994 ) ; underspecified discourse representation theory ( reyle 1993 ) ; and the ` ` glue language approach of Dalrymple et al. ( 1996 )']","['then go on to compare the current approach with that of some other theories with similar aims : the ` ` standard version of quasi - logical form implemented in the core language engine, as rationally reconstructed by #AUTHOR_TAG and Crouch and Pulman ( 1994 ) ; underspecified discourse representation theory ( reyle 1993 ) ; and the ` ` glue language approach of Dalrymple et al. ( 1996 )']","['then go on to compare the current approach with that of some other theories with similar aims : the ` ` standard version of quasi - logical form implemented in the core language engine, as rationally reconstructed by #AUTHOR_TAG and Crouch and Pulman ( 1994 ) ; underspecified discourse representation theory ( reyle 1993 ) ; and the ` ` glue language approach of Dalrymple et al. ( 1996 )']",1
"['starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi - logical form as described in #AUTHOR_TAG, 1992 ),']","['starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi - logical form as described in #AUTHOR_TAG, 1992 ),']","['starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi - logical form as described in #AUTHOR_TAG, 1992 ),']","[""starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi - logical form as described in #AUTHOR_TAG, 1992 ), and implemented in sri's core language engine ( cle )."", 'in the cle - qlf approach, as ra - tionally reconstructed by Alshawi and Crouch (1992) and Crouch and Pulman (1994), the context - independent meaning of a sentence is given by one or more qlfs that are built directly from syntactic and semantic rules.', 'just as here, these qlfs represent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context']",1
"['assume that every determiner has its own equivalence, which resolves it as a quantifier : sometimes this can be quite a complicated matter, as with any ( #AUTHOR_TAG ), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity']","['assume that every determiner has its own equivalence, which resolves it as a quantifier : sometimes this can be quite a complicated matter, as with any ( #AUTHOR_TAG ), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity']","['assume that every determiner has its own equivalence, which resolves it as a quantifier : sometimes this can be quite a complicated matter, as with any ( #AUTHOR_TAG ), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity']","['assume that every determiner has its own equivalence, which resolves it as a quantifier : sometimes this can be quite a complicated matter, as with any ( #AUTHOR_TAG ), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity']",0
"['types of ambiguity, even syntactic ambiguity.', ""the more conservative approach is to try to integrate existing statistical disambiguation schemes for qlfs, either individually or in a ` ` packed'' structure ( #AUTHOR_TAG ), with the resolution process as described here."", '']","['types of ambiguity, even syntactic ambiguity.', ""the more conservative approach is to try to integrate existing statistical disambiguation schemes for qlfs, either individually or in a ` ` packed'' structure ( #AUTHOR_TAG ), with the resolution process as described here."", 'alternatively, i believe it is worth exploring the approach to disambiguation described in Pulman (2000), which would mesh nicely with the theory presented here']","['.', 'one is to adopt pinkal\'s "" radical underspecification "" approach ( pinkal 1995 ) and use underspecified representations for all types of ambiguity, even syntactic ambiguity.', ""the more conservative approach is to try to integrate existing statistical disambiguation schemes for qlfs, either individually or in a ` ` packed'' structure ( #AUTHOR_TAG ), with the resolution process as described here."", '']","['are several stategies that might be pursued.', 'one is to adopt pinkal\'s "" radical underspecification "" approach ( pinkal 1995 ) and use underspecified representations for all types of ambiguity, even syntactic ambiguity.', ""the more conservative approach is to try to integrate existing statistical disambiguation schemes for qlfs, either individually or in a ` ` packed'' structure ( #AUTHOR_TAG ), with the resolution process as described here."", 'alternatively, i believe it is worth exploring the approach to disambiguation described in Pulman (2000), which would mesh nicely with the theory presented here']",3
"['can implement a deductive theory of quantifier scope using the conditional equivalence mechanism.', 'the version proposed here combines a basic insight from Lewin ( 1990 ) with higher - order unification to give an analysis that has a strong resemblance to that proposed in #AUTHOR_TAG, 1991 ), with some differences that are commented on below.', ""like pereira's approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by lewin."", 'we analyze quantified nps at the qlf level as illustrated in the qlf for : we assume that every determiner has its own equivalence, which resolves it as a quantifier : sometimes this can be quite a complicated matter, as with any ( alshawi 1990 ), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity.', '']","['can implement a deductive theory of quantifier scope using the conditional equivalence mechanism.', 'the version proposed here combines a basic insight from Lewin ( 1990 ) with higher - order unification to give an analysis that has a strong resemblance to that proposed in #AUTHOR_TAG, 1991 ), with some differences that are commented on below.', ""like pereira's approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by lewin."", 'we analyze quantified nps at the qlf level as illustrated in the qlf for : we assume that every determiner has its own equivalence, which resolves it as a quantifier : sometimes this can be quite a complicated matter, as with any ( alshawi 1990 ), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity.', '']","['can implement a deductive theory of quantifier scope using the conditional equivalence mechanism.', 'the version proposed here combines a basic insight from Lewin ( 1990 ) with higher - order unification to give an analysis that has a strong resemblance to that proposed in #AUTHOR_TAG, 1991 ), with some differences that are commented on below.', ""like pereira's approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by lewin."", 'we analyze quantified nps at the qlf level as illustrated in the qlf for : we assume that every determiner has its own equivalence, which resolves it as a quantifier : sometimes this can be quite a complicated matter, as with any ( alshawi 1990 ), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity.', '']","['can implement a deductive theory of quantifier scope using the conditional equivalence mechanism.', 'the version proposed here combines a basic insight from Lewin ( 1990 ) with higher - order unification to give an analysis that has a strong resemblance to that proposed in #AUTHOR_TAG, 1991 ), with some differences that are commented on below.', ""like pereira's approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by lewin."", 'we analyze quantified nps at the qlf level as illustrated in the qlf for : we assume that every determiner has its own equivalence, which resolves it as a quantifier : sometimes this can be quite a complicated matter, as with any ( alshawi 1990 ), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity.', '']",1
"['is interesting to compare this analysis with that described in dalrymple, shieber, and Pereira ( 1991 ) and #AUTHOR_TAG, 1991 ).', 'recall that in their treatment, quantified noun phrases are treated in two stages : firstly, what they call a "" free variable "" of type e is introduced in the np position, with an associated "" quantifier assumption, "" which is added as a kind of premise.', 'at a later stage the quantifier assumption is "" discharged, "" capturing all occurrences of the free variable.', 'thus their analysis of something like every manager disappeared would proceed as follows']","['is interesting to compare this analysis with that described in dalrymple, shieber, and Pereira ( 1991 ) and #AUTHOR_TAG, 1991 ).', 'recall that in their treatment, quantified noun phrases are treated in two stages : firstly, what they call a "" free variable "" of type e is introduced in the np position, with an associated "" quantifier assumption, "" which is added as a kind of premise.', 'at a later stage the quantifier assumption is "" discharged, "" capturing all occurrences of the free variable.', 'thus their analysis of something like every manager disappeared would proceed as follows']","['is interesting to compare this analysis with that described in dalrymple, shieber, and Pereira ( 1991 ) and #AUTHOR_TAG, 1991 ).', 'recall that in their treatment, quantified noun phrases are treated in two stages : firstly, what they call a "" free variable "" of type e is introduced in the np position, with an associated "" quantifier assumption, "" which is added as a kind of premise.', 'at a later stage the quantifier assumption is "" discharged, "" capturing all occurrences of the free variable.', 'thus their analysis of something like every manager disappeared would proceed as follows']","['is interesting to compare this analysis with that described in dalrymple, shieber, and Pereira ( 1991 ) and #AUTHOR_TAG, 1991 ).', 'recall that in their treatment, quantified noun phrases are treated in two stages : firstly, what they call a "" free variable "" of type e is introduced in the np position, with an associated "" quantifier assumption, "" which is added as a kind of premise.', 'at a later stage the quantifier assumption is "" discharged, "" capturing all occurrences of the free variable.', 'thus their analysis of something like every manager disappeared would proceed as follows']",1
"['the available five relative scopings of the quantifiers are produced ( #AUTHOR_TAG, 47 ), but without the need for a free variable constraint - - the hou algorithm will not produce any solutions in which a previously bound variable becomes free ; a¢ the equivalences are reversible, and thus the above sentences cart be generated from scoped logical forms']","['the available five relative scopings of the quantifiers are produced ( #AUTHOR_TAG, 47 ), but without the need for a free variable constraint - - the hou algorithm will not produce any solutions in which a previously bound variable becomes free ; a¢ the equivalences are reversible, and thus the above sentences cart be generated from scoped logical forms']","['the available five relative scopings of the quantifiers are produced ( #AUTHOR_TAG, 47 ), but without the need for a free variable constraint - - the hou algorithm will not produce any solutions in which a previously bound variable becomes free ; a¢ the equivalences are reversible, and thus the above sentences cart be generated from scoped logical forms']","['the available five relative scopings of the quantifiers are produced ( #AUTHOR_TAG, 47 ), but without the need for a free variable constraint - - the hou algorithm will not produce any solutions in which a previously bound variable becomes free ; a¢ the equivalences are reversible, and thus the above sentences cart be generated from scoped logical forms ; a¢ partial scopings are permitted ( see reyle [ 19961 ) a¢ scoping can be freely interleaved with other types of reference resolution ; a¢ unscoped or partially scoped forms are available for inference or for generation at every stage']",0
"['a calculus for reasoning with qlfs is too large a task to be undertaken here.', 'but the general outlines are reasonably clear, and we can adapt some of the udrs ( #AUTHOR_TAG ) work to our own framework.', 'reyle points out that many of the inferences involving underspecified']","['a calculus for reasoning with qlfs is too large a task to be undertaken here.', 'but the general outlines are reasonably clear, and we can adapt some of the udrs ( #AUTHOR_TAG ) work to our own framework.', 'reyle points out that many of the inferences involving underspecified']","['a calculus for reasoning with qlfs is too large a task to be undertaken here.', 'but the general outlines are reasonably clear, and we can adapt some of the udrs ( #AUTHOR_TAG ) work to our own framework.', 'reyle points out that many of the inferences involving underspecified representations that we would like to capture rely on the assumption that whatever context disambiguates the premise also disambiguates the conclusion, even if we do not know what that context']","['a calculus for reasoning with qlfs is too large a task to be undertaken here.', 'but the general outlines are reasonably clear, and we can adapt some of the udrs ( #AUTHOR_TAG ) work to our own framework.', 'reyle points out that many of the inferences involving underspecified representations that we would like to capture rely on the assumption that whatever context disambiguates the premise also disambiguates the conclusion, even if we do not know what that context or disambiguation is.', 'his example is']",5
"['present an illustrative first - order fragment along these lines and are able to supply a coherent formal semantics for the clf - qlfs themselves, using a technique essentially equivalent to supervaluations : a qlf is true iff all its possible rqlfs are, false iff they are all false, and undefined otherwise']","['present an illustrative first - order fragment along these lines and are able to supply a coherent formal semantics for the clf - qlfs themselves, using a technique essentially equivalent to supervaluations : a qlf is true iff all its possible rqlfs are, false iff they are all false, and undefined otherwise']","['present an illustrative first - order fragment along these lines and are able to supply a coherent formal semantics for the clf - qlfs themselves, using a technique essentially equivalent to supervaluations : a qlf is true iff all its possible rqlfs are, false iff they are all false, and undefined otherwise']","['present an illustrative first - order fragment along these lines and are able to supply a coherent formal semantics for the clf - qlfs themselves, using a technique essentially equivalent to supervaluations : a qlf is true iff all its possible rqlfs are, false iff they are all false, and undefined otherwise']",0
"['#AUTHOR_TAG ; mitkov 1996, 1998b )']","['#AUTHOR_TAG ; mitkov 1996, 1998b )']","['#AUTHOR_TAG ; mitkov 1996, 1998b )']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( sidner 1979 ; carter 1987 ; rich and luperfoy 1988 ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge - poor anaphora resolution strategies.', 'a number of proposals in the 1990s deliberately limited the extent to which they relied on domain and / or linguistic knowledge and reported promising results in knowledge - poor operational environments ( dagan and itai 1990, 1991 ; lappin and leass 1994 ; nasukawa 1994 ; kennedy and boguraev 1996 ; williams, harvey, and preston 1996 ; #AUTHOR_TAG ; mitkov 1996, 1998b )']",0
['##ube and hahn 1996 ; #AUTHOR_TAG ; tetrea'],"['strube and hahn 1996 ; #AUTHOR_TAG ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a, 2001b ).', '']","['##ube and hahn 1996 ; #AUTHOR_TAG ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a, 2001b ).', '']","['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abracos and lopes 1994 ; strube and hahn 1996 ; #AUTHOR_TAG ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a, 2001b ).', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
"[""##eault's contribution features comparative evaluation involving the author's own centering - based pronoun resolution algorithm called the left - right centering algorithm ( lrc ) as well as three other pronoun resolution methods : hobbs's naive algorithm ( hobbs 1978 ), bfp ( brennan, friedman, and pollard 1987 ), and strube's 5list approach ( #AUTHOR_TAG )."", 'the lrc is an']","[""##eault's contribution features comparative evaluation involving the author's own centering - based pronoun resolution algorithm called the left - right centering algorithm ( lrc ) as well as three other pronoun resolution methods : hobbs's naive algorithm ( hobbs 1978 ), bfp ( brennan, friedman, and pollard 1987 ), and strube's 5list approach ( #AUTHOR_TAG )."", 'the lrc is an']","[""##eault's contribution features comparative evaluation involving the author's own centering - based pronoun resolution algorithm called the left - right centering algorithm ( lrc ) as well as three other pronoun resolution methods : hobbs's naive algorithm ( hobbs 1978 ), bfp ( brennan, friedman, and pollard 1987 ), and strube's 5list approach ( #AUTHOR_TAG )."", 'the lrc is an']","[""##eault's contribution features comparative evaluation involving the author's own centering - based pronoun resolution algorithm called the left - right centering algorithm ( lrc ) as well as three other pronoun resolution methods : hobbs's naive algorithm ( hobbs 1978 ), bfp ( brennan, friedman, and pollard 1987 ), and strube's 5list approach ( #AUTHOR_TAG )."", 'the lrc is an alternative to the original bfp algorithm in that it processes utterances incrementally.', 'it works by first searching for an antecedent in the current sentence ; if none can be found, it continues the search on the cf - list of the previous and the other preceding utterances in a left - to - right fashion']",0
"['1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( #AUTHOR_TAG a, 2001b ).', '']","['hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( #AUTHOR_TAG a, 2001b ).', '']","['##hn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( #AUTHOR_TAG a, 2001b ).', '']","['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abracos and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( #AUTHOR_TAG a, 2001b ).', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
"['#AUTHOR_TAG ; williams, harvey, and preston 1996 ; baldwin 1997 ; mitkov 1996, 1998b )']","['#AUTHOR_TAG ; williams, harvey, and preston 1996 ; baldwin 1997 ; mitkov 1996, 1998b )']","['#AUTHOR_TAG ; williams, harvey, and preston 1996 ; baldwin 1997 ; mitkov 1996, 1998b )']","['of the earlier work in anaphora resolution heavily exploited domain and lin - guistic knowledge ( sidner 1979 ; carter 1987 ; rich and luperfoy 1988 ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge - poor anaphora resolution strategies.', 'a number of proposals in the 1990s deliberately limited the extent to which they relied on domain and / or linguistic knowledge and reported promising results in knowledge - poor operational environments ( dagan and itai 1990, 1991 ; lappin and leass 1994 ; nasukawa 1994 ; #AUTHOR_TAG ; williams, harvey, and preston 1996 ; baldwin 1997 ; mitkov 1996, 1998b )']",0
['#AUTHOR_TAG'],['#AUTHOR_TAG'],['#AUTHOR_TAG'],"['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abracos and lopes 1994 ; #AUTHOR_TAG ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a, 2001b )']",0
['#AUTHOR_TAG'],['#AUTHOR_TAG'],['#AUTHOR_TAG'],"['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; #AUTHOR_TAG ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abracos and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a, 2001b ).', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
"['#AUTHOR_TAG ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995']","['#AUTHOR_TAG ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn']","['#AUTHOR_TAG ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a']","['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; #AUTHOR_TAG ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a (Mitkov , 2001 b.', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
['#AUTHOR_TAG'],['#AUTHOR_TAG'],['#AUTHOR_TAG'],"['of the earlier work in anaphora resolution heavily exploited domain and lin - guistic knowledge ( sidner 1979 ; carter 1987 ; rich and luperfoy 1988 ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge - poor anaphora resolution strategies.', 'a number of proposals in the 1990s deliberately limited the extent to which they relied on domain and / or linguistic knowledge and reported promising results in knowledge - poor operational environments ( dagan and itai 1990, 1991 ; lappin and leass 1994 ; #AUTHOR_TAG ; kennedy and boguraev 1996 ; williams, harvey, and preston 1996 ; baldwin 1997 ; mitkov 1996, 1998b )']",0
"['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( sidner 1979 ; carter 1987 ; #AUTHOR_TAG ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( sidner 1979 ; carter 1987 ; #AUTHOR_TAG ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( sidner 1979 ; carter 1987 ; #AUTHOR_TAG ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( sidner 1979 ; carter 1987 ; #AUTHOR_TAG ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge - poor anaphora resolution strategies.', 'a number of proposals in the 1990s deliberately limited the extent to which they relied on domain and / or linguistic knowledge and reported promising results in knowledge - poor operational environments']",0
['#AUTHOR_TAG'],['#AUTHOR_TAG'],['#AUTHOR_TAG'],"['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; #AUTHOR_TAG ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a (Mitkov , 2001 b.', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
"['last years have seen considerable advances in the field of anaphora resolution, but a number of outstanding issues either remain unsolved or need more attention and, as a consequence, represent major challenges to the further development of the field ( #AUTHOR_TAG a ).', 'a fundamental question that needs further investigation is how far the performance of anaphora resolution algorithms can go and what the limitations of knowledge - poor methods are.', 'in particular, more research should be carried out on the factors influencing the performance of these algorithms.', 'one of the impediments to the evaluation or fuller utilization of machine learning techniques is the lack of widely available corpora annotated for anaphoric or coreferential links.', 'more work toward the proposal of consistent and comprehensive evaluation is necessary ; so too is work in multilingual contexts.', 'some of these challenges have been addressed in the papers published in this issue, but ongoing research will continue to address them in the near future']","['last years have seen considerable advances in the field of anaphora resolution, but a number of outstanding issues either remain unsolved or need more attention and, as a consequence, represent major challenges to the further development of the field ( #AUTHOR_TAG a ).', 'a fundamental question that needs further investigation is how far the performance of anaphora resolution algorithms can go and what the limitations of knowledge - poor methods are.', 'in particular, more research should be carried out on the factors influencing the performance of these algorithms.', 'one of the impediments to the evaluation or fuller utilization of machine learning techniques is the lack of widely available corpora annotated for anaphoric or coreferential links.', 'more work toward the proposal of consistent and comprehensive evaluation is necessary ; so too is work in multilingual contexts.', 'some of these challenges have been addressed in the papers published in this issue, but ongoing research will continue to address them in the near future']","['last years have seen considerable advances in the field of anaphora resolution, but a number of outstanding issues either remain unsolved or need more attention and, as a consequence, represent major challenges to the further development of the field ( #AUTHOR_TAG a ).', 'a fundamental question that needs further investigation is how far the performance of anaphora resolution algorithms can go and what the limitations of knowledge - poor methods are.', 'in particular, more research should be carried out on the factors influencing the performance of these algorithms.', 'one of the impediments to the evaluation or fuller utilization of machine learning techniques is the lack of widely available corpora annotated for anaphoric or coreferential links.', 'more work toward the proposal of consistent and comprehensive evaluation is necessary ; so too is work in multilingual contexts.', 'some of these challenges have been addressed in the papers published in this issue, but ongoing research will continue to address them in the near future']","['last years have seen considerable advances in the field of anaphora resolution, but a number of outstanding issues either remain unsolved or need more attention and, as a consequence, represent major challenges to the further development of the field ( #AUTHOR_TAG a ).', 'a fundamental question that needs further investigation is how far the performance of anaphora resolution algorithms can go and what the limitations of knowledge - poor methods are.', 'in particular, more research should be carried out on the factors influencing the performance of these algorithms.', 'one of the impediments to the evaluation or fuller utilization of machine learning techniques is the lack of widely available corpora annotated for anaphoric or coreferential links.', 'more work toward the proposal of consistent and comprehensive evaluation is necessary ; so too is work in multilingual contexts.', 'some of these challenges have been addressed in the papers published in this issue, but ongoing research will continue to address them in the near future']",3
"['#AUTHOR_TAG ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a, 2001b ).', '']","['#AUTHOR_TAG ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a, 2001b ).', '']","['#AUTHOR_TAG ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a, 2001b ).', '']","['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abracos and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; #AUTHOR_TAG ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a, 2001b ).', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
"['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( sidner 1979 ; carter 1987 ; rich and luperfoy 1988 ; #AUTHOR_TAG ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( sidner 1979 ; carter 1987 ; rich and luperfoy 1988 ; #AUTHOR_TAG ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( sidner 1979 ; carter 1987 ; rich and luperfoy 1988 ; #AUTHOR_TAG ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( sidner 1979 ; carter 1987 ; rich and luperfoy 1988 ; #AUTHOR_TAG ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge - poor anaphora resolution strategies.', 'a number of proposals in the 1990s deliberately limited the extent to which they relied on domain and / or linguistic knowledge and reported promising results in knowledge - poor operational environments']",0
"['impetus to the development of coreference resolution algorithms and systems, such as those described in #AUTHOR_TAG, Gaizauskas and Humphreys ( 1996 ), and Kameyama ( 1997 ).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multil']","['impetus to the development of coreference resolution algorithms and systems, such as those described in #AUTHOR_TAG, Gaizauskas and Humphreys ( 1996 ), and Kameyama ( 1997 ).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn']","['7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in #AUTHOR_TAG, Gaizauskas and Humphreys ( 1996 ), and Kameyama ( 1997 ).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, hum']","['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in #AUTHOR_TAG, Gaizauskas and Humphreys ( 1996 ), and Kameyama ( 1997 ).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a (Mitkov , 2001 b.', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
['#AUTHOR_TAG'],['#AUTHOR_TAG'],['#AUTHOR_TAG'],"['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; #AUTHOR_TAG ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a (Mitkov , 2001 b.', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
"['1997 ; #AUTHOR_TAG, 1998b )']","['1997 ; #AUTHOR_TAG, 1998b )']","['##win 1997 ; #AUTHOR_TAG, 1998b )']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( sidner 1979 ; carter 1987 ; rich and luperfoy 1988 ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge - poor anaphora resolution strategies.', 'a number of proposals in the 1990s deliberately limited the extent to which they relied on domain and / or linguistic knowledge and reported promising results in knowledge - poor operational environments ( dagan and itai 1990, 1991 ; lappin and leass 1994 ; nasukawa 1994 ; kennedy and boguraev 1996 ; williams, harvey, and preston 1996 ; baldwin 1997 ; #AUTHOR_TAG, 1998b )']",0
"[""##eault's contribution features comparative evaluation involving the author's own centering - based pronoun resolution algorithm called the left - right centering algorithm ( lrc ) as well as three other pronoun resolution methods : hobbs's naive algorithm ( #AUTHOR_TAG ), bfp ( brennan, friedman, and pollard 1987 ), and strube's 5list approach ( strube""]","[""##eault's contribution features comparative evaluation involving the author's own centering - based pronoun resolution algorithm called the left - right centering algorithm ( lrc ) as well as three other pronoun resolution methods : hobbs's naive algorithm ( #AUTHOR_TAG ), bfp ( brennan, friedman, and pollard 1987 ), and strube's 5list approach ( strube""]","[""##eault's contribution features comparative evaluation involving the author's own centering - based pronoun resolution algorithm called the left - right centering algorithm ( lrc ) as well as three other pronoun resolution methods : hobbs's naive algorithm ( #AUTHOR_TAG ), bfp ( brennan, friedman, and pollard 1987 ), and strube's 5list approach ( strube 1998 )."", 'the lrc is an']","[""##eault's contribution features comparative evaluation involving the author's own centering - based pronoun resolution algorithm called the left - right centering algorithm ( lrc ) as well as three other pronoun resolution methods : hobbs's naive algorithm ( #AUTHOR_TAG ), bfp ( brennan, friedman, and pollard 1987 ), and strube's 5list approach ( strube 1998 )."", 'the lrc is an alternative to the original bfp algorithm in that it processes utterances incrementally.', 'it works by first searching for an antecedent in the current sentence ; if none can be found, it continues the search on the cf - list of the previous and the other preceding utterances in a left - to - right fashion']",0
"['#AUTHOR_TAG ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995']","['#AUTHOR_TAG ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn']","['#AUTHOR_TAG ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and char']","['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; #AUTHOR_TAG ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a (Mitkov , 2001 b.', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
"['impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. ( 1995 ), Gaizauskas and Humphreys ( 1996 ), and #AUTHOR_TAG.', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multil']","['impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. ( 1995 ), Gaizauskas and Humphreys ( 1996 ), and #AUTHOR_TAG.', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone']","['7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. ( 1995 ), Gaizauskas and Humphreys ( 1996 ), and #AUTHOR_TAG.', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, hum']","['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. ( 1995 ), Gaizauskas and Humphreys ( 1996 ), and #AUTHOR_TAG.', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a (Mitkov , 2001 b.', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
"['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; carter 1987 ; rich and luperfoy 1988 ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; carter 1987 ; rich and luperfoy 1988 ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; carter 1987 ; rich and luperfoy 1988 ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; carter 1987 ; rich and luperfoy 1988 ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge - poor anaphora resolution strategies.', 'a number of proposals in the 1990s deliberately limited the extent to which they relied on domain and / or linguistic knowledge and reported promising results in knowledge - poor operational environments']",0
['( #AUTHOR_TAG 1998 ) of'],['( #AUTHOR_TAG 1998 ) of'],"['names are the main concern of the named - entity recognition subtask ( #AUTHOR_TAG 1998 ) of information extraction.', 'the main objective of this subtask is the identification of proper names and also their classification into semantic categories ( person, organization, location, etc. ). 1', 'there the disambiguation of the first word in a sentence ( and in other ambiguous positions ) is one of the central problems : about 20 % of named entities occur in ambiguous positions.', 'for instance, the word black in the sentenceinitial position can stand for a persons surname but can also refer to the color.', 'even in multiword capital']","['names are the main concern of the named - entity recognition subtask ( #AUTHOR_TAG 1998 ) of information extraction.', 'the main objective of this subtask is the identification of proper names and also their classification into semantic categories ( person, organization, location, etc. ). 1', 'there the disambiguation of the first word in a sentence ( and in other ambiguous positions ) is one of the central problems : about 20 % of named entities occur in ambiguous positions.', 'for instance, the word black in the sentenceinitial position can stand for a persons surname but can also refer to the color.', 'even in multiword capitalized phrases, the first word can belong to the rest of the phrase or can be just an external modifier.', 'in the sentence daily, mason and partners lost their court case, it is clear that daily, mason and partners is the name of a company.', 'in the sentence unfortunately, mason and partners lost their court case, the name of the company does not include the word unfortunately, but the word daily is just as common a word as unfortunately']",0
"[""necessarily be sentences, since the majority of tagging paradigms ( e. g., hidden markov model [ hmm ] [ #AUTHOR_TAG ], brill's [ brill 1995a ], and maxent [ ratnaparkhi 1996""]","[""necessarily be sentences, since the majority of tagging paradigms ( e. g., hidden markov model [ hmm ] [ #AUTHOR_TAG ], brill's [ brill 1995a ], and maxent [ ratnaparkhi 1996""]","[""necessarily be sentences, since the majority of tagging paradigms ( e. g., hidden markov model [ hmm ] [ #AUTHOR_TAG ], brill's [ brill 1995a ], and maxent [ ratnaparkhi 1996""]","['our markup convention ( section 2 ), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations.', 'consequently a pos tagger can naturally treat them similarly to any other ambiguous words.', 'there is, however, one difference in the implementation of such a tagger.', 'normally, a pos tagger operates on text spans that form a sentence.', 'this requires resolving sentence boundaries before tagging.', ""we see no good reason, however, why such text spans should necessarily be sentences, since the majority of tagging paradigms ( e. g., hidden markov model [ hmm ] [ #AUTHOR_TAG ], brill's [ brill 1995a ], and maxent [ ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens."", 'the only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which pos information does not depend on the previous and following history.', 'this issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'for instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.', 'at the same time since this token is unambiguous, it is not affected by the history.', 'a trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other']",1
"['the abbreviations : through a document - centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document.', 'this is implemented as a cascade of simple strategies, which were briefly described in #AUTHOR_TAG']","['the abbreviations : through a document - centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document.', 'this is implemented as a cascade of simple strategies, which were briefly described in #AUTHOR_TAG']","['the abbreviations : through a document - centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document.', 'this is implemented as a cascade of simple strategies, which were briefly described in #AUTHOR_TAG']","['second key task of our approach is the disambiguation of capitalized words that follow a potential sentence boundary punctuation sign.', 'apart from being an important component in the task of text normalization, information about whether or not a capitalized word that follows a period is a common word is crucial for the sbd task, as we showed in section 3. we tackle capitalized words in a similar fashion as we tackled the abbreviations : through a document - centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document.', 'this is implemented as a cascade of simple strategies, which were briefly described in #AUTHOR_TAG']",5
"['are two corpora normally used for evaluation in a number of text - processing tasks : the brown corpus ( #AUTHOR_TAG ) and the wall street journal ( wsj ) corpus, both part of the penn treebank ( marcus, marcinkiewicz, and santorini 1993 ).', 'the brown corpus represents general english.', 'it contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech.', 'the brown corpus is rich in out - of - vocabulary ( unknown ) words, spelling errors, and ungrammatical sentences with complex internal structure.', 'altogether there are about 500 documents in the brown corpus, with an average length of 2, 300 word tokens']","['are two corpora normally used for evaluation in a number of text - processing tasks : the brown corpus ( #AUTHOR_TAG ) and the wall street journal ( wsj ) corpus, both part of the penn treebank ( marcus, marcinkiewicz, and santorini 1993 ).', 'the brown corpus represents general english.', 'it contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech.', 'the brown corpus is rich in out - of - vocabulary ( unknown ) words, spelling errors, and ungrammatical sentences with complex internal structure.', 'altogether there are about 500 documents in the brown corpus, with an average length of 2, 300 word tokens']","['are two corpora normally used for evaluation in a number of text - processing tasks : the brown corpus ( #AUTHOR_TAG ) and the wall street journal ( wsj ) corpus, both part of the penn treebank ( marcus, marcinkiewicz, and santorini 1993 ).', 'the brown corpus represents general english.', 'it contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech.', 'the brown corpus is rich in out - of - vocabulary ( unknown ) words, spelling errors, and ungrammatical sentences with complex internal structure.', 'altogether there are about 500 documents in the brown corpus, with an average length of 2, 300 word tokens']","['are two corpora normally used for evaluation in a number of text - processing tasks : the brown corpus ( #AUTHOR_TAG ) and the wall street journal ( wsj ) corpus, both part of the penn treebank ( marcus, marcinkiewicz, and santorini 1993 ).', 'the brown corpus represents general english.', 'it contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech.', 'the brown corpus is rich in out - of - vocabulary ( unknown ) words, spelling errors, and ungrammatical sentences with complex internal structure.', 'altogether there are about 500 documents in the brown corpus, with an average length of 2, 300 word tokens']",5
"[').', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse ( "" one sense per discourse "" ).', 'since then this idea has been applied to several tasks, including word sense disambiguation ( yarowsky 1995 ) and named - entity recognition ( #AUTHOR_TAG ).', ""gale, church, and yarowsky's""]","['heavily rely on pre - existing word lists.', 'it has been applied not only to the identification of proper names, as described in this article, but also to their classification ( mikheev, grover, and moens 1998 ).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse ( "" one sense per discourse "" ).', 'since then this idea has been applied to several tasks, including word sense disambiguation ( yarowsky 1995 ) and named - entity recognition ( #AUTHOR_TAG ).', ""gale, church, and yarowsky's""]","['##ev, grover, and moens 1998 ).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse ( "" one sense per discourse "" ).', 'since then this idea has been applied to several tasks, including word sense disambiguation ( yarowsky 1995 ) and named - entity recognition ( #AUTHOR_TAG ).', ""gale, church, and yarowsky's observation is also used in our dca, especially for the identification of abbreviations."", 'in capital']","['use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'kuhn and de Mori (1998) proposed a cache model that works as a kind of short - term memory by which the probability of the most recent n words is increased over the probability of a general - purpose bigram or trigram model.', 'within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'the dca system is similar in spirit to such dynamic adaptation : it applies word n - grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'but unlike the cache model, it uses a multipass strategy.', ""Clarkson and Robinson (1997) developed a way of incorporating standard n - grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'in our experiments we applied simple linear interpolation to incorporate the dca system into a pos tagger.', 'instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'for handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', 'mani and mac Millan (1995) pointed out that little attention had been paid in the named - entity recognition field to the discourse properties of proper names.', 'they proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text - driven processing rather than reliance on pre - existing lists.', 'the dca outlined in this article also uses nonlocal discourse context and does not heavily rely on pre - existing word lists.', 'it has been applied not only to the identification of proper names, as described in this article, but also to their classification ( mikheev, grover, and moens 1998 ).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse ( "" one sense per discourse "" ).', 'since then this idea has been applied to several tasks, including word sense disambiguation ( yarowsky 1995 ) and named - entity recognition ( #AUTHOR_TAG ).', ""gale, church, and yarowsky's observation is also used in our dca, especially for the identification of abbreviations."", 'in capitalized - word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts ( n - grams ).', 'this is similar to "" one']",0
"['trainable software is generally seen as a way of producing sys - tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( #AUTHOR_TAG ), neural networks ( palmer and hearst 1994 ), and maximum - entropy modeling ( reynar and ratnaparkhi 1997 ).', 'ma - chine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con - text of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']","['trainable software is generally seen as a way of producing sys - tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( #AUTHOR_TAG ), neural networks ( palmer and hearst 1994 ), and maximum - entropy modeling ( reynar and ratnaparkhi 1997 ).', 'ma - chine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con - text of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']","['trainable software is generally seen as a way of producing sys - tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( #AUTHOR_TAG ), neural networks ( palmer and hearst 1994 ), and maximum - entropy modeling ( reynar and ratnaparkhi 1997 ).', 'ma - chine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con - text of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']","['trainable software is generally seen as a way of producing sys - tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( #AUTHOR_TAG ), neural networks ( palmer and hearst 1994 ), and maximum - entropy modeling ( reynar and ratnaparkhi 1997 ).', 'ma - chine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con - text of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']",0
"['trainable software is generally seen as a way of producing sys - tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( riley 1989 ), neural networks ( #AUTHOR_TAG ), and maximum - entropy modeling ( reynar and ratnaparkhi 1997 ).', 'ma - chine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con - text of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']","['trainable software is generally seen as a way of producing sys - tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( riley 1989 ), neural networks ( #AUTHOR_TAG ), and maximum - entropy modeling ( reynar and ratnaparkhi 1997 ).', 'ma - chine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con - text of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']","['trainable software is generally seen as a way of producing sys - tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( riley 1989 ), neural networks ( #AUTHOR_TAG ), and maximum - entropy modeling ( reynar and ratnaparkhi 1997 ).', 'ma - chine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con - text of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']","['trainable software is generally seen as a way of producing sys - tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( riley 1989 ), neural networks ( #AUTHOR_TAG ), and maximum - entropy modeling ( reynar and ratnaparkhi 1997 ).', 'ma - chine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con - text of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']",0
"[""necessarily be sentences, since the majority of tagging paradigms ( e. g., hidden markov model [ hmm ] [ kupiec 1992 ], brill's [ #AUTHOR_TAG a ], and maxent [ ratnaparkhi 1996""]","[""necessarily be sentences, since the majority of tagging paradigms ( e. g., hidden markov model [ hmm ] [ kupiec 1992 ], brill's [ #AUTHOR_TAG a ], and maxent [ ratnaparkhi 1996""]","[""necessarily be sentences, since the majority of tagging paradigms ( e. g., hidden markov model [ hmm ] [ kupiec 1992 ], brill's [ #AUTHOR_TAG a ], and maxent [ ratnaparkhi 1996""]","['our markup convention ( section 2 ), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations.', 'consequently a pos tagger can naturally treat them similarly to any other ambiguous words.', 'there is, however, one difference in the implementation of such a tagger.', 'normally, a pos tagger operates on text spans that form a sentence.', 'this requires resolving sentence boundaries before tagging.', ""we see no good reason, however, why such text spans should necessarily be sentences, since the majority of tagging paradigms ( e. g., hidden markov model [ hmm ] [ kupiec 1992 ], brill's [ #AUTHOR_TAG a ], and maxent [ ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens."", 'the only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which pos information does not depend on the previous and following history.', 'this issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'for instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.', 'at the same time since this token is unambiguous, it is not affected by the history.', 'a trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other']",1
"[""necessarily be sentences, since the majority of tagging paradigms ( e. g., hidden markov model [ hmm ] [ kupiec 1992 ], brill's [ brill 1995a ], and maxent [ #AUTHOR_TAG""]","[""necessarily be sentences, since the majority of tagging paradigms ( e. g., hidden markov model [ hmm ] [ kupiec 1992 ], brill's [ brill 1995a ], and maxent [ #AUTHOR_TAG""]","[""necessarily be sentences, since the majority of tagging paradigms ( e. g., hidden markov model [ hmm ] [ kupiec 1992 ], brill's [ brill 1995a ], and maxent [ #AUTHOR_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens."", 'the only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which pos information does not depend on the previous and following history.', 'this issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'for instance, a bigram tagger operates within a window of two tokens,']","['our markup convention ( section 2 ), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations.', 'consequently a pos tagger can naturally treat them similarly to any other ambiguous words.', 'there is, however, one difference in the implementation of such a tagger.', 'normally, a pos tagger operates on text spans that form a sentence.', 'this requires resolving sentence boundaries before tagging.', ""we see no good reason, however, why such text spans should necessarily be sentences, since the majority of tagging paradigms ( e. g., hidden markov model [ hmm ] [ kupiec 1992 ], brill's [ brill 1995a ], and maxent [ #AUTHOR_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens."", 'the only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which pos information does not depend on the previous and following history.', 'this issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'for instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.', 'at the same time since this token is unambiguous, it is not affected by the history.', 'a trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other']",1
"['c of table 4 summarizes the highest results known to us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( #AUTHOR_TAG ) with the alembic system ( aberdeen et al. 1995 )']","['c of table 4 summarizes the highest results known to us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( #AUTHOR_TAG ) with the alembic system ( aberdeen et al. 1995 )']","['c of table 4 summarizes the highest results known to us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( #AUTHOR_TAG ) with the alembic system ( aberdeen et al. 1995 )']","['c of table 4 summarizes the highest results known to us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( #AUTHOR_TAG ) with the alembic system ( aberdeen et al. 1995 ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in Mikheev (2000).', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']",1
"['##d systems : rule based and machine learning.', 'the rule - based systems use manually built rules that are usually encoded in terms of regular - expression grammars supplemented with lists of abbreviations, common words, proper names, etc.', 'to put together a few rules is fast and easy, but to develop a rule - based system with good performance is quite a labor - consuming enterprise.', 'for instance, the alembic workbench ( #AUTHOR_TAG ) contains a sentence - splitting module that employs over 100 regular - expression rules written in flex.', 'another well - acknowledged shortcoming of rule - based systems is that such systems are usually closely tailored']","['exist two large classes of sbd systems : rule based and machine learning.', 'the rule - based systems use manually built rules that are usually encoded in terms of regular - expression grammars supplemented with lists of abbreviations, common words, proper names, etc.', 'to put together a few rules is fast and easy, but to develop a rule - based system with good performance is quite a labor - consuming enterprise.', 'for instance, the alembic workbench ( #AUTHOR_TAG ) contains a sentence - splitting module that employs over 100 regular - expression rules written in flex.', 'another well - acknowledged shortcoming of rule - based systems is that such systems are usually closely tailored']","['##d systems : rule based and machine learning.', 'the rule - based systems use manually built rules that are usually encoded in terms of regular - expression grammars supplemented with lists of abbreviations, common words, proper names, etc.', 'to put together a few rules is fast and easy, but to develop a rule - based system with good performance is quite a labor - consuming enterprise.', 'for instance, the alembic workbench ( #AUTHOR_TAG ) contains a sentence - splitting module that employs over 100 regular - expression rules written in flex.', 'another well - acknowledged shortcoming of rule - based systems is that such systems are usually closely tailored']","['exist two large classes of sbd systems : rule based and machine learning.', 'the rule - based systems use manually built rules that are usually encoded in terms of regular - expression grammars supplemented with lists of abbreviations, common words, proper names, etc.', 'to put together a few rules is fast and easy, but to develop a rule - based system with good performance is quite a labor - consuming enterprise.', 'for instance, the alembic workbench ( #AUTHOR_TAG ) contains a sentence - splitting module that employs over 100 regular - expression rules written in flex.', 'another well - acknowledged shortcoming of rule - based systems is that such systems are usually closely tailored to a particular corpus or sublanguage and are not easily portable across domains']",0
"['its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'the error rate on sentence boundaries in the brown corpus was not significantly worse than the lowest quoted before ( riley 1989 : 0. 28 % vs. 0. 20 % error rate ).', 'on the wsj corpus our system performed slightly better than the combination of the alembic and satz systems described in #AUTHOR_TAG ( 0. 44 % vs. 0. 5 % error rate ).', 'although these error rates seem to be very small, they are quite significant.', 'unlike general pos tagging, in which it is unfair to expect an error rate of less than 2 % because even human annotators have a disagreement rate of about 3 %, sentence boundaries are much less ambiguous ( with a disagreement of about 1 in 5, 000 ).', 'this shows that an error rate of 1 in 200 ( 0. 5 % ) is still far from reaching the disagreement level.', 'on the other hand, one error in 200 periods means that there is one error in every two documents in the brown corpus and one error in every four documents in the wsj corpus']","['its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'the error rate on sentence boundaries in the brown corpus was not significantly worse than the lowest quoted before ( riley 1989 : 0. 28 % vs. 0. 20 % error rate ).', 'on the wsj corpus our system performed slightly better than the combination of the alembic and satz systems described in #AUTHOR_TAG ( 0. 44 % vs. 0. 5 % error rate ).', 'although these error rates seem to be very small, they are quite significant.', 'unlike general pos tagging, in which it is unfair to expect an error rate of less than 2 % because even human annotators have a disagreement rate of about 3 %, sentence boundaries are much less ambiguous ( with a disagreement of about 1 in 5, 000 ).', 'this shows that an error rate of 1 in 200 ( 0. 5 % ) is still far from reaching the disagreement level.', 'on the other hand, one error in 200 periods means that there is one error in every two documents in the brown corpus and one error in every four documents in the wsj corpus']","['its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'the error rate on sentence boundaries in the brown corpus was not significantly worse than the lowest quoted before ( riley 1989 : 0. 28 % vs. 0. 20 % error rate ).', 'on the wsj corpus our system performed slightly better than the combination of the alembic and satz systems described in #AUTHOR_TAG ( 0. 44 % vs. 0. 5 % error rate ).', 'although these error rates seem to be very small, they are quite significant.', 'unlike general pos tagging, in which it is unfair to expect an error rate of less than 2 % because even human annotators have a disagreement rate of about 3 %, sentence boundaries are much less ambiguous ( with a disagreement of about 1 in 5, 000 ).', 'this shows that an error rate of 1 in 200 ( 0. 5 % ) is still far from reaching the disagreement level.', 'on the other hand, one error in 200 periods means that there is one error in every two documents in the brown corpus and one error in every four documents in the wsj corpus']","['its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'the error rate on sentence boundaries in the brown corpus was not significantly worse than the lowest quoted before ( riley 1989 : 0. 28 % vs. 0. 20 % error rate ).', 'on the wsj corpus our system performed slightly better than the combination of the alembic and satz systems described in #AUTHOR_TAG ( 0. 44 % vs. 0. 5 % error rate ).', 'although these error rates seem to be very small, they are quite significant.', 'unlike general pos tagging, in which it is unfair to expect an error rate of less than 2 % because even human annotators have a disagreement rate of about 3 %, sentence boundaries are much less ambiguous ( with a disagreement of about 1 in 5, 000 ).', 'this shows that an error rate of 1 in 200 ( 0. 5 % ) is still far from reaching the disagreement level.', 'on the other hand, one error in 200 periods means that there is one error in every two documents in the brown corpus and one error in every four documents in the wsj corpus']",1
"['words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99. 8 %.', ""this was done because purely unsupervised techniques ( e. g., baum - welch [ baum and petrie 1966 ] or brill's [ #AUTHOR_TAG b""]","['decided to train the tagger with the minimum of preannotated resources.', 'first, we used 20, 000 tagged words to "" bootstrap "" the training process, because purely unsupervised techniques, at least for the hmm class of taggers, yield lower precision.', 'we also used our dca system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99. 8 %.', ""this was done because purely unsupervised techniques ( e. g., baum - welch [ baum and petrie 1966 ] or brill's [ #AUTHOR_TAG b""]","['decided to train the tagger with the minimum of preannotated resources.', 'first, we used 20, 000 tagged words to "" bootstrap "" the training process, because purely unsupervised techniques, at least for the hmm class of taggers, yield lower precision.', 'we also used our dca system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99. 8 %.', ""this was done because purely unsupervised techniques ( e. g., baum - welch [ baum and petrie 1966 ] or brill's [ #AUTHOR_TAG b ] ) enable regularities to be induced for word classes which contain many entries,""]","['decided to train the tagger with the minimum of preannotated resources.', 'first, we used 20, 000 tagged words to "" bootstrap "" the training process, because purely unsupervised techniques, at least for the hmm class of taggers, yield lower precision.', 'we also used our dca system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99. 8 %.', ""this was done because purely unsupervised techniques ( e. g., baum - welch [ baum and petrie 1966 ] or brill's [ #AUTHOR_TAG b ] ) enable regularities to be induced for word classes which contain many entries, exploiting the fact that individual words that belong to a pos class occur in different ambiguity patterns."", 'counting all possible pos combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent.', 'periods as many other closed - class words cannot be successfully covered by such technique']",1
"['c of table 4 summarizes the highest results known to us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( aberdeen et al. 1995 ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in #AUTHOR_TAG.', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']","['c of table 4 summarizes the highest results known to us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( aberdeen et al. 1995 ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in #AUTHOR_TAG.', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']","['c of table 4 summarizes the highest results known to us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( aberdeen et al. 1995 ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in #AUTHOR_TAG.', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']","['c of table 4 summarizes the highest results known to us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( aberdeen et al. 1995 ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in #AUTHOR_TAG.', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']",1
"['words is usually handled by pos taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus.', 'as #AUTHOR_TAG rightly pointed out, however, ` ` proper nouns and capitalized words are particularly']","['##uation of capitalized words is usually handled by pos taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus.', 'as #AUTHOR_TAG rightly pointed out, however, ` ` proper nouns and capitalized words are particularly']","['##uation of capitalized words is usually handled by pos taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus.', 'as #AUTHOR_TAG rightly pointed out, however, ` ` proper nouns and capitalized words are particularly']","['##uation of capitalized words is usually handled by pos taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus.', 'as #AUTHOR_TAG rightly pointed out, however, ` ` proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not.', 'estimates from the brown corpus can be misleading.', ""for example, the capitalized word'acts'is found twice in the brown corpus, both times as a proper noun ( in a title )."", ""it would be misleading to infer from this evidence that the word'acts'is always a proper noun.""]",1
"['words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99. 8 %.', ""this was done because purely unsupervised techniques ( e. g., baum - welch [ #AUTHOR_TAG ] or brill's [ brill 1995b""]","['decided to train the tagger with the minimum of preannotated resources.', 'first, we used 20, 000 tagged words to "" bootstrap "" the training process, because purely unsupervised techniques, at least for the hmm class of taggers, yield lower precision.', 'we also used our dca system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99. 8 %.', ""this was done because purely unsupervised techniques ( e. g., baum - welch [ #AUTHOR_TAG ] or brill's [ brill 1995b""]","['decided to train the tagger with the minimum of preannotated resources.', 'first, we used 20, 000 tagged words to "" bootstrap "" the training process, because purely unsupervised techniques, at least for the hmm class of taggers, yield lower precision.', 'we also used our dca system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99. 8 %.', ""this was done because purely unsupervised techniques ( e. g., baum - welch [ #AUTHOR_TAG ] or brill's [ brill 1995b""]","['decided to train the tagger with the minimum of preannotated resources.', 'first, we used 20, 000 tagged words to "" bootstrap "" the training process, because purely unsupervised techniques, at least for the hmm class of taggers, yield lower precision.', 'we also used our dca system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99. 8 %.', ""this was done because purely unsupervised techniques ( e. g., baum - welch [ #AUTHOR_TAG ] or brill's [ brill 1995b ] ) enable regularities to be induced for word classes which contain many entries, exploiting the fact that individual words that belong to a pos class occur in different ambiguity patterns."", 'counting all possible pos combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent.', 'periods as many other closed - class words cannot be successfully covered by such technique']",1
"[', unlike english, russian is a highly inflected language, we had to deal with the case normalization issue.', 'before using the dca method, we applied a russian morphological processor ( #AUTHOR_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives, infinitive for verbs, etc..', 'for words that could be normalized to several main forms ( polysemy ), when secondary forms of different words coincided, we retained all the main']","[', unlike english, russian is a highly inflected language, we had to deal with the case normalization issue.', 'before using the dca method, we applied a russian morphological processor ( #AUTHOR_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives, infinitive for verbs, etc..', 'for words that could be normalized to several main forms ( polysemy ), when secondary forms of different words coincided, we retained all the main forms.', 'since the documents in the bbc news corpus were rather short, we applied the cache module, as described in section 11. 1.', 'this allowed us to reuse information across the documents']","[', unlike english, russian is a highly inflected language, we had to deal with the case normalization issue.', 'before using the dca method, we applied a russian morphological processor ( #AUTHOR_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives, infinitive for verbs, etc..', 'for words that could be normalized to several main forms ( polysemy ), when secondary forms of different words coincided, we retained all the main forms.', 'since the documents in the bbc news corpus were rather short, we applied the cache module, as described in section 11. 1.', 'this allowed us to reuse information across the documents']","[', unlike english, russian is a highly inflected language, we had to deal with the case normalization issue.', 'before using the dca method, we applied a russian morphological processor ( #AUTHOR_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives, infinitive for verbs, etc..', 'for words that could be normalized to several main forms ( polysemy ), when secondary forms of different words coincided, we retained all the main forms.', 'since the documents in the bbc news corpus were rather short, we applied the cache module, as described in section 11. 1.', 'this allowed us to reuse information across the documents']",5
"['n - grams ). this is similar to "" one sense per collocation "" idea of #AUTHOR_TAG']","['n - grams ). this is similar to "" one sense per collocation "" idea of #AUTHOR_TAG']","['is also used in our dc', '##a, especially for the identification of abbreviations. in capitalized - word disambiguation, however, we use this assumption with caution and first', 'apply strategies that rely not just on single words but on words together with their local contexts ( n - grams ). this is similar to "" one sense per collocation "" idea of #AUTHOR_TAG']","['kuhn and de Mori (1998) proposed a cache model that works as a kind of short - term', 'memory by which the probability of the most recent n words is increased over the probability of a general - purpose bigram or trigram model. within certain limits, such a model can adapt itself to changes in word frequencies, depending on', 'the topic of the text passage. the dca system is similar in spirit to such dynamic adaptation : it applies word n -', 'grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists. but unlike the cache model, it uses a multipass strategy. Clarkson and Robinson (1997) developed a way of', 'incorporating standard n - grams into the cache model, using mixtures of', ""language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence. in our experiments we applied simple linear interpolation to incorporate the dca system into a pos tag"", '##ger. instead of decaying nonlocal information, we opted for not propagating it from one document for', 'processing of another. for handling very long documents with our method, however, the information decay strategy seems to be', 'the right way to proceed. mani and mac Millan (1995) pointed out that little attention had been paid in the named - entity', 'recognition field to the discourse properties of proper names. they proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context', ', advocating text - driven processing rather than reliance on pre - existing lists. the dca outlined in this article also uses nonlocal discourse context and does not heavily rely', 'on pre - existing word lists. it has been applied not only to the identification of proper names, as described in this article', ', but also to their classification ( mikheev, grover, and moens 1998 ). Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse ( ""', 'one sense per discourse "" ). since then this idea has been applied to several tasks, including word sense disambiguation ( ya', ""##rowsky 1995 ) and named - entity recognition ( cucerzan and yarowsky 1999 ). gale, church, and yarowsky's observation is also used in our dc"", '##a, especially for the identification of abbreviations. in capitalized - word disambiguation, however, we use this assumption with caution and first', 'apply strategies that rely not just on single words but on words together with their local contexts ( n - grams ). this is similar to "" one sense per collocation "" idea of #AUTHOR_TAG']",1
"['first list on which our method relies is a list of common words.', 'this list includes common words for a given language, but no supplementary information such as pos or morphological information is required to be present in this list.', 'a variety of such lists for many languages are already available ( e. g., #AUTHOR_TAG ).', 'words in such lists are usually supplemented with morphological and pos information ( which is not required by our method ).', 'we do']","['first list on which our method relies is a list of common words.', 'this list includes common words for a given language, but no supplementary information such as pos or morphological information is required to be present in this list.', 'a variety of such lists for many languages are already available ( e. g., #AUTHOR_TAG ).', 'words in such lists are usually supplemented with morphological and pos information ( which is not required by our method ).', 'we do']","['first list on which our method relies is a list of common words.', 'this list includes common words for a given language, but no supplementary information such as pos or morphological information is required to be present in this list.', 'a variety of such lists for many languages are already available ( e. g., #AUTHOR_TAG ).', 'words in such lists are usually supplemented with morphological and pos information ( which is not required by our method ).', 'we do not have to rely on pre - existing resources, however.', 'a list of common words can be easily obtained automatically from a']","['first list on which our method relies is a list of common words.', 'this list includes common words for a given language, but no supplementary information such as pos or morphological information is required to be present in this list.', 'a variety of such lists for many languages are already available ( e. g., #AUTHOR_TAG ).', 'words in such lists are usually supplemented with morphological and pos information ( which is not required by our method ).', 'we do not have to rely on pre - existing resources, however.', 'a list of common words can be easily obtained automatically from a raw ( unannotated in any way ) text collection by simply collecting and counting lowercased words in it.', 'we generated such list from the nyt collection.', 'to account for potential spelling and capitalization errors, we included in the list of common words only words that occurred lower - cased at least three times in the nyt texts.', 'the list of common words that we developed from the nyt collection contained about 15, 000 english words']",0
"['its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'the error rate on sentence boundaries in the brown corpus was not significantly worse than the lowest quoted before ( #AUTHOR_TAG : 0. 28 % vs. 0. 20 % error rate ).', 'on the wsj corpus our system performed slightly better than the combination of the alembic and satz systems described in Palmer and Hearst (1997) ( 0. 44 % vs. 0. 5 % error rate ).', 'although these error rates seem to be very small, they are quite significant.', 'unlike general pos tagging, in which it is unfair to expect an error rate of less than 2 % because even human annotators have a disagreement rate of about 3 %, sentence boundaries are much less ambiguous ( with a disagreement of about 1 in 5, 000 ).', 'this shows that an error rate of 1 in 200 ( 0. 5 % ) is still far from reaching the disagreement level.', 'on the other hand, one error in 200 periods means that there is one error in every two documents in the brown corpus and one error in every four documents in the wsj corpus']","['its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'the error rate on sentence boundaries in the brown corpus was not significantly worse than the lowest quoted before ( #AUTHOR_TAG : 0. 28 % vs. 0. 20 % error rate ).', 'on the wsj corpus our system performed slightly better than the combination of the alembic and satz systems described in Palmer and Hearst (1997) ( 0. 44 % vs. 0. 5 % error rate ).', 'although these error rates seem to be very small, they are quite significant.', 'unlike general pos tagging, in which it is unfair to expect an error rate of less than 2 % because even human annotators have a disagreement rate of about 3 %, sentence boundaries are much less ambiguous ( with a disagreement of about 1 in 5, 000 ).', 'this shows that an error rate of 1 in 200 ( 0. 5 % ) is still far from reaching the disagreement level.', 'on the other hand, one error in 200 periods means that there is one error in every two documents in the brown corpus and one error in every four documents in the wsj corpus']","['its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'the error rate on sentence boundaries in the brown corpus was not significantly worse than the lowest quoted before ( #AUTHOR_TAG : 0. 28 % vs. 0. 20 % error rate ).', 'on the wsj corpus our system performed slightly better than the combination of the alembic and satz systems described in Palmer and Hearst (1997) ( 0. 44 % vs. 0. 5 % error rate ).', 'although these error rates seem to be very small, they are quite significant.', 'unlike general pos tagging, in which it is unfair to expect an error rate of less than 2 % because even human annotators have a disagreement rate of about 3 %, sentence boundaries are much less ambiguous ( with a disagreement of about 1 in 5, 000 ).', 'this shows that an error rate of 1 in 200 ( 0. 5 % ) is still far from reaching the disagreement level.', 'on the other hand, one error in 200 periods means that there is one error in every two documents in the brown corpus and one error in every four documents in the wsj corpus']","['its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'the error rate on sentence boundaries in the brown corpus was not significantly worse than the lowest quoted before ( #AUTHOR_TAG : 0. 28 % vs. 0. 20 % error rate ).', 'on the wsj corpus our system performed slightly better than the combination of the alembic and satz systems described in Palmer and Hearst (1997) ( 0. 44 % vs. 0. 5 % error rate ).', 'although these error rates seem to be very small, they are quite significant.', 'unlike general pos tagging, in which it is unfair to expect an error rate of less than 2 % because even human annotators have a disagreement rate of about 3 %, sentence boundaries are much less ambiguous ( with a disagreement of about 1 in 5, 000 ).', 'this shows that an error rate of 1 in 200 ( 0. 5 % ) is still far from reaching the disagreement level.', 'on the other hand, one error in 200 periods means that there is one error in every two documents in the brown corpus and one error in every four documents in the wsj corpus']",1
"['##a and hausman 1998 ).', 'in some systems such dependencies are learned from labeled examples ( #AUTHOR_TAG ).', 'the advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class.', 'the disadvantage is in the cost of building a wide - coverage set of contextual clues manually or producing annotated training data.', 'also, the contextual clues are usually highly specific to the domain and text genre, making such systems very difficult to port']","['the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name.', 'named - entity recognition systems usually use sets of complex hand - crafted rules that employ a gazetteer and a local context ( krupa and hausman 1998 ).', 'in some systems such dependencies are learned from labeled examples ( #AUTHOR_TAG ).', 'the advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class.', 'the disadvantage is in the cost of building a wide - coverage set of contextual clues manually or producing annotated training data.', 'also, the contextual clues are usually highly specific to the domain and text genre, making such systems very difficult to port']","['##a and hausman 1998 ).', 'in some systems such dependencies are learned from labeled examples ( #AUTHOR_TAG ).', 'the advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class.', 'the disadvantage is in the cost of building a wide - coverage set of contextual clues manually or producing annotated training data.', 'also, the contextual clues are usually highly specific to the domain and text genre, making such systems very difficult to port']","['the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name.', 'named - entity recognition systems usually use sets of complex hand - crafted rules that employ a gazetteer and a local context ( krupa and hausman 1998 ).', 'in some systems such dependencies are learned from labeled examples ( #AUTHOR_TAG ).', 'the advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class.', 'the disadvantage is in the cost of building a wide - coverage set of contextual clues manually or producing annotated training data.', 'also, the contextual clues are usually highly specific to the domain and text genre, making such systems very difficult to port']",0
"['abbreviation ( as opposed to regular word ) these four lists can be acquired completely automatically from raw ( unlabeled ) texts.', 'for the development of these lists we used a collection of texts of about 300, 000 words derived from the new york times ( nyt ) corpus that was supplied as training data for the 7th message understanding conference ( muc - 7 ) ( #AUTHOR_TAG ).', 'we used these texts because the approach described in this article was initially designed to be part of a named - entity recognition system ( mikheev, grover, and moens 1998 ) developed for muc - 7.', 'although the corpus size of 300, 000 words can be seen as large, the']","['abbreviation ( as opposed to regular word ) these four lists can be acquired completely automatically from raw ( unlabeled ) texts.', 'for the development of these lists we used a collection of texts of about 300, 000 words derived from the new york times ( nyt ) corpus that was supplied as training data for the 7th message understanding conference ( muc - 7 ) ( #AUTHOR_TAG ).', 'we used these texts because the approach described in this article was initially designed to be part of a named - entity recognition system ( mikheev, grover, and moens 1998 ) developed for muc - 7.', 'although the corpus size of 300, 000 words can be seen as large, the']","['abbreviation ( as opposed to regular word ) these four lists can be acquired completely automatically from raw ( unlabeled ) texts.', 'for the development of these lists we used a collection of texts of about 300, 000 words derived from the new york times ( nyt ) corpus that was supplied as training data for the 7th message understanding conference ( muc - 7 ) ( #AUTHOR_TAG ).', 'we used these texts because the approach described in this article was initially designed to be part of a named - entity recognition system ( mikheev, grover, and moens 1998 ) developed for muc - 7.', 'although the corpus size of 300, 000 words can be seen as large, the fact that this corpus does not have to be annotated in any way and that a corpus of similar size can be easily collected from on - line sources ( including the internet ) makes this resource cheap to obtain']","['abbreviation ( as opposed to regular word ) these four lists can be acquired completely automatically from raw ( unlabeled ) texts.', 'for the development of these lists we used a collection of texts of about 300, 000 words derived from the new york times ( nyt ) corpus that was supplied as training data for the 7th message understanding conference ( muc - 7 ) ( #AUTHOR_TAG ).', 'we used these texts because the approach described in this article was initially designed to be part of a named - entity recognition system ( mikheev, grover, and moens 1998 ) developed for muc - 7.', 'although the corpus size of 300, 000 words can be seen as large, the fact that this corpus does not have to be annotated in any way and that a corpus of similar size can be easily collected from on - line sources ( including the internet ) makes this resource cheap to obtain']",5
"['c of table 4 summarizes the highest results known to us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( aberdeen et al. 1995 ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by #AUTHOR_TAG, who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in Mikheev (2000).', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']","['c of table 4 summarizes the highest results known to us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( aberdeen et al. 1995 ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by #AUTHOR_TAG, who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in Mikheev (2000).', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']","['c of table 4 summarizes the highest results known to us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( aberdeen et al. 1995 ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by #AUTHOR_TAG, who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in Mikheev (2000).', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']","['c of table 4 summarizes the highest results known to us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( aberdeen et al. 1995 ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by #AUTHOR_TAG, who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in Mikheev (2000).', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']",1
"['much information has been published on abbreviation identification.', 'one of the better - known approaches is described in #AUTHOR_TAG, which suggested that abbreviations first be extracted from a corpus using abbreviation - guessing heuristics akin to those described in section 6 and then']","['much information has been published on abbreviation identification.', 'one of the better - known approaches is described in #AUTHOR_TAG, which suggested that abbreviations first be extracted from a corpus using abbreviation - guessing heuristics akin to those described in section 6 and then']","['much information has been published on abbreviation identification.', 'one of the better - known approaches is described in #AUTHOR_TAG, which suggested that abbreviations first be extracted from a corpus using abbreviation - guessing heuristics akin to those described in section 6 and then']","['much information has been published on abbreviation identification.', 'one of the better - known approaches is described in #AUTHOR_TAG, which suggested that abbreviations first be extracted from a corpus using abbreviation - guessing heuristics akin to those described in section 6 and then reused in further processing.', 'this is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level.', 'the main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.', 'Park and Byrd (2001) recently described a hybrid method for finding abbreviations and their definitions.', 'this method first applies an "" abbreviation recognizer "" that generates a set of "" candidate abbreviations "" for a document.', 'then for this set of candidates the system tries to find in the text their definitions ( e. g., united kingdom for uk ).', 'the abbreviation recognizer for these purposes is allowed to overgenerate significantly.', 'there is no harm ( apart from the performance issues ) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained.', 'therefore the abbreviation recognizer treats as a candidate any token of two to ten characters that contains at least one capital letter.', 'candidates then are filtered through a set of known common words and proper names.', 'at the same time many good abbreviations and acronyms are filtered out because not for all of them will definitions exist in the current document']",0
['word sense disambiguation ( #AUTHOR_TAG ) and named'],"['word sense disambiguation ( #AUTHOR_TAG ) and named - entity recognition ( cucerzan and yarowsky 1999 ).', ""gale, church, and yarowsky's""]",['including word sense disambiguation ( #AUTHOR_TAG ) and named'],"['use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'kuhn and de Mori (1998) proposed a cache model that works as a kind of short - term memory by which the probability of the most recent n words is increased over the probability of a general - purpose bigram or trigram model.', 'within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'the dca system is similar in spirit to such dynamic adaptation : it applies word n - grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'but unlike the cache model, it uses a multipass strategy.', ""Clarkson and Robinson (1997) developed a way of incorporating standard n - grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'in our experiments we applied simple linear interpolation to incorporate the dca system into a pos tagger.', 'instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'for handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', 'mani and mac Millan (1995) pointed out that little attention had been paid in the named - entity recognition field to the discourse properties of proper names.', 'they proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text - driven processing rather than reliance on pre - existing lists.', 'the dca outlined in this article also uses nonlocal discourse context and does not heavily rely on pre - existing word lists.', 'it has been applied not only to the identification of proper names, as described in this article, but also to their classification ( mikheev, grover, and moens 1998 ).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse ( "" one sense per discourse "" ).', 'since then this idea has been applied to several tasks, including word sense disambiguation ( #AUTHOR_TAG ) and named - entity recognition ( cucerzan and yarowsky 1999 ).', ""gale, church, and yarowsky's observation is also used in our dca, especially for the identification of abbreviations."", 'in capitalized - word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts ( n - grams ).', 'this']",0
"['important task of text normalization is sentence boundary disambiguation ( sbd ) or sentence splitting.', 'segmenting text into sentences is an important aspect in developing many applications : syntactic parsing, information extraction, machine translation, question answering, text alignment, document summarization, etc. sentence splitting in most cases is a simple matter : a period, an exclamation mark, or a question mark usually signals a sentence boundary.', 'in certain cases, however, a period denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily signal a sentence boundary.', 'furthermore, an abbreviation itself can be the last token in a sentence in which case its period acts at the same time as part of this abbreviation and as the end - of - sentence indicator ( fullstop ).', 'a detailed introduction to the sbd problem can be found in #AUTHOR_TAG']","['important task of text normalization is sentence boundary disambiguation ( sbd ) or sentence splitting.', 'segmenting text into sentences is an important aspect in developing many applications : syntactic parsing, information extraction, machine translation, question answering, text alignment, document summarization, etc. sentence splitting in most cases is a simple matter : a period, an exclamation mark, or a question mark usually signals a sentence boundary.', 'in certain cases, however, a period denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily signal a sentence boundary.', 'furthermore, an abbreviation itself can be the last token in a sentence in which case its period acts at the same time as part of this abbreviation and as the end - of - sentence indicator ( fullstop ).', 'a detailed introduction to the sbd problem can be found in #AUTHOR_TAG']","['important task of text normalization is sentence boundary disambiguation ( sbd ) or sentence splitting.', 'segmenting text into sentences is an important aspect in developing many applications : syntactic parsing, information extraction, machine translation, question answering, text alignment, document summarization, etc. sentence splitting in most cases is a simple matter : a period, an exclamation mark, or a question mark usually signals a sentence boundary.', 'in certain cases, however, a period denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily signal a sentence boundary.', 'furthermore, an abbreviation itself can be the last token in a sentence in which case its period acts at the same time as part of this abbreviation and as the end - of - sentence indicator ( fullstop ).', 'a detailed introduction to the sbd problem can be found in #AUTHOR_TAG']","['important task of text normalization is sentence boundary disambiguation ( sbd ) or sentence splitting.', 'segmenting text into sentences is an important aspect in developing many applications : syntactic parsing, information extraction, machine translation, question answering, text alignment, document summarization, etc. sentence splitting in most cases is a simple matter : a period, an exclamation mark, or a question mark usually signals a sentence boundary.', 'in certain cases, however, a period denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily signal a sentence boundary.', 'furthermore, an abbreviation itself can be the last token in a sentence in which case its period acts at the same time as part of this abbreviation and as the end - of - sentence indicator ( fullstop ).', 'a detailed introduction to the sbd problem can be found in #AUTHOR_TAG']",0
"['instance, we do not expect our system to perform well on single - cased texts ( e. g., texts written in all capital or all lower - cased letters ) or on optical character reader - generated texts.', 'we noted in section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'this is where robust syntactic systems like satz ( palmer and hearst 1997 ) or the pos tagger reported in #AUTHOR_TAG, which do']","['instance, we do not expect our system to perform well on single - cased texts ( e. g., texts written in all capital or all lower - cased letters ) or on optical character reader - generated texts.', 'we noted in section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'this is where robust syntactic systems like satz ( palmer and hearst 1997 ) or the pos tagger reported in #AUTHOR_TAG, which do']","['instance, we do not expect our system to perform well on single - cased texts ( e. g., texts written in all capital or all lower - cased letters ) or on optical character reader - generated texts.', 'we noted in section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'this is where robust syntactic systems like satz ( palmer and hearst 1997 ) or the pos tagger reported in #AUTHOR_TAG, which do not heavily rely on word capitalization and are not sensitive to document length, have an advantage']","['all its strong points, there are a number of restrictions to the proposed approach.', 'first, in its present form it is suitable only for processing of reasonably "" wellbehaved "" texts that consistently use capitalization ( mixed case ) and do not contain much noisy data.', 'thus, for instance, we do not expect our system to perform well on single - cased texts ( e. g., texts written in all capital or all lower - cased letters ) or on optical character reader - generated texts.', 'we noted in section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'this is where robust syntactic systems like satz ( palmer and hearst 1997 ) or the pos tagger reported in #AUTHOR_TAG, which do not heavily rely on word capitalization and are not sensitive to document length, have an advantage']",1
"['for not propagating it from one document for processing of another.', 'for handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', '#AUTHOR_TAG pointed out that little attention had been paid in the named - entity recognition field to the discourse properties of proper names.', '']","[""standard n - grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'in our experiments we applied simple linear interpolation to incorporate the dca system into a pos tagger.', 'instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'for handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', '#AUTHOR_TAG pointed out that little attention had been paid in the named - entity recognition field to the discourse properties of proper names.', '']","['##hn and de Mori (1998) proposed a cache model that works as a kind of short - term memory by which the probability of the most recent n words is increased over the probability of a general - purpose bigram or trigram model.', 'within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'the dca system is similar in spirit to such dynamic adaptation : it applies word n - grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'but unlike the cache model, it uses a multipass strategy.', ""Clarkson and Robinson (1997) developed a way of incorporating standard n - grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'in our experiments we applied simple linear interpolation to incorporate the dca system into a pos tagger.', 'instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'for handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', '#AUTHOR_TAG pointed out that little attention had been paid in the named - entity recognition field to the discourse properties of proper names.', 'they proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text - driven processing']","['use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'kuhn and de Mori (1998) proposed a cache model that works as a kind of short - term memory by which the probability of the most recent n words is increased over the probability of a general - purpose bigram or trigram model.', 'within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'the dca system is similar in spirit to such dynamic adaptation : it applies word n - grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'but unlike the cache model, it uses a multipass strategy.', ""Clarkson and Robinson (1997) developed a way of incorporating standard n - grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'in our experiments we applied simple linear interpolation to incorporate the dca system into a pos tagger.', 'instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'for handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', '#AUTHOR_TAG pointed out that little attention had been paid in the named - entity recognition field to the discourse properties of proper names.', 'they proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text - driven processing rather than reliance on pre - existing lists.', 'the dca outlined in this article also uses nonlocal discourse context and does not heavily rely on pre - existing word lists.', 'it has been applied not only to the identification of proper names, as described in this article, but also to their classification ( mikheev, grover, and moens 1998 ).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse ( "" one sense per discourse "" ).', 'since then this idea has been applied to several tasks, including word sense disambiguation ( yarowsky 1995 ) and named - entity recognition ( cucerzan and yarowsky 1999 ).', ""gale, church, and yarowsky's observation is also used in our dca, especially for the identification of abbreviations."", 'in capitalized - word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts ( n - grams ).', '']",5
"['test our hypothesis that dca can be used as a complement to a local - context approach, we combined our main configuration ( evaluated in row d of table 4 ) with a pos tagger.', 'unlike other pos taggers, this pos tagger ( #AUTHOR_TAG ) was also trained to disambiguate sentence boundaries']","['test our hypothesis that dca can be used as a complement to a local - context approach, we combined our main configuration ( evaluated in row d of table 4 ) with a pos tagger.', 'unlike other pos taggers, this pos tagger ( #AUTHOR_TAG ) was also trained to disambiguate sentence boundaries']","['test our hypothesis that dca can be used as a complement to a local - context approach, we combined our main configuration ( evaluated in row d of table 4 ) with a pos tagger.', 'unlike other pos taggers, this pos tagger ( #AUTHOR_TAG ) was also trained to disambiguate sentence boundaries']","['test our hypothesis that dca can be used as a complement to a local - context approach, we combined our main configuration ( evaluated in row d of table 4 ) with a pos tagger.', 'unlike other pos taggers, this pos tagger ( #AUTHOR_TAG ) was also trained to disambiguate sentence boundaries']",5
"['c of table 4 summarizes the highest results known to us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( #AUTHOR_TAG ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in Mikheev (2000).', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']","['c of table 4 summarizes the highest results known to us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( #AUTHOR_TAG ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in Mikheev (2000).', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']","['c of table 4 summarizes the highest results known to us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( #AUTHOR_TAG ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in Mikheev (2000).', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']","['c of table 4 summarizes the highest results known to us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( #AUTHOR_TAG ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in Mikheev (2000).', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']",1
"['to the proposed approach.', 'first, in its present form it is suitable only for processing of reasonably "" wellbehaved "" texts that consistently use capitalization ( mixed case ) and do not contain much noisy data.', 'thus, for instance, we do not expect our system to perform well on single - cased texts ( e. g., texts written in all capital or all lower - cased letters ) or on optical character reader - generated texts.', 'we noted in section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'this is where robust syntactic systems like satz ( #AUTHOR_TAG ) or the pos tagger reported in Mikheev ( 2000 ), which do']","['to the proposed approach.', 'first, in its present form it is suitable only for processing of reasonably "" wellbehaved "" texts that consistently use capitalization ( mixed case ) and do not contain much noisy data.', 'thus, for instance, we do not expect our system to perform well on single - cased texts ( e. g., texts written in all capital or all lower - cased letters ) or on optical character reader - generated texts.', 'we noted in section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'this is where robust syntactic systems like satz ( #AUTHOR_TAG ) or the pos tagger reported in Mikheev ( 2000 ), which do']","['to the proposed approach.', 'first, in its present form it is suitable only for processing of reasonably "" wellbehaved "" texts that consistently use capitalization ( mixed case ) and do not contain much noisy data.', 'thus, for instance, we do not expect our system to perform well on single - cased texts ( e. g., texts written in all capital or all lower - cased letters ) or on optical character reader - generated texts.', 'we noted in section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'this is where robust syntactic systems like satz ( #AUTHOR_TAG ) or the pos tagger reported in Mikheev ( 2000 ), which do not heavily rely on word capitalization and are not sensitive to document length, have an advantage']","['all its strong points, there are a number of restrictions to the proposed approach.', 'first, in its present form it is suitable only for processing of reasonably "" wellbehaved "" texts that consistently use capitalization ( mixed case ) and do not contain much noisy data.', 'thus, for instance, we do not expect our system to perform well on single - cased texts ( e. g., texts written in all capital or all lower - cased letters ) or on optical character reader - generated texts.', 'we noted in section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'this is where robust syntactic systems like satz ( #AUTHOR_TAG ) or the pos tagger reported in Mikheev ( 2000 ), which do not heavily rely on word capitalization and are not sensitive to document length, have an advantage']",1
"['##hn and de Mori (1998) proposed a cache model that works as a kind of short - term memory by which the probability of the most recent n words is increased over the probability of a general - purpose bigram or trigram model.', 'within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'the dca system is similar in spirit to such dynamic adaptation : it applies word n - grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'but unlike the cache model, it uses a multipass strategy.', '#AUTHOR_TAG developed a way of']","['use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'kuhn and de Mori (1998) proposed a cache model that works as a kind of short - term memory by which the probability of the most recent n words is increased over the probability of a general - purpose bigram or trigram model.', 'within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'the dca system is similar in spirit to such dynamic adaptation : it applies word n - grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'but unlike the cache model, it uses a multipass strategy.', '#AUTHOR_TAG developed a way of']","['##hn and de Mori (1998) proposed a cache model that works as a kind of short - term memory by which the probability of the most recent n words is increased over the probability of a general - purpose bigram or trigram model.', 'within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'the dca system is similar in spirit to such dynamic adaptation : it applies word n - grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'but unlike the cache model, it uses a multipass strategy.', ""#AUTHOR_TAG developed a way of incorporating standard n - grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last"", 'in our experiments we applied simple linear interpolation to']","['use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'kuhn and de Mori (1998) proposed a cache model that works as a kind of short - term memory by which the probability of the most recent n words is increased over the probability of a general - purpose bigram or trigram model.', 'within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'the dca system is similar in spirit to such dynamic adaptation : it applies word n - grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'but unlike the cache model, it uses a multipass strategy.', ""#AUTHOR_TAG developed a way of incorporating standard n - grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last"", 'in our experiments we applied simple linear interpolation to incorporate the dca system into a pos tagger.', 'instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'for handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', 'mani and mac Millan (1995) pointed out that little attention had been paid in the named - entity recognition field to the discourse properties of proper names.', 'they proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text - driven processing rather than reliance on pre - existing lists.', 'the dca outlined in this article also uses nonlocal discourse context and does not heavily rely on pre - existing word lists.', 'it has been applied not only to the identification of proper names, as described in this article, but also to their classification ( mikheev, grover, and moens 1998 ).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse ( "" one sense per discourse "" ).', 'since then this idea has been applied to several tasks, including word sense disambiguation ( yarowsky 1995 ) and named - entity recognition ( cucerzan and yarowsky 1999 ).', ""gale, church, and yarowsky's observation is also used in our dca, especially for the identification of abbreviations."", 'in capitalized - word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts ( n - grams']",2
['of the eagle workbench for linguistic engineering ( #AUTHOR_TAG )'],['of the eagle workbench for linguistic engineering ( #AUTHOR_TAG )'],"['description of the eagle workbench for linguistic engineering ( #AUTHOR_TAG ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower - cased in the same document.', 'this heuristic also employs a database of bigrams and unigrams of lower - cased and capitalized words found in unambiguous positions.', 'it is quite similar to our method for capitalized - word disambiguation.', 'the description of the eagle case normalization module provided by baldwin et al. is, however, very brief and provides no performance evaluation or other details']","['description of the eagle workbench for linguistic engineering ( #AUTHOR_TAG ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower - cased in the same document.', 'this heuristic also employs a database of bigrams and unigrams of lower - cased and capitalized words found in unambiguous positions.', 'it is quite similar to our method for capitalized - word disambiguation.', 'the description of the eagle case normalization module provided by baldwin et al. is, however, very brief and provides no performance evaluation or other details']",1
"['1989 ), neural networks ( palmer and hearst 1994 ), and maximum - entropy modeling ( #AUTHOR_TAG ).', 'machine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']","['trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( riley 1989 ), neural networks ( palmer and hearst 1994 ), and maximum - entropy modeling ( #AUTHOR_TAG ).', 'machine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']","['trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( riley 1989 ), neural networks ( palmer and hearst 1994 ), and maximum - entropy modeling ( #AUTHOR_TAG ).', 'machine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']","['trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( riley 1989 ), neural networks ( palmer and hearst 1994 ), and maximum - entropy modeling ( #AUTHOR_TAG ).', 'machine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']",0
"['elephants are....', 'the disambiguation of capitalized words in ambiguous positions leads to the identification of proper names ( or their derivatives ), and in this article we will use these two terms and the term case normalization interchangeably.', '#AUTHOR_TAG, p.']","['elephants are....', 'the disambiguation of capitalized words in ambiguous positions leads to the identification of proper names ( or their derivatives ), and in this article we will use these two terms and the term case normalization interchangeably.', '#AUTHOR_TAG, p.']","['.', 'capitalized words in these and some other positions present a case of ambiguity : they can stand for proper names, as in white later said...', ', or they can be just capitalized common words, as in white elephants are....', 'the disambiguation of capitalized words in ambiguous positions leads to the identification of proper names ( or their derivatives ), and in this article we will use these two terms and the term case normalization interchangeably.', '#AUTHOR_TAG, p.']","['##uation of capitalized words in mixed - case texts has received little attention in the natural language processing and information retrieval communities, but in fact it plays an important role in many tasks.', 'in mixed - case texts capitalized words usually denote proper names ( names of organizations, locations, people, artifacts, etc. ), but there are special positions in the text where capitalization is expected.', 'such mandatory positions include the first word in a sentence, words in titles with all significant words capitalized or table entries, a capitalized word after a colon or open quote, and the first word in a list entry, among others.', 'capitalized words in these and some other positions present a case of ambiguity : they can stand for proper names, as in white later said...', ', or they can be just capitalized common words, as in white elephants are....', 'the disambiguation of capitalized words in ambiguous positions leads to the identification of proper names ( or their derivatives ), and in this article we will use these two terms and the term case normalization interchangeably.', ""#AUTHOR_TAG, p. 294 ) studied, among other simple text normalization techniques, the effect of case normalization for different words and showed that ` ` sometimes case variants refer to the same thing ( hurricane and hurricane ), sometimes they refer to different things ( continental and continental ) and sometimes they don't refer to much of anything ( e. g., anytime and anytime ).''"", 'obviously these differences arise because some capitalized words stand for proper names ( such as continental, the name of an airline ) and some do not']",0
"['a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions.', 'this method first applies an "" abbreviation recognizer "" that generates a set of "" candidate abbreviations "" for a document.', 'then for this set of candidates the system tries to find in the text their definitions ( e. g., united kingdom for uk ).', 'the abbreviation recognizer for these purposes is allowed to overgenerate significantly.', 'there is no harm ( apart from the performance issues ) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained.', 'therefore the abbreviation recognizer treats as a candidate any token of two to ten characters that contains at least one capital letter.', 'candidates then are filtered through a set of known common words and proper names.', 'at the same time many good abbreviations and acronyms are']","['a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions.', 'this method first applies an "" abbreviation recognizer "" that generates a set of "" candidate abbreviations "" for a document.', 'then for this set of candidates the system tries to find in the text their definitions ( e. g., united kingdom for uk ).', 'the abbreviation recognizer for these purposes is allowed to overgenerate significantly.', 'there is no harm ( apart from the performance issues ) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained.', 'therefore the abbreviation recognizer treats as a candidate any token of two to ten characters that contains at least one capital letter.', 'candidates then are filtered through a set of known common words and proper names.', 'at the same time many good abbreviations and acronyms are']","['a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions.', 'this method first applies an "" abbreviation recognizer "" that generates a set of "" candidate abbreviations "" for a document.', 'then for this set of candidates the system tries to find in the text their definitions ( e. g., united kingdom for uk ).', 'the abbreviation recognizer for these purposes is allowed to overgenerate significantly.', 'there is no harm ( apart from the performance issues ) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained.', 'therefore the abbreviation recognizer treats as a candidate any token of two to ten characters that contains at least one capital letter.', 'candidates then are filtered through a set of known common words and proper names.', 'at the same time many good abbreviations and acronyms']","['much information has been published on abbreviation identification.', 'one of the better - known approaches is described in Grefenstette and Tapanainen (1994), which suggested that abbreviations first be extracted from a corpus using abbreviation - guessing heuristics akin to those described in section 6 and then reused in further processing.', 'this is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level.', 'the main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions.', 'this method first applies an "" abbreviation recognizer "" that generates a set of "" candidate abbreviations "" for a document.', 'then for this set of candidates the system tries to find in the text their definitions ( e. g., united kingdom for uk ).', 'the abbreviation recognizer for these purposes is allowed to overgenerate significantly.', 'there is no harm ( apart from the performance issues ) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained.', 'therefore the abbreviation recognizer treats as a candidate any token of two to ten characters that contains at least one capital letter.', 'candidates then are filtered through a set of known common words and proper names.', 'at the same time many good abbreviations and acronyms are filtered out because not for all of them will definitions exist in the current document']",0
"['human intervention.', 'the four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation.', 'although some sbd systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance.', 'for instance, #AUTHOR_TAG report that the satz system ( decision tree variant ) was trained on a set of about 800 labeled periods, which corresponds to a corpus of about 16, 000 words.', ""this is a relatively small training set that can be manually marked in a few hours'time."", 'but the error rate ( 1. 5 % ) of the decision tree classifier trained on this small sample was about 50 % higher than that when trained on 6, 000 labeled examples ( 1. 0 % )']","['human intervention.', 'the four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation.', 'although some sbd systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance.', 'for instance, #AUTHOR_TAG report that the satz system ( decision tree variant ) was trained on a set of about 800 labeled periods, which corresponds to a corpus of about 16, 000 words.', ""this is a relatively small training set that can be manually marked in a few hours'time."", 'but the error rate ( 1. 5 % ) of the decision tree classifier trained on this small sample was about 50 % higher than that when trained on 6, 000 labeled examples ( 1. 0 % )']","['human intervention.', 'the four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation.', 'although some sbd systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance.', 'for instance, #AUTHOR_TAG report that the satz system ( decision tree variant ) was trained on a set of about 800 labeled periods, which corresponds to a corpus of about 16, 000 words.', ""this is a relatively small training set that can be manually marked in a few hours'time."", 'but the error rate ( 1. 5 % ) of the decision tree classifier trained on this small sample was about 50 % higher than that when trained on 6, 000 labeled examples ( 1. 0 % )']","['significant advantage of this approach is that it can be targeted to new domains completely automatically, without human intervention.', 'the four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation.', 'although some sbd systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance.', 'for instance, #AUTHOR_TAG report that the satz system ( decision tree variant ) was trained on a set of about 800 labeled periods, which corresponds to a corpus of about 16, 000 words.', ""this is a relatively small training set that can be manually marked in a few hours'time."", 'but the error rate ( 1. 5 % ) of the decision tree classifier trained on this small sample was about 50 % higher than that when trained on 6, 000 labeled examples ( 1. 0 % )']",1
"['sato and nagao 1990 ; veale and way 1997 ; #AUTHOR_TAG ). 7 as an example, consider the translation into french of the house collapsed']","['sato and nagao 1990 ; veale and way 1997 ; #AUTHOR_TAG ). 7 as an example, consider the translation into french of the house collapsed']","['##xxx sato and nagao 1990 ; veale and way 1997 ; #AUTHOR_TAG ). 7 as an example, consider the translation into french of the house collapsed']","['translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'in order to calculate a ranking for each tl sentence produced, we multiply the weights of each chunk used in its construction.', 'note that this ensures that greater importance is attributed to longer chunks, as is usual in most ebmt systems ( cfxxx sato and nagao 1990 ; veale and way 1997 ; #AUTHOR_TAG ). 7 as an example, consider the translation into french of the house collapsed']",0
