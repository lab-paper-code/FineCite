unique_id,citing_id,citing_title,cited_title,cited_authors,section_title,cited_abstract,citation_context,cite_context_paragraph,citation_class_label,cite_context_-1_sent,cite_context_sent_+1,cite_context_-1_sent_+1,cite_context_-2_sent,cite_context_sent_+2,cite_context_-2_sent_+1,cite_context_-1_sent_+2,cite_context_-3_sent,cite_context_sent_+3
CC1160,P97-1063,A word-to-word model of translational equivalence,a geometric approach to mapping bitext correspondencequot,['I D Melamed'],method,,"Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( #AUTHOR_TAGc ) .","['2We could just as easily use other symmetric ""association"" measures, such as Â¢2  or the Dice coefficient (Smadja, 1992).', 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.', 'Then vk and uk+l will also co-occur more often than expected by chance.', 'The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk.', 'Models of translational equivalence that are ignorant of indirect associations have ""a tendency ... to be confused by collocates"" (Dagan et al., 1993).', 'Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( #AUTHOR_TAGc ) .', ""The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest."", 'The competitive linking algorithm implements this heuristic:']",0,"['However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. ( #AUTHOR_TAG ; Melamed , 1996b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. ( #AUTHOR_TAG ; Melamed , 1996b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. ( #AUTHOR_TAG ; Melamed , 1996b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. ( #AUTHOR_TAG ; Melamed , 1996b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. ( #AUTHOR_TAG ; Melamed , 1996b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. ( #AUTHOR_TAG ; Melamed , 1996b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. ( #AUTHOR_TAG ; Melamed , 1996b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. ( #AUTHOR_TAG ; Melamed , 1996b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. ( #AUTHOR_TAG ; Melamed , 1996b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"
CC1161,P97-1063,A word-to-word model of translational equivalence,a statistical approach to machine translationquot,"['P F Brown', 'J Cocke', 'S Della Pietra', 'V Della Pietra', 'F Jelinek', 'R Mercer', 'P Roossin']",introduction,,"Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; #AUTHOR_TAG ; Brown et al. , 1993a ) .","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; #AUTHOR_TAG ; Brown et al. , 1993a ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.', '(Macklovitch, 1994;Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']",0,"['The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .']","['This imbalance foils thresholding strategies , clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']","['The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']","['Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .']","['This imbalance foils thresholding strategies , clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']","['Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']","['The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']","['Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .']","['This imbalance foils thresholding strategies , clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']"
CC1162,P97-1063,A word-to-word model of translational equivalence,robust word alignment for machine aided translationquot,"['I Dagan', 'K Church', 'SZ W Gale']",method,,Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates '' ( #AUTHOR_TAG ) .,"['2We could just as easily use other symmetric ""association"" measures, such as Â¢2  or the Dice coefficient (Smadja, 1992).', 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.', 'Then vk and uk+l will also co-occur more often than expected by chance.', 'The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk.', ""Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates '' ( #AUTHOR_TAG ) ."", 'Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c).', ""The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest."", 'The competitive linking algorithm implements this heuristic:']",0,"['However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"
CC1163,P97-1063,A word-to-word model of translational equivalence,using bitextual alignment for translation validation the transcheck systemquot,['E Macklovitch'],introduction,,"Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. ( #AUTHOR_TAG ; Melamed , 1996b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).","['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. ( #AUTHOR_TAG ; Melamed , 1996b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']",0,"['However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #AUTHOR_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #AUTHOR_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #AUTHOR_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #AUTHOR_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #AUTHOR_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #AUTHOR_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #AUTHOR_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #AUTHOR_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #AUTHOR_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"
CC1164,P97-1063,A word-to-word model of translational equivalence,a program for aligning sentences in bilingual corporaquot,"['W Gale', 'K W Church']",,,"This imbalance foils thresholding strategies , clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .","['Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']",0,"['2We could just as easily use other symmetric ""association"" measures, such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( #AUTHOR_TAG ) .']","['2We could just as easily use other symmetric ""association"" measures, such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( #AUTHOR_TAG ) .', 'co-occur is called a direct association.']","['2We could just as easily use other symmetric ""association"" measures, such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( #AUTHOR_TAG ) .', 'co-occur is called a direct association.']","['2We could just as easily use other symmetric ""association"" measures, such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( #AUTHOR_TAG ) .']","['2We could just as easily use other symmetric ""association"" measures, such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( #AUTHOR_TAG ) .', 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.']","['2We could just as easily use other symmetric ""association"" measures, such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( #AUTHOR_TAG ) .', 'co-occur is called a direct association.']","['2We could just as easily use other symmetric ""association"" measures, such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( #AUTHOR_TAG ) .', 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.']","['2We could just as easily use other symmetric ""association"" measures, such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( #AUTHOR_TAG ) .']","['2We could just as easily use other symmetric ""association"" measures, such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( #AUTHOR_TAG ) .', 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.', 'Then vk and uk+l will also co-occur more often than expected by chance.']"
CC1165,P97-1063,A word-to-word model of translational equivalence,a program for aligning sentences in bilingual corporaquot,"['W Gale', 'K W Church']",introduction,,"Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).","['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']",0,"['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993a ) .']","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993a ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.']","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993a ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.']","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993a ) .']","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993a ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.']","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993a ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.']","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993a ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.']","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993a ) .']","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993a ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.', '(Macklovitch, 1994;Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"
CC1166,P97-1063,A word-to-word model of translational equivalence,a geometric approach to mapping bitext correspondencequot,['I D Melamed'],introduction,,"Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #AUTHOR_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).","['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #AUTHOR_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']",0,"['Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAGa ) .']","['The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAGa ) .', 'Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.']","['Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAGa ) .', 'Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.']","['A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAGa ) .']","['The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAGa ) .', 'Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.', 'The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995;.']","['A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAGa ) .', 'Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.']","['Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAGa ) .', 'Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.', 'The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995;.']","['Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAGa ) .']","['The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAGa ) .', 'Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.', 'The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995;.']"
CC1167,P97-1063,A word-to-word model of translational equivalence,how to compile a bilingual collocational lexicon automaticallyquot,['F Smadja'],method,,"2We could just as easily use other symmetric ""association"" measures, such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( #AUTHOR_TAG ) .","['2We could just as easily use other symmetric ""association"" measures, such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( #AUTHOR_TAG ) .', 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.', 'Then vk and uk+l will also co-occur more often than expected by chance.', 'The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk.', 'Models of translational equivalence that are ignorant of indirect associations have ""a tendency ... to be confused by collocates"" (Dagan et al., 1993).', 'Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c).', ""The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest."", 'The competitive linking algorithm implements this heuristic:']",1,"['We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( #AUTHOR_TAG ) .']","['We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( #AUTHOR_TAG ) .']","['We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( #AUTHOR_TAG ) .']","['We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( #AUTHOR_TAG ) .']","['We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( #AUTHOR_TAG ) .']","['We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( #AUTHOR_TAG ) .']","['We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( #AUTHOR_TAG ) .']","['We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( #AUTHOR_TAG ) .']","['We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( #AUTHOR_TAG ) .']"
CC1168,P97-1063,A word-to-word model of translational equivalence,a statistical approach to language translationquot,"['P F Brown', 'J Cocke', 'S Della Pietra', 'V Della Pietra', 'F Jelinek', 'R Mercer', 'P Roossin']",introduction,,"Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993a ) .","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993a ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.', '(Macklovitch, 1994;Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']",0,"['This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ) .']","['It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']","['This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ) .']","['It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']","['This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ) .']","['It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']"
CC1169,P97-1063,A word-to-word model of translational equivalence,a geometric approach to mapping bitext correspondencequot,['I D Melamed'],introduction,,"The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAGa ) .","['Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAGa ) .', 'Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.', 'The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995;.']",0,"['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .']","['#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .', 'Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.']","['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .']","['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .']","['#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .', 'Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'For many applications, this is the desired behavior.']","['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .', 'Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.']","['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .', 'Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'For many applications, this is the desired behavior.']","['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .']","['#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .', 'Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'For many applications, this is the desired behavior.', ""The most detailed evaluation of link tokens to date was performed by (Macklovitch & Hannan, 1996), who trained Brown et al.'s Model 2 on 74 million words of the Canadian Hansards.""]"
CC1170,P97-1063,A word-to-word model of translational equivalence,a program for aligning sentences in bilingual corporaquot,"['W Gale', 'K W Church']",,,"We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( #AUTHOR_TAG ) .","['We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( #AUTHOR_TAG ) .']",5,"['Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).']","['Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.']","['Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.']","['Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).']","['Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.']","['Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.']","['Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.']","['Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).']","['Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other Melamed, 1996a).']"
CC1171,P97-1063,A word-to-word model of translational equivalence,robust word alignment for machine aided translationquot,"['I Dagan', 'K Church', 'SZ W Gale']",method,,"It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ) .","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']",0,"['However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including ""crummy"" MT on the World Wide Web ( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996b ) ) , concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including ""crummy"" MT on the World Wide Web ( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996b ) ) , concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including ""crummy"" MT on the World Wide Web ( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996b ) ) , concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Over the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including ""crummy"" MT on the World Wide Web ( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996b ) ) , concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including ""crummy"" MT on the World Wide Web ( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996b ) ) , concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Over the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including ""crummy"" MT on the World Wide Web ( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996b ) ) , concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including ""crummy"" MT on the World Wide Web ( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996b ) ) , concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Over the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including ""crummy"" MT on the World Wide Web ( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996b ) ) , concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']","['Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including ""crummy"" MT on the World Wide Web ( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996b ) ) , concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"
CC1172,P97-1063,A word-to-word model of translational equivalence,semiautomatic acquisition of domainspecific translation lexiconsquot,['personal communication Nasr'],,,#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .,"['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .', 'Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'For many applications, this is the desired behavior.', ""The most detailed evaluation of link tokens to date was performed by (Macklovitch & Hannan, 1996), who trained Brown et al.'s Model 2 on 74 million words of the Canadian Hansards."", 'These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.', 'The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words.', 'Therefore, we ignored English words that were linked to nothing.']",0,"['Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy .', ""Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #AUTHOR_TAG ) .""]","[""Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #AUTHOR_TAG ) ."", 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.']","['Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy .', ""Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #AUTHOR_TAG ) ."", 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.']","['Even better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy .', ""Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #AUTHOR_TAG ) .""]","[""Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #AUTHOR_TAG ) ."", 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.', 'In this manner, the model can account for a wider range of translation phenomena.']","['Even better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy .', ""Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #AUTHOR_TAG ) ."", 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.']","['Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy .', ""Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #AUTHOR_TAG ) ."", 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.', 'In this manner, the model can account for a wider range of translation phenomena.']","['Even better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy .', ""Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #AUTHOR_TAG ) .""]","[""Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #AUTHOR_TAG ) ."", 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.', 'In this manner, the model can account for a wider range of translation phenomena.']"
CC1173,P97-1063,A word-to-word model of translational equivalence,a program for aligning sentences in bilingual corporaquot,"['W Gale', 'K W Church']",introduction,,"Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).","['Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other Melamed, 1996a).', 'Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.', 'The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995;.']",0,"['L(u,v) represents the likelihood that u and v can be mutual translations.', 'For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( Ã¢\x80\x9e , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #AUTHOR_TAG ) 2 .']","['For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( Ã¢\x80\x9e , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #AUTHOR_TAG ) 2 .', ""When the L(u, v) are re-estimated, the model's hidden parameters come into play.""]","['L(u,v) represents the likelihood that u and v can be mutual translations.', 'For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( Ã¢\x80\x9e , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #AUTHOR_TAG ) 2 .', ""When the L(u, v) are re-estimated, the model's hidden parameters come into play.""]","['The two hidden parameters are the probabilities of the model generating true and false positives in the data.', 'L(u,v) represents the likelihood that u and v can be mutual translations.', 'For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( Ã¢\x80\x9e , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #AUTHOR_TAG ) 2 .']","['For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( Ã¢\x80\x9e , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #AUTHOR_TAG ) 2 .', ""When the L(u, v) are re-estimated, the model's hidden parameters come into play.""]","['The two hidden parameters are the probabilities of the model generating true and false positives in the data.', 'L(u,v) represents the likelihood that u and v can be mutual translations.', 'For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( Ã¢\x80\x9e , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #AUTHOR_TAG ) 2 .', ""When the L(u, v) are re-estimated, the model's hidden parameters come into play.""]","['L(u,v) represents the likelihood that u and v can be mutual translations.', 'For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( Ã¢\x80\x9e , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #AUTHOR_TAG ) 2 .', ""When the L(u, v) are re-estimated, the model's hidden parameters come into play.""]","['Our translation model consists of the hidden parameters A+ and A-, and likelihood ratios L(u, v).', 'The two hidden parameters are the probabilities of the model generating true and false positives in the data.', 'L(u,v) represents the likelihood that u and v can be mutual translations.', 'For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( Ã¢\x80\x9e , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #AUTHOR_TAG ) 2 .']","['For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( Ã¢\x80\x9e , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #AUTHOR_TAG ) 2 .', ""When the L(u, v) are re-estimated, the model's hidden parameters come into play.""]"
CC1174,P97-1063,A word-to-word model of translational equivalence,deriving translation data from bilingual textsquot,"['R Catizone', 'G Russell', 'S Warwick']",introduction,,"Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including ""crummy"" MT on the World Wide Web ( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996b ) ) , concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).","['Over the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including ""crummy"" MT on the World Wide Web ( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996b ) ) , concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']",0,"['With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; #AUTHOR_TAG ) .']","['With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; #AUTHOR_TAG ) .', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.']","['With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; #AUTHOR_TAG ) .', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.']","['With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; #AUTHOR_TAG ) .']","['With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; #AUTHOR_TAG ) .', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.']","['With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; #AUTHOR_TAG ) .', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.']","['With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; #AUTHOR_TAG ) .', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.']","['With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; #AUTHOR_TAG ) .']","['With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; #AUTHOR_TAG ) .', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other (Gale & Church, 1991; Melamed, 1996a).']"
CC1175,P97-1063,A word-to-word model of translational equivalence,how to compile a bilingual collocational lexicon automaticallyquot,['F Smadja'],conclusion,,Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #AUTHOR_TAG ) .,"['Even better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy .', ""Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #AUTHOR_TAG ) ."", 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.', 'In this manner, the model can account for a wider range of translation phenomena.']",3,"['This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ) .']","['It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']","['This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ) .']","['It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']","['This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ) .']","['It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']"
CC1176,P97-1063,A word-to-word model of translational equivalence,accurate methods for the statistics of surprise and coincidencequot,['T Dunning'],method,,"For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( Ã¢ÂÂ , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #AUTHOR_TAG ) 2 .","['Our translation model consists of the hidden parameters A+ and A-, and likelihood ratios L(u, v).', 'The two hidden parameters are the probabilities of the model generating true and false positives in the data.', 'L(u,v) represents the likelihood that u and v can be mutual translations.', 'For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( Ã¢\x80\x9e , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #AUTHOR_TAG ) 2 .', ""When the L(u, v) are re-estimated, the model's hidden parameters come into play.""]",5,"[""One advantage that Brown et al.'s Model i has over our word-to-word model is that their objective function has no local maxima."", 'By using the EM algorithm ( #AUTHOR_TAG ) , they can guarantee convergence towards the globally optimum parameter set .']","['By using the EM algorithm ( #AUTHOR_TAG ) , they can guarantee convergence towards the globally optimum parameter set .', 'In contrast, the dynamic nature of the competitive linking algorithm changes the Pr(datalmodel ) in a non-monotonic fashion.']","[""One advantage that Brown et al.'s Model i has over our word-to-word model is that their objective function has no local maxima."", 'By using the EM algorithm ( #AUTHOR_TAG ) , they can guarantee convergence towards the globally optimum parameter set .', 'In contrast, the dynamic nature of the competitive linking algorithm changes the Pr(datalmodel ) in a non-monotonic fashion.']","[""One advantage that Brown et al.'s Model i has over our word-to-word model is that their objective function has no local maxima."", 'By using the EM algorithm ( #AUTHOR_TAG ) , they can guarantee convergence towards the globally optimum parameter set .']","['By using the EM algorithm ( #AUTHOR_TAG ) , they can guarantee convergence towards the globally optimum parameter set .', 'In contrast, the dynamic nature of the competitive linking algorithm changes the Pr(datalmodel ) in a non-monotonic fashion.', 'We have adopted the simple heuristic that the model ""has converged"" when this probability stops increasing.']","[""One advantage that Brown et al.'s Model i has over our word-to-word model is that their objective function has no local maxima."", 'By using the EM algorithm ( #AUTHOR_TAG ) , they can guarantee convergence towards the globally optimum parameter set .', 'In contrast, the dynamic nature of the competitive linking algorithm changes the Pr(datalmodel ) in a non-monotonic fashion.']","[""One advantage that Brown et al.'s Model i has over our word-to-word model is that their objective function has no local maxima."", 'By using the EM algorithm ( #AUTHOR_TAG ) , they can guarantee convergence towards the globally optimum parameter set .', 'In contrast, the dynamic nature of the competitive linking algorithm changes the Pr(datalmodel ) in a non-monotonic fashion.', 'We have adopted the simple heuristic that the model ""has converged"" when this probability stops increasing.']","[""One advantage that Brown et al.'s Model i has over our word-to-word model is that their objective function has no local maxima."", 'By using the EM algorithm ( #AUTHOR_TAG ) , they can guarantee convergence towards the globally optimum parameter set .']","['By using the EM algorithm ( #AUTHOR_TAG ) , they can guarantee convergence towards the globally optimum parameter set .', 'In contrast, the dynamic nature of the competitive linking algorithm changes the Pr(datalmodel ) in a non-monotonic fashion.', 'We have adopted the simple heuristic that the model ""has converged"" when this probability stops increasing.']"
CC1177,P97-1063,A word-to-word model of translational equivalence,automatic evaluation and uniform filter cascades for inducing nbest translation lexiconsquot,['I D Melamed'],introduction,"This paper shows how to induce an N-best translation lexicon from a bilingual text corpus using statistical properties of the corpus together with four external knowledge sources. The knowledge sources are cast as filters, so that any subset of them can be cascaded in a uniform framework. A new objective evaluation measure is used to compare the quality of lexicons induced with different filter cascades. The best filter cascades improve lexicon quality by up to 137% over the plain vanilla statistical method, and approach human performance. Drastically reducing the size of the training corpus has a much smaller impact on lexicon quality when these knowledge sources are used. This makes it practical to train on small hand-built corpora for language pairs where large bilingual corpora are unavailable. Moreover, three of the four filters prove useful even when used with large training corpora.","With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; #AUTHOR_TAG ) .","['With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; #AUTHOR_TAG ) .', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other (Gale & Church, 1991; Melamed, 1996a).', 'Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.', 'The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993; Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995; Melamed, 1997).']",0,"['Even better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #AUTHOR_TAG ) .']","['Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #AUTHOR_TAG ) .', 'Another interesting extension is to broaden the definition of a ""word"" to include multi-word lexical units (Smadja, 1992).']","['Even better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #AUTHOR_TAG ) .']","['Even better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #AUTHOR_TAG ) .']","['Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #AUTHOR_TAG ) .', 'Another interesting extension is to broaden the definition of a ""word"" to include multi-word lexical units (Smadja, 1992).', 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.']","['Even better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #AUTHOR_TAG ) .', 'Another interesting extension is to broaden the definition of a ""word"" to include multi-word lexical units (Smadja, 1992).']","['Even better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #AUTHOR_TAG ) .', 'Another interesting extension is to broaden the definition of a ""word"" to include multi-word lexical units (Smadja, 1992).', 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.']","['Even better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #AUTHOR_TAG ) .']","['Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #AUTHOR_TAG ) .', 'Another interesting extension is to broaden the definition of a ""word"" to include multi-word lexical units (Smadja, 1992).', 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.', 'In this manner, the model can account for a wider range of translation phenomena.']"
CC1178,P97-1063,A word-to-word model of translational equivalence,building probabilistic models for natural language,['S Chen'],method,"Building models of language is a central task in natural language processing. Traditionally, language has been modeled with manually-constructed grammars that describe which strings are grammatical and which are not; however, with the recent availability of massive amounts of on-line text, statistically-trained models are an attractive alternative. These models are generally probabilistic, yielding a score reflecting sentence frequency instead of a binary grammaticality judgement. Probabilistic models of language are a fundamental tool in speech recognition for resolving acoustically ambiguous utterances. For example, we prefer the transcription forbear to four bear as the former string is far more frequent in English text. Probabilistic models also have application in optical character recognition, handwriting recognition, spelling correction, part-of-speech tagging, and machine translation. In this thesis, we investigate three problems involving the probabilistic modeling of language: smoothing n-gram models, statistical grammar induction, and bilingual sentence alignment. These three problems employ models at three different levels of language; they involve word-based, constituent-based, and sentence-based models, respectively. We describe techniques for improving the modeling of language at each of these levels, and surpass the performance of existing algorithms for each problem. We approach the three problems using three different frameworks. We relate each of these frameworks to the Bayesian paradigm, and show why each framework used was appropriate for the given problem. Finally, we show how our research addresses two central issues in probabilistic modeling: the sparse data problem and the problem of inducing hidden structure.Engineering and Applied Science","It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ) .","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']",1,"['More accurate models can be induced by taking into account various features of the linked tokens.', 'For example , frequent words are translated less consistently than rare words ( #AUTHOR_TAG ) .']","['For example , frequent words are translated less consistently than rare words ( #AUTHOR_TAG ) .']","['More accurate models can be induced by taking into account various features of the linked tokens.', 'For example , frequent words are translated less consistently than rare words ( #AUTHOR_TAG ) .']","['In the basic word-to-word model, the hidden parameters A + and A-depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'More accurate models can be induced by taking into account various features of the linked tokens.', 'For example , frequent words are translated less consistently than rare words ( #AUTHOR_TAG ) .']","['For example , frequent words are translated less consistently than rare words ( #AUTHOR_TAG ) .']","['In the basic word-to-word model, the hidden parameters A + and A-depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'More accurate models can be induced by taking into account various features of the linked tokens.', 'For example , frequent words are translated less consistently than rare words ( #AUTHOR_TAG ) .']","['More accurate models can be induced by taking into account various features of the linked tokens.', 'For example , frequent words are translated less consistently than rare words ( #AUTHOR_TAG ) .']","['In the basic word-to-word model, the hidden parameters A + and A-depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'More accurate models can be induced by taking into account various features of the linked tokens.', 'For example , frequent words are translated less consistently than rare words ( #AUTHOR_TAG ) .']","['For example , frequent words are translated less consistently than rare words ( #AUTHOR_TAG ) .']"
CC1179,P97-1063,A word-to-word model of translational equivalence,maximum likelihood from incomplete data via the em algorithmquot,"['A P Dempster', 'N M Laird', 'D B Rubin']",,,"By using the EM algorithm ( #AUTHOR_TAG ) , they can guarantee convergence towards the globally optimum parameter set .","[""One advantage that Brown et al.'s Model i has over our word-to-word model is that their objective function has no local maxima."", 'By using the EM algorithm ( #AUTHOR_TAG ) , they can guarantee convergence towards the globally optimum parameter set .', 'In contrast, the dynamic nature of the competitive linking algorithm changes the Pr(datalmodel ) in a non-monotonic fashion.', 'We have adopted the simple heuristic that the model ""has converged"" when this probability stops increasing.']",0,"['This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ) .']","['It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']","['This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ) .']","['It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']","['This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ) .']","['It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']"
CC1180,P97-1063,A word-to-word model of translational equivalence,measuring semantic entropyquot,['I D Melamed'],conclusion,,"Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #AUTHOR_TAG ) .","['Even better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #AUTHOR_TAG ) .', 'Another interesting extension is to broaden the definition of a ""word"" to include multi-word lexical units (Smadja, 1992).', 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.', 'In this manner, the model can account for a wider range of translation phenomena.']",3,"[""2We could just as easily use other symmetric `` association '' measures , such as 02 ( #AUTHOR_TAG ) or the Dice coefficient ( Smadja , 1992 ) .""]","[""2We could just as easily use other symmetric `` association '' measures , such as 02 ( #AUTHOR_TAG ) or the Dice coefficient ( Smadja , 1992 ) ."", 'co-occur is called a direct association.']","[""2We could just as easily use other symmetric `` association '' measures , such as 02 ( #AUTHOR_TAG ) or the Dice coefficient ( Smadja , 1992 ) ."", 'co-occur is called a direct association.']","[""2We could just as easily use other symmetric `` association '' measures , such as 02 ( #AUTHOR_TAG ) or the Dice coefficient ( Smadja , 1992 ) .""]","[""2We could just as easily use other symmetric `` association '' measures , such as 02 ( #AUTHOR_TAG ) or the Dice coefficient ( Smadja , 1992 ) ."", 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.']","[""2We could just as easily use other symmetric `` association '' measures , such as 02 ( #AUTHOR_TAG ) or the Dice coefficient ( Smadja , 1992 ) ."", 'co-occur is called a direct association.']","[""2We could just as easily use other symmetric `` association '' measures , such as 02 ( #AUTHOR_TAG ) or the Dice coefficient ( Smadja , 1992 ) ."", 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.']","[""2We could just as easily use other symmetric `` association '' measures , such as 02 ( #AUTHOR_TAG ) or the Dice coefficient ( Smadja , 1992 ) .""]","[""2We could just as easily use other symmetric `` association '' measures , such as 02 ( #AUTHOR_TAG ) or the Dice coefficient ( Smadja , 1992 ) ."", 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.', 'Then vk and uk+l will also co-occur more often than expected by chance.']"
CC1181,P97-1063,A word-to-word model of translational equivalence,measuring semantic entropyquot,['I D Melamed'],method,,"For example , frequent words are translated less consistently than rare words ( #AUTHOR_TAG ) .","['In the basic word-to-word model, the hidden parameters A + and A-depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'More accurate models can be induced by taking into account various features of the linked tokens.', 'For example , frequent words are translated less consistently than rare words ( #AUTHOR_TAG ) .']",0,"['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .']","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.']","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.']","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .']","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.']","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.']","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.']","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .']","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.', '(Macklovitch, 1994;Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"
CC1182,P97-1063,A word-to-word model of translational equivalence,a statistical approach to machine translationquot,"['P F Brown', 'J Cocke', 'S Della Pietra', 'V Della Pietra', 'F Jelinek', 'R Mercer', 'P Roossin']",method,,"It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ) .","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']",1,"['The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .']","['This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']","['The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']","['Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .']","['This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']","['Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']","['The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']","['Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .']","['This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']"
CC1183,P97-1063,A word-to-word model of translational equivalence,a program for aligning sentences in bilingual corporaquot,"['W Gale', 'K W Church']",method,,"2We could just as easily use other symmetric `` association '' measures , such as 02 ( #AUTHOR_TAG ) or the Dice coefficient ( Smadja , 1992 ) .","[""2We could just as easily use other symmetric `` association '' measures , such as 02 ( #AUTHOR_TAG ) or the Dice coefficient ( Smadja , 1992 ) ."", 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.', 'Then vk and uk+l will also co-occur more often than expected by chance.', 'The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk.', 'Models of translational equivalence that are ignorant of indirect associations have ""a tendency ... to be confused by collocates"" (Dagan et al., 1993).', 'Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c).', ""The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest."", 'The competitive linking algorithm implements this heuristic:']",1,"['Just as easily, we can model links that coincide with entries in a pre-existing translation lexicon separately from those that do not.', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .']","['This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .', 'for their models (Brown et al., 1993b).']","['Just as easily, we can model links that coincide with entries in a pre-existing translation lexicon separately from those that do not.', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .', 'for their models (Brown et al., 1993b).']","['Word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences.', 'Just as easily, we can model links that coincide with entries in a pre-existing translation lexicon separately from those that do not.', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .']","['This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .', 'for their models (Brown et al., 1993b).', 'When the hidden parameters are conditioned on different link classes, the estimation method does not change; it is just repeated for each link class.']","['Word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences.', 'Just as easily, we can model links that coincide with entries in a pre-existing translation lexicon separately from those that do not.', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .', 'for their models (Brown et al., 1993b).']","['Just as easily, we can model links that coincide with entries in a pre-existing translation lexicon separately from those that do not.', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .', 'for their models (Brown et al., 1993b).', 'When the hidden parameters are conditioned on different link classes, the estimation method does not change; it is just repeated for each link class.']","['Similarly, the hidden parameters can be conditioned on the linked parts of speech.', 'Word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences.', 'Just as easily, we can model links that coincide with entries in a pre-existing translation lexicon separately from those that do not.', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .']","['This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .', 'for their models (Brown et al., 1993b).', 'When the hidden parameters are conditioned on different link classes, the estimation method does not change; it is just repeated for each link class.']"
CC1184,P97-1063,A word-to-word model of translational equivalence,the mathematics of statistical machine translation parameter estimationquot,"['P F Brown', 'V J Della Pietra', 'S A Della Pietra', 'R L Mercer']",introduction,,"Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.', '(Macklovitch, 1994;Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']",0,"['For many applications, this is the desired behavior.', ""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .""]","[""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .""]","['For many applications, this is the desired behavior.', ""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .""]","['Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'For many applications, this is the desired behavior.', ""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .""]","[""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .""]","['Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'For many applications, this is the desired behavior.', ""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .""]","['For many applications, this is the desired behavior.', ""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .""]","['Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%.', 'Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'For many applications, this is the desired behavior.', ""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .""]","[""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards ."", 'These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.', 'The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words.']"
CC1185,P97-1063,A word-to-word model of translational equivalence,building probabilistic models for natural language,['S Chen'],,"Building models of language is a central task in natural language processing. Traditionally, language has been modeled with manually-constructed grammars that describe which strings are grammatical and which are not; however, with the recent availability of massive amounts of on-line text, statistically-trained models are an attractive alternative. These models are generally probabilistic, yielding a score reflecting sentence frequency instead of a binary grammaticality judgement. Probabilistic models of language are a fundamental tool in speech recognition for resolving acoustically ambiguous utterances. For example, we prefer the transcription forbear to four bear as the former string is far more frequent in English text. Probabilistic models also have application in optical character recognition, handwriting recognition, spelling correction, part-of-speech tagging, and machine translation. In this thesis, we investigate three problems involving the probabilistic modeling of language: smoothing n-gram models, statistical grammar induction, and bilingual sentence alignment. These three problems employ models at three different levels of language; they involve word-based, constituent-based, and sentence-based models, respectively. We describe techniques for improving the modeling of language at each of these levels, and surpass the performance of existing algorithms for each problem. We approach the three problems using three different frameworks. We relate each of these frameworks to the Bayesian paradigm, and show why each framework used was appropriate for the given problem. Finally, we show how our research addresses two central issues in probabilistic modeling: the sparse data problem and the problem of inducing hidden structure.Engineering and Applied Science","This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .","['Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']",0,"['We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 From ( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .']","['11 From ( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .', 'Thus, to be convenient, we only conduct experiments with the SAMT system.']","['We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 From ( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .', 'Thus, to be convenient, we only conduct experiments with the SAMT system.']","['We built an s2t translation system with the achieved U-trees after the 1000th iteration.', 'We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 From ( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .']","['11 From ( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .', 'Thus, to be convenient, we only conduct experiments with the SAMT system.']","['We built an s2t translation system with the achieved U-trees after the 1000th iteration.', 'We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 From ( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .', 'Thus, to be convenient, we only conduct experiments with the SAMT system.']","['We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 From ( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .', 'Thus, to be convenient, we only conduct experiments with the SAMT system.']","['For the hyperparameters, we set I to 0.1 and p expand = 1/3 to give a preference to the rules with small fragments.', 'We built an s2t translation system with the achieved U-trees after the 1000th iteration.', 'We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 From ( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .']","['11 From ( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .', 'Thus, to be convenient, we only conduct experiments with the SAMT system.']"
CC1186,P97-1063,A word-to-word model of translational equivalence,the mathematics of statistical machine translation parameter estimationquot,"['P F Brown', 'V J Della Pietra', 'S A Della Pietra', 'R L Mercer']",method,,This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .,"['To account for this difference, we can estimate separate values of X + and A-for different ranges of n(u,v).', 'Similarly, the hidden parameters can be conditioned on the linked parts of speech.', 'Word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences.', 'Just as easily, we can model links that coincide with entries in a pre-existing translation lexicon separately from those that do not.', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .', 'for their models (Brown et al., 1993b).', 'When the hidden parameters are conditioned on different link classes, the estimation method does not change; it is just repeated for each link class.']",1,"['In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler.', 'Differently , #AUTHOR_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .']","['Differently , #AUTHOR_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .', 'We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.']","['In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler.', 'Differently , #AUTHOR_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .', 'We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.']","['For example, the bold italic nodes with shadows in Figure 2 are frontier nodes.', 'In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler.', 'Differently , #AUTHOR_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .']","['Differently , #AUTHOR_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .', 'We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'This subject will be one of our future work topics.']","['For example, the bold italic nodes with shadows in Figure 2 are frontier nodes.', 'In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler.', 'Differently , #AUTHOR_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .', 'We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.']","['In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler.', 'Differently , #AUTHOR_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .', 'We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'This subject will be one of our future work topics.']","['Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'For example, the bold italic nodes with shadows in Figure 2 are frontier nodes.', 'In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler.', 'Differently , #AUTHOR_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .']","['Differently , #AUTHOR_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .', 'We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'This subject will be one of our future work topics.']"
CC1187,P97-1063,A word-to-word model of translational equivalence,line em up advances in alignment technology and their impact on translation support toolsquot,"['E Macklovitch', 'M-L Hannan']",,"We present a quantitative evaluation of one well-known word-alignment algorithm, as well as an analysis of frequent errors in terms of this model's underlying assumptions. Despite error rates that range from 22% to 32%, we argue that this technology can be put to good use in certain automated aids for human translators. We support our contention by pointing to several successful applications and outline ways in which text alignments below the sentence level would allow us to improve the performance of other translation support tools.","The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .","['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', 'Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%.', 'Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'For many applications, this is the desired behavior.', ""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards ."", 'These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.', 'The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words.', 'Therefore, we ignored English words that were linked to nothing.']",1,"['Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #AUTHOR_TAG ) .']",['The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #AUTHOR_TAG ) .'],"['Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #AUTHOR_TAG ) .']","['This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #AUTHOR_TAG ) .']",['The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #AUTHOR_TAG ) .'],"['This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #AUTHOR_TAG ) .']","['Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #AUTHOR_TAG ) .']","['Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #AUTHOR_TAG ) .']","['The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #AUTHOR_TAG ) .', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.']"
CC1188,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a wordclass approach to labeling pscfg rules for machine translation,"['Andreas Zollmann', 'Stephan Vogel']",experiments,"In this work we propose methods to label probabilistic synchronous context-free grammar (PSCFG) rules using only word tags, generated by either part-of-speech analysis or unsupervised word class induction. The proposals range from simple tag-combination schemes to a phrase clustering model that can incorporate an arbitrary number of features.    Our models improve translation quality over the single generic label approach of Chiang (2005) and perform on par with the syntactically motivated approach from Zollmann and Venugopal (2006) on the NIST large Chinese-to-English translation task. These results persist when using automatically learned word tags, suggesting broad applicability of our technique across diverse language pairs for which syntactic resources are not available.","11 From ( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .","['For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus.', 'The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine.', 'For the hyperparameters, we set Ä® to 0.1 and p expand = 1/3 to give a preference to the rules with small fragments.', 'We built an s2t translation system with the achieved U-trees after the 1000th iteration.', 'We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 From ( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .', 'Thus, to be convenient, we only conduct experiments with the SAMT system.']",4,"['This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', '#AUTHOR_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .']","['#AUTHOR_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .']","['This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', '#AUTHOR_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .']","['Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', '#AUTHOR_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .']","['#AUTHOR_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .']","['Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', '#AUTHOR_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .']","['This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', '#AUTHOR_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .']","['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', '#AUTHOR_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .']","['#AUTHOR_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.']"
CC1189,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a bayesian model of syntaxdirected tree to string grammar induction,"['Trevor Cohn', 'Phil Blunsom']",method,"Tree based translation models are a compelling means of integrating linguistic information into machine translation. Syntax can inform lexical selection and reordering choices and thereby improve translation quality. Research to date has focussed primarily on decoding with such models, but less on the difficult problem of inducing the bilingual grammar from data. We propose a generative Bayesian model of tree-to-string translation which induces grammars that are both smaller and produce better translations than the previous heuristic two-stage approach which employs a separate word alignment step.","Differently , #AUTHOR_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .","['Using the initial target U-trees, source sentences and word alignment, we extract minimal GHKM translation rules 7 in terms of frontier nodes (Galley et al., 2004).', 'Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'For example, the bold italic nodes with shadows in Figure 2 are frontier nodes.', 'In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler.', 'Differently , #AUTHOR_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .', 'We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'This subject will be one of our future work topics.']",4,"['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ) .']","['The system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']","['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']","['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ) .']","['The system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']","['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']","['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']","['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ) .']","['The system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']"
CC1190,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,hierarchical phrasebased translation,['David Chiang'],related work,"Left-to-right (LR) decoding (Watanabe et al., 2006) is promising decoding algorithm for hierarchical phrase-based translation (Hiero) that visits input spans in arbitrary order producing the output translation in left to right order. This leads to far fewer language model calls, but while LR decod-ing is more efficient than CKY decoding, it is unable to capture some hierarchical phrase alignments reachable using CKY decoding and suffers from lower transla-tion quality as a result. This paper in-troduces two improvements to LR decod-ing that make it comparable in translation quality to CKY-based Hiero.",The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #AUTHOR_TAG ) .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #AUTHOR_TAG ) .', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work ( #AUTHOR_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models .']","['Our previous work ( #AUTHOR_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models .', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.']","['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work ( #AUTHOR_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models .']","['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work ( #AUTHOR_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models .']","['Our previous work ( #AUTHOR_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models .', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.']","['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work ( #AUTHOR_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models .', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.']","['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work ( #AUTHOR_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models .', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.']","['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work ( #AUTHOR_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models .']","['Our previous work ( #AUTHOR_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models .', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).']"
CC1191,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,bayesian synchronous grammar induction,"['Phil Blunsom', 'Trevor Cohn', 'Miles Osborne']",related work,"We present a novel method for inducing synchronous context free grammars (SCFGs) from a corpus of parallel string pairs. SCFGs can model equivalence between strings in terms of substitutions, insertions and deletions, and the reordering of sub-strings. We develop a non-parametric Bayesian model and apply it to a machine translation task, using priors to replace the various heuristics commonly used in this field. Using a variational Bayes training procedure, we learn the latent structure of translation equivalence through the induction of synchronous grammar categories for phrasal translations, showing improvements in translation performance over maximum likelihood models.","#AUTHOR_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .","['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', '#AUTHOR_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al., 2006).', 'Then , we binarize the English parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system .']","['Then , we binarize the English parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system .']","['To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al., 2006).', 'Then , we binarize the English parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system .']","['To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al., 2006).', 'Then , we binarize the English parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system .']","['Then , we binarize the English parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system .']","['To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al., 2006).', 'Then , we binarize the English parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system .']","['To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al., 2006).', 'Then , we binarize the English parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system .']","['To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al., 2006).', 'Then , we binarize the English parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system .']","['Then , we binarize the English parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system .']"
CC1192,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,spmt statistical machine translation with syntactified target language phrases,"['Daniel Marcu', 'Wei Wang', 'Abdessamad Echihabi', 'Kevin Knight']",experiments,"We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a human-based quality metric that ranks translations on a scale from 1 to 5.","The system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ) .","['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']",5,"['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']"
CC1193,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,treebased translation without using parse trees,"['Feifei Zhai', 'Jiajun Zhang', 'Yu Zhou', 'Chengqing Zong']",related work,"Parse trees are indispensable to the existing tree- based translation models. However, there exist two major challenges in utilizing parse trees: 1) Fo r most language pairs, it is hard to get parse trees due to the lack of syntactic resources for training. 2) Numero us parse trees are not compatible with word alignment which is generally learned by GIZA++. Therefore, a number of useful translation rules are often excluded. To overcome these two problems, in this paper we make a great effort to bypass the parse trees and induce effective unsupervised trees for treebased translation models. Our unsupervised trees depend only on the word alignment without utilizing any syntactic resource or linguistic pars er. Hence, they are very beneficial for the translation between resource-poor languages. Our experimental results have shown that the string-to-tree translation system using our unsupervised trees significantly outperforms th e stringto-tree system using parse trees.",Our previous work ( #AUTHOR_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work ( #AUTHOR_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models .', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011b ) .']","['Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011b ) .']","['Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011b ) .']","['Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011b ) .']"
CC1194,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,binarizing syntax trees to improve syntaxbased machine translation accuracy,"['Wei Wang', 'Kevin Knight', 'Daniel Marcu']",experiments,We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result.,"Then , we binarize the English parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system .","['To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al., 2006).', 'Then , we binarize the English parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system .']",5,"['They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', '#AUTHOR_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .']",['#AUTHOR_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .'],"['They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', '#AUTHOR_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .']","['Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', '#AUTHOR_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .']",['#AUTHOR_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .'],"['Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', '#AUTHOR_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .']","['They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', '#AUTHOR_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .']","['Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', '#AUTHOR_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .']","['#AUTHOR_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.']"
CC1195,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,spmt statistical machine translation with syntactified target language phrases,"['Daniel Marcu', 'Wei Wang', 'Abdessamad Echihabi', 'Kevin Knight']",introduction,"We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a human-based quality metric that ranks translations on a scale from 1 to 5.","Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']",0,"['The base distribution 0 ( | ) P r N is designed to assign prior probabilities to the STSG production rules.', 'Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #AUTHOR_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows :']","['Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #AUTHOR_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows :']","['The base distribution 0 ( | ) P r N is designed to assign prior probabilities to the STSG production rules.', 'Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #AUTHOR_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows :']","['The base distribution 0 ( | ) P r N is designed to assign prior probabilities to the STSG production rules.', 'Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #AUTHOR_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows :']","['Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #AUTHOR_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows :']","['The base distribution 0 ( | ) P r N is designed to assign prior probabilities to the STSG production rules.', 'Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #AUTHOR_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows :']","['The base distribution 0 ( | ) P r N is designed to assign prior probabilities to the STSG production rules.', 'Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #AUTHOR_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows :']","['The base distribution 0 ( | ) P r N is designed to assign prior probabilities to the STSG production rules.', 'Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #AUTHOR_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows :']","['Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #AUTHOR_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows :']"
CC1196,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a new stringtodependency machine translation algorithm with a target dependency language model,"['Libin Shen', 'Jinxi Xu', 'Ralph Weischedel']",introduction,"In this paper, we propose a novel string-todependency algorithm for statistical machine translation. With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model. Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set.","Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011b ) .","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011b ) .']",0,"['Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', '#AUTHOR_TAG further labeled the SCFG rules with POS tags and unsupervised word classes .']","['#AUTHOR_TAG further labeled the SCFG rules with POS tags and unsupervised word classes .', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']","['Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', '#AUTHOR_TAG further labeled the SCFG rules with POS tags and unsupervised word classes .', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']","['Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', '#AUTHOR_TAG further labeled the SCFG rules with POS tags and unsupervised word classes .']","['#AUTHOR_TAG further labeled the SCFG rules with POS tags and unsupervised word classes .', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']","['Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', '#AUTHOR_TAG further labeled the SCFG rules with POS tags and unsupervised word classes .', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']","['Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', '#AUTHOR_TAG further labeled the SCFG rules with POS tags and unsupervised word classes .', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']","['Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', '#AUTHOR_TAG further labeled the SCFG rules with POS tags and unsupervised word classes .']","['#AUTHOR_TAG further labeled the SCFG rules with POS tags and unsupervised word classes .', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']"
CC1197,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a bayesian model of syntaxdirected tree to string grammar induction,"['Trevor Cohn', 'Phil Blunsom']",related work,"Tree based translation models are a compelling means of integrating linguistic information into machine translation. Syntax can inform lexical selection and reordering choices and thereby improve translation quality. Research to date has focussed primarily on decoding with such models, but less on the difficult problem of inducing the bilingual grammar from data. We propose a generative Bayesian model of tree-to-string translation which induces grammars that are both smaller and produce better translations than the previous heuristic two-stage approach which employs a separate word alignment step.",#AUTHOR_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', '#AUTHOR_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #AUTHOR_TAG ) .']","['Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #AUTHOR_TAG ) .', 'Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.']","['Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #AUTHOR_TAG ) .', 'Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.']","['Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #AUTHOR_TAG ) .']","['Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #AUTHOR_TAG ) .', 'Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'For example, the bold italic nodes with shadows in Figure 2 are frontier nodes.']","['Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #AUTHOR_TAG ) .', 'Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.']","['Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #AUTHOR_TAG ) .', 'Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'For example, the bold italic nodes with shadows in Figure 2 are frontier nodes.']","['Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #AUTHOR_TAG ) .']","['Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #AUTHOR_TAG ) .', 'Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'For example, the bold italic nodes with shadows in Figure 2 are frontier nodes.', 'In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler.']"
CC1198,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a bayesian model of syntaxdirected tree to string grammar induction,"['Trevor Cohn', 'Phil Blunsom']",method,"Tree based translation models are a compelling means of integrating linguistic information into machine translation. Syntax can inform lexical selection and reordering choices and thereby improve translation quality. Research to date has focussed primarily on decoding with such models, but less on the difficult problem of inducing the bilingual grammar from data. We propose a generative Bayesian model of tree-to-string translation which induces grammars that are both smaller and produce better translations than the previous heuristic two-stage approach which employs a separate word alignment step.","Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #AUTHOR_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows :","['The base distribution 0 ( | ) P r N is designed to assign prior probabilities to the STSG production rules.', 'Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #AUTHOR_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows :']",5,"['adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', '#AUTHOR_TAG re-trained the linguistic parsers bilingually based on word alignment .']",['#AUTHOR_TAG re-trained the linguistic parsers bilingually based on word alignment .'],"['adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', '#AUTHOR_TAG re-trained the linguistic parsers bilingually based on word alignment .']","['They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', '#AUTHOR_TAG re-trained the linguistic parsers bilingually based on word alignment .']",['#AUTHOR_TAG re-trained the linguistic parsers bilingually based on word alignment .'],"['They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', '#AUTHOR_TAG re-trained the linguistic parsers bilingually based on word alignment .']","['adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', '#AUTHOR_TAG re-trained the linguistic parsers bilingually based on word alignment .']","['Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', '#AUTHOR_TAG re-trained the linguistic parsers bilingually based on word alignment .']","['#AUTHOR_TAG re-trained the linguistic parsers bilingually based on word alignment .', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.']"
CC1199,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a wordclass approach to labeling pscfg rules for machine translation,"['Andreas Zollmann', 'Stephan Vogel']",related work,"In this work we propose methods to label probabilistic synchronous context-free grammar (PSCFG) rules using only word tags, generated by either part-of-speech analysis or unsupervised word class induction. The proposals range from simple tag-combination schemes to a phrase clustering model that can incorporate an arbitrary number of features.    Our models improve translation quality over the single generic label approach of Chiang (2005) and perform on par with the syntactically motivated approach from Zollmann and Venugopal (2006) on the NIST large Chinese-to-English translation task. These results persist when using automatically learned word tags, suggesting broad applicability of our technique across diverse language pairs for which syntactic resources are not available.",#AUTHOR_TAG further labeled the SCFG rules with POS tags and unsupervised word classes .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', '#AUTHOR_TAG further labeled the SCFG rules with POS tags and unsupervised word classes .', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty.', 'The statistical significance test is performed by the re-sampling approach ( #AUTHOR_TAG ) .']",['The statistical significance test is performed by the re-sampling approach ( #AUTHOR_TAG ) .'],"['The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty.', 'The statistical significance test is performed by the re-sampling approach ( #AUTHOR_TAG ) .']","['Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set.', 'The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty.', 'The statistical significance test is performed by the re-sampling approach ( #AUTHOR_TAG ) .']",['The statistical significance test is performed by the re-sampling approach ( #AUTHOR_TAG ) .'],"['Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set.', 'The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty.', 'The statistical significance test is performed by the re-sampling approach ( #AUTHOR_TAG ) .']","['The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty.', 'The statistical significance test is performed by the re-sampling approach ( #AUTHOR_TAG ) .']","['We use MERT (Och, 2004) to tune parameters.', 'Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set.', 'The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty.', 'The statistical significance test is performed by the re-sampling approach ( #AUTHOR_TAG ) .']",['The statistical significance test is performed by the re-sampling approach ( #AUTHOR_TAG ) .']
CC1200,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,whatâs in a translation rule,"['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']",method,"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.","Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #AUTHOR_TAG ) .","['Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #AUTHOR_TAG ) .', 'Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'For example, the bold italic nodes with shadows in Figure 2 are frontier nodes.', 'In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler.', 'Differently,  designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment.', 'We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'This subject will be one of our future work topics.']",5,"['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( #AUTHOR_TAG ) and ( Marcu et al. 2006 ) .']","['The system is implemented based on ( #AUTHOR_TAG ) and ( Marcu et al. 2006 ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']","['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( #AUTHOR_TAG ) and ( Marcu et al. 2006 ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']","['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( #AUTHOR_TAG ) and ( Marcu et al. 2006 ) .']","['The system is implemented based on ( #AUTHOR_TAG ) and ( Marcu et al. 2006 ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']","['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( #AUTHOR_TAG ) and ( Marcu et al. 2006 ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']","['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( #AUTHOR_TAG ) and ( Marcu et al. 2006 ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']","['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( #AUTHOR_TAG ) and ( Marcu et al. 2006 ) .']","['The system is implemented based on ( #AUTHOR_TAG ) and ( Marcu et al. 2006 ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']"
CC1201,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,retraining monolingual parser bilingually for syntactic smt,"['Shujie Liu', 'Chi-Ho Li', 'Mu Li', 'Ming Zhou']",related work,"The training of most syntactic SMT approaches involves two essential components, word alignment and monolingual parser. In the current state of the art these two components are mutually independent, thus causing problems like lack of rule generalization, and violation of syntactic correspondence in translation rules. In this paper, we propose two ways of re-training monolingual parser with the target of maximizing the consistency between parse trees and alignment matrices. One is targeted self-training with a simple evaluation function; the other is based on training data selection from forced alignment of bilingual data. We also propose an auxiliary method for boosting alignment quality, by symmetrizing alignment matrices with respect to parse trees. The best combination of these novel methods achieves 3 Bleu point gain in an IWSLT task and more than 1 Bleu point gain in NIST tasks.",#AUTHOR_TAG re-trained the linguistic parsers bilingually based on word alignment .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', '#AUTHOR_TAG re-trained the linguistic parsers bilingually based on word alignment .', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', '#AUTHOR_TAG employed a Bayesian method to learn discontinuous SCFG rules .']",['#AUTHOR_TAG employed a Bayesian method to learn discontinuous SCFG rules .'],"['The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', '#AUTHOR_TAG employed a Bayesian method to learn discontinuous SCFG rules .']","['Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', '#AUTHOR_TAG employed a Bayesian method to learn discontinuous SCFG rules .']",['#AUTHOR_TAG employed a Bayesian method to learn discontinuous SCFG rules .'],"['Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', '#AUTHOR_TAG employed a Bayesian method to learn discontinuous SCFG rules .']","['The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', '#AUTHOR_TAG employed a Bayesian method to learn discontinuous SCFG rules .']","['This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', '#AUTHOR_TAG employed a Bayesian method to learn discontinuous SCFG rules .']","['#AUTHOR_TAG employed a Bayesian method to learn discontinuous SCFG rules .', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.']"
CC1202,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,statistical significance tests for machine translation evaluation,['Philipp Koehn'],experiments,"If two translation systems differ differ in perfor-mance on a test set, can we trust that this indicates a difference in true system quality? To answer this question, we describe bootstrap resampling meth-ods to compute statistical significance of test results, and validate them on the concrete example of the BLEU score. Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real.",The statistical significance test is performed by the re-sampling approach ( #AUTHOR_TAG ) .,"['The experiments are conducted on Chinese-to-English translation.', 'The training data are the FBIS corpus with approximately 7.1 million Chinese words and 9.2 million English words.', 'We obtain the bidirectional word alignment with GIZA++, and then adopt the grow-diag-final-and strategy to obtain the final symmetric alignment.', 'We train a 5gram language model on the Xinhua portion of the English Gigaword corpus and the English part of the training data.', 'For tuning and testing, we use the NIST MT 2003 evaluation data as the development set, and use the NIST MT04 and MT05 data as the test set.', 'We use MERT (Och, 2004) to tune parameters.', 'Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set.', 'The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty.', 'The statistical significance test is performed by the re-sampling approach ( #AUTHOR_TAG ) .']",5,"['Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', '#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment .']",['#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment .'],"['Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', '#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment .']","['This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', '#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment .']",['#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment .'],"['This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', '#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment .']","['Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', '#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment .']","['Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', '#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment .']","['#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment .', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.']"
CC1203,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,scalable inference and training of contextrich syntactic translation models,"['Michel Galley', 'Jonathan Graehl', 'Kevin Knight', 'Daniel Marcu', 'Steve DeNeefe', 'Wei Wang', 'Ignacio Thayer']",experiments,"Statistical MT has made great progress in the last few years, but current translation models are weak on re-ordering and target language fluency. Syntactic approaches seek to remedy these problems. In this paper, we take the framework for acquiring multi-level syntactic translation rules of (Galley et al., 2004) from aligned tree-string pairs, and present two main extensions of their approach: first, instead of merely computing a single derivation that minimally explains a sentence pair, we construct a large number of derivations that include contextually richer rules, and account for multiple interpretations of unaligned words. Second, we propose probability estimates and a training procedure for weighting these rules. We contrast different approaches on real examples, show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated, and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules.",The system is implemented based on ( #AUTHOR_TAG ) and ( Marcu et al. 2006 ) .,"['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( #AUTHOR_TAG ) and ( Marcu et al. 2006 ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']",5,"['Our final experiments verify this point and we will conduct a much detailed analysis in future.', '9 We only use the minimal GHKM rules ( #AUTHOR_TAG ) here to reduce the complexity of the sampler .']",['9 We only use the minimal GHKM rules ( #AUTHOR_TAG ) here to reduce the complexity of the sampler .'],"['Our final experiments verify this point and we will conduct a much detailed analysis in future.', '9 We only use the minimal GHKM rules ( #AUTHOR_TAG ) here to reduce the complexity of the sampler .']","['Thus, compared with the conventional TMs, we believe that our final TM would not be worse due to AEs.', 'Our final experiments verify this point and we will conduct a much detailed analysis in future.', '9 We only use the minimal GHKM rules ( #AUTHOR_TAG ) here to reduce the complexity of the sampler .']",['9 We only use the minimal GHKM rules ( #AUTHOR_TAG ) here to reduce the complexity of the sampler .'],"['Thus, compared with the conventional TMs, we believe that our final TM would not be worse due to AEs.', 'Our final experiments verify this point and we will conduct a much detailed analysis in future.', '9 We only use the minimal GHKM rules ( #AUTHOR_TAG ) here to reduce the complexity of the sampler .']","['Our final experiments verify this point and we will conduct a much detailed analysis in future.', '9 We only use the minimal GHKM rules ( #AUTHOR_TAG ) here to reduce the complexity of the sampler .']","['Besides, our sampler encourages the correct alignments and simultaneously discourages the infrequent AEs.', 'Thus, compared with the conventional TMs, we believe that our final TM would not be worse due to AEs.', 'Our final experiments verify this point and we will conduct a much detailed analysis in future.', '9 We only use the minimal GHKM rules ( #AUTHOR_TAG ) here to reduce the complexity of the sampler .']",['9 We only use the minimal GHKM rules ( #AUTHOR_TAG ) here to reduce the complexity of the sampler .']
CC1204,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a bayesian model for learning scfgs with discontiguous rules,"['Abby Levenberg', 'Chris Dyer', 'Phil Blunsom']",related work,"We describe a nonparametric model and corresponding inference algorithm for learning Synchronous Context Free Grammar derivations for parallel text. The model employs a Pitman-Yor Process prior which uses a novel base distribution over synchronous grammar rules. Through both synthetic grammar induction and statistical machine translation experiments, we show that our model learns complex translational correspondences--- including discontiguous, many-to-many alignments---and produces competitive translation results. Further, inference is efficient and we present results on significantly larger corpora than prior work.",#AUTHOR_TAG employed a Bayesian method to learn discontinuous SCFG rules .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', '#AUTHOR_TAG employed a Bayesian method to learn discontinuous SCFG rules .', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['The system is implemented based on (Galley et al., 2006) and ).', 'In the system , we extract both the minimal GHKM rules ( #AUTHOR_TAG ) , and the rules of SPMT Model 1 ( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side .']","['In the system , we extract both the minimal GHKM rules ( #AUTHOR_TAG ) , and the rules of SPMT Model 1 ( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side .']","['The system is implemented based on (Galley et al., 2006) and ).', 'In the system , we extract both the minimal GHKM rules ( #AUTHOR_TAG ) , and the rules of SPMT Model 1 ( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side .']","['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on (Galley et al., 2006) and ).', 'In the system , we extract both the minimal GHKM rules ( #AUTHOR_TAG ) , and the rules of SPMT Model 1 ( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side .']","['In the system , we extract both the minimal GHKM rules ( #AUTHOR_TAG ) , and the rules of SPMT Model 1 ( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side .']","['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on (Galley et al., 2006) and ).', 'In the system , we extract both the minimal GHKM rules ( #AUTHOR_TAG ) , and the rules of SPMT Model 1 ( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side .']","['The system is implemented based on (Galley et al., 2006) and ).', 'In the system , we extract both the minimal GHKM rules ( #AUTHOR_TAG ) , and the rules of SPMT Model 1 ( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side .']","['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on (Galley et al., 2006) and ).', 'In the system , we extract both the minimal GHKM rules ( #AUTHOR_TAG ) , and the rules of SPMT Model 1 ( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side .']","['In the system , we extract both the minimal GHKM rules ( #AUTHOR_TAG ) , and the rules of SPMT Model 1 ( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side .']"
CC1205,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,two languages are better than one for syntactic parsing,"['David Burkett', 'Dan Klein']",related work,"We show that jointly parsing a bitext can substantially improve parse quality on both sides. In a maximum entropy bitext parsing model, we define a distribution over source trees, target trees, and node-to-node alignments between them. Features include monolingual parse scores and various measures of syntactic divergence. Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables. The resulting bitext parser outperforms state-of-the-art monolingual parser baselines by 2.5 F1 at predicting English side trees and 1.8 F1 at predicting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation.",#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', '#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment .', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['Otherwise, we change its state to the right state O , and transform the U-tree to Figure 3 Obviously, towards an s-node for sampling, the two values of O would define two different U-trees.', 'Using the GHKM algorithm ( #AUTHOR_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .']","['Using the GHKM algorithm ( #AUTHOR_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .']","['Otherwise, we change its state to the right state O , and transform the U-tree to Figure 3 Obviously, towards an s-node for sampling, the two values of O would define two different U-trees.', 'Using the GHKM algorithm ( #AUTHOR_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .']","['For example, in Figure 3(a), the s-node is currently in the left VWDWHO :HVDPSOHWKHORIWKLVQRGHDQGLI WKHVDPSOHGYDOXHRIOLVZHNHHSWKHVWUXFWXUH unchanged, i.e., in the left state.', 'Otherwise, we change its state to the right state O , and transform the U-tree to Figure 3 Obviously, towards an s-node for sampling, the two values of O would define two different U-trees.', 'Using the GHKM algorithm ( #AUTHOR_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .']","['Using the GHKM algorithm ( #AUTHOR_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .']","['For example, in Figure 3(a), the s-node is currently in the left VWDWHO :HVDPSOHWKHORIWKLVQRGHDQGLI WKHVDPSOHGYDOXHRIOLVZHNHHSWKHVWUXFWXUH unchanged, i.e., in the left state.', 'Otherwise, we change its state to the right state O , and transform the U-tree to Figure 3 Obviously, towards an s-node for sampling, the two values of O would define two different U-trees.', 'Using the GHKM algorithm ( #AUTHOR_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .']","['Otherwise, we change its state to the right state O , and transform the U-tree to Figure 3 Obviously, towards an s-node for sampling, the two values of O would define two different U-trees.', 'Using the GHKM algorithm ( #AUTHOR_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .']","['Our first Gibbs operator, Rotate, just works by sampling value of the Oparameters, one at a time, and changing the U-tree accordingly.', 'For example, in Figure 3(a), the s-node is currently in the left VWDWHO :HVDPSOHWKHORIWKLVQRGHDQGLI WKHVDPSOHGYDOXHRIOLVZHNHHSWKHVWUXFWXUH unchanged, i.e., in the left state.', 'Otherwise, we change its state to the right state O , and transform the U-tree to Figure 3 Obviously, towards an s-node for sampling, the two values of O would define two different U-trees.', 'Using the GHKM algorithm ( #AUTHOR_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .']","['Using the GHKM algorithm ( #AUTHOR_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .', 'Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own.', ""In the two derivations, the STSG rules defined by the two states include the one rooted at the s-node's lowest ancestor frontier node, and the one rooted at the s-node if it is a frontier node."", 'For instance, in Figure 3(a), as the s-node is not a frontier node, the left state (O ) defines only one rule: Using these STSG rules, the two derivations are evaluated as follows (We use the value of O to denote the corresponding STSG derivation):']"
CC1206,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,whatâs in a translation rule,"['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']",method,"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.",9 We only use the minimal GHKM rules ( #AUTHOR_TAG ) here to reduce the complexity of the sampler .,"['The sampler might reinforce the frequent alignment errors (AE), which would harm the translation model (TM).', 'Actually, the frequent AEs also greatly impair the conventional TM.', 'Besides, our sampler encourages the correct alignments and simultaneously discourages the infrequent AEs.', 'Thus, compared with the conventional TMs, we believe that our final TM would not be worse due to AEs.', 'Our final experiments verify this point and we will conduct a much detailed analysis in future.', '9 We only use the minimal GHKM rules ( #AUTHOR_TAG ) here to reduce the complexity of the sampler .']",5,"['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']"
CC1207,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,whatâs in a translation rule,"['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']",experiments,"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.","In the system , we extract both the minimal GHKM rules ( #AUTHOR_TAG ) , and the rules of SPMT Model 1 ( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side .","['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on (Galley et al., 2006) and ).', 'In the system , we extract both the minimal GHKM rules ( #AUTHOR_TAG ) , and the rules of SPMT Model 1 ( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side .']",5,"['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; #AUTHOR_TAG ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; #AUTHOR_TAG ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; #AUTHOR_TAG ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; #AUTHOR_TAG ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; #AUTHOR_TAG ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; #AUTHOR_TAG ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; #AUTHOR_TAG ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; #AUTHOR_TAG ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; #AUTHOR_TAG ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']"
CC1208,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,whatâs in a translation rule,"['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']",method,"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.","Using the GHKM algorithm ( #AUTHOR_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .","['Our first Gibbs operator, Rotate, just works by sampling value of the Èparameters, one at a time, and changing the U-tree accordingly.', 'For example, in Figure 3(a), the s-node is currently in the left VWDWHÈ :HVDPSOHWKHÈRIWKLVQRGHDQGLI WKHVDPSOHGYDOXHRIÈLVZHNHHSWKHVWUXFWXUH unchanged, i.e., in the left state.', 'Otherwise, we change its state to the right state È , and transform the U-tree to Figure 3 Obviously, towards an s-node for sampling, the two values of È would define two different U-trees.', 'Using the GHKM algorithm ( #AUTHOR_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .', 'Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own.', ""In the two derivations, the STSG rules defined by the two states include the one rooted at the s-node's lowest ancestor frontier node, and the one rooted at the s-node if it is a frontier node."", 'For instance, in Figure 3(a), as the s-node is not a frontier node, the left state (È ) defines only one rule: Using these STSG rules, the two derivations are evaluated as follows (We use the value of È to denote the corresponding STSG derivation):']",5,"['To create the baseline system , we use the opensource Joshua 4.0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #AUTHOR_TAG ) respectively .']","['To create the baseline system , we use the opensource Joshua 4.0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #AUTHOR_TAG ) respectively .']","['To create the baseline system , we use the opensource Joshua 4.0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #AUTHOR_TAG ) respectively .']","['To create the baseline system , we use the opensource Joshua 4.0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #AUTHOR_TAG ) respectively .']","['To create the baseline system , we use the opensource Joshua 4.0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #AUTHOR_TAG ) respectively .']","['To create the baseline system , we use the opensource Joshua 4.0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #AUTHOR_TAG ) respectively .']","['To create the baseline system , we use the opensource Joshua 4.0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #AUTHOR_TAG ) respectively .']","['To create the baseline system , we use the opensource Joshua 4.0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #AUTHOR_TAG ) respectively .']","['To create the baseline system , we use the opensource Joshua 4.0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #AUTHOR_TAG ) respectively .']"
CC1209,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,whatâs in a translation rule,"['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']",introduction,"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.","Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']",0,"['To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #AUTHOR_TAG ) .']","['To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #AUTHOR_TAG ) .', 'Then, we binarize the English parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system.']","['To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #AUTHOR_TAG ) .', 'Then, we binarize the English parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system.']","['To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #AUTHOR_TAG ) .']","['To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #AUTHOR_TAG ) .', 'Then, we binarize the English parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system.']","['To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #AUTHOR_TAG ) .', 'Then, we binarize the English parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system.']","['To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #AUTHOR_TAG ) .', 'Then, we binarize the English parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system.']","['To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #AUTHOR_TAG ) .']","['To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #AUTHOR_TAG ) .', 'Then, we binarize the English parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system.']"
CC1210,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,dependency treelet translation syntactically informed phrasal smt,"['Chris Quirk', 'Arul Menezes', 'Colin Cherry']",introduction,"We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation. This method requires a source-language dependency parser, target language word segmentation and an unsupervised word alignment component. We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model. We describe an efficient decoder and show that using these treebased models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser. 1","Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; #AUTHOR_TAG ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; #AUTHOR_TAG ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']",0,"['Differently, we require that each multi-word non-terminal node must have two child nodes.', 'This is because the binary structure has been verified to be very effective for tree-based translation ( #AUTHOR_TAG ; Zhang et al. , 2011a ) .']","['This is because the binary structure has been verified to be very effective for tree-based translation ( #AUTHOR_TAG ; Zhang et al. , 2011a ) .']","['Differently, we require that each multi-word non-terminal node must have two child nodes.', 'This is because the binary structure has been verified to be very effective for tree-based translation ( #AUTHOR_TAG ; Zhang et al. , 2011a ) .']","['To generate frag,  used a geometric prior to decide how many child nodes to assign each node.', 'Differently, we require that each multi-word non-terminal node must have two child nodes.', 'This is because the binary structure has been verified to be very effective for tree-based translation ( #AUTHOR_TAG ; Zhang et al. , 2011a ) .']","['This is because the binary structure has been verified to be very effective for tree-based translation ( #AUTHOR_TAG ; Zhang et al. , 2011a ) .']","['To generate frag,  used a geometric prior to decide how many child nodes to assign each node.', 'Differently, we require that each multi-word non-terminal node must have two child nodes.', 'This is because the binary structure has been verified to be very effective for tree-based translation ( #AUTHOR_TAG ; Zhang et al. , 2011a ) .']","['Differently, we require that each multi-word non-terminal node must have two child nodes.', 'This is because the binary structure has been verified to be very effective for tree-based translation ( #AUTHOR_TAG ; Zhang et al. , 2011a ) .']","['is the probability of producing the target tree fragment frag.', 'To generate frag,  used a geometric prior to decide how many child nodes to assign each node.', 'Differently, we require that each multi-word non-terminal node must have two child nodes.', 'This is because the binary structure has been verified to be very effective for tree-based translation ( #AUTHOR_TAG ; Zhang et al. , 2011a ) .']","['This is because the binary structure has been verified to be very effective for tree-based translation ( #AUTHOR_TAG ; Zhang et al. , 2011a ) .']"
CC1211,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,syntax augmented machine translation via chart parsing,"['Andreas Zollmann', 'Ashish Venugopal']",experiments,"We present translation results on the shared task ""Exploiting Parallel Texts for Statistical Machine Translation"" generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories. We use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence. Considering phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar. We present results on the French-to-English task for this workshop, representing significant improvements over the workshop's baseline system. Our translation system is available open-source under the GNU General Public License.","To create the baseline system , we use the opensource Joshua 4.0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #AUTHOR_TAG ) respectively .","['To create the baseline system , we use the opensource Joshua 4.0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #AUTHOR_TAG ) respectively .']",5,"['In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']","['Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']"
CC1212,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,learning accurate compact and interpretable tree annotation,"['Slav Petrov', 'Leon Barrett', 'Romain Thibaux', 'Dan Klein']",experiments,"We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank. Starting with a simple Xbar grammar, we learn a new grammar whose nonterminals are subsymbols of the original nonterminals. In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data. Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation. On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation. Despite its simplicity, our best grammar achieves an F1 of 90.2 % on the Penn Treebank, higher than fully lexicalized systems.","To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #AUTHOR_TAG ) .","['To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #AUTHOR_TAG ) .', 'Then, we binarize the English parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system.']",5,"['2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #AUTHOR_TAG ) .']",['This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #AUTHOR_TAG ) .'],"['2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #AUTHOR_TAG ) .']","['However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training.', '2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #AUTHOR_TAG ) .']",['This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #AUTHOR_TAG ) .'],"['However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training.', '2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #AUTHOR_TAG ) .']","['2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #AUTHOR_TAG ) .']","['However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training.', '2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #AUTHOR_TAG ) .']",['This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #AUTHOR_TAG ) .']
CC1213,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,binarizing syntax trees to improve syntaxbased machine translation accuracy,"['Wei Wang', 'Kevin Knight', 'Daniel Marcu']",method,We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result.,"This is because the binary structure has been verified to be very effective for tree-based translation ( #AUTHOR_TAG ; Zhang et al. , 2011a ) .","['is the probability of producing the target tree fragment frag.', 'To generate frag,  used a geometric prior to decide how many child nodes to assign each node.', 'Differently, we require that each multi-word non-terminal node must have two child nodes.', 'This is because the binary structure has been verified to be very effective for tree-based translation ( #AUTHOR_TAG ; Zhang et al. , 2011a ) .']",4,"['Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein ( 2008 ) and #AUTHOR_TAG focused on joint parsing and alignment .']",['Burkett and Klein ( 2008 ) and #AUTHOR_TAG focused on joint parsing and alignment .'],"['Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein ( 2008 ) and #AUTHOR_TAG focused on joint parsing and alignment .']","['This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein ( 2008 ) and #AUTHOR_TAG focused on joint parsing and alignment .']",['Burkett and Klein ( 2008 ) and #AUTHOR_TAG focused on joint parsing and alignment .'],"['This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein ( 2008 ) and #AUTHOR_TAG focused on joint parsing and alignment .']","['Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein ( 2008 ) and #AUTHOR_TAG focused on joint parsing and alignment .']","['Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein ( 2008 ) and #AUTHOR_TAG focused on joint parsing and alignment .']","['Burkett and Klein ( 2008 ) and #AUTHOR_TAG focused on joint parsing and alignment .', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.']"
CC1214,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,treetostring alignment template for statistical machine translation,"['Yang Liu', 'Qun Liu', 'Shouxun Lin']",introduction,"We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models.","Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .","['In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']",0,"['Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', '#AUTHOR_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .']",['#AUTHOR_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .'],"['Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', '#AUTHOR_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .']","['adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', '#AUTHOR_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .']",['#AUTHOR_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .'],"['adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', '#AUTHOR_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .']","['Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', '#AUTHOR_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .']","['They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', '#AUTHOR_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .']","['#AUTHOR_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.']"
CC1215,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,restructuring relabeling and realigning for syntaxbased machine translation,"['Wei Wang', 'Jonathan May', 'Kevin Knight', 'Daniel Marcu']",introduction,,This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #AUTHOR_TAG ) .,"['However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training.', '2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #AUTHOR_TAG ) .']",0,"['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']","['Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']","['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']","['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']","['Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']","['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']","['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']","['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']","['Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']"
CC1216,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,joint parsing and alignment with weakly synchronized grammars,"['David Burkett', 'John Blitzer', 'Dan Klein']",related work,"Syntactic machine translation systems extract rules from bilingual, word-aligned, syntacti-cally parsed text, but current systems for pars-ing and word alignment are at best cascaded and at worst totally independent of one an-other. This work presents a unified joint model for simultaneous parsing and word alignment. To flexibly model syntactic divergence, we de-velop a discriminative log-linear model over two parse trees and an ITG derivation which is encouraged but not forced to synchronize with the parses. Our model gives absolute improvements of 3.3 F1 for English pars-ing, 2.1 F1 for Chinese parsing, and 5.5 F1 for word alignment over each task's indepen-dent baseline, giving the best reported results for both Chinese-English word alignment and joint parsing on the parallel portion of the Chi-nese treebank. We also show an improvement of 1.2 BLEU in downstream MT evaluation over basic HMM alignments.",Burkett and Klein ( 2008 ) and #AUTHOR_TAG focused on joint parsing and alignment .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein ( 2008 ) and #AUTHOR_TAG focused on joint parsing and alignment .', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']","['Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']","['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']","['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']","['Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']","['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']","['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']","['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']","['Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']"
CC1217,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,transforming trees to improve syntactic convergence,"['David Burkett', 'Dan Klein']",related work,"We describe a transformation-based learning method for learning a sequence of mono-lingual tree transformations that improve the agreement between constituent trees and word alignments in bilingual corpora. Using the manually annotated English Chinese Transla-tion Treebank, we show how our method au-tomatically discovers transformations that ac-commodate differences in English and Chi-nese syntax. Furthermore, when transforma-tions are learned on automatically generated trees and alignments from the same domain as the training data for a syntactic MT system, the transformed trees achieve a 0.9 BLEU im-provement over baseline trees.",#AUTHOR_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', '#AUTHOR_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .']","['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .', 'This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.']","['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .', 'This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.']","['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .']","['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .', 'This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.', 'Considering the above reasons, we believe that the STSG-based learning procedure would result in a better translation grammar for tree-based models.']","['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .', 'This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.']","['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .', 'This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.', 'Considering the above reasons, we believe that the STSG-based learning procedure would result in a better translation grammar for tree-based models.']","['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .']","['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .', 'This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.', 'Considering the above reasons, we believe that the STSG-based learning procedure would result in a better translation grammar for tree-based models.']"
CC1218,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a bayesian model of syntaxdirected tree to string grammar induction,"['Trevor Cohn', 'Phil Blunsom']",method,"Tree based translation models are a compelling means of integrating linguistic information into machine translation. Syntax can inform lexical selection and reordering choices and thereby improve translation quality. Research to date has focussed primarily on decoding with such models, but less on the difficult problem of inducing the bilingual grammar from data. We propose a generative Bayesian model of tree-to-string translation which induces grammars that are both smaller and produce better translations than the previous heuristic two-stage approach which employs a separate word alignment step.","Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .","['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']",4,"['Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .']","['#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.']","['Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.']","['Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .']","['#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']","['Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.']","['Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']","['Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .']","['#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']"
CC1219,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a gibbs sampler for phrasal synchronous grammar induction,"['Phil Blunsom', 'Trevor Cohn', 'Chris Dyer', 'Miles Osborne']",method,"We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches.","Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .","['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']",4,"['Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan (Kobayashi et al., 1992).', 'This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #AUTHOR_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses .']","['This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #AUTHOR_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses .']","['Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan (Kobayashi et al., 1992).', 'This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #AUTHOR_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses .']","['As the recogn/fion engine, either VoiceRex, developed by NTI"" (Noda et al., 1998), or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan (Kobayashi et al., 1992).', 'This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #AUTHOR_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses .']","['This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #AUTHOR_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses .']","['As the recogn/fion engine, either VoiceRex, developed by NTI"" (Noda et al., 1998), or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan (Kobayashi et al., 1992).', 'This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #AUTHOR_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses .']","['Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan (Kobayashi et al., 1992).', 'This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #AUTHOR_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses .']","['word hypotheses.', 'As the recogn/fion engine, either VoiceRex, developed by NTI"" (Noda et al., 1998), or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan (Kobayashi et al., 1992).', 'This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #AUTHOR_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses .']","['This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #AUTHOR_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses .', 'This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible.', 'This module continuously runs and outputs recognition results when it detects a speech interval.', 'This enables the language generation module to react immediately to user interruptions while the system is speaking.']"
CC1220,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a gibbs sampler for phrasal synchronous grammar induction,"['Phil Blunsom', 'Trevor Cohn', 'Chris Dyer', 'Miles Osborne']",,"We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches.","In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .","['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .', 'This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.', 'Considering the above reasons, we believe that the STSG-based learning procedure would result in a better translation grammar for tree-based models.']",1,"['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']"
CC1221,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,syntax augmented machine translation via chart parsing,"['Andreas Zollmann', 'Ashish Venugopal']",related work,"We present translation results on the shared task ""Exploiting Parallel Texts for Statistical Machine Translation"" generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories. We use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence. Considering phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar. We present results on the French-to-English task for this workshop, representing significant improvements over the workshop's baseline system. Our translation system is available open-source under the GNU General Public License.",#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ) .']","['There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ) .', 'Incorporating such techniques would deo crease the system developer workload.']","['There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ) .', 'Incorporating such techniques would deo crease the system developer workload.']","['There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ) .']","['There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ) .', 'Incorporating such techniques would deo crease the system developer workload.', 'However, there has been no work on domain-independent response generation for robust spoken dialogue systems that can deal with utterances that might include pauses in the middle of a sentence, which WIT handles well.']","['There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ) .', 'Incorporating such techniques would deo crease the system developer workload.']","['There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ) .', 'Incorporating such techniques would deo crease the system developer workload.', 'However, there has been no work on domain-independent response generation for robust spoken dialogue systems that can deal with utterances that might include pauses in the middle of a sentence, which WIT handles well.']","['There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ) .']","['There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ) .', 'Incorporating such techniques would deo crease the system developer workload.', 'However, there has been no work on domain-independent response generation for robust spoken dialogue systems that can deal with utterances that might include pauses in the middle of a sentence, which WIT handles well.', 'Therefore incorporating those techniques remains as a future work.']"
CC1222,W00-1017,WIT,implementation of coordinative nodding behavior on spoken dialogue systems,"['Jun-ichi Hirasawa', 'Noboru Miyazaki', 'Mikio Nakano', 'Takeshi Kawabata']",,"This paper proposes a mechanism that contributes to the implementation of a spoken dialogue system with which a user can communicate e ortlessly. In a dialogue, exchanges between participants promote the establishment of shared information and this leads to e ortless communication. This is called  dialogue coordination"". In particular, revealing the respondent's internal state, such as through nodding and back-channel feedback, promotes the establishment of shared information. This is called  manifestation"", which is one aspect of coordinative behavior, and a mechanism for handling manifestation is introduced. In a human-human dialogue, the listener's manifestative behavior often occurs during a speaker's utterance. However, systems using conventional speech recognition technologies cannot respond during the speaker's utterance. In order to solve this problem, the proposed mechanism, ISTAR protocol transmission, utilizes the intermediate speech recognition results without waiting for the end of the speaker's utterance. This realizes a system with exible manifestative behavior.","This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #AUTHOR_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses .","['word hypotheses.', 'As the recogn/fion engine, either VoiceRex, developed by NTI"" (Noda et al., 1998), or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan (Kobayashi et al., 1992).', 'This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #AUTHOR_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses .', 'This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible.', 'This module continuously runs and outputs recognition results when it detects a speech interval.', 'This enables the language generation module to react immediately to user interruptions while the system is speaking.', 'The language model for speech recognition is a network (regular) grammar, and it allows each speech interval to be an arbitrary number of phrases.', 'A phrase is a sequence of words, which is to be defined in a domain-dependent way.', 'Sentences can be decomposed into a couple of phrases.', 'The reason we use a repetition of phrases instead of a sentence grammar for the language model is that the speech recognition module of a robust spoken dialogue system sometimes has to recognize spontaneously spoken utterances, which include self-repairs and repetition.', 'In Japanese, bunsetsu is appropriate for defining phrases.', 'A bunsetsu consists of one content word and a number (possibly zero) of function words.', 'In the meeting room reservation system we have developed, examples of defined phrases are bunsetsu to specify the room to be reserved and the time of the reservation and bunsetsu to express affirmation and negation.']",0,"['Functions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG .']",['They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG .'],"['Functions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG .']","['Functions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG .']",['They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG .'],"['Functions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG .']","['Functions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG .']","['Functions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG .']",['They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG .']
CC1223,W00-1017,WIT,the philips automatic train timetable information system,"['Harald Aust', 'Martin Oerder', 'Frank Seide', 'Volker Steinbiss']",introduction,,"The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']",0,"['As the recogn/fion engine, either VoiceRex, developed by NTI"" (Noda et al., 1998), or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #AUTHOR_TAG ) .']",['Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #AUTHOR_TAG ) .'],"['As the recogn/fion engine, either VoiceRex, developed by NTI"" (Noda et al., 1998), or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #AUTHOR_TAG ) .']","['word hypotheses.', 'As the recogn/fion engine, either VoiceRex, developed by NTI"" (Noda et al., 1998), or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #AUTHOR_TAG ) .']",['Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #AUTHOR_TAG ) .'],"['word hypotheses.', 'As the recogn/fion engine, either VoiceRex, developed by NTI"" (Noda et al., 1998), or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #AUTHOR_TAG ) .']","['As the recogn/fion engine, either VoiceRex, developed by NTI"" (Noda et al., 1998), or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #AUTHOR_TAG ) .']","['word hypotheses.', 'As the recogn/fion engine, either VoiceRex, developed by NTI"" (Noda et al., 1998), or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #AUTHOR_TAG ) .']","['Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #AUTHOR_TAG ) .', 'This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search (Hirasawa et al., 1998) using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses.', 'This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible.', 'This module continuously runs and outputs recognition results when it detects a speech interval.']"
CC1224,W00-1017,WIT,gus a frame driven dialog system,"['Daniel G Bobrow', 'Ronald M Kaplan', 'Martin Kay', 'Donald A Norman', 'Henry Thompson', 'Teny Winograd']",conclusion,,"There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ) .","['There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ) .', 'Incorporating such techniques would deo crease the system developer workload.', 'However, there has been no work on domain-independent response generation for robust spoken dialogue systems that can deal with utterances that might include pauses in the middle of a sentence, which WIT handles well.', 'Therefore incorporating those techniques remains as a future work.']",3,"['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ) .']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ) .']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ) .']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']"
CC1225,W00-1017,WIT,mimic an adaptive mixed initiative spoken dialogue system for information queries,['Junnifer Chu-Carroll'],,"This paper describes MIMIC, an adaptive mixed initiative spoken dialogue system that provides movie showtime information. MIMIC improves upon previous dialogue systems in two respects. First, it employs initiative-oriented strategy adaptation to automatically adapt response generation strategies based on the cumulative effect of information dynamically extracted from user utterances during the dialogue. Second, MIMIC's dialogue management architecture decouples its initiative module from the goal and response strategy selection processes, providing a general framework for developing spoken dialogue systems with different adaptation behavior.",They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG .,"['Functions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG .']",1,"['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ) .']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ) .']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ) .']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']"
CC1226,W00-1017,WIT,asj continuous speech corpus for research,"['Tetsunori Kobayashi', 'Shuichi Itahashi', 'Satoru Hayamizu', 'Toshiyuld Takezawa']",,,Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #AUTHOR_TAG ) .,"['word hypotheses.', 'As the recogn/fion engine, either VoiceRex, developed by NTI"" (Noda et al., 1998), or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #AUTHOR_TAG ) .', 'This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search (Hirasawa et al., 1998) using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses.', 'This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible.', 'This module continuously runs and outputs recognition results when it detects a speech interval.', 'This enables the language generation module to react immediately to user interruptions while the system is speaking.', 'The language model for speech recognition is a network (regular) grammar, and it allows each speech interval to be an arbitrary number of phrases.', 'A phrase is a sequence of words, which is to be defined in a domain-dependent way.', 'Sentences can be decomposed into a couple of phrases.', 'The reason we use a repetition of phrases instead of a sentence grammar for the language model is that the speech recognition module of a robust spoken dialogue system sometimes has to recognize spontaneously spoken utterances, which include self-repairs and repetition.', 'In Japanese, bunsetsu is appropriate for defining phrases.', 'A bunsetsu consists of one content word and a number (possibly zero) of function words.', 'In the meeting room reservation system we have developed, examples of defined phrases are bunsetsu to specify the room to be reserved and the time of the reservation and bunsetsu to express affirmation and negation.']",5,"['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #AUTHOR_TAGb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) .']","['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #AUTHOR_TAGb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) .', 'The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.']","['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #AUTHOR_TAGb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) .', 'The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.']","['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #AUTHOR_TAGb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) .']","['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #AUTHOR_TAGb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) .', 'The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'A sample dialogue between this system and a naive user is shown in Figure 2.']","['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #AUTHOR_TAGb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) .', 'The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.']","['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #AUTHOR_TAGb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) .', 'The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'A sample dialogue between this system and a naive user is shown in Figure 2.']","['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #AUTHOR_TAGb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) .']","['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #AUTHOR_TAGb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) .', 'The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'A sample dialogue between this system and a naive user is shown in Figure 2.', 'This system employs HTK as the speech recognition engine.']"
CC1227,W00-1017,WIT,learning to predict problematic situations in a spoken dialogue system experiments with how may i help you,"['Marilyn Walker', 'Irene Langkilde', 'Jerry Wright Allen Gorin', 'Diane Litman']",introduction,"Current spoken dialogue systems are deficient in their strategies for preventing, identifying and repairing problems that arise in the conversation. This paper reports results on learning to automatically identify and predict problematic human-computer dialogues in a corpus of 4774 dialogues collected with the How May I Help You spoken dialogue system. Our expectation is that the ability to predict problematic dialogues will allow the system's dialogue manager to modify its behavior to repair problems, and even perhaps, to prevent them. We train a problematic dialogue classifier using automatically-obtainable features that can identify problematic dialogues significantly better (23%) than the baseline. A classifier trained with only automatic features from the first exchange in the dialogue can predict problematic dialogues 7% more accurately than the baseline, and one trained with automatic features from the first two exchanges can perform 14% better than the baseline.","The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ) .","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']",0,"['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .']","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']"
CC1228,W00-1017,WIT,jupiter a telephonebased conversational interface for weather information,"['Victor Zue', 'Stephanie Seneff', 'James Glass', 'Joseph Polifroni', 'Christine Pao', 'Timothy J Hazen', 'Lee Hetherington']",introduction,,"The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ) .","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']",0,"['Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered.', 'For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ) .']","['For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ) .']","['Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered.', 'For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ) .']","['Recording some dialogue history is also possible.', 'Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered.', 'For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ) .']","['For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ) .']","['Recording some dialogue history is also possible.', 'Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered.', 'For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ) .']","['Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered.', 'For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ) .']","['For example, it is possible to represent a discourse stack whose depth is limited.', 'Recording some dialogue history is also possible.', 'Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered.', 'For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ) .']","['For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ) .', 'The language generation module features Common Lisp functions, so there is no limitation on the description.', 'Some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shirnazu, 1997).', 'It is also possible to build a simple finite-state-model-based dialogue system using WIT.']"
CC1229,W00-1017,WIT,understanding unsegmented user utterances in realtime spoken dialogue systems,"['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']",experiments,"This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries.","WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #AUTHOR_TAGb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) .","['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #AUTHOR_TAGb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) .', 'The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'A sample dialogue between this system and a naive user is shown in Figure 2.', 'This system employs HTK as the speech recognition engine.', ""The weather information system can answer the user's questions about weather forecasts in Japan."", 'The vocabulary size is around 500, and the number of phrase structure rules is 31.', 'The number of attributes in the semantic flame is 11, and the number of the files of the pre-recorded speech is about 13,000.']",2,"['Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAGa ) .']","['Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAGa ) .', 'Thus the system can respond immediately after user pauses when the user has the initiative.']","['Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAGa ) .', 'Thus the system can respond immediately after user pauses when the user has the initiative.']","['Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAGa ) .']","['Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAGa ) .', 'Thus the system can respond immediately after user pauses when the user has the initiative.', 'When the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way (Dohsaka and Shimazu, 1997).']","['Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAGa ) .', 'Thus the system can respond immediately after user pauses when the user has the initiative.']","['Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAGa ) .', 'Thus the system can respond immediately after user pauses when the user has the initiative.', 'When the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way (Dohsaka and Shimazu, 1997).']","['Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAGa ) .']","['Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAGa ) .', 'Thus the system can respond immediately after user pauses when the user has the initiative.', 'When the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way (Dohsaka and Shimazu, 1997).']"
CC1230,W00-1017,WIT,a robust system for natural spoken dialogue,"['James F Allen', 'Bradford W Miller', 'Eric K Ringger', 'Teresa Sikorsld']",introduction,"This paper describes a system that leads us to believe in the feasibility of constructing natural spoken dialogue systems in task-oriented domains. It specifically addresses the issue of robust interpretation of speech in the presence of recognition errors. Robustness is achieved by a combination of statistical error post-correction, syntactically- and semantically-driven robust parsing, and extensive use of the dialogue context. We present an evaluation of the system using time-to-completion and the quality of the final solution that suggests that most native speakers of English can use the system successfully with virtually no training.Comment: uuencoded, gzipped PostScript. Includes extra Appendi","The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']",0,"['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( #AUTHOR_TAG ) .']","['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( #AUTHOR_TAG ) .', 'The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.']","['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( #AUTHOR_TAG ) .', 'The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.']","['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( #AUTHOR_TAG ) .']","['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( #AUTHOR_TAG ) .', 'The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'A sample dialogue between this system and a naive user is shown in Figure 2.']","['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( #AUTHOR_TAG ) .', 'The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.']","['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( #AUTHOR_TAG ) .', 'The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'A sample dialogue between this system and a naive user is shown in Figure 2.']","['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( #AUTHOR_TAG ) .']","['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( #AUTHOR_TAG ) .', 'The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'A sample dialogue between this system and a naive user is shown in Figure 2.', 'This system employs HTK as the speech recognition engine.']"
CC1231,W00-1017,WIT,a grammar and a parser for spontaneous speech,"['Mikio Nakano', 'Akira Shimazu', 'Kiyoshi Kogure']",conclusion,"This paper classifies distinctive phenomena occurring in Japanese spontaneous speech, and proposes a grammar and processing techniques for handling them. Parsers using a grammar for written sentences cannot deal with spontaneous speech because in spontaneous speech there are phenomena that do not occur in written sentences. A grammar based on analysis of transcripts of dialogues was therefore developed. It has two distinctive features: it uses short units as input units instead of using sentences in grammars for written sentences, and it covers utterances including phrases peculiar to spontaneous speech. Since the grammar is an augmentation of a grammar for written sentences, it can also be used to analyze complex utterances. Incorporating the grammar into the distributed natural language processing model described elsewhere enables the handling of utterances including variety of phenomena peculiar to spontaneous speech.","For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ) .","['Whereas previous finite-state-model-based toolkits place many severe restrictions on domain descriptions, WIT has enough descriptive power to build a variety of dialogue systems.', 'Although the dialogue state is represented by a simple attributevalue matrix, since there is no limitation on the number of attributes, it can hold more complicated information.', 'For example, it is possible to represent a discourse stack whose depth is limited.', 'Recording some dialogue history is also possible.', 'Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered.', 'For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ) .', 'The language generation module features Common Lisp functions, so there is no limitation on the description.', 'Some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shirnazu, 1997).', 'It is also possible to build a simple finite-state-model-based dialogue system using WIT.', 'States can be represented by dialogue phases in WIT.']",3,"['This paper presents WIT 1, which is a toolkit IWIT is an acronym of Workable spoken dialogue lnter-for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output.', 'WIT features an incremental understanding method ( #AUTHOR_TAGb ) that makes it possible to build a robust and real-time system .']","['WIT features an incremental understanding method ( #AUTHOR_TAGb ) that makes it possible to build a robust and real-time system .', 'In addition, WIT compiles domain-dependent system specifications into internal knowledge sources so that building systems is easier.']","['This paper presents WIT 1, which is a toolkit IWIT is an acronym of Workable spoken dialogue lnter-for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output.', 'WIT features an incremental understanding method ( #AUTHOR_TAGb ) that makes it possible to build a robust and real-time system .']","['This paper presents WIT 1, which is a toolkit IWIT is an acronym of Workable spoken dialogue lnter-for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output.', 'WIT features an incremental understanding method ( #AUTHOR_TAGb ) that makes it possible to build a robust and real-time system .']","['WIT features an incremental understanding method ( #AUTHOR_TAGb ) that makes it possible to build a robust and real-time system .', 'In addition, WIT compiles domain-dependent system specifications into internal knowledge sources so that building systems is easier.', 'Although WIT requires more domaindependent specifications than finite-state-modelbased toolkits, WIT-based systems are capable of taking full advantage of language processing technology.']","['This paper presents WIT 1, which is a toolkit IWIT is an acronym of Workable spoken dialogue lnter-for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output.', 'WIT features an incremental understanding method ( #AUTHOR_TAGb ) that makes it possible to build a robust and real-time system .', 'In addition, WIT compiles domain-dependent system specifications into internal knowledge sources so that building systems is easier.']","['This paper presents WIT 1, which is a toolkit IWIT is an acronym of Workable spoken dialogue lnter-for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output.', 'WIT features an incremental understanding method ( #AUTHOR_TAGb ) that makes it possible to build a robust and real-time system .', 'In addition, WIT compiles domain-dependent system specifications into internal knowledge sources so that building systems is easier.', 'Although WIT requires more domaindependent specifications than finite-state-modelbased toolkits, WIT-based systems are capable of taking full advantage of language processing technology.']","['This paper presents WIT 1, which is a toolkit IWIT is an acronym of Workable spoken dialogue lnter-for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output.', 'WIT features an incremental understanding method ( #AUTHOR_TAGb ) that makes it possible to build a robust and real-time system .']","['WIT features an incremental understanding method ( #AUTHOR_TAGb ) that makes it possible to build a robust and real-time system .', 'In addition, WIT compiles domain-dependent system specifications into internal knowledge sources so that building systems is easier.', 'Although WIT requires more domaindependent specifications than finite-state-modelbased toolkits, WIT-based systems are capable of taking full advantage of language processing technology.', 'WIT has been implemented and used to build several spoken dialogue systems.']"
CC1232,W00-1017,WIT,understanding unsegmented user utterances in realtime spoken dialogue systems,"['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']",,"This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries.","Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAGa ) .","['Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAGa ) .', 'Thus the system can respond immediately after user pauses when the user has the initiative.', 'When the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way (Dohsaka and Shimazu, 1997).']",0,"['The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.', 'Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .']","['Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .', 'When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase.']","['The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.', 'Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .']","['The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.', 'Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .']","['Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .', 'When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase.', 'The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.']","['The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.', 'Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .', 'When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase.']","['The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.', 'Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .', 'When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase.', 'The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.']","['The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.', 'Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .']","['Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .', 'When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase.', 'The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.', ""Two kinds of sentenees can be considered; domain-related ones that express the user's intention about the reser-vafion and dialogue-related ones that express the user's attitude with respect to the progress of the dialogue, such as confirmation and denial.""]"
CC1233,W00-1017,WIT,an efficient dialogue control method under systems limited knowledge,"['Kohji Dohsaka', 'Norihito Yasuda', 'Noboru Miyazaki', 'Mikio Nakano', 'Kiyoald Aikawa']",experiments,"This paper presents a novel method that controls a dialogue between a spoken dialogue system and a user efficiently so that the system responds as helpfully as possible within the limits of its knowledge. Due to speech recognition errors, a system and user must engage in a ""confirmation dialogue"" to clarify a user's request. Although a confirmation dialogue is unavoidable, it should be as concise as possible. Previous methods do not sufficiently allow for the effect of the limits of the system's knowledge on the efficiency of dialogue. The result is unnecessarily long dialogues to confirm a user's request minutely even if the request is beyond the system's knowledge. This paper describes a method that controls a dialogue efficiently so as to avoid an unnecessary confirmation dialogue and presents a computational efficiency criterion for dialogue control within the limits of the system's knowledge.","WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( #AUTHOR_TAG ) .","['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( #AUTHOR_TAG ) .', 'The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'A sample dialogue between this system and a naive user is shown in Figure 2.', 'This system employs HTK as the speech recognition engine.', ""The weather information system can answer the user's questions about weather forecasts in Japan."", 'The vocabulary size is around 500, and the number of phrase structure rules is 31.', 'The number of attributes in the semantic flame is 11, and the number of the files of the pre-recorded speech is about 13,000.']",2,"['It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper.', 'The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .']","['The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .', ""When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state.""]","['It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper.', 'The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .', ""When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state.""]","['These roles are similar to DCG (Pereira and Warren, 1980) rules; they can include logical variables and these variables can be bound when these rules are applied.', 'It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper.', 'The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .']","['The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .', ""When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state."", 'The command is one of the following:']","['These roles are similar to DCG (Pereira and Warren, 1980) rules; they can include logical variables and these variables can be bound when these rules are applied.', 'It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper.', 'The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .', ""When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state.""]","['It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper.', 'The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .', ""When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state."", 'The command is one of the following:']","['These roles are similar to DCG (Pereira and Warren, 1980) rules; they can include logical variables and these variables can be bound when these rules are applied.', 'It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper.', 'The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .']","['The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .', ""When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state."", 'The command is one of the following:']"
CC1234,W00-1017,WIT,understanding unsegmented user utterances in realtime spoken dialogue systems,"['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']",introduction,"This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries.",WIT features an incremental understanding method ( #AUTHOR_TAGb ) that makes it possible to build a robust and real-time system .,"['This paper presents WIT 1, which is a toolkit IWIT is an acronym of Workable spoken dialogue lnter-for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output.', 'WIT features an incremental understanding method ( #AUTHOR_TAGb ) that makes it possible to build a robust and real-time system .', 'In addition, WIT compiles domain-dependent system specifications into internal knowledge sources so that building systems is easier.', 'Although WIT requires more domaindependent specifications than finite-state-modelbased toolkits, WIT-based systems are capable of taking full advantage of language processing technology.', 'WIT has been implemented and used to build several spoken dialogue systems.']",5,"['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .']","['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model.']","['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model.']","['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .']","['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model.', 'It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain.']","['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model.']","['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model.', 'It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain.']","['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .']","['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model.', 'It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain.', 'However, it limits system functions; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics.']"
CC1235,W00-1017,WIT,constraint projection an efficient treatment of disjunctive feature descriptions,['Mikio Nakano'],,"Unification of disjunctive feature descriptions is important for efficient unification-based parsing. This paper presents constraint projection, a new method for unification of disjunctive feature structures represented by logical constraints. Constraint projection is a generalization of constraint unification, and is more efficient because constraint projection has a mechanism for abandoning information irrelevant to a goal specified by a list of variables.",Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .,"['The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.', 'Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .', 'When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase.', 'The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.', ""Two kinds of sentenees can be considered; domain-related ones that express the user's intention about the reser-vafion and dialogue-related ones that express the user's attitude with respect to the progress of the dialogue, such as confirmation and denial."", 'Considering the meeting room reservation system, examples of domain-related sentences are ""I need to book Room 2 on Wednesday"", ""I need to book Room 2"", and ""Room 2"" and dialogue-related ones are ""yes"", ""no"", and ""Okay"".']",5,"['The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs).', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .']","['The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence.']","['The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs).', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence.']","['The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs).', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .']","['The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence.', 'ISSS also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time.']","['The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs).', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence.']","['The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs).', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence.', 'ISSS also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time.']","['The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs).', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .']","['The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence.', 'ISSS also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time.']"
CC1236,W00-1017,WIT,understanding unsegmented user utterances in realtime spoken dialogue systems,"['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']",experiments,"This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries.",The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .,"['These roles are similar to DCG (Pereira and Warren, 1980) rules; they can include logical variables and these variables can be bound when these rules are applied.', 'It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper.', 'The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .', ""When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state."", 'The command is one of the following:']",5,"['Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).']","['Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.']","['Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.']","['Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).']","['Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997.']","['Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.']","['Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997.']","['Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).']","['Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997.', 'Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR.']"
CC1237,W00-1017,WIT,europa a generic framework for developing spoken dialogue systems,"['Munehiko Sasajima', 'Yakehide Yano', 'Yasuyuld Kono']",introduction,"Voice interfaces are not popular since they are neither useful nor user-friendly for non-specialist users. In this paper, EUROPA, a new framework for developing spoken dialogue systems, is introduced. In developing EUROPA, the authors focused on three points : (1) acceptance of spoken language, (2) portability in terms of domain and task, and (3) practical performance of the applied system. The framework is applied to prototyping a car navigation system called MINOS. MINOS is built on a portable PC, can process over 700 words of recognition vocabulary, and is able to respond to a user's question within a few seconds.","To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .","['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model.', 'It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain.', 'However, it limits system functions; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics.', 'Another is GALAXY-II (Seneffet al., 1998), *Mikio Nakano is currently a visiting scientist at MIT Laboratory for Computer Science. which enables modules in a dialogue system to communicate with each other.', 'It consists of the hub and several servers, such as the speech recognition server and the natural language server, and the hub communicates with these servers.', 'Although it requires more specifications than finitestate-model-based toolkits, it places less limitations on system functions.']",0,"['This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ) .']","['There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ) .']","['This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ) .']","['A second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ) .']","['There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ) .']","['A second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ) .']","['This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ) .']","['A second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ) .']","['There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ) .', 'One such method is to treat different translations of the same term as synonyms.', 'Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms.', 'However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.']"
CC1238,W00-1017,WIT,understanding unsegmented user utterances in realtime spoken dialogue systems,"['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']",,"This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries.","The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .","['The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs).', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence.', 'ISSS also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time.']",5,"['Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries.', 'The one-sided t-test ( #AUTHOR_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .']",['The one-sided t-test ( #AUTHOR_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .'],"['Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries.', 'The one-sided t-test ( #AUTHOR_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .']","['The results in Table 4 show that manual disambiguation improves performance by 17% on Trec5C, 4% on Trec4S, but not at all on Trec6C.', 'Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries.', 'The one-sided t-test ( #AUTHOR_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .']",['The one-sided t-test ( #AUTHOR_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .'],"['The results in Table 4 show that manual disambiguation improves performance by 17% on Trec5C, 4% on Trec4S, but not at all on Trec6C.', 'Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries.', 'The one-sided t-test ( #AUTHOR_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .']","['Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries.', 'The one-sided t-test ( #AUTHOR_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .']","['The results in Table 4 show that manual disambiguation improves performance by 17% on Trec5C, 4% on Trec4S, but not at all on Trec6C.', 'Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries.', 'The one-sided t-test ( #AUTHOR_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .']",['The one-sided t-test ( #AUTHOR_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .']
CC1239,W00-1312,Cross-lingual information retrieval using hidden Markov models,phrasal translation and query expansion techniques for crosslanguage information retrievalquot,"['L Ballesteros', 'W B Croft']",related work,"Dictionary methods for cross-language information retrieval give performance below that for mono-lingual retrieval. Failure to translate multi-term phrases has been shown to be one of the factors responsible for the errors associated with dictionary methods. First, we study the importance of phrasal translation for this approach. Second, we explore the role of phrases in query expansion via local context analysis and local feedback and show how they can be used to significantly reduce the error associated with automatic dictionary translation.","Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).","['Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997.', 'Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR.']",1,"['The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #AUTHOR_TAG ) .']","['The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #AUTHOR_TAG ) .', 'We believe our approach is computationally less costly than (LSI and GVSM) and assumes less resources (WordNet in Diekema et al., 1999).']","['The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #AUTHOR_TAG ) .', 'We believe our approach is computationally less costly than (LSI and GVSM) and assumes less resources (WordNet in Diekema et al., 1999).']","['The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #AUTHOR_TAG ) .']","['The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #AUTHOR_TAG ) .', 'We believe our approach is computationally less costly than (LSI and GVSM) and assumes less resources (WordNet in Diekema et al., 1999).']","['The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #AUTHOR_TAG ) .', 'We believe our approach is computationally less costly than (LSI and GVSM) and assumes less resources (WordNet in Diekema et al., 1999).']","['The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #AUTHOR_TAG ) .', 'We believe our approach is computationally less costly than (LSI and GVSM) and assumes less resources (WordNet in Diekema et al., 1999).']","['The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #AUTHOR_TAG ) .']","['The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #AUTHOR_TAG ) .', 'We believe our approach is computationally less costly than (LSI and GVSM) and assumes less resources (WordNet in Diekema et al., 1999).']"
CC1240,W00-1312,Cross-lingual information retrieval using hidden Markov models,using structured queries for disambiguation in crosslanguage information retrievalquot,['D A Hull'],method,,"There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ) .","['A second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ) .', 'One such method is to treat different translations of the same term as synonyms.', 'Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms.', 'However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'By contrast, our HMM approach supports translation probabilities.', ""The synonym approach is equivalent to changing all non-zero translation probabilities P(W~[ Wy)'s to 1 in our retrieyal function."", 'Even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations.']",1,['Ã¢\x80Â¢ The transition probability a is 0.7 using the EM algorithm ( #AUTHOR_TAG ) on the TREC4 ad-hoc query set .'],['Ã¢\x80Â¢ The transition probability a is 0.7 using the EM algorithm ( #AUTHOR_TAG ) on the TREC4 ad-hoc query set .'],['Ã¢\x80Â¢ The transition probability a is 0.7 using the EM algorithm ( #AUTHOR_TAG ) on the TREC4 ad-hoc query set .'],['Ã¢\x80Â¢ The transition probability a is 0.7 using the EM algorithm ( #AUTHOR_TAG ) on the TREC4 ad-hoc query set .'],['Ã¢\x80Â¢ The transition probability a is 0.7 using the EM algorithm ( #AUTHOR_TAG ) on the TREC4 ad-hoc query set .'],['Ã¢\x80Â¢ The transition probability a is 0.7 using the EM algorithm ( #AUTHOR_TAG ) on the TREC4 ad-hoc query set .'],['Ã¢\x80Â¢ The transition probability a is 0.7 using the EM algorithm ( #AUTHOR_TAG ) on the TREC4 ad-hoc query set .'],['Ã¢\x80Â¢ The transition probability a is 0.7 using the EM algorithm ( #AUTHOR_TAG ) on the TREC4 ad-hoc query set .'],['Ã¢\x80Â¢ The transition probability a is 0.7 using the EM algorithm ( #AUTHOR_TAG ) on the TREC4 ad-hoc query set .']
CC1241,W00-1312,Cross-lingual information retrieval using hidden Markov models,using statistical testing in the evaluation of retrieval experimentsquot,['D Hull'],,,The one-sided t-test ( #AUTHOR_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .,"['The results in Table 4 show that manual disambiguation improves performance by 17% on Trec5C, 4% on Trec4S, but not at all on Trec6C.', 'Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries.', 'The one-sided t-test ( #AUTHOR_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .']",5,"['While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997.', '#AUTHOR_TAG studied the issue of disambiguation for mono-lingual M.']",['#AUTHOR_TAG studied the issue of disambiguation for mono-lingual M.'],"['While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997.', '#AUTHOR_TAG studied the issue of disambiguation for mono-lingual M.']","['(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997.', '#AUTHOR_TAG studied the issue of disambiguation for mono-lingual M.']",['#AUTHOR_TAG studied the issue of disambiguation for mono-lingual M.'],"['(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997.', '#AUTHOR_TAG studied the issue of disambiguation for mono-lingual M.']","['While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997.', '#AUTHOR_TAG studied the issue of disambiguation for mono-lingual M.']","['Another common approach is term translation, e.g., via a bilingual lexicon.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997.', '#AUTHOR_TAG studied the issue of disambiguation for mono-lingual M.']",['#AUTHOR_TAG studied the issue of disambiguation for mono-lingual M.']
CC1242,W00-1312,Cross-lingual information retrieval using hidden Markov models,translingual information retrieval a comparative evaluationquot,"['J Carbonell', 'Y Yang', 'R Frederlcing', 'R Brown', 'Y Geng', 'D Lee']",related work,Translingual information retrieval TIR con sists of providing a query in one language and searching document collections in one or more di erent languages This paper introduces new TIR methods and reports on comparative TIR experiments with these new methods and with previously reported ones in a realistic setting Methods fall into two categories query trans lation based and statistical IR approaches es tablishing translingual associations The re sults show that using bilingual corpora for au tomated extraction of term equivalences in con text outperforms other methods Translin gual versions of the Generalized Vector Space Model GVSM and Latent Semantic Indexing LSI perform relatively well as does translin gual pseudo relevance feedback PRF All showed relatively small performance loss be tween monolingual and translingual versions Query translation based on a general machine readable bilingual dictionary heretofore the most popular method did not match the per formance of other more sophisticated methods Also the previous very high LSI results in the literature were discon rmed by more realistic relevance based evaluations,"The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #AUTHOR_TAG ) .","['The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #AUTHOR_TAG ) .', 'We believe our approach is computationally less costly than (LSI and GVSM) and assumes less resources (WordNet in Diekema et al., 1999).']",1,"['While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; #AUTHOR_TAG .']","['Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; #AUTHOR_TAG .', 'Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR.']","['While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; #AUTHOR_TAG .', 'Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR.']","['(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; #AUTHOR_TAG .']","['Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; #AUTHOR_TAG .', 'Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR.']","['(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; #AUTHOR_TAG .', 'Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR.']","['While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; #AUTHOR_TAG .', 'Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR.']","['Another common approach is term translation, e.g., via a bilingual lexicon.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; #AUTHOR_TAG .']","['Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; #AUTHOR_TAG .', 'Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR.']"
CC1243,W00-1312,Cross-lingual information retrieval using hidden Markov models,a tutorial on hidden markov models and selected applications in speech recognitionquot,['L Rabiner'],,,Ã¢ÂÂ¢ The transition probability a is 0.7 using the EM algorithm ( #AUTHOR_TAG ) on the TREC4 ad-hoc query set .,['Ã¢\x80Â¢ The transition probability a is 0.7 using the EM algorithm ( #AUTHOR_TAG ) on the TREC4 ad-hoc query set .'],5,"['Many approaches to cross-lingual IR have been published.', 'One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ) .']","['One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ) .', 'For most languages, there are no MT systems at all.']","['Many approaches to cross-lingual IR have been published.', 'One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ) .', 'For most languages, there are no MT systems at all.']","['Many approaches to cross-lingual IR have been published.', 'One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ) .']","['One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ) .', 'For most languages, there are no MT systems at all.', 'Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived.']","['Many approaches to cross-lingual IR have been published.', 'One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ) .', 'For most languages, there are no MT systems at all.']","['Many approaches to cross-lingual IR have been published.', 'One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ) .', 'For most languages, there are no MT systems at all.', 'Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived.']","['Many approaches to cross-lingual IR have been published.', 'One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ) .']","['One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ) .', 'For most languages, there are no MT systems at all.', 'Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived.']"
CC1244,W00-1312,Cross-lingual information retrieval using hidden Markov models,word sense disambiguation and information retrievalquot,['M Sanderson'],related work,,#AUTHOR_TAG studied the issue of disambiguation for mono-lingual M.,"['Another common approach is term translation, e.g., via a bilingual lexicon.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997.', '#AUTHOR_TAG studied the issue of disambiguation for mono-lingual M.']",0,"['This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .']","['There are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .']","['This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .']","['A second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .']","['There are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .']","['A second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .']","['This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .']","['A second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .']","['There are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .', 'One such method is to treat different translations of the same term as synonyms.', 'Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms.', 'However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.']"
CC1245,W00-1312,Cross-lingual information retrieval using hidden Markov models,using structured queries for disambiguation in crosslanguage information retrievalquot,['D A Hull'],related work,,"Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; #AUTHOR_TAG .","['Another common approach is term translation, e.g., via a bilingual lexicon.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; #AUTHOR_TAG .', 'Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR.']",0,"['That is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at andbl.', ' However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #AUTHOR_TAG ) .']","[' However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #AUTHOR_TAG ) .']","['That is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at andbl.', ' However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #AUTHOR_TAG ) .']","['Also, a document matching different translations of one term in the original query may be ranked higher than a document that matches translations of different terms in the original query.', 'That is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at andbl.', ' However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #AUTHOR_TAG ) .']","[' However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #AUTHOR_TAG ) .']","['Also, a document matching different translations of one term in the original query may be ranked higher than a document that matches translations of different terms in the original query.', 'That is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at andbl.', ' However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #AUTHOR_TAG ) .']","['That is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at andbl.', ' However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #AUTHOR_TAG ) .']","['Since all terms are treated as equal in the translated query, this gives terms with more translations (potentially the more common terms) more credit in retrieval, even though such terms should potentially be given less credit if they are more common.', 'Also, a document matching different translations of one term in the original query may be ranked higher than a document that matches translations of different terms in the original query.', 'That is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at andbl.', ' However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #AUTHOR_TAG ) .']","[' However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #AUTHOR_TAG ) .']"
CC1246,W00-1312,Cross-lingual information retrieval using hidden Markov models,a comparative study of query and document translation for crosslanguage information retrievalquot,['D W Oard'],related work,"Cross-language retrieval systems use queries in one natural language to guide retrieval of documents that might be written in another. Acquisition and representation of translation knowledge plays a central role in this process. This paper explores the utility of two sources of translation knowledge for cross-language retrieval. We have implemented six query translation techniques that use bilingual term lists and one based on direct use of the translation output from an existing machine translation system; these are compared with a document translation technique that uses output from the same machine translation system. Average precision measures on a TREC collection suggest that arbitrarily selecting a single dictionary translation is typically no less effective than using every translation in the dictionary, that query translation using a machine translation system can achieve somewhat better effectiveness than simpler techniques, and that document translation may result in further improvements in retrieval effectiveness under some conditions.","One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ) .","['Many approaches to cross-lingual IR have been published.', 'One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ) .', 'For most languages, there are no MT systems at all.', 'Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived.']",1,"['Each English word has around 1.5 translations on average.', 'A cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem Spanish words .']","['A cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem Spanish words .', 'One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon.']","['Each English word has around 1.5 translations on average.', 'A cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem Spanish words .', 'One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon.']","['containing around 22,000 English words (16,000 English stems) and processed it similarly.', 'Each English word has around 1.5 translations on average.', 'A cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem Spanish words .']","['A cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem Spanish words .', 'One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon.', 'This is useful for translating proper nouns, which often have identical spellings in English and Spanish but are routinely excluded from a lexicon.']","['containing around 22,000 English words (16,000 English stems) and processed it similarly.', 'Each English word has around 1.5 translations on average.', 'A cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem Spanish words .', 'One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon.']","['Each English word has around 1.5 translations on average.', 'A cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem Spanish words .', 'One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon.', 'This is useful for translating proper nouns, which often have identical spellings in English and Spanish but are routinely excluded from a lexicon.']","['For Spanish, we downloaded a bilingual English-Spanish lexicon from the Internet (http://www.activa.arrakis.es)', 'containing around 22,000 English words (16,000 English stems) and processed it similarly.', 'Each English word has around 1.5 translations on average.', 'A cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem Spanish words .']","['A cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem Spanish words .', 'One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon.', 'This is useful for translating proper nouns, which often have identical spellings in English and Spanish but are routinely excluded from a lexicon.']"
CC1247,W00-1312,Cross-lingual information retrieval using hidden Markov models,the effects of query structure and dictionary setups in dictionarybased crosslanguage information retrievalquot,['An Pirkola'],method,,"There are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .","['A second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .', 'One such method is to treat different translations of the same term as synonyms.', 'Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms.', 'However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'By contrast, our HMM approach supports translation probabilities.', ""The synonym approach is equivalent to changing all non-zero translation probabilities P(W~[ Wy)'s to 1 in our retrieyal function."", 'Even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations.']",1,"['Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .']","['Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']","['Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']","['Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .']","['Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']","['Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']","['Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']","['Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .']","['Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']"
CC1248,W00-1312,Cross-lingual information retrieval using hidden Markov models,resolving ambiguity for crosslanguage retrievalquot,"['L Ballesteros', 'W B Croft']",method,," However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #AUTHOR_TAG ) .","['In this section we compare our approach with two other approaches.', 'One approach is ""simple substitution"", i.e., replacing a query term with all its translations and treating the translated query as a bag of words in mono-lingual retrieval.', 'Suppose we have a simple query Q=(a, b), the translations for a are al, a2, a3, and the translations for b are bl, b2. The translated query would be (at, a2, a3, b~, b2).', 'Since all terms are treated as equal in the translated query, this gives terms with more translations (potentially the more common terms) more credit in retrieval, even though such terms should potentially be given less credit if they are more common.', 'Also, a document matching different translations of one term in the original query may be ranked higher than a document that matches translations of different terms in the original query.', 'That is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at andbl.', ' However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #AUTHOR_TAG ) .']",0,"['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .']","['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']","['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']","['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .']","['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']","['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']","['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']","['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .']","['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']"
CC1249,W00-1312,Cross-lingual information retrieval using hidden Markov models,corpusbased stemming using cooccurrence of word variantsquot,"['J Xu', 'W B Croft']",experiments,,A cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem Spanish words .,"['For Spanish, we downloaded a bilingual English-Spanish lexicon from the Internet (http://www.activa.arrakis.es)', 'containing around 22,000 English words (16,000 English stems) and processed it similarly.', 'Each English word has around 1.5 translations on average.', 'A cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem Spanish words .', 'One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon.', 'This is useful for translating proper nouns, which often have identical spellings in English and Spanish but are routinely excluded from a lexicon.']",5,['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .'],"['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.']","['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.']",['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .'],"['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.']","['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.']","['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.']",['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .'],"['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.']"
CC1250,W00-1312,Cross-lingual information retrieval using hidden Markov models,a language modeling approach to information retrievalquot,"['J Ponte', 'W B Croft']",related work,,"Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .","['Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']",1,"['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .']","['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', 'Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ).']","['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', 'Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ).']","['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .']","['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', 'Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ).', 'The approach therefore estimates the probability that a query Q is generated, given the document D is relevant.']","['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', 'Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ).']","['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', 'Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ).', 'The approach therefore estimates the probability that a query Q is generated, given the document D is relevant.']","['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .']","['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', 'Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ).', 'The approach therefore estimates the probability that a query Q is generated, given the document D is relevant.', '(A glossary of symbols used appears below.)']"
CC1251,W00-1312,Cross-lingual information retrieval using hidden Markov models,on relevance probabilistic indexing and information retrievalquot,"['M E Maron', 'K L Kuhns']",related work,,"Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .","['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']",1,"['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #AUTHOR_TAG ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #AUTHOR_TAG ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #AUTHOR_TAG ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #AUTHOR_TAG ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #AUTHOR_TAG ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #AUTHOR_TAG ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #AUTHOR_TAG ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #AUTHOR_TAG ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #AUTHOR_TAG ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1252,W00-1312,Cross-lingual information retrieval using hidden Markov models,finding terminology translations from nonparallel corporaquot,"['P Fung', 'K Mckeown']",,"We present a statistical word feature, the Word Relation Matrix, which can be used to find translated pairs of words and terms from non-parallel corpora, across language groups. Online dictionary entries are used as seed words to generate Word Relation Matrices for the unknown words according to correlation measures. Word Relation Matrices are then mapped across the corpora to find translation pairs. Translation accuracies are around 30% when only the top candidate is counted. Nevertheless, top 20 candidate output give a 50.9% average increase in accuracy on human translator performance.",Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .,"['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.']",0,['The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) .'],"['The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) .', 'In this case, a full parse tree is represented in a flat form, producing a representation as in the example above.']","['The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) .', 'In this case, a full parse tree is represented in a flat form, producing a representation as in the example above.']",['The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) .'],"['The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) .', 'In this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'The goal in this case is therefore to accurately predict a collection of C/ PS C/ different types of phrases.']","['The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) .', 'In this case, a full parse tree is represented in a flat form, producing a representation as in the example above.']","['The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) .', 'In this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'The goal in this case is therefore to accurately predict a collection of C/ PS C/ different types of phrases.']",['The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) .'],"['The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) .', 'In this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'The goal in this case is therefore to accurately predict a collection of C/ PS C/ different types of phrases.', 'The chunk types are based on the syntactic category part of the bracket label in the Treebank.']"
CC1253,W00-1312,Cross-lingual information retrieval using hidden Markov models,a hidden markov model information retrieval systemquot,"['D Miller', 'T Leek', 'R Schwartz']",,"We present a new method for information retrieval using hidden Markov models (HMMs). We develop a general framework for incorporating multiple word generation mechanisms within the same model. We then demonstrate that an extremely simple realization of this model substantially outperforms standard tf :idf ranking on both the TREC-6 and TREC-7 ad hoc retrieval tasks. We go on to present a novel method for performing blind feedback in the HMM framework, a more complex HMM that models bigram production, and several other algorithmic re nements. Together, these methods form a state-of-the-art retrieval system that ranked among the best on the TREC-7 ad hoc retrieval task.","Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .","['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', 'Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ).', 'The approach therefore estimates the probability that a query Q is generated, given the document D is relevant.', '(A glossary of symbols used appears below.)']",5,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .']","['Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.']","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .']","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .']","['Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""']","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.']","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""']","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .']","['Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.']"
CC1254,W01-0706,Exploring evidence for shallow parsing,three generative lexicalised models for statistical parsing,['M Collins'],introduction,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #AUTHOR_TAG ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #AUTHOR_TAG ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1255,W01-0706,Exploring evidence for shallow parsing,introduction to the conll2000 shared task chunking,"['E F Tjong Kim Sang', 'S Buchholz']",experiments,,The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) .,"['The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) .', 'In this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'The goal in this case is therefore to accurately predict a collection of Â¢ Â£ Â¢ different types of phrases.', 'The chunk types are based on the syntactic category part of the bracket label in the Treebank.', 'Roughly, a chunk contains everything to the left of and including the syntactic head of the constituent of the same name.', 'The phrases are: adjective phrase (ADJP), adverb phrase (ADVP), conjunction phrase (CONJP), interjection phrase (INTJ), list marker (LST), noun phrase (NP), preposition phrase (PP), particle (PRT), subordinated clause (SBAR), unlike coordinated phrase (UCP), verb phrase (VP).', '(See details in (Tjong Kim Sang and Buchholz, 2000).)']",5,"['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 ) .']","['However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 ) .']","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 ) .']","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 ) .']","['However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 ) .']","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 ) .']","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 ) .']","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 ) .']","['However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 ) .']"
CC1256,W01-0706,Exploring evidence for shallow parsing,cooccurrence and transformation in linguistic structure,['Z S Harris'],introduction,,"Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997;Charniak, 1997a;Charniak, 1997b;Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns -syntactic phrases or words that participate in a syntactic relationship (Church, 1988;Ramshaw and Marcus, 1995;Argamon et al., 1998;Cardie and Pierce, 1998;Munoz et al., 1999;Punyakanok and Roth, 2001;Buchholz et al., 1999;Tjong Kim Sang and Buchholz, 2000).']",0,"['We perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #AUTHOR_TAG ) -- one of the most accurate full parsers around .']","['For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #AUTHOR_TAG ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.']","['We perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #AUTHOR_TAG ) -- one of the most accurate full parsers around .']","['We perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #AUTHOR_TAG ) -- one of the most accurate full parsers around .']","['For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #AUTHOR_TAG ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.']","['We perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #AUTHOR_TAG ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.']","['We perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #AUTHOR_TAG ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.']","['We perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #AUTHOR_TAG ) -- one of the most accurate full parsers around .']","['For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #AUTHOR_TAG ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'After that, it will choose the candidate parse tree with the highest probability as output.']"
CC1257,W01-0706,Exploring evidence for shallow parsing,statistical parsing with a contextfree grammar and word statistics,['E Charniak'],introduction,,"Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #AUTHOR_TAGb ; Charniak , 1997a ; Collins , 1997 ; Ratnaparkhi , 1997 ) .']","['However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #AUTHOR_TAGb ; Charniak , 1997a ; Collins , 1997 ; Ratnaparkhi , 1997 ) .']","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #AUTHOR_TAGb ; Charniak , 1997a ; Collins , 1997 ; Ratnaparkhi , 1997 ) .']","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #AUTHOR_TAGb ; Charniak , 1997a ; Collins , 1997 ; Ratnaparkhi , 1997 ) .']","['However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #AUTHOR_TAGb ; Charniak , 1997a ; Collins , 1997 ; Ratnaparkhi , 1997 ) .']","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #AUTHOR_TAGb ; Charniak , 1997a ; Collins , 1997 ; Ratnaparkhi , 1997 ) .']","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #AUTHOR_TAGb ; Charniak , 1997a ; Collins , 1997 ; Ratnaparkhi , 1997 ) .']","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #AUTHOR_TAGb ; Charniak , 1997a ; Collins , 1997 ; Ratnaparkhi , 1997 ) .']","['However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #AUTHOR_TAGb ; Charniak , 1997a ; Collins , 1997 ; Ratnaparkhi , 1997 ) .']"
CC1258,W01-0706,Exploring evidence for shallow parsing,three generative lexicalised models for statistical parsing,['M Collins'],introduction,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97","However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 ) .","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 ) .']",0,"['The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'SNoW ( #AUTHOR_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .']","['SNoW ( #AUTHOR_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.']","['The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'SNoW ( #AUTHOR_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .']","['The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'SNoW ( #AUTHOR_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .']","['SNoW ( #AUTHOR_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.']","['The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'SNoW ( #AUTHOR_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.']","['The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'SNoW ( #AUTHOR_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.']","['The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'SNoW ( #AUTHOR_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .']","['SNoW ( #AUTHOR_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', 'However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.']"
CC1259,W01-0706,Exploring evidence for shallow parsing,three generative lexicalised models for statistical parsing,['M Collins'],experiments,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97","For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #AUTHOR_TAG ) -- one of the most accurate full parsers around .","['We perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #AUTHOR_TAG ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'After that, it will choose the candidate parse tree with the highest probability as output.', 'The experiments use the version that was trained (by Collins) on sections 02-21 of the Penn Treebank.', 'The reported results for the full parse tree (on section 23) are recall/precision of 88.1/87.5 (Collins, 1997).']",5,"['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1260,W01-0706,Exploring evidence for shallow parsing,statistical parsing with a contextfree grammar and word statistics,['E Charniak'],introduction,,"However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #AUTHOR_TAGb ; Charniak , 1997a ; Collins , 1997 ; Ratnaparkhi , 1997 ) .","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #AUTHOR_TAGb ; Charniak , 1997a ; Collins , 1997 ; Ratnaparkhi , 1997 ) .']",0,"['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1261,W01-0706,Exploring evidence for shallow parsing,the snow learning architecture,"['A Carleson', 'C Cumby', 'J Rosen', 'D Roth']",experiments,,"SNoW ( #AUTHOR_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .","['The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'SNoW ( #AUTHOR_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', 'However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.).', ""The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.""]",5,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ) .']","['Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.']","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ) .']","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ) .']","['Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""']","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.']","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""']","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ) .']","['Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.']"
CC1262,W01-0706,Exploring evidence for shallow parsing,a stochastic parts program and noun phrase parser for unrestricted text,['Kenneth W Church'],introduction,A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.>,"Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ) .']","['Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.']","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ) .']","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ) .']","['Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""']","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.']","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""']","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ) .']","['Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.']"
CC1263,W01-0706,Exploring evidence for shallow parsing,the use of classifiers in sequential inference,"['V Punyakanok', 'D Roth']",introduction,"We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ) .']","['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ) .', 'Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.']","['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ) .', 'Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.']","['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ) .']","['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ) .', 'Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis.']","['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ) .', 'Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.']","['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ) .', 'Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis.']","['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ) .']","['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ) .', 'Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis.', 'This can be augmented later if more information is available.']"
CC1264,W01-0706,Exploring evidence for shallow parsing,parsing by chunks,['S P Abney'],introduction,"I begin with an intuition: when I read a sentence, I read it a chunk at a time. For example, the previous sentence breaks up something like this:    (1)    [I begin] [with an intuition]: [when I read] [a sentence], [I read it] [a chunk] [at a time]              These chunks correspond in some way to prosodic patterns. It appears, for instance, that the strongest stresses in the sentence fall one to a chunk, and pauses are most likely to fall between chunks. Chunks also represent a grammatical watershed of sorts. The typical chunk consists of a single content word surrounded by a constellation of function words, matching a fixed template. A simple context-free grammar is quite adequate to describe the structure of chunks. By contrast, the relationships between chunks are mediated more by lexical selection than by rigid templates. Co-occurrence of chunks is determined not just by their syntactic categories, but is sensitive to the precise words that head them; and the order in which chunks occur is much more flexible than the order of words within chunks.","Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997;Charniak, 1997a;Charniak, 1997b;Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns -syntactic phrases or words that participate in a syntactic relationship (Church, 1988;Ramshaw and Marcus, 1995;Argamon et al., 1998;Cardie and Pierce, 1998;Munoz et al., 1999;Punyakanok and Roth, 2001;Buchholz et al., 1999;Tjong Kim Sang and Buchholz, 2000).']",0,"['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) to compare it to other shallow parsers .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) to compare it to other shallow parsers .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) to compare it to other shallow parsers .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']"
CC1265,W01-0706,Exploring evidence for shallow parsing,evaluation techniques for automatic semantic extraction comparing semantic and window based approaches,['G Greffenstette'],introduction,"As large on-line corpora become more prevalent, a number of attempts have been made to automatically extract thesaurus-like relations directly from text using knowledge poor methods. In the absence of any specific application, comparing the results of these attempts is difficult. Here we propose an evaluation method using gold standards, i.e., pre-existing hand-compiled resources, as a means of comparing extraction techniques. Using this evaluation method, we compare two semantic extraction techniques which produce similar word lists, one using syntactic context of words , and the other using windows of heuristically tagged words. The two techniques are very similar except that in one case selective natural language processing, a partial syntactic analysis, is performed. On a 4 megabyte corpus, syntactic contexts produce significantly better results against the gold standards for the most characteristk: words in the corpus, while windows produce better results for rare words.","Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997;Charniak, 1997a;Charniak, 1997b;Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns -syntactic phrases or words that participate in a syntactic relationship (Church, 1988;Ramshaw and Marcus, 1995;Argamon et al., 1998;Cardie and Pierce, 1998;Munoz et al., 1999;Punyakanok and Roth, 2001;Buchholz et al., 1999;Tjong Kim Sang and Buchholz, 2000).']",0,"['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1266,W01-0706,Exploring evidence for shallow parsing,fastus a finitestate processor for information extraction from realworld text,"['D Appelt', 'J Hobbs', 'J Bear', 'D Israel', 'M Tyson']",introduction,,"First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ) .","['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ) .', 'Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis.', 'This can be augmented later if more information is available.', 'Finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low -sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes.']",0,"['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; #AUTHOR_TAG ) .']","['However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; #AUTHOR_TAG ) .']","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; #AUTHOR_TAG ) .']","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; #AUTHOR_TAG ) .']","['However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; #AUTHOR_TAG ) .']","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; #AUTHOR_TAG ) .']","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; #AUTHOR_TAG ) .']","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; #AUTHOR_TAG ) .']","['However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; #AUTHOR_TAG ) .']"
CC1267,W01-0706,Exploring evidence for shallow parsing,introduction to the conll2000 shared task chunking,"['E F Tjong Kim Sang', 'S Buchholz']",experiments,,"Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) to compare it to other shallow parsers .","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']",5,"['The shallow parser used is the SNoW-based CSCL parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ) .']","['The shallow parser used is the SNoW-based CSCL parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ) .', 'SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.']","['The shallow parser used is the SNoW-based CSCL parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ) .', 'SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.']","['The shallow parser used is the SNoW-based CSCL parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ) .']","['The shallow parser used is the SNoW-based CSCL parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ) .', 'SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.']","['The shallow parser used is the SNoW-based CSCL parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ) .', 'SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.']","['The shallow parser used is the SNoW-based CSCL parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ) .', 'SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.']","['The shallow parser used is the SNoW-based CSCL parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ) .']","['The shallow parser used is the SNoW-based CSCL parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ) .', 'SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.']"
CC1268,W01-0706,Exploring evidence for shallow parsing,a learning approach to shallow parsing,"['M Munoz', 'V Punyakanok', 'D Roth', 'D Zimak']",introduction,"A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally. The approach learns to identify syntactic patterns by combining simple predictors to produce a coherent inference. Two instantiations of this approach are studied and experimental results for Noun-Phrases (NP) and Subject-Verb (SV) phrases that compare favorably with the best published results are presented. In doing that, we compare two ways of modeling the problem of learning to recognize patterns and suggest that shallow parsing patterns are better learned using open/close predictors than using inside/outside predictors.} thus contribute to the understanding of how to model shallow parsing tasks as learning problems.","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['We start by reporting the results in which we compare the full parser and the shallow parser on the ""clean"" WSJ data.', 'Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #AUTHOR_TAG ) terminology .']","['Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #AUTHOR_TAG ) terminology .', 'The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.']","['We start by reporting the results in which we compare the full parser and the shallow parser on the ""clean"" WSJ data.', 'Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #AUTHOR_TAG ) terminology .']","['We start by reporting the results in which we compare the full parser and the shallow parser on the ""clean"" WSJ data.', 'Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #AUTHOR_TAG ) terminology .']","['Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #AUTHOR_TAG ) terminology .', 'The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.', 'Next, we compared the performance of the parsers on the task of identifying atomic phrases 2 .']","['We start by reporting the results in which we compare the full parser and the shallow parser on the ""clean"" WSJ data.', 'Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #AUTHOR_TAG ) terminology .', 'The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.']","['We start by reporting the results in which we compare the full parser and the shallow parser on the ""clean"" WSJ data.', 'Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #AUTHOR_TAG ) terminology .', 'The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.', 'Next, we compared the performance of the parsers on the task of identifying atomic phrases 2 .']","['We start by reporting the results in which we compare the full parser and the shallow parser on the ""clean"" WSJ data.', 'Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #AUTHOR_TAG ) terminology .']","['Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #AUTHOR_TAG ) terminology .', 'The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.', 'Next, we compared the performance of the parsers on the task of identifying atomic phrases 2 .', 'Here, again, the shallow parser exhibits significantly better performance.']"
CC1269,W01-0706,Exploring evidence for shallow parsing,a linear observed time statistical parser based on maximum entropy models,['A Ratnaparkhi'],introduction,"This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.","However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; #AUTHOR_TAG ) .","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; #AUTHOR_TAG ) .']",0,"['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1270,W01-0706,Exploring evidence for shallow parsing,the use of classifiers in sequential inference,"['V Punyakanok', 'D Roth']",experiments,"We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.","The shallow parser used is the SNoW-based CSCL parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ) .","['The shallow parser used is the SNoW-based CSCL parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ) .', 'SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', 'However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.).', ""The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.""]",5,"['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']"
CC1271,W01-0706,Exploring evidence for shallow parsing,introduction to the conll2000 shared task chunking,"['E F Tjong Kim Sang', 'S Buchholz']",experiments,,Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #AUTHOR_TAG ) terminology .,"['We start by reporting the results in which we compare the full parser and the shallow parser on the ""clean"" WSJ data.', 'Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #AUTHOR_TAG ) terminology .', 'The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.', 'Next, we compared the performance of the parsers on the task of identifying atomic phrases 2 .', 'Here, again, the shallow parser exhibits significantly better performance.', 'Table 3 shows the results of extracting atomic phrases.']",5,"['Research on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint .']","['Research on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint .']","['Research on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint .']","['Research on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint .']","['Research on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint .']","['Research on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint .']","['Research on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint .']","['Research on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint .']","['Research on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint .']"
CC1272,W01-0706,Exploring evidence for shallow parsing,text chunking using transformationbased learning,"['L A Ramshaw', 'M P Marcus']",introduction,"Transformation-based learning, a technique introduced by Eric Brill (1993b), has been shown to do part-of-speech tagging with fairly high accuracy. This same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive ""baseNP"" chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 93% for baseNP chunks (trained on 950K words) and 88% for somewhat more complex chunks that partition the sentence (trained on 200K words). Working in this new application and with larger template and training sets has also required some interesting adaptations to the transformation-based learning approach.","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['We perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins ( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .']","['For the full parser , we use the one developed by Michael Collins ( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.']","['We perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins ( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .']","['We perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins ( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .']","['For the full parser , we use the one developed by Michael Collins ( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.']","['We perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins ( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.']","['We perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins ( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.']","['We perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins ( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .']","['For the full parser , we use the one developed by Michael Collins ( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'After that, it will choose the candidate parse tree with the highest probability as output.']"
CC1273,W01-0706,Exploring evidence for shallow parsing,a learning approach to shallow parsing,"['M Munoz', 'V Punyakanok', 'D Roth', 'D Zimak']",experiments,"A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally. The approach learns to identify syntactic patterns by combining simple predictors to produce a coherent inference. Two instantiations of this approach are studied and experimental results for Noun-Phrases (NP) and Subject-Verb (SV) phrases that compare favorably with the best published results are presented. In doing that, we compare two ways of modeling the problem of learning to recognize patterns and suggest that shallow parsing patterns are better learned using open/close predictors than using inside/outside predictors.} thus contribute to the understanding of how to model shallow parsing tasks as learning problems.","Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']",2,"['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1274,W01-0706,Exploring evidence for shallow parsing,performance structuresa psycholinguistic and linguistic appraisal cognitive psychology,"['J P Gee', 'F Grosjean']",introduction,,"Research on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint .","['Research on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint .']",0,"['The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) .']","['The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) .', 'SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.']","['The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) .', 'SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.']","['The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) .']","['The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) .', 'SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.']","['The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) .', 'SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.']","['The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) .', 'SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.']","['The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) .']","['The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) .', 'SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.']"
CC1275,W01-0706,Exploring evidence for shallow parsing,a new statistical parser based on bigram lexical dependencies,['M Collins'],experiments,"This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.  1 Introduction  Lexical information has been shown to be crucial for many parsing decisions, such as prepositional-phrase attachment (for example (Hindle and Rooth 93)). However, early approaches to probabilistic parsing (Pereira and Schabes 92; Magerman and Marcus 91; Briscoe and Carroll 93) conditioned probabil..","For the full parser , we use the one developed by Michael Collins ( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .","['We perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins ( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'After that, it will choose the candidate parse tree with the highest probability as output.', 'The experiments use the version that was trained (by Collins) on sections 02-21 of the Penn Treebank.', 'The reported results for the full parse tree (on section 23) are recall/precision of 88.1/87.5 (Collins, 1997).']",5,"['The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'SNoW ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .']","['SNoW ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.']","['The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'SNoW ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .']","['The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'SNoW ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .']","['SNoW ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.']","['The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'SNoW ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.']","['The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'SNoW ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.']","['The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'SNoW ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .']","['SNoW ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', 'However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.']"
CC1276,W01-0706,Exploring evidence for shallow parsing,errordriven pruning of treebanks grammars for base noun phrase identification,"['C Cardie', 'D Pierce']",introduction,"Finding simple, non-recursive, base noun phrases is an important subtask for many natural language processing applications. While previous empirical methods for base NP identification have been rather complex, this paper instead proposes a very simple algorithm that is tailored to the relative simplicity of the task. In particular, we present a corpus-based approach for finding base NPs by matching part-of-speech tag sequences. The training phase of the algorithm is based on two successful techniques: first the base NP grammar is read from a ""treebank"" corpus; then the grammar is improved by selecting rules with high ""benefit"" scores. Using this simple algorithm with a naive heuristic for matching rules, we achieve surprising accuracy in an evaluation on the Penn Treebank Wall Street Journal.","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['For example, the sentence He reckons the current account deficit will narrow to only $ 1.8 billion in September .', 'would be chunked as follows ( Tjong Kim #AUTHOR_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .']",['would be chunked as follows ( Tjong Kim #AUTHOR_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .'],"['For example, the sentence He reckons the current account deficit will narrow to only $ 1.8 billion in September .', 'would be chunked as follows ( Tjong Kim #AUTHOR_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .']","['A lot of recent work on shallow parsing has been influenced by Abneys work (Abney, 1991), who has suggested to chunk sentences to base level phrases.', 'For example, the sentence He reckons the current account deficit will narrow to only $ 1.8 billion in September .', 'would be chunked as follows ( Tjong Kim #AUTHOR_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .']",['would be chunked as follows ( Tjong Kim #AUTHOR_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .'],"['A lot of recent work on shallow parsing has been influenced by Abneys work (Abney, 1991), who has suggested to chunk sentences to base level phrases.', 'For example, the sentence He reckons the current account deficit will narrow to only $ 1.8 billion in September .', 'would be chunked as follows ( Tjong Kim #AUTHOR_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .']","['For example, the sentence He reckons the current account deficit will narrow to only $ 1.8 billion in September .', 'would be chunked as follows ( Tjong Kim #AUTHOR_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .']","['Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957; Abney, 1991; Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abneys work (Abney, 1991), who has suggested to chunk sentences to base level phrases.', 'For example, the sentence He reckons the current account deficit will narrow to only $ 1.8 billion in September .', 'would be chunked as follows ( Tjong Kim #AUTHOR_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .']",['would be chunked as follows ( Tjong Kim #AUTHOR_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .']
CC1277,W01-0706,Exploring evidence for shallow parsing,a learning approach to shallow parsing,"['M Munoz', 'V Punyakanok', 'D Roth', 'D Zimak']",experiments,"A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally. The approach learns to identify syntactic patterns by combining simple predictors to produce a coherent inference. Two instantiations of this approach are studied and experimental results for Noun-Phrases (NP) and Subject-Verb (SV) phrases that compare favorably with the best published results are presented. In doing that, we compare two ways of modeling the problem of learning to recognize patterns and suggest that shallow parsing patterns are better learned using open/close predictors than using inside/outside predictors.} thus contribute to the understanding of how to model shallow parsing tasks as learning problems.","The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) .","['The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) .', 'SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', 'However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.).', ""The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.""]",5,"['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ) .']","['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ) .', 'Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.']","['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ) .', 'Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.']","['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ) .']","['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ) .', 'Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis.']","['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ) .', 'Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.']","['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ) .', 'Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis.']","['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ) .']","['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ) .', 'Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis.', 'This can be augmented later if more information is available.']"
CC1278,W01-0706,Exploring evidence for shallow parsing,learning to resolve natural language ambiguities a unified approach,['D Roth'],experiments,"We analyze a few of the commonly used statistics based and machine learning algorithms for natural language disambiguation tasks and observe that they can be recast as learning linear separators in the feature space. Each of the methods makes a priori assumptions which it employs, given the data, when searching for its hypothesis. Nevertheless, as we show, it searches a space that is as rich as the space of all linear separators. We use this to build an argument for a data driven approach which merely searches for a good linear separator in the feature space, without further assumptions on the domain or a specific problem.We present such an approach - a sparse network of linear separators, utilizing the Winnow learning algorithm - and show how to use it in a variety of ambiguity resolution problems. The learning approach presented is attribute-efficient and, therefore, appropriate for domains having very large number of attributes.In particular, we present an extensive experimental comparison of our approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging. In all cases we show that our approach either outperforms other methods tried for these tasks or performs comparably to the best.","SNoW ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .","['The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'SNoW ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', 'However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.).', ""The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.""]",5,"['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1279,W01-0706,Exploring evidence for shallow parsing,introduction to the conll2000 shared task chunking,"['E F Tjong Kim Sang', 'S Buchholz']",introduction,,would be chunked as follows ( Tjong Kim #AUTHOR_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957; Abney, 1991; Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abneyï¿½s work (Abney, 1991), who has suggested to ï¿½chunkï¿½ sentences to base level phrases.', 'For example, the sentence ï¿½He reckons the current account deficit will narrow to only $ 1.8 billion in September .ï¿½', 'would be chunked as follows ( Tjong Kim #AUTHOR_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .']",0,"['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']","['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']","['For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']","['For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']","['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']","['A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1280,W01-0706,Exploring evidence for shallow parsing,the nyu system for muc6 or whereâs syntax in,['R Grishman'],introduction,,"First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ) .","['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ) .', 'Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis.', 'This can be augmented later if more information is available.', 'Finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low -sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes.']",0,"['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']","['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1281,W01-0706,Exploring evidence for shallow parsing,a memorybased approach to learning shallow natural language patterns,"['S Argamon', 'I Dagan', 'Y Krymolowski']",introduction,"Recognizing shallow linguistic patterns, such as basic syntactic relationships between words, is a common task in applied natural language and text processing. The common practice for approaching this task is by tedious manual definition of possible pattern structures, often in the form of regular expressions or finite automata. This paper presents a novel memory-based learning method that recognizes shallow patterns in new text based on a bracketed training corpus. The training data are stored as-is, in efficient suffix-tree data structures. Generalization is performed on-line at recognition time by comparing subsequences of the new text to positive and negative evidence in the corpus. This way, no information in the training is lost, as can happen in other learning systems that construct a single generalized model at the time of training. The paper presents experimental results for recognizing noun phrase, subject-verb and verb-object patterns in English. Since the learning approach enables easy porting to new domains, we plan to apply it to syntactic patterns in other languages and to sub-language patterns for information extraction.","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .']","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']"
CC1282,W01-0706,Exploring evidence for shallow parsing,cascaded grammatical relation assignment,"['S Buchholz', 'J Veenstra', 'W Daelemans']",introduction,"In this paper we discuss cascaded Memory-Based grammatical relations assignment. In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal). In the last stage, we assign grammatical relations to pairs of chunks. We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder.","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .']","['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .', 'To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations.']","['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .', 'To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations.']","['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .']","['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .', 'To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations.', 'This is done using the ""Chunklink"" program provided for CoNLL-2000 (Tjong Kim Sang andBuchholz, 2000).']","['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .', 'To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations.']","['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .', 'To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations.', 'This is done using the ""Chunklink"" program provided for CoNLL-2000 (Tjong Kim Sang andBuchholz, 2000).']","['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .']","['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .', 'To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations.', 'This is done using the ""Chunklink"" program provided for CoNLL-2000 (Tjong Kim Sang andBuchholz, 2000).']"
CC1283,W01-0706,Exploring evidence for shallow parsing,a linear observed time statistical parser based on maximum entropy models,['A Ratnaparkhi'],introduction,"This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,['Tateisi et al. also translated LTAG into HPSG ( #AUTHOR_TAG ) .'],"['Tateisi et al. also translated LTAG into HPSG ( #AUTHOR_TAG ) .', 'However, their method depended on translators intuitive analy- sis of the original grammar.']","['Tateisi et al. also translated LTAG into HPSG ( #AUTHOR_TAG ) .', 'However, their method depended on translators intuitive analy- sis of the original grammar.']",['Tateisi et al. also translated LTAG into HPSG ( #AUTHOR_TAG ) .'],"['Tateisi et al. also translated LTAG into HPSG ( #AUTHOR_TAG ) .', 'However, their method depended on translators intuitive analy- sis of the original grammar.', 'Thus the transla- tion was manual and grammar dependent.']","['Tateisi et al. also translated LTAG into HPSG ( #AUTHOR_TAG ) .', 'However, their method depended on translators intuitive analy- sis of the original grammar.']","['Tateisi et al. also translated LTAG into HPSG ( #AUTHOR_TAG ) .', 'However, their method depended on translators intuitive analy- sis of the original grammar.', 'Thus the transla- tion was manual and grammar dependent.']",['Tateisi et al. also translated LTAG into HPSG ( #AUTHOR_TAG ) .'],"['Tateisi et al. also translated LTAG into HPSG ( #AUTHOR_TAG ) .', 'However, their method depended on translators intuitive analy- sis of the original grammar.', 'Thus the transla- tion was manual and grammar dependent.', 'The manual translation demanded considerable efforts from the translator, and obscures the equiva- lence between the original and obtained gram- mars.']"
CC1284,W01-0706,Exploring evidence for shallow parsing,the use of classifiers in sequential inference,"['V Punyakanok', 'D Roth']",experiments,"We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.","Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']",2,"['There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ) , and ones on programming/grammar-development environ -  (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).']","['There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ) , and ones on programming/grammar-development environ -  (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.']","['There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ) , and ones on programming/grammar-development environ -  (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.']","['There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ) , and ones on programming/grammar-development environ -  (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).']","['There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ) , and ones on programming/grammar-development environ -  (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']","['There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ) , and ones on programming/grammar-development environ -  (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.']","['There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ) , and ones on programming/grammar-development environ -  (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']","['There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ) , and ones on programming/grammar-development environ -  (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).']","['There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ) , and ones on programming/grammar-development environ -  (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']"
CC1285,W01-0706,Exploring evidence for shallow parsing,building a large annotated corpus of english the penn treebank,"['M P Marcus', 'B Santorini', 'M Marcinkiewicz']",experiments,"Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant.","Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .","['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .', 'To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations.', 'This is done using the ""Chunklink"" program provided for CoNLL-2000 (Tjong Kim Sang andBuchholz, 2000).']",5,"['We apply our system to the latest version of the XTAG English grammar (The XTAG Research Group, 2001), which is a large-scale FB-LTAG grammar.', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #AUTHOR_TAG ) .']","['A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #AUTHOR_TAG ) .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.']","['We apply our system to the latest version of the XTAG English grammar (The XTAG Research Group, 2001), which is a large-scale FB-LTAG grammar.', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #AUTHOR_TAG ) .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.']","['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We apply our system to the latest version of the XTAG English grammar (The XTAG Research Group, 2001), which is a large-scale FB-LTAG grammar.', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #AUTHOR_TAG ) .']","['A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #AUTHOR_TAG ) .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']","['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We apply our system to the latest version of the XTAG English grammar (The XTAG Research Group, 2001), which is a large-scale FB-LTAG grammar.', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #AUTHOR_TAG ) .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.']","['We apply our system to the latest version of the XTAG English grammar (The XTAG Research Group, 2001), which is a large-scale FB-LTAG grammar.', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #AUTHOR_TAG ) .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']","['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We apply our system to the latest version of the XTAG English grammar (The XTAG Research Group, 2001), which is a large-scale FB-LTAG grammar.', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #AUTHOR_TAG ) .']","['A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #AUTHOR_TAG ) .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']"
CC1286,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,translating the xtag english grammar to hpsg,"['Yuka Tateisi', 'Kentaro Torisawa', 'Yusuke Miyao', 'Junâichi Tsujii']",introduction,,Tateisi et al. also translated LTAG into HPSG ( #AUTHOR_TAG ) .,"['Tateisi et al. also translated LTAG into HPSG ( #AUTHOR_TAG ) .', 'However, their method depended on translatorï¿½s intuitive analy- sis of the original grammar.', 'Thus the transla- tion was manual and grammar dependent.', 'The manual translation demanded considerable efforts from the translator, and obscures the equiva- lence between the original and obtained gram- mars.', 'Other works (Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars.', 'However, given the greater ex- pressive power of HPSG, it is impossible to con- vert an arbitrary HPSG grammar into an LTAG grammar.', 'Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capac- ity.', 'Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above ad- vantages.']",1,"['In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2).', 'TNT refers to the HPSG parser ( #AUTHOR_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .']","['TNT refers to the HPSG parser ( #AUTHOR_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .']","['In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2).', 'TNT refers to the HPSG parser ( #AUTHOR_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .']","['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2).', 'TNT refers to the HPSG parser ( #AUTHOR_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .']","['TNT refers to the HPSG parser ( #AUTHOR_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .']","['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2).', 'TNT refers to the HPSG parser ( #AUTHOR_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .']","['In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2).', 'TNT refers to the HPSG parser ( #AUTHOR_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .']","['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2).', 'TNT refers to the HPSG parser ( #AUTHOR_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .']","['TNT refers to the HPSG parser ( #AUTHOR_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .', 'Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']"
CC1287,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,hybrid japanese parser with handcrafted grammar and statistics,"['Hiroshi Kanayama', 'Kentaro Torisawa', 'Yutaka Mitsuisi', 'Junâichi Tsujii']",introduction,,"There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ) , and ones on programming/grammar-development environ -  (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).","['There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ) , and ones on programming/grammar-development environ -  (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']",0,"['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #AUTHOR_TAG ) by a method of grammar conversion .']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #AUTHOR_TAG ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #AUTHOR_TAG ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #AUTHOR_TAG ) by a method of grammar conversion .']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #AUTHOR_TAG ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #AUTHOR_TAG ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #AUTHOR_TAG ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #AUTHOR_TAG ) by a method of grammar conversion .']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #AUTHOR_TAG ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']"
CC1288,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,efficient ltag parsing using hpsg parsers,"['Naoki Yoshinaga', 'Yusuke Miyao', 'Kentaro Torisawa', 'Junâichi Tsujii']",introduction,,A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #AUTHOR_TAG ) .,"['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We apply our system to the latest version of the XTAG English grammar (The XTAG Research Group, 2001), which is a large-scale FB-LTAG grammar.', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #AUTHOR_TAG ) .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']",1,"['Adjunction grafts an auxiliary tree with the root node and foot node labeled U onto an internal node of another tree with the same symbol U (Figure 4).', 'FBLTAG ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the LTAG formalism .']","['FBLTAG ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the LTAG formalism .']","['Adjunction grafts an auxiliary tree with the root node and foot node labeled U onto an internal node of another tree with the same symbol U (Figure 4).', 'FBLTAG ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the LTAG formalism .']","['Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled U onto an internal node of another tree with the same symbol U (Figure 4).', 'FBLTAG ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the LTAG formalism .']","['FBLTAG ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the LTAG formalism .']","['Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled U onto an internal node of another tree with the same symbol U (Figure 4).', 'FBLTAG ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the LTAG formalism .']","['Adjunction grafts an auxiliary tree with the root node and foot node labeled U onto an internal node of another tree with the same symbol U (Figure 4).', 'FBLTAG ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the LTAG formalism .']","['In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled U onto an internal node of another tree with the same symbol U (Figure 4).', 'FBLTAG ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the LTAG formalism .']","['FBLTAG ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the LTAG formalism .', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.']"
CC1289,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,an hpsg parser with cfg filtering,"['Kentaro Torisawa', 'Kenji Nishida', 'Yusuke Miyao', 'Junâichi Tsujii']",experiments,,"TNT refers to the HPSG parser ( #AUTHOR_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .","['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2).', 'TNT refers to the HPSG parser ( #AUTHOR_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .', 'Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.', 'Another paper  describes the detailed analysis on the factor of the difference of parsing performance.']",1,"['ment ( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) .']","['ment ( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) .', 'These works are re- stricted to each closed community, and the rela- tion between them is not well discussed.']","['ment ( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) .', 'These works are re- stricted to each closed community, and the rela- tion between them is not well discussed.']","['ment ( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) .']","['ment ( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) .', 'These works are re- stricted to each closed community, and the rela- tion between them is not well discussed.', 'Investi- gating the relation will be apparently valuable for both communities.']","['ment ( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) .', 'These works are re- stricted to each closed community, and the rela- tion between them is not well discussed.']","['ment ( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) .', 'These works are re- stricted to each closed community, and the rela- tion between them is not well discussed.', 'Investi- gating the relation will be apparently valuable for both communities.']","['ment ( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) .']","['ment ( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) .', 'These works are re- stricted to each closed community, and the rela- tion between them is not well discussed.', 'Investi- gating the relation will be apparently valuable for both communities.']"
CC1290,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,headdriven phrase structure grammar,"['Carl Pollard', 'Ivan A Sag']",introduction,,"This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #AUTHOR_TAG ) by a method of grammar conversion .","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #AUTHOR_TAG ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']",0,"['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']"
CC1291,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,feature structures based tree adjoining grammars,"['K Vijay-Shanker', 'Aravind K Joshi']",introduction,"We have embedded Tree Adjoining Grammars (TAG) in a  feature structure based unification system. The resulting system, Feature Structure based Tree Adjoining Grammars (FTAG), captures the principle of factoring dependencies and recursion, fundamental to TAG u27s. We show that FTAG has an enhanced descriptive capacity compared to TAG formalism. We consider some restricted versions of this system and some possible linguistic stipulations that can be made. We briefly describe a calculus to represent the structures used by this system, extending on the work of Rounds, and Kasper [Rounds et al. 1986, Kasper et al. 1986)involving the logical formulation of feature structures","FBLTAG ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the LTAG formalism .","['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with Â¥).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with Â£).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled Ã onto an internal node of another tree with the same symbol Ã (Figure 4).', 'FBLTAG ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the LTAG formalism .', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (AbeillÃ© and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']",0,['LTAG ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG grammar'],['LTAG ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG grammar'],['LTAG ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG grammar'],['LTAG ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG grammar'],['LTAG ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG grammar'],['LTAG ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG grammar'],['LTAG ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG grammar'],['LTAG ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG grammar'],['LTAG ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG grammar']
CC1292,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,typing as a means for validating feature structures,"['Anoop Sarkar', 'Shuly Wintner']",introduction,"We present a method for validating the consistency of feature structure speci cations by imposing a type discipline. A typed system facilitates a great number of compile-time checks: many possible errors can be detected before the grammar is used for parsing. We have constructed a type signature for an existing broad-coverage grammar of English, and implemented a type inference algorithm that operates on the feature structure speci cations in the grammar. The algorithm reports occurrences of incompatibility with the type signature. We have detected a large number of errors in the grammar; four types of errors are described in the paper.","ment ( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) .","['ment ( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) .', 'These works are re- stricted to each closed community, and the rela- tion between them is not well discussed.', 'Investi- gating the relation will be apparently valuable for both communities.']",0,"['Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #AUTHOR_TAG ) .']","['In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #AUTHOR_TAG ) .', 'Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000).']","['Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #AUTHOR_TAG ) .', 'Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000).']","['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #AUTHOR_TAG ) .']","['In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #AUTHOR_TAG ) .', 'Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000).']","['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #AUTHOR_TAG ) .', 'Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000).']","['Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #AUTHOR_TAG ) .', 'Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000).']","['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #AUTHOR_TAG ) .']","['In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #AUTHOR_TAG ) .', 'Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000).']"
CC1293,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,feature structures based tree adjoining grammars,"['K Vijay-Shanker', 'Aravind K Joshi']",introduction,"We have embedded Tree Adjoining Grammars (TAG) in a  feature structure based unification system. The resulting system, Feature Structure based Tree Adjoining Grammars (FTAG), captures the principle of factoring dependencies and recursion, fundamental to TAG u27s. We show that FTAG has an enhanced descriptive capacity compared to TAG formalism. We consider some restricted versions of this system and some possible linguistic stipulations that can be made. We briefly describe a calculus to represent the structures used by this system, extending on the work of Rounds, and Kasper [Rounds et al. 1986, Kasper et al. 1986)involving the logical formulation of feature structures","This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']",0,"['In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese ( #AUTHOR_TAG ) , which is used in a high-accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) .']","['Our group has developed a wide-coverage HPSG grammar for Japanese ( #AUTHOR_TAG ) , which is used in a high-accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) .']","['In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese ( #AUTHOR_TAG ) , which is used in a high-accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) .']","['Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese ( #AUTHOR_TAG ) , which is used in a high-accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) .']","['Our group has developed a wide-coverage HPSG grammar for Japanese ( #AUTHOR_TAG ) , which is used in a high-accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) .']","['Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese ( #AUTHOR_TAG ) , which is used in a high-accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) .']","['In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese ( #AUTHOR_TAG ) , which is used in a high-accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) .']","['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese ( #AUTHOR_TAG ) , which is used in a high-accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) .']","['Our group has developed a wide-coverage HPSG grammar for Japanese ( #AUTHOR_TAG ) , which is used in a high-accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) .']"
CC1294,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,parsing strategies with âlexicalizedâ grammars application to tree adjoining grammars,"['Yves Schabes', 'Anne Abeille', 'Aravind K Joshi']",introduction,"In this paper we present a general parsing strategy that arose from the development of an Earley-type parsing algorithm for TAGs (Schabes and Joshi 1988) and from recent linguistic work in TAGs (Abeille 1988).In our approach elementary structures are associated with their lexical heads. These structures specify extended domains of locality (as compared to a context-free grammar) over which constraints can be stated. These constraints either hold within the elementary structure itself or specify what other structures can be composed with a given elementary structure.We state the conditions under which context-free based grammars can be 'lexicalized' without changing the linguistic structures originally produced. We argue that even if one extends the domain of locality of CFGs to trees, using only substitution does not give the freedom to choose the head of each structure. We show how adjunction allows us to 'lexicalize' a CFG freely.We then show how a 'lexicalized' grammar naturally follows from the extended domain of locality of TAGs and present some of the linguistic advantages of our approach.A novel general parsing strategy for 'lexicalized' grammars is discussed. In a first stage, the parser builds a set structures corresponding to the input sentence and in a second stage, the sentence is parsed with respect to this set. The strategy is independent of the linguistic theory adopted and of the underlying grammar formalism. However, we focus our attention on TAGs. Since the set of trees needed to parse an input sentence is supposed to be finite, the parser can use in principle any search strategy. Thus, in particular, a top-down strategy can be used since problems due to recursive structures are eliminated. The parser is also able to use non-local information to guide the search.We then explain how the Earley-type parser for TAGs can be modified to take advantage of this approach.",LTAG ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG grammar,['LTAG ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG grammar'],0,"['The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars.', 'Other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert HPSG grammars into LTAG grammars .']","['Other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert HPSG grammars into LTAG grammars .']","['The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars.', 'Other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert HPSG grammars into LTAG grammars .']","['Thus the translation was manual and grammar dependent.', 'The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars.', 'Other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert HPSG grammars into LTAG grammars .']","['Other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert HPSG grammars into LTAG grammars .']","['Thus the translation was manual and grammar dependent.', 'The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars.', 'Other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert HPSG grammars into LTAG grammars .']","['The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars.', 'Other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert HPSG grammars into LTAG grammars .']","[""However, their method depended on translator's intuitive analysis of the original grammar."", 'Thus the translation was manual and grammar dependent.', 'The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars.', 'Other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert HPSG grammars into LTAG grammars .']","['Other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert HPSG grammars into LTAG grammars .', 'However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar.', 'Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity.', 'Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages.']"
CC1295,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,verbmobil a translation system for facetoface dialog,"['M Kay', 'J Gawron', 'P Norvig']",introduction,,"In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #AUTHOR_TAG ) .","['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #AUTHOR_TAG ) .', 'Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000).']",0,"['In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer ( #AUTHOR_TAG ) .']","['Our group has developed a wide-coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer ( #AUTHOR_TAG ) .']","['In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer ( #AUTHOR_TAG ) .']","['Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer ( #AUTHOR_TAG ) .']","['Our group has developed a wide-coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer ( #AUTHOR_TAG ) .']","['Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer ( #AUTHOR_TAG ) .']","['In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer ( #AUTHOR_TAG ) .']","['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer ( #AUTHOR_TAG ) .']","['Our group has developed a wide-coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer ( #AUTHOR_TAG ) .']"
CC1296,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,hpsgstyle underspecified japanese grammar with wide coverage,"['Yutaka Mitsuishi', 'Kentaro Torisawa', 'Junâichi Tsujii']",introduction,"This paper describes a wide-coverage Japanese grammar based on HPSG. The aim of this work is to see the coverage and accuracy attainable using an underspecified grammar. Underspecification, allowed in a typed feature structure formalism, enables us to write down a wide-coverage grammar concisely. The grammar we have implemented consists of only 6 ID schemata, 68 lexical entries (assigned to functional words), and 63 lexical entry templates (assigned to parts of speech (POSs) ). Furthermore, word-specific constraints such as subcategorization of verbs are not fixed in the grammar. However, this grammar can generate parse trees for 87% of the 10000 sentences in the Japanese EDR corpus. The dependency accuracy is 78% when a parser uses the heuristic that every bunsetsu is attached to the nearest possible one.","Our group has developed a wide-coverage HPSG grammar for Japanese ( #AUTHOR_TAG ) , which is used in a high-accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) .","['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese ( #AUTHOR_TAG ) , which is used in a high-accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) .']",0,"['An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #AUTHOR_TAG ) .']","['An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #AUTHOR_TAG ) .', 'A lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category.']","['An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #AUTHOR_TAG ) .', 'A lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category.']","['An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #AUTHOR_TAG ) .']","['An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #AUTHOR_TAG ) .', 'A lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category.', 'An ID grammar rule represents a relation between a mother and its daughters, and is independent of lexical characteristics.']","['An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #AUTHOR_TAG ) .', 'A lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category.']","['An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #AUTHOR_TAG ) .', 'A lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category.', 'An ID grammar rule represents a relation between a mother and its daughters, and is independent of lexical characteristics.']","['An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #AUTHOR_TAG ) .']","['An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #AUTHOR_TAG ) .', 'A lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category.', 'An ID grammar rule represents a relation between a mother and its daughters, and is independent of lexical characteristics.', 'Figure 6 illustrates an example of bottom-up parsing with an HPSG grammar.']"
CC1297,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,adapting hpsgtotag compilation to widecoverage grammars,"['Tilman Becker', 'Patrice Lopez']",introduction,,"Other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert HPSG grammars into LTAG grammars .","['Figure 1 depicts a brief sketch of the RenTAL system.', 'The system consists of the following four modules: Tree converter, Type hierarchy extractor, Lexicon converter and Derivation translator.', 'The tree converter module is a core module of the system, which is an implementation of the grammar conversion algorithm given in Section 3. The type hierarchy extractor module extracts the symbols of the node, features, and feature values from the LTAG elementary tree templates and lexicon, and construct the type hierarchy from them.', 'The lexicon converter module converts LTAG elementary tree templates into HPSG lexical entries.', 'The derivation translator module takes HPSG parse  (Tateisi et al., 1998).', ""However, their method depended on translator's intuitive analysis of the original grammar."", 'Thus the translation was manual and grammar dependent.', 'The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars.', 'Other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert HPSG grammars into LTAG grammars .', 'However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar.', 'Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity.', 'Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages.']",1,"['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .']","['We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .']","['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .']","['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .']","['We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.']","['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .']","['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.']","['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .']","['We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']"
CC1298,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,hybrid japanese parser with handcrafted grammar and statistics,"['Hiroshi Kanayama', 'Kentaro Torisawa', 'Yutaka Mitsuisi', 'Junâichi Tsujii']",introduction,,"Our group has developed a wide-coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer ( #AUTHOR_TAG ) .","['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer ( #AUTHOR_TAG ) .']",0,"['Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group ( #AUTHOR_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .']","['The XTAG group ( #AUTHOR_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .', 'Development of a large-scale French grammar (Abeille and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']","['Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group ( #AUTHOR_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .', 'Development of a large-scale French grammar (Abeille and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']","['In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group ( #AUTHOR_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .']","['The XTAG group ( #AUTHOR_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .', 'Development of a large-scale French grammar (Abeille and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']","['In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group ( #AUTHOR_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .', 'Development of a large-scale French grammar (Abeille and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']","['Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group ( #AUTHOR_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .', 'Development of a large-scale French grammar (Abeille and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']","['FB-LTAG (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) is an extension of the LTAG formalism.', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group ( #AUTHOR_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .']","['The XTAG group ( #AUTHOR_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .', 'Development of a large-scale French grammar (Abeille and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']"
CC1299,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,the logic of typed feature structures,['Bob Carpenter'],introduction,"For those of us who belonged to the ""Bay Area (Computational) Linguistics Community,"" the early eighties were a heady time. Local researchers working on linguistics, computational linguistics, and logic programming were investigating notions of category, type, feature, term, and partial specification that appeared to converge to a powerful new approach for describing (linguistic) objects and their relationships by monotonic accumulation of constraints between their features. The seed notions had almost independently arisen in generalized phrase structure grammar (GPSG) (Gazdar et al. 1985), lexical-functional grammar (LFG) (Bresnan and Kaplan 1982), functionalunification grammar (FUG) (Kay 1985), logic programming (Colmerauer 1978, Pereira and Warren 1980), and terminological reasoning systems (Ait-Kaci 1984). It took, however, a lot of experimental and theoretical work to identify precisely what the core notions were, how particular systems related to the core notions, and what were the most illuminating mathematical accounts of that core. The development of the unificationbased formalism PATR-II (Shieber 1984) was an early step toward the definition of the core, but its mathematical analysis, and the clarification of the connections between the various systems, are only now coming to a reasonable closure. The Logic of Typed Feature Structures is the first monograph that brings all the main theoretical ideas into one place where they can be related and compared in a unified setting. Carpenter's book touches most of the crucial questions of the developments during the decade, provides proofs for central results, and reaches right up to the edge of current research in the field. These contributions alone make it an indispensable compendium for the researcher or graduate student working on constraint-based grammatical formalisms, and they also make it a very useful reference work for researchers in object-oriented databases and logic programming. Having discharged the main obligation of the reviewer of saying who should read the book under review and why, I will now survey each of the book's four parts while raising some more general questions impinging on the whole book as they arise from the discussion of each part.","An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #AUTHOR_TAG ) .","['An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #AUTHOR_TAG ) .', 'A lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category.', 'An ID grammar rule represents a relation between a mother and its daughters, and is independent of lexical characteristics.', 'Figure 6 illustrates an example of bottom-up parsing with an HPSG grammar.', 'First, lexical entries for ""can"" and ""run"" are unified respectively with the daughter feature structures of']",0,"['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoin-ing Grammar (FB-LTAG 1 ) (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) by a method of grammar conversion.', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar ( #AUTHOR_TAG ) .']","['The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar ( #AUTHOR_TAG ) .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoin-ing Grammar (FB-LTAG 1 ) (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) by a method of grammar conversion.', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar ( #AUTHOR_TAG ) .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoin-ing Grammar (FB-LTAG 1 ) (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) by a method of grammar conversion.', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar ( #AUTHOR_TAG ) .']","['The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar ( #AUTHOR_TAG ) .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoin-ing Grammar (FB-LTAG 1 ) (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) by a method of grammar conversion.', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar ( #AUTHOR_TAG ) .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoin-ing Grammar (FB-LTAG 1 ) (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) by a method of grammar conversion.', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar ( #AUTHOR_TAG ) .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoin-ing Grammar (FB-LTAG 1 ) (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) by a method of grammar conversion.', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar ( #AUTHOR_TAG ) .']","['The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar ( #AUTHOR_TAG ) .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']"
CC1300,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,a lexicalized tree adjoining grammar for english,['The XTAG Research Group'],introduction,"This paper presents a sizable grammar for English written in the Tree Adjoining grammar (TAG) formalism. The grammar uses a TAG that is both lexicalized (Schabes, Abeille, Joshi 1988) and feature-based (VijayShankar, Joshi 1988). In this paper, we describe a wide range of phenomena that it covers. A Lexicalized TAG (LTAG) is organized around a lexicon, which associates sets of elementary trees (instead of just simple categories) with the lexical items. A Lexicalized TAG consists of a finite set of trees associated with lexical items, and operations (adjunction and substitution) for composing the trees. A lexical item is called the anchor of its corresponding tree and directly determines both the tree's structure and its syntactic features. In particular, the trees define the domain of locality over which constraints are specified and these constraints are local with respect to their anchor. In this paper, the basic tree structures of the English LTAG are described, along with some relevant features. The interaction between the morphological and the syntactic components of the lexicon is also explained. Next, the properties of the different tree structures are discussed. The use of S complements exclusively allows us to take full advantage of the treatment of unbounded dependencies originally presented in Joshi (1985) and Kroch and Joshi (1985). Structures for auxiliaries and raising-verbs which use adjunction trees are also discussed. We present a representation of prepositional complements that is based on extended elementary trees. This representation avoids the need for preposition incorporation in order to account for double whquestions (preposition stranding and pied-piping) and the pseudo-passive. A treatment of light verb constructions is also given, similar to what Abeille (1988c) has presented. Again, neither noun nor adjective incorporation is needed to handle double passives and to account for CNPC violations in these constructions. TAG'S extended domain of locality allows us to handle, within a single level of syntactic description, phenomena that in other frameworks require either dual analyses or reanalysis. In addition, following Abeille and Schabes (1989), we describe how to deal with semantic non compositionality in verb-particle combinations, light verb constructions and idioms, without losing the internal syntactic composition of these structures. The last sections discuss current work on PRO, case, anaphora and negation, and outline future work on copula constructions and small clauses, optional arguments, adverb movement and the nature of syntactic rules in a lexicalized framework. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-90-24. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/527 A Lexicalized Tree Adjoining Grammar For English MS-CIS-90-24 LINC LAB 170 Anne Abeillh Kathleen Bishop Sharon Cote Yves Schabes Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 19104-6389","We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .","['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']",5,"['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2 , lem refers to the LTAG parser ( #AUTHOR_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .']","['In Table 2 , lem refers to the LTAG parser ( #AUTHOR_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .', 'TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2).']","['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2 , lem refers to the LTAG parser ( #AUTHOR_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .']","['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2 , lem refers to the LTAG parser ( #AUTHOR_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .']","['In Table 2 , lem refers to the LTAG parser ( #AUTHOR_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .', 'TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2).', 'Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.']","['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2 , lem refers to the LTAG parser ( #AUTHOR_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .', 'TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2).']","['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2 , lem refers to the LTAG parser ( #AUTHOR_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .', 'TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2).', 'Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.']","['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2 , lem refers to the LTAG parser ( #AUTHOR_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .']","['In Table 2 , lem refers to the LTAG parser ( #AUTHOR_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .', 'TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2).', 'Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.']"
CC1301,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,evolution of the xtag system,"['Christy Doran', 'Beth Ann Hockey', 'Anoop Sarkar', 'B Srinivas', 'Fei Xia']",introduction,,"The XTAG group ( #AUTHOR_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .","['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with Â¥).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with Â£).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled Ã onto an internal node of another tree with the same symbol Ã (Figure 4).', 'FB-LTAG (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) is an extension of the LTAG formalism.', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group ( #AUTHOR_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .', 'Development of a large-scale French grammar (AbeillÃ© and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']",0,"['Adjunction grafts an auxiliary tree with the root node and foot node labeled U onto an internal node of another tree with the same symbol U (Figure 4).', 'FBLTAG ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .']","['FBLTAG ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .']","['Adjunction grafts an auxiliary tree with the root node and foot node labeled U onto an internal node of another tree with the same symbol U (Figure 4).', 'FBLTAG ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .']","['Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled U onto an internal node of another tree with the same symbol U (Figure 4).', 'FBLTAG ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .']","['FBLTAG ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .']","['Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled U onto an internal node of another tree with the same symbol U (Figure 4).', 'FBLTAG ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .']","['Adjunction grafts an auxiliary tree with the root node and foot node labeled U onto an internal node of another tree with the same symbol U (Figure 4).', 'FBLTAG ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .']","['In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled U onto an internal node of another tree with the same symbol U (Figure 4).', 'FBLTAG ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .']","['FBLTAG ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.']"
CC1302,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,grammar conversion from fbltag to hpsg,"['Naoki Yoshinaga', 'Yusuke Miyao']",introduction,,The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar ( #AUTHOR_TAG ) .,"['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoin-ing Grammar (FB-LTAG 1 ) (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) by a method of grammar conversion.', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar ( #AUTHOR_TAG ) .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']",0,"['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #AUTHOR_TAG ) .']","['Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #AUTHOR_TAG ) .', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).']","['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #AUTHOR_TAG ) .', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).']","['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #AUTHOR_TAG ) .']","['Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #AUTHOR_TAG ) .', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000).']","['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #AUTHOR_TAG ) .', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).']","['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #AUTHOR_TAG ) .', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000).']","['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #AUTHOR_TAG ) .']","['Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #AUTHOR_TAG ) .', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000).']"
CC1303,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,some experiments on indicators of parsing complexity for lexicalized grammars,"['Anoop Sarkar', 'Fei Xia', 'Aravind Joshi']",experiments,"In this paper, we identify syntactic lexical ambiguity and sentence complexity as factors that contribute to parsing complexity in fully lexicalized grammar formalisms such as Lexicalized Tree Adjoining Grammars. We also report on experiments that explore the effects of these factors on parsing complexity. We discuss how these constraints can be exploited in improving efficiency of parsers for such grammar formalisms.","In Table 2 , lem refers to the LTAG parser ( #AUTHOR_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .","['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2 , lem refers to the LTAG parser ( #AUTHOR_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .', 'TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2).', 'Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.', 'Another paper  describes the detailed analysis on the factor of the difference of parsing performance.']",1,"['Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ) .']","['There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.']","['Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.']","['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ) .']","['There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']","['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.']","['Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']","['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ) .']","['There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']"
CC1304,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,a study of tree adjoining grammars,['K Vijay-Shanker'],introduction,"Constrained grammatical system have been the object of study in computational linguistics over the last few years, both with respect to their linguistic adequacy and their computational properties. A Tree Adjoining Grammar (TAG) is a tree rewriting system whose linguistic relevance has been extensively studied. A key property of these systems is that a TAG factors recursion from the co-occurrence restrictions. In this thesis, we study some mathematical properties of TAG u27s. We show that TAG u27s have several interesting properties and are a natural generalization of Context Free Grammars. We show the equivalence of the classes of languages generated by TAG u27s with those generated by Head Grammars and a linear version of Indexed Grammars, which have been studied for their linguistic applicability. We define the embedded pushdown automaton, an extention of the pushdown automaton, and prove that they are equivalent to TAG u27s. We show that the class of Tree Adjoining Languages form a substitution closed abstract family of languages, and that each Tree Adjoining Language is a semilinear language. We show that a TAG can be parsed in polynomial time by adapting the Cocke-Kasami-Younger algorithm for CFL u27s. Feature structures, essentially a set of attribute value pairs, have been used in computational linguistics to make statements of equality to capture some linguistic phenomena such as subcategorization and agreement. We embed TAG u27s in a feature structure based framework. We show that the resulting system has several advantages over TAG u27s. We give a mathematical model of this system based on the logical calculus developed by Rounds, Kasper, and Manaster-Ramer. Finally, we propose a restriction of this system and show how parsing of such a system can be done efficiently","FBLTAG ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .","['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with Â¥).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with Â£).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled Ã onto an internal node of another tree with the same symbol Ã (Figure 4).', 'FBLTAG ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (AbeillÃ© and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']",0,"['The RenTAL system is implemented in LiL-FeS (Makino et al., 1998) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ) .']","['LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ) .', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.']","['The RenTAL system is implemented in LiL-FeS (Makino et al., 1998) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ) .']","['The RenTAL system is implemented in LiL-FeS (Makino et al., 1998) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ) .']","['LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ) .', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus (Marcus et al., 1994) 6 (the average length is 6.32 words).']","['The RenTAL system is implemented in LiL-FeS (Makino et al., 1998) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ) .', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.']","['The RenTAL system is implemented in LiL-FeS (Makino et al., 1998) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ) .', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus (Marcus et al., 1994) 6 (the average length is 6.32 words).']","['The RenTAL system is implemented in LiL-FeS (Makino et al., 1998) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ) .']","['LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ) .', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus (Marcus et al., 1994) 6 (the average length is 6.32 words).', 'This result empirically attested the strong equivalence of our algorithm.']"
CC1305,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,on building a more efficient grammar by exploiting types,['Dan Flickinger'],introduction,"Modern grammar development platforms often support multiple devices for representing properties of a natural language, giving the grammar writer some freedom in implementing analyses of linguistic phenomena. These design alternatives can have dramatic consequences for efficiency both in processing and in grammar building. In this paper I report on three experiments in making systematic modifications to a broad-coverage grammar of English in order to gain efficiency without loss of linguistic elegance. While the experiments are to some degree both platform-dependant and theory-bound, the kinds of modifications reported should be applicable to any unification-based grammar which makes use of types. The results make a strong case for a more visible role for the linguist in the collaborative effort to achieve greater processing efficiency.","Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #AUTHOR_TAG ) .","['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #AUTHOR_TAG ) .', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000).']",0,"['We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.', 'Another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance .']",['Another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance .'],"['We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.', 'Another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance .']","['This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.', 'Another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance .']",['Another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance .'],"['This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.', 'Another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance .']","['We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.', 'Another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance .']","['Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.', 'Another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance .']",['Another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance .']
CC1306,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,evolution of the xtag system,"['Christy Doran', 'Beth Ann Hockey', 'Anoop Sarkar', 'B Srinivas', 'Fei Xia']",introduction,,"There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ) .","['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']",0,"['We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #AUTHOR_TAG ) 6 ( the average length is 6.32 words ) .']","['The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #AUTHOR_TAG ) 6 ( the average length is 6.32 words ) .', 'This result empirically attested the strong equivalence of our algorithm.']","['We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #AUTHOR_TAG ) 6 ( the average length is 6.32 words ) .', 'This result empirically attested the strong equivalence of our algorithm.']","['LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;.', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #AUTHOR_TAG ) 6 ( the average length is 6.32 words ) .']","['The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #AUTHOR_TAG ) 6 ( the average length is 6.32 words ) .', 'This result empirically attested the strong equivalence of our algorithm.']","['LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;.', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #AUTHOR_TAG ) 6 ( the average length is 6.32 words ) .', 'This result empirically attested the strong equivalence of our algorithm.']","['We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #AUTHOR_TAG ) 6 ( the average length is 6.32 words ) .', 'This result empirically attested the strong equivalence of our algorithm.']","['The RenTAL system is implemented in LiL-FeS (Makino et al., 1998) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;.', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #AUTHOR_TAG ) 6 ( the average length is 6.32 words ) .']","['The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #AUTHOR_TAG ) 6 ( the average length is 6.32 words ) .', 'This result empirically attested the strong equivalence of our algorithm.']"
CC1307,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,an hpsg parser with cfg filtering,"['Kentaro Torisawa', 'Kenji Nishida', 'Yusuke Miyao', 'Junâichi Tsujii']",experiments,,"LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ) .","['The RenTAL system is implemented in LiL-FeS (Makino et al., 1998) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ) .', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus (Marcus et al., 1994) 6 (the average length is 6.32 words).', 'This result empirically attested the strong equivalence of our algorithm.']",0,['The RenTAL system is implemented in LiLFeS ( #AUTHOR_TAG ) 2 .'],"['The RenTAL system is implemented in LiLFeS ( #AUTHOR_TAG ) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;.']","['The RenTAL system is implemented in LiLFeS ( #AUTHOR_TAG ) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;.']",['The RenTAL system is implemented in LiLFeS ( #AUTHOR_TAG ) 2 .'],"['The RenTAL system is implemented in LiLFeS ( #AUTHOR_TAG ) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;.', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.']","['The RenTAL system is implemented in LiLFeS ( #AUTHOR_TAG ) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;.']","['The RenTAL system is implemented in LiLFeS ( #AUTHOR_TAG ) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;.', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.']",['The RenTAL system is implemented in LiLFeS ( #AUTHOR_TAG ) 2 .'],"['The RenTAL system is implemented in LiLFeS ( #AUTHOR_TAG ) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;.', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus (Marcus et al., 1994) 6 (the average length is 6.32 words).']"
CC1308,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,efficient ltag parsing using hpsg parsers,"['Naoki Yoshinaga', 'Yusuke Miyao', 'Kentaro Torisawa', 'Junâichi Tsujii']",experiments,,Another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance .,"['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2).', 'TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2).', 'Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.', 'Another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance .']",0,"['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .']","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']"
CC1309,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,building a large annotated corpus of english the penn treebank computational linguistics,"['Mitchell Marcus', 'Beatrice Santorini', 'Mary Ann Marcinkiewicz']",experiments,,The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #AUTHOR_TAG ) 6 ( the average length is 6.32 words ) .,"['The RenTAL system is implemented in LiL-FeS (Makino et al., 1998) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;.', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #AUTHOR_TAG ) 6 ( the average length is 6.32 words ) .', 'This result empirically attested the strong equivalence of our algorithm.']",5,"['Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ) .']","['There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.']","['Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.']","['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ) .']","['There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']","['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.']","['Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']","['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ) .']","['There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']"
CC1310,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,lilfes â towards a practical hpsg parsers,"['Takaki Makino', 'Minoru Yoshida', 'Kentaro Torisawa', 'Junâichi Tsujii']",experiments,,The RenTAL system is implemented in LiLFeS ( #AUTHOR_TAG ) 2 .,"['The RenTAL system is implemented in LiLFeS ( #AUTHOR_TAG ) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;.', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus (Marcus et al., 1994) 6 (the average length is 6.32 words).', 'This result empirically attested the strong equivalence of our algorithm.']",5,"['Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).']","['There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.']","['Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.']","['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).']","['There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']","['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.']","['Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']","['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).']","['There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']"
CC1311,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,a study of tree adjoining grammars,['K Vijay-Shanker'],introduction,"Constrained grammatical system have been the object of study in computational linguistics over the last few years, both with respect to their linguistic adequacy and their computational properties. A Tree Adjoining Grammar (TAG) is a tree rewriting system whose linguistic relevance has been extensively studied. A key property of these systems is that a TAG factors recursion from the co-occurrence restrictions. In this thesis, we study some mathematical properties of TAG u27s. We show that TAG u27s have several interesting properties and are a natural generalization of Context Free Grammars. We show the equivalence of the classes of languages generated by TAG u27s with those generated by Head Grammars and a linear version of Indexed Grammars, which have been studied for their linguistic applicability. We define the embedded pushdown automaton, an extention of the pushdown automaton, and prove that they are equivalent to TAG u27s. We show that the class of Tree Adjoining Languages form a substitution closed abstract family of languages, and that each Tree Adjoining Language is a semilinear language. We show that a TAG can be parsed in polynomial time by adapting the Cocke-Kasami-Younger algorithm for CFL u27s. Feature structures, essentially a set of attribute value pairs, have been used in computational linguistics to make statements of equality to capture some linguistic phenomena such as subcategorization and agreement. We embed TAG u27s in a feature structure based framework. We show that the resulting system has several advantages over TAG u27s. We give a mathematical model of this system based on the logical calculus developed by Rounds, Kasper, and Manaster-Ramer. Finally, we propose a restriction of this system and show how parsing of such a system can be done efficiently","This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']",0,['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .'],['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .'],['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .'],['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .'],['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .'],['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .'],['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .'],['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .'],['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .']
CC1312,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,lilfes â towards a practical hpsg parsers,"['Takaki Makino', 'Minoru Yoshida', 'Kentaro Torisawa', 'Junâichi Tsujii']",introduction,,"There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ) .","['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']",0,"['In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .']","['Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.']","['In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.']","['FB-LTAG (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) is an extension of the LTAG formalism.', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .']","['Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeille and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']","['FB-LTAG (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) is an extension of the LTAG formalism.', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.']","['In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeille and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']","['Adjunction grafts an auxiliary tree with the root node and foot node labeled U onto an internal node of another tree with the same symbol U (Figure 4).', 'FB-LTAG (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) is an extension of the LTAG formalism.', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .']","['Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeille and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']"
CC1313,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,twostep tag parsing revisited,"['Peter Poller', 'Tilman Becker']",introduction,"Based on the work in (Poller, 1994) and a minor assumption about a normal form for TAGs, we present a highly simplified version of the twostep parsing approach for TAGs which allows for a much easier analysis of run-time and space complexity. It also snggests how restrictions on the grammars might result in improvements in run-time complexity. The main advantage of a two-step parsing system shows in practical applications like Verbmobil (Bub et al., 1997) where the parser must look at multiple hypotheses supplied by a speech recognizer (encoded in a word hypotheses lattice) and filter out illicit hypotheses as early as possible. The first (context-free) step of our parser filters out some illicit hypotheses fast (O(n3 )); the constructed parsing matrix is then reused for the second step, the complete (O(n6 )) TAG parse.","There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).","['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']",0,"['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .']","['We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .']","['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .']","['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .']","['We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.']","['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .']","['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.']","['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .']","['We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']"
CC1314,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,grammar conversion from fbltag to hpsg,"['Naoki Yoshinaga', 'Yusuke Miyao']",,,The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .,['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .'],0,"['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']"
CC1315,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,a lexicalized tree adjoining grammar for english,['The XTAG Research Group'],introduction,"This paper presents a sizable grammar for English written in the Tree Adjoining grammar (TAG) formalism. The grammar uses a TAG that is both lexicalized (Schabes, Abeille, Joshi 1988) and feature-based (VijayShankar, Joshi 1988). In this paper, we describe a wide range of phenomena that it covers. A Lexicalized TAG (LTAG) is organized around a lexicon, which associates sets of elementary trees (instead of just simple categories) with the lexical items. A Lexicalized TAG consists of a finite set of trees associated with lexical items, and operations (adjunction and substitution) for composing the trees. A lexical item is called the anchor of its corresponding tree and directly determines both the tree's structure and its syntactic features. In particular, the trees define the domain of locality over which constraints are specified and these constraints are local with respect to their anchor. In this paper, the basic tree structures of the English LTAG are described, along with some relevant features. The interaction between the morphological and the syntactic components of the lexicon is also explained. Next, the properties of the different tree structures are discussed. The use of S complements exclusively allows us to take full advantage of the treatment of unbounded dependencies originally presented in Joshi (1985) and Kroch and Joshi (1985). Structures for auxiliaries and raising-verbs which use adjunction trees are also discussed. We present a representation of prepositional complements that is based on extended elementary trees. This representation avoids the need for preposition incorporation in order to account for double whquestions (preposition stranding and pied-piping) and the pseudo-passive. A treatment of light verb constructions is also given, similar to what Abeille (1988c) has presented. Again, neither noun nor adjective incorporation is needed to handle double passives and to account for CNPC violations in these constructions. TAG'S extended domain of locality allows us to handle, within a single level of syntactic description, phenomena that in other frameworks require either dual analyses or reanalysis. In addition, following Abeille and Schabes (1989), we describe how to deal with semantic non compositionality in verb-particle combinations, light verb constructions and idioms, without losing the internal syntactic composition of these structures. The last sections discuss current work on PRO, case, anaphora and negation, and outline future work on copula constructions and small clauses, optional arguments, adverb movement and the nature of syntactic rules in a lexicalized framework. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-90-24. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/527 A Lexicalized Tree Adjoining Grammar For English MS-CIS-90-24 LINC LAB 170 Anne Abeillh Kathleen Bishop Sharon Cote Yves Schabes Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 19104-6389","Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .","['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with Â¥).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with Â£).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled Ã onto an internal node of another tree with the same symbol Ã (Figure 4).', 'FB-LTAG (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) is an extension of the LTAG formalism.', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (AbeillÃ© and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']",0,"['This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG ) .']","['From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG ) .']","['This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG ) .']","['When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG ) .']","['From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG ) .']","['When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG ) .']","['This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG ) .']","['This has been reported for other languages, too, dependent on the generality of the chosen approach (Jappinen and Niemisto, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekcioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG ) .']","['From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG ) .']"
CC1316,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,a lexicalized tree adjoining grammar for english,['The XTAG Research Group'],experiments,"This paper presents a sizable grammar for English written in the Tree Adjoining grammar (TAG) formalism. The grammar uses a TAG that is both lexicalized (Schabes, Abeille, Joshi 1988) and feature-based (VijayShankar, Joshi 1988). In this paper, we describe a wide range of phenomena that it covers. A Lexicalized TAG (LTAG) is organized around a lexicon, which associates sets of elementary trees (instead of just simple categories) with the lexical items. A Lexicalized TAG consists of a finite set of trees associated with lexical items, and operations (adjunction and substitution) for composing the trees. A lexical item is called the anchor of its corresponding tree and directly determines both the tree's structure and its syntactic features. In particular, the trees define the domain of locality over which constraints are specified and these constraints are local with respect to their anchor. In this paper, the basic tree structures of the English LTAG are described, along with some relevant features. The interaction between the morphological and the syntactic components of the lexicon is also explained. Next, the properties of the different tree structures are discussed. The use of S complements exclusively allows us to take full advantage of the treatment of unbounded dependencies originally presented in Joshi (1985) and Kroch and Joshi (1985). Structures for auxiliaries and raising-verbs which use adjunction trees are also discussed. We present a representation of prepositional complements that is based on extended elementary trees. This representation avoids the need for preposition incorporation in order to account for double whquestions (preposition stranding and pied-piping) and the pseudo-passive. A treatment of light verb constructions is also given, similar to what Abeille (1988c) has presented. Again, neither noun nor adjective incorporation is needed to handle double passives and to account for CNPC violations in these constructions. TAG'S extended domain of locality allows us to handle, within a single level of syntactic description, phenomena that in other frameworks require either dual analyses or reanalysis. In addition, following Abeille and Schabes (1989), we describe how to deal with semantic non compositionality in verb-particle combinations, light verb constructions and idioms, without losing the internal syntactic composition of these structures. The last sections discuss current work on PRO, case, anaphora and negation, and outline future work on copula constructions and small clauses, optional arguments, adverb movement and the nature of syntactic rules in a lexicalized framework. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-90-24. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/527 A Lexicalized Tree Adjoining Grammar For English MS-CIS-90-24 LINC LAB 170 Anne Abeillh Kathleen Bishop Sharon Cote Yves Schabes Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 19104-6389","We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .","['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']",5,"[""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance .']","['mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance .']","[""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance .']","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance .']","['mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance .']","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance .']","[""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance .']","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance .']","['mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance .', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jappinen and Niemisto, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekcioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.']"
CC1317,W02-0309,Biomedical text retrieval in languages with a complex morphology,how effective is suffixing,['D Harman'],conclusion,"s and titles from the Cranfield collection (with 225 queries and 1400 documents), comprised the major test collection for this study. The Medlars collection (30 queries and 1033 documents), and the CACM collection (64 queries and 3204 documents) were used to provide information about the variation of stemming performance across different subject areas and test collections. In addition to the standard recall/precision measures, with SMART system averaging (Salton, 1971), several methods more suited to an interactive retrieval environment were adopted. The interactive environment returns lists of the top ranked documents, and allows the users to scan titles of a group of documents a screenful at a time, so that the ranking of individual documents within the screenful is not as important as the total number of relevant titles within a screen. Furthermore, the number of relevant documents in the first few screens is far more important for the user than the number of relevant in the last screenfuls. Three measures were selected which evaluate performance at given rank cutoff points, such as those corresponding to a screenful of document titles. The first measure, the E measure (Van Rijsbergen, 1979), is a weighted combination of recall and precision that evaluates a set of retrieved documents at a given cutoff, ignoring the ranking within that set. The measure may have weights of 0.5, 1.0, and 2.0 which correspond, respectively, to attaching half the importance to recall as to precision, equal importance to both, and double importance to recall. A lower E value indicates a more effective performance. A second measure, the total number of relevant documents retrieved by a given cutoff, was also calculated. Cutoffs of 10 and 30 documents were used, with ten reflecting a minimum number a user might be expected to TABLE 2. Retrieval performance for Cranfteld 225. scan, and 30 being an assumed upper limit of what a user would scan before query modification. The third measure applicable to the interactive environment is the number of queries that retrieve no relevant documents by the given cutoff. This measure is important because many types of query modification techniques, such as relevance feedback, require relevant documents to be in the retrieved set to work well. These measures were all used in Croft (1983) as complementary measures to the standard recall/precision evaluation.","There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']",0,"['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']"
CC1318,W02-0309,Biomedical text retrieval in languages with a complex morphology,morphosemantic parsing of medical expressions,"['R Baud', 'C Lovis', 'A-M Rassinoux', 'J-R Scherrer']",introduction,,"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG ) .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 'Â¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (JÃ¤ppinen and NiemistÃ¶, 1988;Choueka, 1990;Popovic and Willett, 1992;EkmekÃ§ioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG ) .']",0,"[""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance .']","['mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance .']","[""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance .']","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance .']","['mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance .']","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance .']","[""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance .']","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance .']","['mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance .', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jappinen and Niemisto, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekcioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.']"
CC1319,W02-0309,Biomedical text retrieval in languages with a complex morphology,development of a stemming algorithm,['J Lovins'],introduction,"A stemming algorithm, a procedure to reduce all words with the same stem to a common form, is useful in many areas of computational linguistics and information-retrieval work. While the form of the algorithm varies with its application, certain linguistic problems are common to any stemming procedure. As a basis for evaluation of previous attempts to deal with these problems, this paper first discusses the theoretical and practical attributes of stemming algorithms. Then a new version of a context-sensitive, longest-match stemming algorithm for English is proposed; though developed for use in a library information transfer system, it is of general application. A major linguistic problem in stemming, variation in spelling of stems, is discussed in some detail and several feasible programmed solutions are outlined, along with sample results of one of these methods.","mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 'Â¡ ' denotes the string concatenation operator."", 'mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance .', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (JÃ¤ppinen and NiemistÃ¶, 1988;Choueka, 1990;Popovic and Willett, 1992;EkmekÃ§ioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998).']",0,"['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( Choueka , 1990 ; J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; #AUTHOR_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .']","['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( Choueka , 1990 ; J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; #AUTHOR_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure']).""]","['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( Choueka , 1990 ; J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; #AUTHOR_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure']).""]","['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( Choueka , 1990 ; J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; #AUTHOR_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .']","['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( Choueka , 1990 ; J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; #AUTHOR_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure'])."", ""The goal is to map all occurring morphological variants to some canonical base forme.g., 'search' in the examples from above.""]","['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( Choueka , 1990 ; J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; #AUTHOR_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure']).""]","['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( Choueka , 1990 ; J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; #AUTHOR_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure'])."", ""The goal is to map all occurring morphological variants to some canonical base forme.g., 'search' in the examples from above.""]","['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( Choueka , 1990 ; J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; #AUTHOR_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .']","['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( Choueka , 1990 ; J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; #AUTHOR_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure'])."", ""The goal is to map all occurring morphological variants to some canonical base forme.g., 'search' in the examples from above.""]"
CC1320,W02-0309,Biomedical text retrieval in languages with a complex morphology,stemming algorithms a case study for detailed evaluation,['D A Hull'],conclusion,"The majority of information retrieval experiments are evaluated by measures such as average precision and average recall. Fundamental decisions about the superiority of one retrieval technique over another are made solely on the basis of these measures. We claim that average performance figures need to be validated with a careful statistical analysis and that there is a great deal of additional information that can be uncovered by looking closely at the results of individual queries. This article is a case study of stemming algorithms which describes a number of novel approaches to evaluation and demonstrates their value. (c) 1996 John Wiley & Sons, Inc.","There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']",0,"['This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['This has been reported for other languages, too, dependent on the generality of the chosen approach (Jappinen and Niemisto, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekcioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']"
CC1321,W02-0309,Biomedical text retrieval in languages with a complex morphology,an algorithm for suffix stripping,['M Porter'],introduction,"Purpose - The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. This work was originally published in Program in 1980 and is republished as part of a series of articles commemorating the 40th anniversary of the journal. Design/methodology/approach - An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL. Findings - Although simple, it performs slightly better than a much more elaborate system with which it has been compared. It effectively works by treating complex suffixes as compounds made up of simple suffixes, and removing the simple suffixes in a number of steps. In each step the removal of the suffix is made to depend upon the form of the remaining stem, which usually involves a measure of its syllable length. Originality/value - The piece provides a useful historical document on information retrieval.","mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 'Â¡ ' denotes the string concatenation operator."", 'mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance .', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (JÃ¤ppinen and NiemistÃ¶, 1988;Choueka, 1990;Popovic and Willett, 1992;EkmekÃ§ioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998).']",0,"['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']"
CC1322,W02-0309,Biomedical text retrieval in languages with a complex morphology,viewing stemming as recall enhancement,"['W Kraaij', 'R Pohlmann']",introduction,"Previous research on stemming has shown both positive and negative effects on retrieval performance. This paper describes an experiment in which several linguistic and non-linguistic stemmers are evaluated on a Dutch test collection. Experiments especially focus on the measurement of Recall. Results show that linguistic stemming restricted to inflection yields a significant improvement over full linguistic and non-linquistic stemming, both in average Precision and R-Recall. Best results are obtained with a linguistic stemmer which is enhanced with compound analysis. This version has a significantly better Recall than a system without stemming, without a significant deterioration of Precision.","Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( Choueka , 1990 ; J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; #AUTHOR_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .","['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( Choueka , 1990 ; J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; #AUTHOR_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure'])."", ""The goal is to map all occurring morphological variants to some canonical base forme.g., 'search' in the examples from above.""]",0,"['), but at high ones its precision decreases almost dramatically.', 'Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method .']",['Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method .'],"['), but at high ones its precision decreases almost dramatically.', 'Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method .']","['), but at high ones its precision decreases almost dramatically.', 'Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method .']",['Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method .'],"['), but at high ones its precision decreases almost dramatically.', 'Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method .']","['), but at high ones its precision decreases almost dramatically.', 'Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method .']","['), but at high ones its precision decreases almost dramatically.', 'Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method .']",['Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method .']
CC1323,W02-0309,Biomedical text retrieval in languages with a complex morphology,the use of morphosemantic regularities in the medical vocabulary for automatic lexical coding methods ofinformation in,['S Wolff'],introduction,The use of morphosemantic regularities in the medical vocabulary for automatic lexical coding. -,"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 'Â¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (JÃ¤ppinen and NiemistÃ¶, 1988;Choueka, 1990;Popovic and Willett, 1992;EkmekÃ§ioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']",0,"['This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['This has been reported for other languages, too, dependent on the generality of the chosen approach (Jappinen and Niemisto, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekcioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']"
CC1324,W02-0309,Biomedical text retrieval in languages with a complex morphology,an algorithm for suffix stripping,['M Porter'],conclusion,"Purpose - The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. This work was originally published in Program in 1980 and is republished as part of a series of articles commemorating the 40th anniversary of the journal. Design/methodology/approach - An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL. Findings - Although simple, it performs slightly better than a much more elaborate system with which it has been compared. It effectively works by treating complex suffixes as compounds made up of simple suffixes, and removing the simple suffixes in a number of steps. In each step the removal of the suffix is made to depend upon the form of the remaining stem, which usually involves a measure of its syllable length. Originality/value - The piece provides a useful historical document on information retrieval.","There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']",0,"['The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 ) .']","['Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 ) .']","['The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 ) .']","['There has been some controversy, at least for simple stemmers (Lovins, 1968;Porter, 1980), about the effectiveness of morphological analysis for document retrieval (Harman, 1991;Krovetz, 1993;Hull, 1996).', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 ) .']","['Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 ) .']","['There has been some controversy, at least for simple stemmers (Lovins, 1968;Porter, 1980), about the effectiveness of morphological analysis for document retrieval (Harman, 1991;Krovetz, 1993;Hull, 1996).', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 ) .']","['The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 ) .']","['There has been some controversy, at least for simple stemmers (Lovins, 1968;Porter, 1980), about the effectiveness of morphological analysis for document retrieval (Harman, 1991;Krovetz, 1993;Hull, 1996).', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 ) .']","['Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 ) .']"
CC1325,W02-0309,Biomedical text retrieval in languages with a complex morphology,automated coding of diagnoses three methods compared,"['P Franz', 'A Zaiss', 'S Schulz', 'U Hahn', 'R Klar']",experiments,"In Germany, new legal requirements have raised the importance of the accurate encoding of admission and discharge diseases for in- and outpatients. In response to emerging needs for computer-supported tools we examined three methods for automated coding of German-language free-text diagnosis phrases. We compared a language-independent lexicon-free n-gram approach with one which uses a dictionary of medical morphemes and refines the query by a mapping to SNOMED codes. Both techniques produced a ranked output of possible diagnoses within a vector space framework for retrieval. The results did not reveal any significant difference: The correct diagnosis was found in approximately 40% for three-digit codes, and 30% for four-digit codes. The lexicon-based method was then modified by substituting the vector space ranking by a heuristic approach that capitalizes on the semantic structure of SNOMED, thus raising the number of correct diagnoses significantly (approximately 50% for three-digit codes, and 40% for four-digit codes). As a result, we claim that lexicon-based retrieval methods do not perform better than the lexicon-free ones, unless conceptual knowledge is added.",Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method .,"['), but at high ones its precision decreases almost dramatically.', 'Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method .']",1,"['It crawls text/HTML files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf-idf metric.', 'The retrieval process relies on the vector space model ( #AUTHOR_TAG ) , with the cosine measure expressing the similarity between a query and a document .']","['The retrieval process relies on the vector space model ( #AUTHOR_TAG ) , with the cosine measure expressing the similarity between a query and a document .', 'The search engine produces a ranked output of documents.']","['It crawls text/HTML files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf-idf metric.', 'The retrieval process relies on the vector space model ( #AUTHOR_TAG ) , with the cosine measure expressing the similarity between a query and a document .', 'The search engine produces a ranked output of documents.']","['For unbiased evaluation of our approach, we used a home-grown search engine (implemented in the PYTHON script language).', 'It crawls text/HTML files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf-idf metric.', 'The retrieval process relies on the vector space model ( #AUTHOR_TAG ) , with the cosine measure expressing the similarity between a query and a document .']","['The retrieval process relies on the vector space model ( #AUTHOR_TAG ) , with the cosine measure expressing the similarity between a query and a document .', 'The search engine produces a ranked output of documents.']","['For unbiased evaluation of our approach, we used a home-grown search engine (implemented in the PYTHON script language).', 'It crawls text/HTML files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf-idf metric.', 'The retrieval process relies on the vector space model ( #AUTHOR_TAG ) , with the cosine measure expressing the similarity between a query and a document .', 'The search engine produces a ranked output of documents.']","['It crawls text/HTML files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf-idf metric.', 'The retrieval process relies on the vector space model ( #AUTHOR_TAG ) , with the cosine measure expressing the similarity between a query and a document .', 'The search engine produces a ranked output of documents.']","['For unbiased evaluation of our approach, we used a home-grown search engine (implemented in the PYTHON script language).', 'It crawls text/HTML files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf-idf metric.', 'The retrieval process relies on the vector space model ( #AUTHOR_TAG ) , with the cosine measure expressing the similarity between a query and a document .']","['The retrieval process relies on the vector space model ( #AUTHOR_TAG ) , with the cosine measure expressing the similarity between a query and a document .', 'The search engine produces a ranked output of documents.']"
CC1326,W02-0309,Biomedical text retrieval in languages with a complex morphology,morphologic analysis of compound words,['F Wingert'],introduction,,"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 'Â¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (JÃ¤ppinen and NiemistÃ¶, 1988;Choueka, 1990;Popovic and Willett, 1992;EkmekÃ§ioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']",0,"['There has been some controversy , at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .']","['There has been some controversy , at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.']","['There has been some controversy , at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.']","['There has been some controversy , at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .']","['There has been some controversy , at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']","['There has been some controversy , at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.']","['There has been some controversy , at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']","['There has been some controversy , at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .']","['There has been some controversy , at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']"
CC1327,W02-0309,Biomedical text retrieval in languages with a complex morphology,viewing stemming as recall enhancement,"['W Kraaij', 'R Pohlmann']",conclusion,"Previous research on stemming has shown both positive and negative effects on retrieval performance. This paper describes an experiment in which several linguistic and non-linguistic stemmers are evaluated on a Dutch test collection. Experiments especially focus on the measurement of Recall. Results show that linguistic stemming restricted to inflection yields a significant improvement over full linguistic and non-linquistic stemming, both in average Precision and R-Recall. Best results are obtained with a linguistic stemmer which is enhanced with compound analysis. This version has a significantly better Recall than a system without stemming, without a significant deterioration of Precision.","Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 ) .","['There has been some controversy, at least for simple stemmers (Lovins, 1968;Porter, 1980), about the effectiveness of morphological analysis for document retrieval (Harman, 1991;Krovetz, 1993;Hull, 1996).', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 ) .']",0,"['Furthermore, medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language (e.g., German), often referred to as neo-classical compounding (Mc-Cray et al., 1988).', 'While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( #AUTHOR_TAG ) .']","['While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( #AUTHOR_TAG ) .']","['Furthermore, medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language (e.g., German), often referred to as neo-classical compounding (Mc-Cray et al., 1988).', 'While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( #AUTHOR_TAG ) .']","['Furthermore, medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language (e.g., German), often referred to as neo-classical compounding (Mc-Cray et al., 1988).', 'While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( #AUTHOR_TAG ) .']","['While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( #AUTHOR_TAG ) .']","['Furthermore, medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language (e.g., German), often referred to as neo-classical compounding (Mc-Cray et al., 1988).', 'While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( #AUTHOR_TAG ) .']","['Furthermore, medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language (e.g., German), often referred to as neo-classical compounding (Mc-Cray et al., 1988).', 'While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( #AUTHOR_TAG ) .']","['Furthermore, medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language (e.g., German), often referred to as neo-classical compounding (Mc-Cray et al., 1988).', 'While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( #AUTHOR_TAG ) .']","['While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( #AUTHOR_TAG ) .']"
CC1328,W02-0309,Biomedical text retrieval in languages with a complex morphology,automatic text processing the transformation analysis and retrieval ofinformation by computer,['Gerard Salton'],experiments,,"The retrieval process relies on the vector space model ( #AUTHOR_TAG ) , with the cosine measure expressing the similarity between a query and a document .","['For unbiased evaluation of our approach, we used a home-grown search engine (implemented in the PYTHON script language).', 'It crawls text/HTML files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf-idf metric.', 'The retrieval process relies on the vector space model ( #AUTHOR_TAG ) , with the cosine measure expressing the similarity between a query and a document .', 'The search engine produces a ranked output of documents.']",5,"['It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #AUTHOR_TAG ) ) are incorporated into our system .']","['This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #AUTHOR_TAG ) ) are incorporated into our system .', 'Alternatively, we may think of user-centered comparative studies (Hersh et al., 1995).']","['It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #AUTHOR_TAG ) ) are incorporated into our system .', 'Alternatively, we may think of user-centered comparative studies (Hersh et al., 1995).']","['We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5).', 'It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #AUTHOR_TAG ) ) are incorporated into our system .']","['This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #AUTHOR_TAG ) ) are incorporated into our system .', 'Alternatively, we may think of user-centered comparative studies (Hersh et al., 1995).']","['We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5).', 'It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #AUTHOR_TAG ) ) are incorporated into our system .', 'Alternatively, we may think of user-centered comparative studies (Hersh et al., 1995).']","['It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #AUTHOR_TAG ) ) are incorporated into our system .', 'Alternatively, we may think of user-centered comparative studies (Hersh et al., 1995).']","['A question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing.', 'We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5).', 'It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #AUTHOR_TAG ) ) are incorporated into our system .']","['This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #AUTHOR_TAG ) ) are incorporated into our system .', 'Alternatively, we may think of user-centered comparative studies (Hersh et al., 1995).']"
CC1329,W02-0309,Biomedical text retrieval in languages with a complex morphology,development of a stemming algorithm,['J Lovins'],conclusion,"A stemming algorithm, a procedure to reduce all words with the same stem to a common form, is useful in many areas of computational linguistics and information-retrieval work. While the form of the algorithm varies with its application, certain linguistic problems are common to any stemming procedure. As a basis for evaluation of previous attempts to deal with these problems, this paper first discusses the theoretical and practical attributes of stemming algorithms. Then a new version of a context-sensitive, longest-match stemming algorithm for English is proposed; though developed for use in a library information transfer system, it is of general application. A major linguistic problem in stemming, variation in spelling of stems, is discussed in some detail and several feasible programmed solutions are outlined, along with sample results of one of these methods.","There has been some controversy , at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .","['There has been some controversy , at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']",0,"['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ) .']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ) .']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ) .']","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']"
CC1330,W02-0309,Biomedical text retrieval in languages with a complex morphology,the use of morphosemantic regularities in the medical vocabulary for automatic lexical coding methods ofinformation in,['S Wolff'],introduction,The use of morphosemantic regularities in the medical vocabulary for automatic lexical coding. -,"While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( #AUTHOR_TAG ) .","['Furthermore, medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language (e.g., German), often referred to as neo-classical compounding (Mc-Cray et al., 1988).', 'While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( #AUTHOR_TAG ) .']",0,"['For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers (Lovins, 1968; Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Choueka , 1990 ; #AUTHOR_TAG ; Ekmekc ÃÂ¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) .']","['This has been reported for other languages , too , dependent on the generality of the chosen approach ( J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Choueka , 1990 ; #AUTHOR_TAG ; Ekmekc ÃÂ¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) .']","['For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers (Lovins, 1968; Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Choueka , 1990 ; #AUTHOR_TAG ; Ekmekc ÃÂ¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) .']","['The efforts required for performing morphologi- cal analysis vary from language to language.', 'For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers (Lovins, 1968; Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Choueka , 1990 ; #AUTHOR_TAG ; Ekmekc ÃÂ¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) .']","['This has been reported for other languages , too , dependent on the generality of the chosen approach ( J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Choueka , 1990 ; #AUTHOR_TAG ; Ekmekc ÃÂ¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) .']","['The efforts required for performing morphologi- cal analysis vary from language to language.', 'For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers (Lovins, 1968; Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Choueka , 1990 ; #AUTHOR_TAG ; Ekmekc ÃÂ¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) .']","['For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers (Lovins, 1968; Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Choueka , 1990 ; #AUTHOR_TAG ; Ekmekc ÃÂ¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) .']","['The efforts required for performing morphologi- cal analysis vary from language to language.', 'For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers (Lovins, 1968; Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Choueka , 1990 ; #AUTHOR_TAG ; Ekmekc ÃÂ¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) .']","['This has been reported for other languages , too , dependent on the generality of the chosen approach ( J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Choueka , 1990 ; #AUTHOR_TAG ; Ekmekc ÃÂ¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) .', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980; Norton and Pacak, 1983; Wolff, 1984; Wingert, 1985; Dujols et al., 1991; Baud et al., 1998).']"
CC1331,W02-0309,Biomedical text retrieval in languages with a complex morphology,medical subject headings,['NLM'],experiments,"Automatically assigning MeSH (Medical Subject Headings) to articles is an active research topic. Recent work demonstrated the feasibility of improving the existing automated Medical Text Indexer (MTI) system, developed at the National Library of Medicine (NLM). Encouraged by this work, we propose a novel data-driven approach that uses semantic distances in the MeSH ontology for automated MeSH assignment. Specifically, we developed a graphical model to propagate belief through a citation network to provide robust MeSH main heading (MH) recommendation. Our preliminary results indicate that this approach can reach high Mean Average Precision (MAP) in some scenarios","This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #AUTHOR_TAG ) ) are incorporated into our system .","['Generalizing the interpretation of our data in the light of these findings, we recognize a substantial increase of retrieval performance when query and text tokens are segmented according to the principles of the subword model.', 'The gain is still not overwhelming.', ""With regard to orthographic normalization, we expected a higher performance benefit because of the well-known spelling problems for German medical terms of Latin or Greek origin (such as in 'ZÃ¤kum ', 'CÃ¤kum', 'Zaekum', 'Caekum', 'Zaecum', 'Caecum')."", 'For our experiments, however, we used quite a homogeneous document collection following the spelling standards of medical publishers.', 'The same standards apparently applied to the original multiple choice questions, by which the acquisition of expert queries was guided (cf.', 'Section 3).', 'In the layman queries, there were only few Latin or Greek terms, and, therefore, they did not take advantage of the spelling normalization.', 'However, the experience with medical text retrieval (especially on medical reports which exhibit a high rate of spelling variations) shows that orthographic normalization is a desider-  Whereas the usefulness of Subword Indexing became evident, we could not provide sufficient evidence for Synonym Class Indexing, so far.', 'However, synonym mapping is still incomplete in the current state of our subword dictionary.', 'A question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing.', 'We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5).', 'It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #AUTHOR_TAG ) ) are incorporated into our system .', 'Alternatively, we may think of user-centered comparative studies (Hersh et al., 1995).']",3,"['This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['This has been reported for other languages, too, dependent on the generality of the chosen approach (Jappinen and Niemisto, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekcioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']"
CC1332,W02-0309,Biomedical text retrieval in languages with a complex morphology,viewing morphology as an inference process,['R Krovetz'],conclusion,"AbstractMorphology is the area of linguistics concerned with the internal structure of words. Information retrieval has generally not paid much attention to word structure, other than to account for some of the variability in word forms via the use of stemmers. We report on our experiments to determine the importance of morphology, and the effect that it has on performance. We found that grouping morphological variants makes a significant improvement in retrieval performance. Improvements are seen by grouping inflectional as well as derivational variants. We also found that performance was enhanced by recognizing lexical phrases. We describe the interaction between morphology and lexical ambiguity, and how resolving that ambiguity will lead to further improvements in performance","There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ) .","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']",0,"['This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['This has been reported for other languages, too, dependent on the generality of the chosen approach (Jappinen and Niemisto, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekcioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']","['From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']"
CC1333,W02-0309,Biomedical text retrieval in languages with a complex morphology,the effectiveness of stemming for natural language access to slovene textual data,"['M Popovic', 'P Willett']",introduction,"There have been several studies of the use of stemming algorithms for conflating morphological variants in freetext retrieval systems. Comparison of stemmed and nonconflated searches suggests that there are no significant increases in the effectiveness of retrieval when stemming is applied to English-language documents and queries. This article reports the use of stemming on Slovene-language documents and queries, and demonstrates that the use of an appropriate stemming algorithm results in a large, and statistically significant, increase in retrieval effectiveness when compared with nonconflated processing; similar comments apply to the use of manual, right-hand truncation. A comparison is made with stemming of English versions of the same documents and queries and it is concluded that the effectiveness of a stemming algorithm is determined by the morphological complexity of the language that it is designed to process.","This has been reported for other languages , too , dependent on the generality of the chosen approach ( J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Choueka , 1990 ; #AUTHOR_TAG ; Ekmekc ÃÂ¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) .","['The efforts required for performing morphologi- cal analysis vary from language to language.', 'For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers (Lovins, 1968; Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Choueka , 1990 ; #AUTHOR_TAG ; Ekmekc ÃÂ¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) .', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980; Norton and Pacak, 1983; Wolff, 1984; Wingert, 1985; Dujols et al., 1991; Baud et al., 1998).']",0,"['This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus (MeSH, (NLM, 2001)) are incorporated into our system.', 'Alternatively , we may think of user-centered comparative studies ( #AUTHOR_TAG ) .']","['Alternatively , we may think of user-centered comparative studies ( #AUTHOR_TAG ) .']","['This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus (MeSH, (NLM, 2001)) are incorporated into our system.', 'Alternatively , we may think of user-centered comparative studies ( #AUTHOR_TAG ) .']","['It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus (MeSH, (NLM, 2001)) are incorporated into our system.', 'Alternatively , we may think of user-centered comparative studies ( #AUTHOR_TAG ) .']","['Alternatively , we may think of user-centered comparative studies ( #AUTHOR_TAG ) .']","['It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus (MeSH, (NLM, 2001)) are incorporated into our system.', 'Alternatively , we may think of user-centered comparative studies ( #AUTHOR_TAG ) .']","['This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus (MeSH, (NLM, 2001)) are incorporated into our system.', 'Alternatively , we may think of user-centered comparative studies ( #AUTHOR_TAG ) .']","['We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5).', 'It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus (MeSH, (NLM, 2001)) are incorporated into our system.', 'Alternatively , we may think of user-centered comparative studies ( #AUTHOR_TAG ) .']","['Alternatively , we may think of user-centered comparative studies ( #AUTHOR_TAG ) .']"
CC1334,W02-0309,Biomedical text retrieval in languages with a complex morphology,morphosemantic analysis of compound word forms denoting surgical procedures methods ofinformation in medicine,"['L Norton', 'M Pacak']",introduction,,"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 'Â¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (JÃ¤ppinen and NiemistÃ¶, 1988;Choueka, 1990;Popovic and Willett, 1992;EkmekÃ§ioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']",0,"['The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG ) .']","['Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG ) .']","['The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG ) .']","['There has been some controversy, at least for simple stemmers (Lovins, 1968;Porter, 1980), about the effectiveness of morphological analysis for document retrieval (Harman, 1991;Krovetz, 1993;Hull, 1996).', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG ) .']","['Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG ) .']","['There has been some controversy, at least for simple stemmers (Lovins, 1968;Porter, 1980), about the effectiveness of morphological analysis for document retrieval (Harman, 1991;Krovetz, 1993;Hull, 1996).', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG ) .']","['The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG ) .']","['There has been some controversy, at least for simple stemmers (Lovins, 1968;Porter, 1980), about the effectiveness of morphological analysis for document retrieval (Harman, 1991;Krovetz, 1993;Hull, 1996).', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG ) .']","['Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG ) .']"
CC1335,W02-0309,Biomedical text retrieval in languages with a complex morphology,morphosemantic analysis of itis forms in medical language,"['M Pacak', 'L Norton', 'G Dunham']",introduction,,"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 'Â¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (JÃ¤ppinen and NiemistÃ¶, 1988;Choueka, 1990;Popovic and Willett, 1992;EkmekÃ§ioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']",0,"['The problem one faces from an IR point of view is that besides fairly standardized nominal compounds, which already form a regular part of the sublanguage proper, a myriad of ad hoc compounds are formed on the fly which cannot be anticipated when formulating a retrieval query though they appear in relevant documents.', 'Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #AUTHOR_TAG ) , turns out to be infeasible , at least for German and related languages .']","['Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #AUTHOR_TAG ) , turns out to be infeasible , at least for German and related languages .']","['The problem one faces from an IR point of view is that besides fairly standardized nominal compounds, which already form a regular part of the sublanguage proper, a myriad of ad hoc compounds are formed on the fly which cannot be anticipated when formulating a retrieval query though they appear in relevant documents.', 'Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #AUTHOR_TAG ) , turns out to be infeasible , at least for German and related languages .']","['This problem becomes even more pressing for technical sublanguages, such as medical German (e.g., Blut druck mess gera__t translates to device for measuring blood pressure).', 'The problem one faces from an IR point of view is that besides fairly standardized nominal compounds, which already form a regular part of the sublanguage proper, a myriad of ad hoc compounds are formed on the fly which cannot be anticipated when formulating a retrieval query though they appear in relevant documents.', 'Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #AUTHOR_TAG ) , turns out to be infeasible , at least for German and related languages .']","['Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #AUTHOR_TAG ) , turns out to be infeasible , at least for German and related languages .']","['This problem becomes even more pressing for technical sublanguages, such as medical German (e.g., Blut druck mess gera__t translates to device for measuring blood pressure).', 'The problem one faces from an IR point of view is that besides fairly standardized nominal compounds, which already form a regular part of the sublanguage proper, a myriad of ad hoc compounds are formed on the fly which cannot be anticipated when formulating a retrieval query though they appear in relevant documents.', 'Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #AUTHOR_TAG ) , turns out to be infeasible , at least for German and related languages .']","['The problem one faces from an IR point of view is that besides fairly standardized nominal compounds, which already form a regular part of the sublanguage proper, a myriad of ad hoc compounds are formed on the fly which cannot be anticipated when formulating a retrieval query though they appear in relevant documents.', 'Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #AUTHOR_TAG ) , turns out to be infeasible , at least for German and related languages .']","['While one may argue that single-word compounds are quite rare in English (which is not the case in the medical domain either), this is certainly not true for German and other basically agglutinative languages known for excessive single-word nominal compounding.', 'This problem becomes even more pressing for technical sublanguages, such as medical German (e.g., Blut druck mess gera__t translates to device for measuring blood pressure).', 'The problem one faces from an IR point of view is that besides fairly standardized nominal compounds, which already form a regular part of the sublanguage proper, a myriad of ad hoc compounds are formed on the fly which cannot be anticipated when formulating a retrieval query though they appear in relevant documents.', 'Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #AUTHOR_TAG ) , turns out to be infeasible , at least for German and related languages .']","['Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #AUTHOR_TAG ) , turns out to be infeasible , at least for German and related languages .']"
CC1336,W02-0309,Biomedical text retrieval in languages with a complex morphology,towards new measures of information retrieval evaluation,"['W Hersh', 'D Elliot', 'D Hickam', 'S Wolf', 'A Molnar', 'C Leichtenstien']",experiments,"All of the methods currently used to assess information retrieval (IR) systems have limitations in their ability to measure how well users are able to acquire information. We utilized a new approach to assessing information obtained, based on a short-answer test given to senior medical students. Students took the ten-question test and then searched one of two IR systems on the five questions for which they were least certain of their answer Our results showed that pre-searching scores on the test were low but that searching yielded a high proportion of answers with both systems. These methods are able to measure information obtained, and will be used in subsequent studies to assess differences among IR systems.","Alternatively , we may think of user-centered comparative studies ( #AUTHOR_TAG ) .","['Generalizing the interpretation of our data in the light of these findings, we recognize a substantial increase of retrieval performance when query and text tokens are segmented according to the principles of the subword model.', 'The gain is still not overwhelming.', ""With regard to orthographic normalization, we expected a higher performance benefit because of the well-known spelling problems for German medical terms of Latin or Greek origin (such as in 'ZÃ¤kum ', 'CÃ¤kum', 'Zaekum', 'Caekum', 'Zaecum', 'Caecum')."", 'For our experiments, however, we used quite a homogeneous document collection following the spelling standards of medical publishers.', 'The same standards apparently applied to the original multiple choice questions, by which the acquisition of expert queries was guided (cf.', 'Section 3).', 'In the layman queries, there were only few Latin or Greek terms, and, therefore, they did not take advantage of the spelling normalization.', 'However, the experience with medical text retrieval (especially on medical reports which exhibit a high rate of spelling variations) shows that orthographic normalization is a desider-  Whereas the usefulness of Subword Indexing became evident, we could not provide sufficient evidence for Synonym Class Indexing, so far.', 'However, synonym mapping is still incomplete in the current state of our subword dictionary.', 'A question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing.', 'We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5).', 'It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus (MeSH, (NLM, 2001)) are incorporated into our system.', 'Alternatively , we may think of user-centered comparative studies ( #AUTHOR_TAG ) .']",3,"['mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc ÃÂ¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #AUTHOR_TAG ) .']","['This has been reported for other languages , too , dependent on the generality of the chosen approach ( J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc ÃÂ¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #AUTHOR_TAG ) .']","['mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc ÃÂ¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #AUTHOR_TAG ) .']","[""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc ÃÂ¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #AUTHOR_TAG ) .']","['This has been reported for other languages , too , dependent on the generality of the chosen approach ( J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc ÃÂ¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #AUTHOR_TAG ) .']","[""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc ÃÂ¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #AUTHOR_TAG ) .']","['mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc ÃÂ¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #AUTHOR_TAG ) .']","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc ÃÂ¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #AUTHOR_TAG ) .']","['This has been reported for other languages , too , dependent on the generality of the chosen approach ( J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc ÃÂ¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #AUTHOR_TAG ) .', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998).']"
CC1337,W02-0309,Biomedical text retrieval in languages with a complex morphology,effective use of natural language processing techniques for automatic conflation of multiword terms the role of derivational morphology part of speech tagging and shallow parsing,"['E Tzoukermann', 'J Klavans', 'C Jacquemin']",conclusion,,"Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG ) .","['There has been some controversy, at least for simple stemmers (Lovins, 1968;Porter, 1980), about the effectiveness of morphological analysis for document retrieval (Harman, 1991;Krovetz, 1993;Hull, 1996).', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG ) .']",0,"['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .']","['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure']).""]","['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure']).""]","['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .']","['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure'])."", ""The goal is to map all occurring morphological variants to some canonical base forme.g., 'search' in the examples from above.""]","['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure']).""]","['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure'])."", ""The goal is to map all occurring morphological variants to some canonical base forme.g., 'search' in the examples from above.""]","['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .']","['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure'])."", ""The goal is to map all occurring morphological variants to some canonical base forme.g., 'search' in the examples from above.""]"
CC1338,W02-0309,Biomedical text retrieval in languages with a complex morphology,the contribution of morphological knowledge to french mesh mapping for information retrieval,"['P Zweigenbaum', 'S Darmoni', 'N Grabar']",introduction,"MeSH-indexed Internet health directories must provide a mapping from natural language queries to MeSH terms so that both health professionals and the general public can query their contents. We describe here the design of lexical knowledge bases for mapping French expressions to MeSH terms, and the initial evaluation of their contribution to Doc'CISMeF, the search tool of a MeSH-indexed directory of French-language medical Internet resources. The observed trend is in favor of the use of morphological knowledge as a moderate (approximately 5%) but effective factor for improving query to term mapping capabilities.","Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #AUTHOR_TAG ) , turns out to be infeasible , at least for German and related languages .","['While one may argue that single-word compounds are quite rare in English (which is not the case in the medical domain either), this is certainly not true for German and other basically agglutinative languages known for excessive single-word nominal compounding.', 'This problem becomes even more pressing for technical sublanguages, such as medical German (e.g., ï¿½Blut druck mess gera__tï¿½ translates to ï¿½device for measuring blood pressureï¿½).', 'The problem one faces from an IR point of view is that besides fairly standardized nominal compounds, which already form a regular part of the sublanguage proper, a myriad of ad hoc compounds are formed on the fly which cannot be anticipated when formulating a retrieval query though they appear in relevant documents.', 'Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #AUTHOR_TAG ) , turns out to be infeasible , at least for German and related languages .']",0,"['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .']","['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .', 'While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting (Wolff, 1984).']","['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .', 'While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting (Wolff, 1984).']","['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .']","['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .', 'While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting (Wolff, 1984).']","['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .', 'While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting (Wolff, 1984).']","['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .', 'While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting (Wolff, 1984).']","['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .']","['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .', 'While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting (Wolff, 1984).']"
CC1339,W02-0309,Biomedical text retrieval in languages with a complex morphology,morphological typology of languages for ir,['A Pirkola'],introduction,"This paper presents a morphological classification of languages from the IR perspective. Linguistic typology research has shown that the morphological complexity of every language in the world can be described by two variables, index of synthesis and index of fusion. These variables provide a theoretical basis for IR research handling morphological issues. A common theoretical framework is needed in particular because of the increasing significance of cross-language retrieval research and CLIR systems processing different languages. The paper elaborates the linguistic morphological typology for the purposes of IR research. It studies how the indexes of synthesis and fusion could be used as practical tools in mono- and cross-lingual IR research. The need for semantic and syntactic typologies is discussed. The paper also reviews studies made in different languages on the effects of morphology and stemming in IR.","This has been reported for other languages , too , dependent on the generality of the chosen approach ( J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc ÃÂ¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #AUTHOR_TAG ) .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 'Â¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc ÃÂ¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #AUTHOR_TAG ) .', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998).']",0,"['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( #AUTHOR_TAG ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( #AUTHOR_TAG ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( #AUTHOR_TAG ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( #AUTHOR_TAG ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( #AUTHOR_TAG ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( #AUTHOR_TAG ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( #AUTHOR_TAG ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( #AUTHOR_TAG ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( #AUTHOR_TAG ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"
CC1340,W02-0309,Biomedical text retrieval in languages with a complex morphology,responsa an operational fulltext retrieval system with linguistic components for large corpora,['Y Choueka'],introduction,,"Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .","['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J ÃÂ¨ appinen and Niemist ÃÂ¨ o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure'])."", ""The goal is to map all occurring morphological variants to some canonical base forme.g., 'search' in the examples from above.""]",0,"['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( #AUTHOR_TAG ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( #AUTHOR_TAG ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( #AUTHOR_TAG ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( #AUTHOR_TAG ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( #AUTHOR_TAG ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( #AUTHOR_TAG ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( #AUTHOR_TAG ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( #AUTHOR_TAG ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( #AUTHOR_TAG ) .']"
CC1341,W02-0309,Biomedical text retrieval in languages with a complex morphology,the semantic structure of neoclassical compounds,"['A McCray', 'A Browne', 'D Moore']",introduction,,"Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .","['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .', 'While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting (Wolff, 1984).']",0,"['In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .']","['In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .', 'The correspondence between the string and its associated representation tree structure is defined in terms of the sub-correspondence between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation.']","['In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .', 'The correspondence between the string and its associated representation tree structure is defined in terms of the sub-correspondence between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation.']","['In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .']","['In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .', 'The correspondence between the string and its associated representation tree structure is defined in terms of the sub-correspondence between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation.', 'Such correspondence is defined in a way that is able to handle some non-standard cases (e.g.']","['In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .', 'The correspondence between the string and its associated representation tree structure is defined in terms of the sub-correspondence between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation.']","['In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .', 'The correspondence between the string and its associated representation tree structure is defined in terms of the sub-correspondence between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation.', 'Such correspondence is defined in a way that is able to handle some non-standard cases (e.g.']","['In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .']","['In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .', 'The correspondence between the string and its associated representation tree structure is defined in terms of the sub-correspondence between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation.', 'Such correspondence is defined in a way that is able to handle some non-standard cases (e.g.', 'non-projective correspondence).']"
CC1342,W02-1601,A synchronization structure of SSTC and its applications in machine translation,finding structural correspondences from bilingual parsed corpus for corpusbased translation,"['H Watanabe', 'S Kurohashi', 'E Aramaki']",,"In this paper, we describe a system and methods for finding structural correspondences from the paired dependency structures of a source sentence and its translation in a target language. The system we have developed finds word correspondences first, then finds phrasal correspondences based on word correspondences. We have also developed a GUI system with which a user can check and correct the correspondences retrieved by the system. These structural correspondences will be used as raw translation patterns in a corpus-based translation system.","For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( #AUTHOR_TAG ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( #AUTHOR_TAG ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']",0,"['The SSTC is a general structure that can associate an arbitrary tree structure to string in a language as desired by the annotator to be the interpretation structure of the string, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be nonprojective (Boitet & Zaharin, 1988).', 'These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( #AUTHOR_TAG ) .']","['These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( #AUTHOR_TAG ) .', 'crossed dependencies (Tang & Zaharin, 1995).']","['The SSTC is a general structure that can associate an arbitrary tree structure to string in a language as desired by the annotator to be the interpretation structure of the string, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be nonprojective (Boitet & Zaharin, 1988).', 'These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( #AUTHOR_TAG ) .', 'crossed dependencies (Tang & Zaharin, 1995).']","['The SSTC is a general structure that can associate an arbitrary tree structure to string in a language as desired by the annotator to be the interpretation structure of the string, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be nonprojective (Boitet & Zaharin, 1988).', 'These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( #AUTHOR_TAG ) .']","['These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( #AUTHOR_TAG ) .', 'crossed dependencies (Tang & Zaharin, 1995).']","['The SSTC is a general structure that can associate an arbitrary tree structure to string in a language as desired by the annotator to be the interpretation structure of the string, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be nonprojective (Boitet & Zaharin, 1988).', 'These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( #AUTHOR_TAG ) .', 'crossed dependencies (Tang & Zaharin, 1995).']","['The SSTC is a general structure that can associate an arbitrary tree structure to string in a language as desired by the annotator to be the interpretation structure of the string, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be nonprojective (Boitet & Zaharin, 1988).', 'These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( #AUTHOR_TAG ) .', 'crossed dependencies (Tang & Zaharin, 1995).']","['The SSTC is a general structure that can associate an arbitrary tree structure to string in a language as desired by the annotator to be the interpretation structure of the string, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be nonprojective (Boitet & Zaharin, 1988).', 'These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( #AUTHOR_TAG ) .']","['These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( #AUTHOR_TAG ) .', 'crossed dependencies (Tang & Zaharin, 1995).']"
CC1343,W02-1601,A synchronization structure of SSTC and its applications in machine translation,examplebased machine translation based on the synchronous sstc annotation schema,"['M H Al-Adhaileh', 'E K Tang']",,"In this paper, we describe an Example-Based Machine Translation (EBMT) system for English-Malay translation. Our approach is an example-based approach which relies sorely on example translations kept in a Bilingual Knowledge Bank (BKB). In our approach, a flexible annotation schema called Structured String-Tree Correspondence (SSTC) is used to annotate both the source and target sentences of a translation pair. Each SSTC describes a sentence, a representation tree as well as the correspondences between substrings in the sentence and subtrees in the representation tree. With both the source and target SSTCs established, a translation example in the BKB can then be represented effectively in terms of a pair of synchronous SSTCs. In the process of translation, we first try to build the representation tree for the source sentence (English) based on the example-based parsing algorithm as presented in [1]. By referring to the resultant source parse tree, we then proceed to synthesis the target sentence (Malay) based on the target SSTCs as pointed to by the synchronous SSTCs which encode the relationship between source and target SSTCs.","For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( #AUTHOR_TAG ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( #AUTHOR_TAG ) .']",0,"['non-projective correspondence), see Figure 1.', 'Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #AUTHOR_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .']","['Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #AUTHOR_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .']","['non-projective correspondence), see Figure 1.', 'Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #AUTHOR_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .']","['Hence, it is very much desired to define the correspondence in a way to be able to handle the non-standard cases (e.g.', 'non-projective correspondence), see Figure 1.', 'Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #AUTHOR_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .']","['Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #AUTHOR_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .']","['Hence, it is very much desired to define the correspondence in a way to be able to handle the non-standard cases (e.g.', 'non-projective correspondence), see Figure 1.', 'Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #AUTHOR_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .']","['non-projective correspondence), see Figure 1.', 'Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #AUTHOR_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .']","['scrambling, cross serial dependencies, etc.).', 'Hence, it is very much desired to define the correspondence in a way to be able to handle the non-standard cases (e.g.', 'non-projective correspondence), see Figure 1.', 'Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #AUTHOR_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .']","['Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #AUTHOR_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .']"
CC1344,W02-1601,A synchronization structure of SSTC and its applications in machine translation,representation trees and stringtree correspondences,"['C Boitet', 'Y Zaharin']",introduction,,"In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .","['In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .', 'The correspondence between the string and its associated representation tree structure is defined in terms of the sub-correspondence between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation.', 'Such correspondence is defined in a way that is able to handle some non-standard cases (e.g.', 'non-projective correspondence).']",0,"['S-SSTC very well suited for the construction of a BKB, which is needed for the EBMT applications.', '#AUTHOR_TAG presented an approach for constructing a BKB based on the S-SSTC .']",['#AUTHOR_TAG presented an approach for constructing a BKB based on the S-SSTC .'],"['S-SSTC very well suited for the construction of a BKB, which is needed for the EBMT applications.', '#AUTHOR_TAG presented an approach for constructing a BKB based on the S-SSTC .']","[""The proposed S-SSTC annotation schema can fulfill this need, and it is flexible enough to handle different type of relations that may happen between different languages' structures."", 'S-SSTC very well suited for the construction of a BKB, which is needed for the EBMT applications.', '#AUTHOR_TAG presented an approach for constructing a BKB based on the S-SSTC .']",['#AUTHOR_TAG presented an approach for constructing a BKB based on the S-SSTC .'],"[""The proposed S-SSTC annotation schema can fulfill this need, and it is flexible enough to handle different type of relations that may happen between different languages' structures."", 'S-SSTC very well suited for the construction of a BKB, which is needed for the EBMT applications.', '#AUTHOR_TAG presented an approach for constructing a BKB based on the S-SSTC .']","['S-SSTC very well suited for the construction of a BKB, which is needed for the EBMT applications.', '#AUTHOR_TAG presented an approach for constructing a BKB based on the S-SSTC .']","['However, what has so far been lacking is a schema or a framework to annotate and express such extracted lexical and structural correspondences in a flexible and powerful manner.', ""The proposed S-SSTC annotation schema can fulfill this need, and it is flexible enough to handle different type of relations that may happen between different languages' structures."", 'S-SSTC very well suited for the construction of a BKB, which is needed for the EBMT applications.', '#AUTHOR_TAG presented an approach for constructing a BKB based on the S-SSTC .']",['#AUTHOR_TAG presented an approach for constructing a BKB based on the S-SSTC .']
CC1345,W02-1601,A synchronization structure of SSTC and its applications in machine translation,handling crossed dependencies with the stcg,"['E K Tang', 'Y Zaharin']",,,"These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( #AUTHOR_TAG ) .","['The SSTC is a general structure that can associate an arbitrary tree structure to string in a language as desired by the annotator to be the interpretation structure of the string, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be nonprojective (Boitet & Zaharin, 1988).', 'These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( #AUTHOR_TAG ) .', 'crossed dependencies (Tang & Zaharin, 1995).']",0,"['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( #AUTHOR_TAG ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( #AUTHOR_TAG ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( #AUTHOR_TAG ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( #AUTHOR_TAG ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( #AUTHOR_TAG ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( #AUTHOR_TAG ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( #AUTHOR_TAG ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( #AUTHOR_TAG ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( #AUTHOR_TAG ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"
CC1346,W02-1601,A synchronization structure of SSTC and its applications in machine translation,representation trees and stringtree correspondences,"['C Boitet', 'Y Zaharin']",,,"Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #AUTHOR_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .","['In this section, we stress on the fact that in order to describe Natural Language (NL) in a natural manner, three distinct components need to be expressed by the linguistic formalisms; namely, the text, its corresponding abstract linguistic representation and the mapping (correspondence) between these two.', 'Actually, NL is not only a correspondence between different representation levels, as stressed by MTT postulates, but also a sub-correspondence between them.', 'For instance, between the string in a language and its representation tree structure, it is important to specify the sub-correspondences between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation in NLP.', 'It is well known that many linguistic constructions are not projective (e.g.', 'scrambling, cross serial dependencies, etc.).', 'Hence, it is very much desired to define the correspondence in a way to be able to handle the non-standard cases (e.g.', 'non-projective correspondence), see Figure 1.', 'Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #AUTHOR_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .']",0,"['Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #AUTHOR_TAG ) .']","['It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #AUTHOR_TAG ) .']","['Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #AUTHOR_TAG ) .']","['S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #AUTHOR_TAG ) .']","['It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #AUTHOR_TAG ) .']","['S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #AUTHOR_TAG ) .']","['Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #AUTHOR_TAG ) .']","['For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeille et al.,1990), (Harbusch & Poller,1996), or for relating a syntactic TAG and semantic one for the same language (Shieber & Schabes,1990).', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #AUTHOR_TAG ) .']","['It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #AUTHOR_TAG ) .', 'As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.', 'In this case only TAL can be formed in each component.', 'This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.']"
CC1347,W02-1601,A synchronization structure of SSTC and its applications in machine translation,converting a bilingual dictionary into a bilingual knowledge bank based on the synchronous sstc annotation schema,"['M H Al-Adhaileh', 'E K Tang']",,"In this paper, we would like to present an approach to construct a huge Bilingual Knowledge Bank (BKB) from an English Malay bilingual dictionary based on the idea of synchronous Structured String-Tree Correspondence (SSTC). The SSTC is a general structure that can associate an arbitrary tree structure to string in a language as desired by the annotator to be the interpretation structure of the string, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be non-projective. With this structure, we are able to match linguistic units at different inter levels of the structure (i.e. define the correspondence between substrings in the sentence, nodes in the tree, subtrees in the tree and sub-correspondences in the SSTC). This flexibility makes synchronous SSTC very well suited for the construction of a Bilingual Knowledge Bank we need for the English-Malay MT application.",#AUTHOR_TAG presented an approach for constructing a BKB based on the S-SSTC .,"['However, what has so far been lacking is a schema or a framework to annotate and express such extracted lexical and structural correspondences in a flexible and powerful manner.', ""The proposed S-SSTC annotation schema can fulfill this need, and it is flexible enough to handle different type of relations that may happen between different languages' structures."", 'S-SSTC very well suited for the construction of a BKB, which is needed for the EBMT applications.', '#AUTHOR_TAG presented an approach for constructing a BKB based on the S-SSTC .']",0,"['From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ) .']","['From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ) .', 'The MTT point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community.']","['From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ) .', 'The MTT point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community.']","['From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ) .']","['From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ) .', 'The MTT point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community.']","['From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ) .', 'The MTT point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community.']","['From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ) .', 'The MTT point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community.']","['From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ) .']","['From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ) .', 'The MTT point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community.']"
CC1348,W02-1601,A synchronization structure of SSTC and its applications in machine translation,chartbased transfer rule application in machine translation,"['A Meyers', 'M Kosaka', 'R Grishman']",,"Transfer-based Machine Translation systems require a procedure for choosing the set of transfer rules for generating a target language translation from a given source language sentence. In an MT system with many competing transfer rules, choosing the best set of transfer rules for translation may involve the evaluation of an explosive number of competing sets. We propose a solution to this problem based on current best-first chart parsing algorithms.","For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( #AUTHOR_TAG ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( #AUTHOR_TAG ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']",0,"['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #AUTHOR_TAG ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #AUTHOR_TAG ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #AUTHOR_TAG ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #AUTHOR_TAG ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #AUTHOR_TAG ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #AUTHOR_TAG ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #AUTHOR_TAG ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #AUTHOR_TAG ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #AUTHOR_TAG ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"
CC1349,W02-1601,A synchronization structure of SSTC and its applications in machine translation,nonisomorphic synchronous tags,"['K Harbusch', 'P Poller']",,"Synchronous tree{adjoining grammars (S{TAGs) combine two standard tree{adjoining grammars (TAGs), e.g., for language transduction in Machine Translation (MT). Recent advances show that the restriction to isomorphic derivation trees (IS{TAGs) ensures eecient transduction because only tree{adjoining languages can be formed in each component. As a result IS{TAGs only allow for  triv-ial"" transfer rules, due to the fact that only isomorphic derivations can be synchronized. This means that only very similar constructions in the two languages can be translated into each other. To overcome these limitations and provide a way of realizing more complex translation phenomena, this paper introduces a new formalism, the dynamic link synchronous tree{adjoining grammars or DLS{TAGs. This formalism allows for the synchronization of non{isomorphic derivation trees by introducing the new concept of dynamic links. DLS{TAGs are more powerful than IS-TAGs. More precisely speaking, DLS{TAGs allow for the formulation of a non{tree{adjoining language in one of the two components. This makes the translation problem more diicult but not untractable as outlined in this paper. However , there remain non{isomorphic translation phenomena which cannot be handled by DLS-TAGs as we also show in this paper.","It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #AUTHOR_TAG ) .","['The idea of parallelized formalisms is widely used one, and one which has been applied in many different ways.', 'The use of synchronous formalisms is motivated by the desire to describe two languages that are closely related to each other but that do not have the same structures.', 'For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (AbeillÃ© et al.,1990), (Harbusch & Poller,1996), or for relating a syntactic TAG and semantic one for the same language (Shieber & Schabes,1990).', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #AUTHOR_TAG ) .', 'As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.', 'In this case only TAL can be formed in each component.', 'This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'Also contrastive well-known translation phenomena exist in different languages, which cannot be expressed by IS-TAG, Figure 3 illustrates some examples (Shieber, 1994).']",0,"['As mentioned earlier, there are some non-standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #AUTHOR_TAG cases ) .']","['In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #AUTHOR_TAG cases ) .', 'Shieber (1994) cases).']","['As mentioned earlier, there are some non-standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #AUTHOR_TAG cases ) .', 'Shieber (1994) cases).']","['As mentioned earlier, there are some non-standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #AUTHOR_TAG cases ) .']","['In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #AUTHOR_TAG cases ) .', 'Shieber (1994) cases).', 'Due to lack of space we will only brief on some of these non-standard cases without going into the details.']","['As mentioned earlier, there are some non-standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #AUTHOR_TAG cases ) .', 'Shieber (1994) cases).']","['As mentioned earlier, there are some non-standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #AUTHOR_TAG cases ) .', 'Shieber (1994) cases).', 'Due to lack of space we will only brief on some of these non-standard cases without going into the details.']","['As mentioned earlier, there are some non-standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #AUTHOR_TAG cases ) .']","['In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #AUTHOR_TAG cases ) .', 'Shieber (1994) cases).', 'Due to lack of space we will only brief on some of these non-standard cases without going into the details.']"
CC1350,W02-1601,A synchronization structure of SSTC and its applications in machine translation,what is a natural language and how to describe it meaningtext approaches in contrast with generative approaches,['S Kahane'],,"The paper expounds the general conceptions of the Meaning- Text theory about what a natural language is and how it must be de- scribed. In a second part, a formalization of these conceptions - the transductive grammars - is proposed and compared with generative ap- proaches.","From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ) .","['From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ) .', 'The MTT point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community.']",0,"['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( #AUTHOR_TAG ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( #AUTHOR_TAG ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( #AUTHOR_TAG ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( #AUTHOR_TAG ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( #AUTHOR_TAG ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( #AUTHOR_TAG ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( #AUTHOR_TAG ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( #AUTHOR_TAG ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( #AUTHOR_TAG ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"
CC1351,W02-1601,A synchronization structure of SSTC and its applications in machine translation,towards memorybased translation,"['S Sato', 'M Nagao']",,,"For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #AUTHOR_TAG ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #AUTHOR_TAG ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']",0,"['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) , such as the relation between syntax and semantic .']","['Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) , such as the relation between syntax and semantic .']","['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) , such as the relation between syntax and semantic .']","['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) , such as the relation between syntax and semantic .']","['Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) , such as the relation between syntax and semantic .']","['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) , such as the relation between syntax and semantic .']","['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) , such as the relation between syntax and semantic .']","['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) , such as the relation between syntax and semantic .']","['Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) , such as the relation between syntax and semantic .']"
CC1352,W02-1601,A synchronization structure of SSTC and its applications in machine translation,restricting the weak generative capacity of synchronous tree adjoining grammar,['S Shieber'],,,"In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #AUTHOR_TAG cases ) .","['As mentioned earlier, there are some non-standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #AUTHOR_TAG cases ) .', 'Shieber (1994) cases).', 'Due to lack of space we will only brief on some of these non-standard cases without going into the details.']",0,"['Note that these constraints can be used to license only the linguistically meaningful synchronous correspondences between the two SSTCs of the S-SSTC (i.e. between the two languages).', ""For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus""]","[""For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus""]","['Note that these constraints can be used to license only the linguistically meaningful synchronous correspondences between the two SSTCs of the S-SSTC (i.e. between the two languages).', ""For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus""]","['This means allowing one-to-one, one-to-many and many-to-many, but the mappings do not overlap.', 'Note that these constraints can be used to license only the linguistically meaningful synchronous correspondences between the two SSTCs of the S-SSTC (i.e. between the two languages).', ""For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus""]","[""For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus""]","['This means allowing one-to-one, one-to-many and many-to-many, but the mappings do not overlap.', 'Note that these constraints can be used to license only the linguistically meaningful synchronous correspondences between the two SSTCs of the S-SSTC (i.e. between the two languages).', ""For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus""]","['Note that these constraints can be used to license only the linguistically meaningful synchronous correspondences between the two SSTCs of the S-SSTC (i.e. between the two languages).', ""For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus""]","[') are governed by the following constraints:  .', 'This means allowing one-to-one, one-to-many and many-to-many, but the mappings do not overlap.', 'Note that these constraints can be used to license only the linguistically meaningful synchronous correspondences between the two SSTCs of the S-SSTC (i.e. between the two languages).', ""For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus""]","[""For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus""]"
CC1353,W02-1601,A synchronization structure of SSTC and its applications in machine translation,structural matching of parallel texts,"['Y Matsumoto', 'H Ishimoto', 'T Utsuro']",,"This paper describes a nethod for finding structural matching between parallel sentences of two lauguages, (such as Japanese and English). Par- allel sentences are analyzed based on unification grammars, and structural matching is performed by making use of a similarity measure of word pairs in the two languages. Syntactic ambiguities are resolved simultaneously in the matching process. The results serve as a useful source for extracting linguistic and lexical knowledge","For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( #AUTHOR_TAG ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( #AUTHOR_TAG ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']",0,"['Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e.', 'Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e.', 'Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e.', 'Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"
CC1354,W02-1601,A synchronization structure of SSTC and its applications in machine translation,synchronous models of language,"['O Rambow', 'G Satta']",,"In synchronous rewriting, the productions of two rewriting systems are paired and applied synchronously in the derivation of a pair of strings. We present a new synchronous rewriting system and argue that it can handle certain phenomena that are not covered by existing synchronous systems. We also prove some interesting formal/computational properties of our system.Comment: 8 pages uuencoded gzipped ps fil","Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) , such as the relation between syntax and semantic .","['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) , such as the relation between syntax and semantic .']",0,"['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) .']",['Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) .'],"['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) .']","['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) .']",['Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) .'],"['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) .']","['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) .']","['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) .']",['Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) .']
CC1355,W02-1601,A synchronization structure of SSTC and its applications in machine translation,pilot implementation of a bilingual knowledge bank,"['V Sadler', 'R Vendelmans']",,"A Bilingual Knowledge Bank is a syntactically and referentially structured pair of corpora, one being a  translation of the other, in which translation units are  cross-codexl between the corpora. A pilot implementation  is described for a corpus of some 20,000 words  each in English, French and Esperanto which has been cross-coded between English and Esperanto and &apos;between Esperanto and French. The aim is to develop a corpus-based general-purpose knowledge sontee for applicatious in machine translation and computer-  aided translation","For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus","[') are governed by the following constraints:  .', 'This means allowing one-to-one, one-to-many and many-to-many, but the mappings do not overlap.', 'Note that these constraints can be used to license only the linguistically meaningful synchronous correspondences between the two SSTCs of the S-SSTC (i.e. between the two languages).', ""For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus""]",0,"['""up"" (4-5) in ""pick-up"" (1-2+4-5)) in the sentence ""He picks the box up"".', 'For more details on the proprieties of SSTC , see #AUTHOR_TAG .']","['For more details on the proprieties of SSTC , see #AUTHOR_TAG .']","['""up"" (4-5) in ""pick-up"" (1-2+4-5)) in the sentence ""He picks the box up"".', 'For more details on the proprieties of SSTC , see #AUTHOR_TAG .']","['The particle ""up"" is featurised into the verb ""pick"" and in discontinuous manner (e.g.', '""up"" (4-5) in ""pick-up"" (1-2+4-5)) in the sentence ""He picks the box up"".', 'For more details on the proprieties of SSTC , see #AUTHOR_TAG .']","['For more details on the proprieties of SSTC , see #AUTHOR_TAG .']","['The particle ""up"" is featurised into the verb ""pick"" and in discontinuous manner (e.g.', '""up"" (4-5) in ""pick-up"" (1-2+4-5)) in the sentence ""He picks the box up"".', 'For more details on the proprieties of SSTC , see #AUTHOR_TAG .']","['""up"" (4-5) in ""pick-up"" (1-2+4-5)) in the sentence ""He picks the box up"".', 'For more details on the proprieties of SSTC , see #AUTHOR_TAG .']","['The case depicted in Figure 2, describes how the SSTC structure treats some non-standard linguistic phenomena.', 'The particle ""up"" is featurised into the verb ""pick"" and in discontinuous manner (e.g.', '""up"" (4-5) in ""pick-up"" (1-2+4-5)) in the sentence ""He picks the box up"".', 'For more details on the proprieties of SSTC , see #AUTHOR_TAG .']","['For more details on the proprieties of SSTC , see #AUTHOR_TAG .']"
CC1356,W02-1601,A synchronization structure of SSTC and its applications in machine translation,a bestfirst algorithm for automatic extraction of transfer mappings from bilingual corpora,"['A Menezes', 'S Richardson']",,"Translation systems that automatically extract transfer mappings (rules or examples) from bilingual corpora have been hampered by the difficulty of achieving accurate alignment and acquiring high quality mappings. We describe an algorithm that uses a best-first strategy and a small alignment grammar to significantly improve the quality of the transfer mappings extracted. For each mapping, frequencies are computed and sufficient context is retained to distinguish competing mappings during translation. Variants of the algorithm are run against a corpus containing 200K sentence pairs and evaluated based on the quality of resulting translations.","For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e.', 'Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']",0,"['An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ) .']","['A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ) .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']","['An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ) .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']","['It contains a nonprojective correspondence.', 'An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ) .']","['A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ) .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']","['It contains a nonprojective correspondence.', 'An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ) .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']","['An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ) .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']","['Figure 2 illustrates the sentence ""John picks the box up"" with its corresponding SSTC.', 'It contains a nonprojective correspondence.', 'An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ) .']","['A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ) .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']"
CC1357,W02-1601,A synchronization structure of SSTC and its applications in machine translation,synchronous models of language,"['O Rambow', 'G Satta']",introduction,"In synchronous rewriting, the productions of two rewriting systems are paired and applied synchronously in the derivation of a pair of strings. We present a new synchronous rewriting system and argue that it can handle certain phenomena that are not covered by existing synchronous systems. We also prove some interesting formal/computational properties of our system.Comment: 8 pages uuencoded gzipped ps fil",Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) .,"['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) .']",0,"['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , ( Al-Adhaileh & Tang , 1999 ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , ( Al-Adhaileh & Tang , 1999 ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , ( Al-Adhaileh & Tang , 1999 ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , ( Al-Adhaileh & Tang , 1999 ) .']"
CC1358,W02-1601,A synchronization structure of SSTC and its applications in machine translation,representation trees and stringtree correspondences,"['C Boitet', 'Y Zaharin']",,,"For more details on the proprieties of SSTC , see #AUTHOR_TAG .","['The case depicted in Figure 2, describes how the SSTC structure treats some non-standard linguistic phenomena.', 'The particle ""up"" is featurised into the verb ""pick"" and in discontinuous manner (e.g.', '""up"" (4-5) in ""pick-up"" (1-2+4-5)) in the sentence ""He picks the box up"".', 'For more details on the proprieties of SSTC , see #AUTHOR_TAG .']",0,"['Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( #AUTHOR_TAG ) , ( Harbusch & Poller , 2000 ) .']","['It allows the construction of a non-TAL ( #AUTHOR_TAG ) , ( Harbusch & Poller , 2000 ) .']","['Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( #AUTHOR_TAG ) , ( Harbusch & Poller , 2000 ) .']","['S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( #AUTHOR_TAG ) , ( Harbusch & Poller , 2000 ) .']","['It allows the construction of a non-TAL ( #AUTHOR_TAG ) , ( Harbusch & Poller , 2000 ) .']","['S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( #AUTHOR_TAG ) , ( Harbusch & Poller , 2000 ) .']","['Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( #AUTHOR_TAG ) , ( Harbusch & Poller , 2000 ) .']","['For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeille et al.,1990), (Harbusch & Poller,1996), or for relating a syntactic TAG and semantic one for the same language (Shieber & Schabes,1990).', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( #AUTHOR_TAG ) , ( Harbusch & Poller , 2000 ) .']","['It allows the construction of a non-TAL ( #AUTHOR_TAG ) , ( Harbusch & Poller , 2000 ) .', 'As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.', 'In this case only TAL can be formed in each component.', 'This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.']"
CC1359,W02-1601,A synchronization structure of SSTC and its applications in machine translation,natural language analysis in machine translation mt based on the stringtree correspondence grammar stcg,['E K Tang'],,"The formalism is argued to be a totally declarative grammar formalism that can associate, to strings in a language, arbitrary tree structures as desired by the grammar writer to be the linguistic representation structures of the strings. More importantly is the facility to specify the correspondence between the string and the associated tree in a very natural manner. These features are very much desired in grammar writing, in particular for the treatment of certain linguistic phenomena which are 'non-standard', namely featurisation, lexicalisation and crossed dependencies [2,3]. Furthermore, a grammar written in this way naturally inherits the desired property of bi-directionality (in fact non-directionality [4]) such that the same grammar can be interpreted for both analysis and generation.",A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ) .,"['Figure 2 illustrates the sentence ""John picks the box up"" with its corresponding SSTC.', 'It contains a nonprojective correspondence.', 'An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ) .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']",5,"['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']","['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"
CC1360,W02-1601,A synchronization structure of SSTC and its applications in machine translation,achieving commercialquality translation with examplebased methods,"['S Richardson', 'W Dolan', 'A Menezes', 'J Pinkham']",,,"For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , ( Al-Adhaileh & Tang , 1999 ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , ( Al-Adhaileh & Tang , 1999 ) .']",0,"['An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .']","['A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']","['An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']","['It contains a nonprojective correspondence.', 'An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .']","['A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']","['It contains a nonprojective correspondence.', 'An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']","['An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']","['Figure 2 illustrates the sentence ""John picks the box up"" with its corresponding SSTC.', 'It contains a nonprojective correspondence.', 'An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .']","['A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']"
CC1361,W02-1601,A synchronization structure of SSTC and its applications in machine translation,restricting the weak generative capacity of synchronous tree adjoining grammar,['S Shieber'],,,"It allows the construction of a non-TAL ( #AUTHOR_TAG ) , ( Harbusch & Poller , 2000 ) .","['The idea of parallelized formalisms is widely used one, and one which has been applied in many different ways.', 'The use of synchronous formalisms is motivated by the desire to describe two languages that are closely related to each other but that do not have the same structures.', 'For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (AbeillÃ© et al.,1990), (Harbusch & Poller,1996), or for relating a syntactic TAG and semantic one for the same language (Shieber & Schabes,1990).', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( #AUTHOR_TAG ) , ( Harbusch & Poller , 2000 ) .', 'As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.', 'In this case only TAL can be formed in each component.', 'This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'Also contrastive well-known translation phenomena exist in different languages, which cannot be expressed by IS-TAG, Figure 3 illustrates some examples (Shieber, 1994).']",0,"['However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #AUTHOR_TAG ) .']","['However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #AUTHOR_TAG ) .', 'Recent work (Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data.']","['However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #AUTHOR_TAG ) .', 'Recent work (Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data.']","['However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #AUTHOR_TAG ) .']","['However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #AUTHOR_TAG ) .', 'Recent work (Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data.', 'Also, many potential applications of NLP will involve processing very large text databases.']","['However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #AUTHOR_TAG ) .', 'Recent work (Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data.']","['However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #AUTHOR_TAG ) .', 'Recent work (Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data.', 'Also, many potential applications of NLP will involve processing very large text databases.']","['However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #AUTHOR_TAG ) .']","['However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #AUTHOR_TAG ) .', 'Recent work (Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data.', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.']"
CC1362,W02-1601,A synchronization structure of SSTC and its applications in machine translation,examplebased machine translation,['S Sato'],,"Example-based machine translation (EBMT) systems, so far, rely on heuristic measures in re-trieving translation examples. Such a heuristic measure costs time to adjust, and might make its algorithm unclear. This paper presents a probabilistic model for EBMT. Under the pro-posed model, the system searches the transla-tion example combination which has the high-est probability. The proposed model clearly for-malizes EBMT process. In addition, the model can naturally incorporate the context similarity of translation examples. The experimental re-sults demonstrate that the proposed model has a slightly better translation quality than state-of-the-art EBMT systems.","For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']",0,"['This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ) .']","['There have already been several attempts to develop distributed NLP systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ) .', 'Web services will allow components developed by different researchers in different locations to be composed to build larger systems.']","['This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ) .', 'Web services will allow components developed by different researchers in different locations to be composed to build larger systems.']","['Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI.', 'This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ) .']","['There have already been several attempts to develop distributed NLP systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ) .', 'Web services will allow components developed by different researchers in different locations to be composed to build larger systems.']","['Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI.', 'This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ) .', 'Web services will allow components developed by different researchers in different locations to be composed to build larger systems.']","['This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ) .', 'Web services will allow components developed by different researchers in different locations to be composed to build larger systems.']","['A web service provides a remote procedure that can be called using XML based encodings (XMLRPC or SOAP) of function names, arguments and results transmitted via internet protocols such as HTTP.', 'Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI.', 'This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ) .']","['There have already been several attempts to develop distributed NLP systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ) .', 'Web services will allow components developed by different researchers in different locations to be composed to build larger systems.']"
CC1363,W02-1601,A synchronization structure of SSTC and its applications in machine translation,representation trees and stringtree correspondences,"['C Boitet', 'Y Zaharin']",,,"A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .","['Figure 2 illustrates the sentence ""John picks the box up"" with its corresponding SSTC.', 'It contains a nonprojective correspondence.', 'An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']",5,"['Efficient training is required because the amount of data available for training will increase significantly.', 'Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( #AUTHOR_TAG ) .']","['Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( #AUTHOR_TAG ) .', 'Processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly.']","['Efficient training is required because the amount of data available for training will increase significantly.', 'Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( #AUTHOR_TAG ) .', 'Processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly.']","['Efficiency is required both in training and processing.', 'Efficient training is required because the amount of data available for training will increase significantly.', 'Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( #AUTHOR_TAG ) .']","['Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( #AUTHOR_TAG ) .', 'Processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly.']","['Efficiency is required both in training and processing.', 'Efficient training is required because the amount of data available for training will increase significantly.', 'Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( #AUTHOR_TAG ) .', 'Processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly.']","['Efficient training is required because the amount of data available for training will increase significantly.', 'Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( #AUTHOR_TAG ) .', 'Processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly.']","['As discussed earlier, there are two main requirements of the system that are covered by ""high performance"": speed and state of the art accuracy.', 'Efficiency is required both in training and processing.', 'Efficient training is required because the amount of data available for training will increase significantly.', 'Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( #AUTHOR_TAG ) .']","['Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( #AUTHOR_TAG ) .', 'Processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly.']"
CC1364,W03-0806,Blueprint for a high performance NLP infrastructure,english gigaword corpus catalogue number ldc2003t05,['Linguistic Data Consortium'],introduction,,"However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #AUTHOR_TAG ) .","['However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #AUTHOR_TAG ) .', 'Recent work (Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data.', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.', 'Other potential applications must process text online or in realtime.', 'For example, Google currently answers 250 million queries per day, thus processing time must be minimised.', 'Clearly, efficient NLP components will need to be developed.', 'At the same time, state-of-the-art performance will be needed for these systems to be of practical use.']",0,"['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #AUTHOR_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .']","['Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #AUTHOR_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).']","['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #AUTHOR_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .']","['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #AUTHOR_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .']","['Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #AUTHOR_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.']","['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #AUTHOR_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).']","['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #AUTHOR_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.']","['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #AUTHOR_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .']","['Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #AUTHOR_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']"
CC1365,W03-0806,Blueprint for a high performance NLP infrastructure,dialogue interaction with the darpa communicator infrastructure the development of useful software,"['Samuel Bayer', 'Christine Doran', 'Bryan George']",,"To support engaging human users in robust, mixed-initiative speech dialogue interactions which reach beyond current capabilities in dialogue systems, the DARPA Communicator program [1] is funding the development of a distributed message-passing infrastructure for dialogue systems which all Communicator participants are using. In this presentation, we describe the features of and requirements for a genuinely useful software infrastructure for this purpose.","There have already been several attempts to develop distributed NLP systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ) .","['The final interface we intend to implement is a collection of web services for NLP.', 'A web service provides a remote procedure that can be called using XML based encodings (XMLRPC or SOAP) of function names, arguments and results transmitted via internet protocols such as HTTP.', 'Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI.', 'This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ) .', 'Web services will allow components developed by different researchers in different locations to be composed to build larger systems.']",0,"['This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ) .']","['There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ) .', 'Web services will allow components developed by different researchers in different locations to be composed to build larger systems.']","['This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ) .', 'Web services will allow components developed by different researchers in different locations to be composed to build larger systems.']","['Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI.', 'This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ) .']","['There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ) .', 'Web services will allow components developed by different researchers in different locations to be composed to build larger systems.']","['Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI.', 'This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ) .', 'Web services will allow components developed by different researchers in different locations to be composed to build larger systems.']","['This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ) .', 'Web services will allow components developed by different researchers in different locations to be composed to build larger systems.']","['A web service provides a remote procedure that can be called using XML based encodings (XMLRPC or SOAP) of function names, arguments and results transmitted via internet protocols such as HTTP.', 'Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI.', 'This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ) .']","['There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ) .', 'Web services will allow components developed by different researchers in different locations to be composed to build larger systems.']"
CC1366,W03-0806,Blueprint for a high performance NLP infrastructure,combining labeled and unlabeled data with cotraining,"['Avrim Blum', 'Tom Mitchell']",,,"Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( #AUTHOR_TAG ) .","['As discussed earlier, there are two main requirements of the system that are covered by ""high performance"": speed and state of the art accuracy.', 'Efficiency is required both in training and processing.', 'Efficient training is required because the amount of data available for training will increase significantly.', 'Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( #AUTHOR_TAG ) .', 'Processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly.']",0,"['The Generative Programming approach to NLP infrastructure development will allow tools such as sentence boundary detectors, POS taggers, chunkers and named entity recognisers to be rapidly composed from many elemental components.', 'For instance , implementing an efficient version of the MXPOST POS tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .']","['For instance , implementing an efficient version of the MXPOST POS tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .']","['The Generative Programming approach to NLP infrastructure development will allow tools such as sentence boundary detectors, POS taggers, chunkers and named entity recognisers to be rapidly composed from many elemental components.', 'For instance , implementing an efficient version of the MXPOST POS tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .']","['The Generative Programming approach to NLP infrastructure development will allow tools such as sentence boundary detectors, POS taggers, chunkers and named entity recognisers to be rapidly composed from many elemental components.', 'For instance , implementing an efficient version of the MXPOST POS tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .']","['For instance , implementing an efficient version of the MXPOST POS tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .']","['The Generative Programming approach to NLP infrastructure development will allow tools such as sentence boundary detectors, POS taggers, chunkers and named entity recognisers to be rapidly composed from many elemental components.', 'For instance , implementing an efficient version of the MXPOST POS tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .']","['The Generative Programming approach to NLP infrastructure development will allow tools such as sentence boundary detectors, POS taggers, chunkers and named entity recognisers to be rapidly composed from many elemental components.', 'For instance , implementing an efficient version of the MXPOST POS tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .']","['The Generative Programming approach to NLP infrastructure development will allow tools such as sentence boundary detectors, POS taggers, chunkers and named entity recognisers to be rapidly composed from many elemental components.', 'For instance , implementing an efficient version of the MXPOST POS tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .']","['For instance , implementing an efficient version of the MXPOST POS tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .']"
CC1367,W03-0806,Blueprint for a high performance NLP infrastructure,gate â a general architecture for text engineering,"['Hamish Cunningham', 'Yorick Wilks', 'Robert J Gaizauskas']",experiments,"This paper presents the design, implementation and evaluation of GATE, a General Architecture for Text Engineering.GATE lies at the intersection of human language computation and software engineering, and constitutes aninfrastructural system supporting research and development of languageprocessing software.","Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #AUTHOR_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .","['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #AUTHOR_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']",0,"['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .']","['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.']","['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.']","['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .']","['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.']","['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.']","['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.']","['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .']","['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999).']"
CC1368,W03-0806,Blueprint for a high performance NLP infrastructure,a distributed architecture for robust automatic speech recognition,"['Kadri Hacioglu', 'Bryan Pellom']",,"In this paper, we attempt to decompose a state-of-the-art speech recognition system into its components and define an infrastructure that allows a flexible, efficient and effective interaction among the components. Motivated by the success of DARPA Communicator program, we select the open source Galaxy architecture as our development test bed. It consists of a hub that allows communication among servers connected to it by message passing and supports the plug-and-play paradigm. In addition to message passing it supports high bandwidth data (binary or audio) transfer between servers via a brokering scheme. For several reasons, we believe that it is the right time to start developing a distributed framework for speech recognition along with data and protocol standards supporting interoperability. We present our work towards that goal using the Colorado University (CU) Sonic recognizer. We divide Sonic into a number of components and structure it around the Hub. We describe the system in some detail and report on its present status with some possibilities for future development. 1","There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ) .","['The final interface we intend to implement is a collection of web services for NLP.', 'A web service provides a remote procedure that can be called using XML based encodings (XMLRPC or SOAP) of function names, arguments and results transmitted via internet protocols such as HTTP.', 'Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI.', 'This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ) .', 'Web services will allow components developed by different researchers in different locations to be composed to build larger systems.']",0,"['The implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .']","['The implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .', 'We have already implemented a P O S tagger, chunker, C C G supertagger and named entity recogniser using the infrastructure.']","['The implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .', 'We have already implemented a P O S tagger, chunker, C C G supertagger and named entity recogniser using the infrastructure.']","['The implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .']","['The implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .', 'We have already implemented a P O S tagger, chunker, C C G supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training ma- terials and tag faster than T N T , the fastest existing P O S tagger.']","['The implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .', 'We have already implemented a P O S tagger, chunker, C C G supertagger and named entity recogniser using the infrastructure.']","['The implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .', 'We have already implemented a P O S tagger, chunker, C C G supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training ma- terials and tag faster than T N T , the fastest existing P O S tagger.']","['The implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .']","['The implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .', 'We have already implemented a P O S tagger, chunker, C C G supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training ma- terials and tag faster than T N T , the fastest existing P O S tagger.', 'These tools use a highly optimised G I S imple- mentation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999).']"
CC1369,W03-0806,Blueprint for a high performance NLP infrastructure,a maximum entropy partofspeech tagger,['Adwait Ratnaparkhi'],conclusion,,"For instance , implementing an efficient version of the MXPOST POS tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .","['The Generative Programming approach to NLP infrastructure development will allow tools such as sentence boundary detectors, POS taggers, chunkers and named entity recognisers to be rapidly composed from many elemental components.', 'For instance , implementing an efficient version of the MXPOST POS tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .']",3,"['However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) .']","['An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).']","['However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).']","['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) .']","['An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']","['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).']","['However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']","['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) .']","['An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']"
CC1370,W03-0806,Blueprint for a high performance NLP infrastructure,bootstrapping postaggers using unlabelled data,"['Stephen Clark', 'James R Curran', 'Miles Osborne']",experiments,"This paper investigates booststrapping part-of-speech taggers using co-training, in which two taggers are iteratively re-trained on each other's output. Since the output of the taggers is noisy, there is a question of which newly labelled examples to add to the training set. We investigate selecting examples by directly maximising tagger agreement on unlabelled data, a method which has been theoretically and empirically motivated in the co-training literature. Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost.","The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .","['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999).', 'We expect even faster training times when we move to conjugate gradient methods.']",4,"['This gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ) .']","['Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ) .', 'However, the source code for these tools is not freely available, so they cannot be extended.']","['This gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ) .', 'However, the source code for these tools is not freely available, so they cannot be extended.']","['These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', 'This gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ) .']","['Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ) .', 'However, the source code for these tools is not freely available, so they cannot be extended.']","['These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', 'This gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ) .', 'However, the source code for these tools is not freely available, so they cannot be extended.']","['This gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ) .', 'However, the source code for these tools is not freely available, so they cannot be extended.']","['For example, the suite of LT tools (Mikheev et al., 1999;Grover et al., 2000) perform tokenization, tagging and chunking on XML marked-up text directly.', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', 'This gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ) .']","['Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ) .', 'However, the source code for these tools is not freely available, so they cannot be extended.']"
CC1371,W03-0806,Blueprint for a high performance NLP infrastructure,scaling context space,"['James R Curran', 'Marc Moens']",experiments,"This paper proposes a computationally feasible method for measuring the context-sensitive semantic distance between words. The distance is computed by adaptive scaling of a semantic space. In the semantic space, each word in the vocabulary V is represented by a multidimensional vector which is extracted from an English dictionary through principal component analysis. Given a word set C which specifies a context, each dimension of the semantic space is scaled up or down according to the distribution of C in the semantic space. In the space thus transformed, the distance between words in V  becomes dependent on the context C. An evaluation through a word prediction task shows that the proposed measurement successfully extracts the context of a text. 1 Introduction  Semantic distance (or similarity) between words is one of the basic measurements used in many fields of natural language processing, information retrieval, etc. Word distance provides bottom-up information for text understandi..","The implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .","['The implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .', 'We have already implemented a P O S tagger, chunker, C C G supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training ma- terials and tag faster than T N T , the fastest existing P O S tagger.', 'These tools use a highly optimised G I S imple- mentation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999).', 'We expect even faster train- ing times when we move to conjugate gradient methods.']",4,"['However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003).', 'Recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data .']","['Recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.']","['However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003).', 'Recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data .']","['However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003).', 'Recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data .']","['Recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.']","['However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003).', 'Recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.']","['However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003).', 'Recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.']","['However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003).', 'Recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data .']","['Recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.', 'Other potential applications must process text online or in realtime.']"
CC1372,W03-0806,Blueprint for a high performance NLP infrastructure,maximum entropy models for natural language ambiguity resolution,['Adwait Ratnaparkhi'],experiments,"This thesis demonstrates that several important kinds of natural language ambiguities can be resolved to state-of-the-art accuracies using a single statistical modeling technique based on the principle of maximum entropy.  We discuss the problems of sentence boundary detection, part-of-speech tagging, prepositional phrase attachment, natural language parsing, and text categorization under the maximum entropy framework. In practice, we have found that maximum entropy models offer the following advantages:  State-of-the-art accuracy. The probability models for all of the tasks discussed perform at or near state-of-the-art accuracies, or outperform competing learning algorithms when trained and tested under similar conditions. Methods which outperform those presented here require much more supervision in the form of additional human involvement or additional supporting resources.  Knowledge-poor features. The facts used to model the data, or features, are linguistically very simple, or ""knowledge-poor"", but yet succeed in approximating complex linguistic relationships.  Reusable software technology. The mathematics of the maximum entropy framework are essentially independent of any particular task, and a single software implementation can be used for all of the probability models in this thesis.  The experiments in this thesis suggest that experimenters can obtain state-of-the-art accuracies on a wide range of natural language tasks, with little task-specific effort, by using maximum entropy probability models.","An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) .","['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']",0,"['Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #AUTHOR_TAG ) , currently used for training POS taggers .']","['For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #AUTHOR_TAG ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations.']","['Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #AUTHOR_TAG ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations.']","['NLP is experiencing an explosion in the quantity of electronic text available.', 'Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #AUTHOR_TAG ) , currently used for training POS taggers .']","['For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #AUTHOR_TAG ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations.']","['NLP is experiencing an explosion in the quantity of electronic text available.', 'Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #AUTHOR_TAG ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations.']","['Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #AUTHOR_TAG ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations.']","['NLP is experiencing an explosion in the quantity of electronic text available.', 'Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #AUTHOR_TAG ) , currently used for training POS taggers .']","['For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #AUTHOR_TAG ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations.']"
CC1373,W03-0806,Blueprint for a high performance NLP infrastructure,a rational design for a weighted finitestate transducer library,"['Mehryar Mohri', 'Fernando C N Pereira', 'Michael Riley']",experiments,,"Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ) .","['A number of stand-alone tools have also been developed.', 'For example, the suite of LT tools (Mikheev et al., 1999;Grover et al., 2000) perform tokenization, tagging and chunking on XML marked-up text directly.', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', 'This gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ) .', 'However, the source code for these tools is not freely available, so they cannot be extended.']",0,"['Python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation.', 'It has already been used to implement a framework for teaching NLP ( #AUTHOR_TAG ) .']",['It has already been used to implement a framework for teaching NLP ( #AUTHOR_TAG ) .'],"['Python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation.', 'It has already been used to implement a framework for teaching NLP ( #AUTHOR_TAG ) .']","['Python has a number of advantages over other options, such as Java and Perl.', 'Python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation.', 'It has already been used to implement a framework for teaching NLP ( #AUTHOR_TAG ) .']",['It has already been used to implement a framework for teaching NLP ( #AUTHOR_TAG ) .'],"['Python has a number of advantages over other options, such as Java and Perl.', 'Python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation.', 'It has already been used to implement a framework for teaching NLP ( #AUTHOR_TAG ) .']","['Python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation.', 'It has already been used to implement a framework for teaching NLP ( #AUTHOR_TAG ) .']","['To overcome this problem we have implemented an interface to the infrastructure in the Python scripting language.', 'Python has a number of advantages over other options, such as Java and Perl.', 'Python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation.', 'It has already been used to implement a framework for teaching NLP ( #AUTHOR_TAG ) .']",['It has already been used to implement a framework for teaching NLP ( #AUTHOR_TAG ) .']
CC1374,W03-0806,Blueprint for a high performance NLP infrastructure,scaling context space,"['James R Curran', 'Marc Moens']",introduction,"This paper proposes a computationally feasible method for measuring the context-sensitive semantic distance between words. The distance is computed by adaptive scaling of a semantic space. In the semantic space, each word in the vocabulary V is represented by a multidimensional vector which is extracted from an English dictionary through principal component analysis. Given a word set C which specifies a context, each dimension of the semantic space is scaled up or down according to the distribution of C in the semantic space. In the space thus transformed, the distance between words in V  becomes dependent on the context C. An evaluation through a word prediction task shows that the proposed measurement successfully extracts the context of a text. 1 Introduction  Semantic distance (or similarity) between words is one of the basic measurements used in many fields of natural language processing, information retrieval, etc. Word distance provides bottom-up information for text understandi..","Recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data .","['However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003).', 'Recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.', 'Other potential applications must process text online or in realtime.', 'For example, Google currently answers 250 million queries per day, thus processing time must be minimised.', 'Clearly, efficient NLP components will need to be developed.', 'At the same time, state-of-the-art performance will be needed for these systems to be of practical use.']",0,"['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( Cunningham et al. , 1997 ) and the Alembic Workbench ( #AUTHOR_TAG ) ) as well as NLP tools and resources that can be manipulated from the GUI .']","['Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( Cunningham et al. , 1997 ) and the Alembic Workbench ( #AUTHOR_TAG ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).']","['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( Cunningham et al. , 1997 ) and the Alembic Workbench ( #AUTHOR_TAG ) ) as well as NLP tools and resources that can be manipulated from the GUI .']","['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( Cunningham et al. , 1997 ) and the Alembic Workbench ( #AUTHOR_TAG ) ) as well as NLP tools and resources that can be manipulated from the GUI .']","['Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( Cunningham et al. , 1997 ) and the Alembic Workbench ( #AUTHOR_TAG ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.']","['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( Cunningham et al. , 1997 ) and the Alembic Workbench ( #AUTHOR_TAG ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).']","['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( Cunningham et al. , 1997 ) and the Alembic Workbench ( #AUTHOR_TAG ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.']","['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( Cunningham et al. , 1997 ) and the Alembic Workbench ( #AUTHOR_TAG ) ) as well as NLP tools and resources that can be manipulated from the GUI .']","['Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( Cunningham et al. , 1997 ) and the Alembic Workbench ( #AUTHOR_TAG ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']"
CC1375,W03-0806,Blueprint for a high performance NLP infrastructure,building a large annotated corpus of english the penn treebank computational linguistics,"['Mitchell Marcus', 'Beatrice Santorini', 'Mary Marcinkiewicz']",introduction,,"For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #AUTHOR_TAG ) , currently used for training POS taggers .","['NLP is experiencing an explosion in the quantity of electronic text available.', 'Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #AUTHOR_TAG ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations.']",0,"['General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ) .']","['For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ) .', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.']","['General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ) .', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.']","['Many provide graphical user interfaces (GUI) for manual annotation (e.g.', 'General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ) .']","['For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ) .', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']","['Many provide graphical user interfaces (GUI) for manual annotation (e.g.', 'General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ) .', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.']","['General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ) .', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']","['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces (GUI) for manual annotation (e.g.', 'General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ) .']","['For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ) .', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']"
CC1376,W03-0806,Blueprint for a high performance NLP infrastructure,nltk the natural language toolkit,"['Edward Loper', 'Steven Bird']",,,It has already been used to implement a framework for teaching NLP ( #AUTHOR_TAG ) .,"['Although C++ is extremely efficient, it is not suitable for rapidly gluing components together to form new tools.', 'To overcome this problem we have implemented an interface to the infrastructure in the Python scripting language.', 'Python has a number of advantages over other options, such as Java and Perl.', 'Python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation.', 'It has already been used to implement a framework for teaching NLP ( #AUTHOR_TAG ) .']",2,['Software engineering research on Generative Programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .'],"['Software engineering research on Generative Programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .', 'Our infrastructure for NLP will provide high performance 1 components inspired by Generative Programming principles.']","['Software engineering research on Generative Programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .', 'Our infrastructure for NLP will provide high performance 1 components inspired by Generative Programming principles.']",['Software engineering research on Generative Programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .'],"['Software engineering research on Generative Programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .', 'Our infrastructure for NLP will provide high performance 1 components inspired by Generative Programming principles.']","['Software engineering research on Generative Programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .', 'Our infrastructure for NLP will provide high performance 1 components inspired by Generative Programming principles.']","['Software engineering research on Generative Programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .', 'Our infrastructure for NLP will provide high performance 1 components inspired by Generative Programming principles.']",['Software engineering research on Generative Programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .'],"['Software engineering research on Generative Programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .', 'Our infrastructure for NLP will provide high performance 1 components inspired by Generative Programming principles.']"
CC1377,W03-0806,Blueprint for a high performance NLP infrastructure,mixedinitiative development of language processing systems,"['David Day', 'John Aberdeen', 'Lynette Hirschman', 'Robyn Kozierok', 'Patricia Robinson', 'Marc Vilain']",experiments,"Historically, tailoring language processing systems to specific domains and languages for which they were not originally built has required a great deal of effort. Recent advances in corpus-based manual and automatic training methods have shown promise in reducing the time and cost of this porting process. These developments have focused even greater attention on the bottleneck of acquiring reliable, manually tagged training data. This paper describes a new set of integrated tools, collectively called the Alembic Workbench, that uses a mixed-initiative approach to ""bootstrapping"" the manual tagging process, with the goal of reducing the overhead associated with corpus development. Initial empirical studies using the Alembic Workbench to annotate ""named entities"" demonstrates that this approach can approximately double the production rate. As an added benefit, the combined efforts of machine and user produce domain specific annotation rules that can be used to annotate similar texts automatically through the Alembic-NLP system. The ultimate goal of this project is to enable end users to generate a practical domain-specific information extraction system within a single session.","Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( Cunningham et al. , 1997 ) and the Alembic Workbench ( #AUTHOR_TAG ) ) as well as NLP tools and resources that can be manipulated from the GUI .","['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( Cunningham et al. , 1997 ) and the Alembic Workbench ( #AUTHOR_TAG ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']",0,"['Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( #AUTHOR_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers .']","['For example , 10 million words of the American National Corpus ( #AUTHOR_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations.']","['Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( #AUTHOR_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations.']","['NLP is experiencing an explosion in the quantity of electronic text available.', 'Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( #AUTHOR_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers .']","['For example , 10 million words of the American National Corpus ( #AUTHOR_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations.']","['NLP is experiencing an explosion in the quantity of electronic text available.', 'Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( #AUTHOR_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations.']","['Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( #AUTHOR_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations.']","['NLP is experiencing an explosion in the quantity of electronic text available.', 'Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( #AUTHOR_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers .']","['For example , 10 million words of the American National Corpus ( #AUTHOR_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations.']"
CC1378,W03-0806,Blueprint for a high performance NLP infrastructure,developing language processing components with gate,"['Hamish Cunningham', 'Diana Maynard', 'C Ursu K Bontcheva', 'V Tablan', 'M Dimitrov']",experiments,"Fluid leakage through soil in a region thereof is controlled by sequentially passing over the region to dig a plurality of parallel, laterally displaced grooves in the surface. Soil dug from each groove is temporarily stored, and a strip of sheet material is laid over a groove as it is created during each pass, the width of the strip being greater than the width of the groove. Thereafter, the temporarily stored soil is deposited on the strip such that it is covered with soil except along one edge, the other edge of the strip overlying the uncovered edge of an adjacent strip laid down during a previous pass over the region. As a consequence, a first layer of overlapping strips of sheet material covered with soil is installed over the region.","For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ) .","['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces (GUI) for manual annotation (e.g.', 'General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ) .', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']",0,"['A number of stand-alone tools have also been developed.', 'For example , the suite of LT tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly .']","['For example , the suite of LT tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.']","['A number of stand-alone tools have also been developed.', 'For example , the suite of LT tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly .']","['A number of stand-alone tools have also been developed.', 'For example , the suite of LT tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly .']","['For example , the suite of LT tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', 'This gives a greater flexibility but the tradeoff is that these tools can run very slowly.']","['A number of stand-alone tools have also been developed.', 'For example , the suite of LT tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.']","['A number of stand-alone tools have also been developed.', 'For example , the suite of LT tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', 'This gives a greater flexibility but the tradeoff is that these tools can run very slowly.']","['A number of stand-alone tools have also been developed.', 'For example , the suite of LT tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly .']","['For example , the suite of LT tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', 'This gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'Other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998).']"
CC1379,W03-0806,Blueprint for a high performance NLP infrastructure,generative programming methods tools and applications,"['Krzysztof Czarnecki', 'Ulrich W Eisenecker']",introduction,"1. What Is This Book About? From Handcrafting to Automated Assembly Lines. Generative Programming. Benefits and Applicability. I. ANALYSIS AND DESIGN METHODS AND TECHNIQUES. 2. Domain Engineering. Why Is This Chapter Worth Reading? What Is Domain Engineering? Domain Analysis. Domain Design and Domain Implementation. Application Engineering. Product-Line Practices. Key Domain Engineering Concepts. Domain. Domain Scope and Scoping. Relationships between Domains. Features and Feature Models. Method Tailoring and Specialization. Survey of Domain Analysis and Domain Engineering Methods. Feature-Oriented Domain Analysis (FODA). Organization Domain Modeling (ODM). Draco. Capture. Domain Analysis and Reuse Environment (DARE). Domain-Specific Software Architecture (DSSA) Approach. Algebraic Approach. Other Approaches. Domain Engineering and Related Approaches. Historical Notes. Summary. 3. Domain Engineering and Object-Oriented Analysis and Design. Why Is This Chapter Worth Reading? OO Technology and Reuse. Solution Space. Problem Space. Relationship between Domain Engineering and Object-Oriented Analysis and Design (OOA/D) Methods. Aspects of Integrating Domain Engineering and OOA/D Methods. Horizontal versus Vertical Methods. Selected Methods. Rational Unified Process. 00ram. Reuse-Driven Software Engineering Business (RSEB). FeatuRSEB. Domain Engineering Method for Reusable Algorithmic Libraries (DEMRAL). 4. Feature Modeling. Why Is This Chapter Worth Reading? Features Revisited. Feature Modeling. Feature Models. Feature Diagrams. Other Infon-Nation Associated with Feature Diagrams in a Feature Model. Assigning Priorities to Variable Features. Availability Sites, Binding Sites, and Binding Modes. Relationship between Feature Diagrams and Other Modeling Notations and Implementation Techniques. Single Inheritance. Multiple Inheritance. Parameterized Inheritance. Static Parameterization. Dynamic Parameterization. Implementing Constraints. Tool Support for Feature Models. Frequently Asked Questions about Feature Diagrams. Feature Modeling Process. How to Find Features. Role of Variability in Modeling. 5. The Process of Generative Programming. Why Is This Chapter Worth Reading? Generative Domain Models. Main Development Steps in Generative Programming. Adapting Domain Engineering for Generative Programming. Domain-Specific Languages. DEMRAL: Example of a Domain Engineering Method for Generative Programming. Outline of DEMRAL. Domain Analysis. Domain Definition. Domain Modeling. Domain Design. Scope Domain Model for Implementation. Identify Packages. Develop Target Architectures and Identify the Implementation Components. Identify User DSLs. Identify Interactions between DSLs. Specify DSLs and Their Translation. Configuration DSLs. Expression DSLs. Domain Implementation. II. IMPLEMENTATION TECHNOLOGIES. 6. Generic Programming. Why Is This Chapter Worth Reading? What Is Generic Programming? Generic versus Generative Programming. Generic Parameters. Parametric versus Subtype Polymorphism. Genericity in Java. Bounded versus Unbounded Polymorphism. A Fresh Look at Polymorphism. Parameterized Components. Parameterized Programming. Types, Interfaces, and Specifications. Adapters. Vertical and Horizontal Parameters. Module Expressions. C++ Standard Template Library. Iterators. Freestanding Functions versus Member Functions. Generic Methodology. Historical Notes. 7. Component-Oriented Template-Based C++ Programming Techniques. Why Is This Chapter Worth Reading? Types of System Configuration. C++ Support for Dynamic Configuration. C++ Support for Static Configuration. Static Typing. Static Binding. Inlining. Templates. Parameterized Inheritance. typedefs. Member Types. Nested Classes. Prohibiting Certain Template Instantiations. Static versus Dynamic Parameterization. Wrappers Based on Parameterized Inheritance. Template Method Based on Parameterized Inheritance. Parameterizing Binding Mode. Consistent Parameterization of Multiple Components. Static Interactions between Components. Components with Influence. Components under Influence. Structured Configurations. Recursive Components. Intelligent Configuration. 8. Aspect-Oriented Decomposition and Composition. Why Is This Chapter Worth Reading? What Is Aspect-Oriented Programming? Aspect-Oriented Decomposition Approaches. Subject-Oriented Programming. Composition Filters. Demeter / Adaptive Programming. Aspect-Oriented Decomposition and Domain Engineering. How Aspects Arise. Composition Mechanisms. Requirements on Composition Mechanisms. Example: Synchronizing a Bounded Buffer. ""Tangled"" Synchronized Stack. Separating Synchronization Using Design Patterns. Separating Synchronization Using SOP. Some Problems with Design Patterns and Some Solutions. Implementing Noninvasive, Dynamic Composition in Smalltalk. Kinds of Crosscutting. How to Express Aspects in Programming Languages. Separating Synchronization Using AspectJ Cool. Implementing Dynamic Cool in Smalltalk. Implementation Technologies for Aspect-Oriented Programming. Technologies for Implementing Aspect-Specific Abstractions. Technologies for Implementing Weaving. AOP and Specialized Language Extensions. AOP and Active Libraries. Final Remarks. 9. Generators. Why Is This Chapter Worth Reading? What Are Generators? Transformational Model of Software Development. Technologies for Building Generators. Compositional versus Transformational Generators. Kinds of Transformations. Compiler Transformations. Source-to-Source Transformations. Transformation Systems. Scheduling Transformations. Existing Transformation Systems and Their Applications. Selected Approaches to Generation. Draco. GenVoca. Approaches Based on Algebraic Specifications. 10. Static Metaprogramming in C++. Why Is This Chapter Worth Reading? What Is Metaprogramming? A Quick Tour of Metaprogramming. Static Metaprogramming. C++ as a Two-Level Language. Functional Flavor of the Static Level. Class Templates as Functions. Integers and Types as Data. Symbolic Names Instead of Variables. Constant Initialization and typedef-Statements Instead of Assignment. Template Recursion Instead of Loops. Conditional Operator and Template Specialization as Conditional Constructs. Template Metaprogramming. Template Metafunctions. Metafinctions as Arguments and Return Values of Other Metafinctions. Representing Metainformation. Member Traits. Traits Classes. Traits Templates. Example: Using Template Metafunctions and Traits Templates to Implement Type Promotions. Compile-Time Lists and Trees as Nested Templates. Compile-Time Control Structures. Explicit Selection Constructs. Template Recursion as a Looping Construct. Explicit Looping Constructs. Code Generation. Simple Code Selection. Composing Templates. Generators Based on Expression Templates. Recursive Code Expansion. Explicit Loops for Generating Code. Example: Using Static Execute Loops to Test Metafunctions. Partial Evaluation in C++. Workarounds for Partial Template Specialization. Problems of Template Metaprogramming. Historical Notes. 11. Intentional Programming. Why Is This Chapter Worth Reading? What Is Intentional Programming? Technology behind IP. System Architecture. Representing Programs in IP: The Source Graph. Source Graph + Methods = Active Source. Working with the IP Programming Environment. Editing. Further Capabilities of the IP Editor. Extending the IP System with New Intentions. Advanced Topics. Questions, Methods, and a Frameworklike Organization. Source-Pattem-Based Polymorphism. Methods as Visitors. Asking Questions Synchronously and Asynchronously. Reduction. The Philosophy behind IP. Why Do We Need Extendible Programming Environments? or What Is the Problem with Fixed Programming Languages? Moving Focus from Fixed Languages to Language Features and the Emergence of an Intention Market. Intentional Programming and Component-Based Development. Frequently Asked Questions. Summary. III. APPLICATION EXAMPLES. 12. List Container. Why Is This Chapter Worth Reading? Overview. Domain Analysis. Domain Design. Implementation Components. Manual Assembly. Specifying Lists. The Generator. Extensions. 13. Bank Account. Why Is This Chapter Worth Reading? The Successful Programming Shop. Design Pattems, Frameworks, and Components. Domain Engineering and Generative Programming. Feature Modeling. Architecture Design. Implementation Components. Configurable Class Hierarchies. Designing a Domain-Specific Language. Bank Account Generator. Testing Generators and Their Products. 14. Generative Matrix Computation Library (GMCL). Why Is This Chapter Worth Reading? Why Matrix Computations? Domain Analysis. Domain Definition. Domain Modeling. Domain Design and Implementation. Matrix Type Generation. Generating Code for Matrix Expressions. Implementing the Matrix Component in IP. APPENDICES. Appendix A: Conceptual Modeling. What Are Concepts? Theories of Concepts. Basic Terminology. The Classical View. The Probabilistic View. The Exemplar View. Summary of the Three Views. Important Issues Concerning Concepts. Stability of Concepts. Concept Core. Informational Contents of Features. Feature Composition and Relationships between Features. Quality of Features. Abstraction and Generalization. Conceptual Modeling, Object-Orientation, and Software Reuse. Appendix B: Instance-Specific Extension Protocol for Smalltalk. Appendix C: Protocol for Attaching Listener Objects in Smalltalk. Appendix D: Glossary of Matrix Computation Terms. Appendix E: Metafunction for Evaluating Dependency Tables. Glossary of Generative Programming Terms. References. Index. 020130977T04062001",Software engineering research on Generative Programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .,"['Software engineering research on Generative Programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .', 'Our infrastructure for NLP will provide high performance 1 components inspired by Generative Programming principles.']",0,"['However, because templates are a static facility we will also provide dynamic versions (using inheritance), which will be slower but accessible from scripting languages and user interfaces.', 'To provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ) , and for the dynamic version we will use configuration classes .']","['To provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ) , and for the dynamic version we will use configuration classes .']","['However, because templates are a static facility we will also provide dynamic versions (using inheritance), which will be slower but accessible from scripting languages and user interfaces.', 'To provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ) , and for the dynamic version we will use configuration classes .']","['Templates will be used heavily to provide generality without significantly impacting on efficiency.', 'However, because templates are a static facility we will also provide dynamic versions (using inheritance), which will be slower but accessible from scripting languages and user interfaces.', 'To provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ) , and for the dynamic version we will use configuration classes .']","['To provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ) , and for the dynamic version we will use configuration classes .']","['Templates will be used heavily to provide generality without significantly impacting on efficiency.', 'However, because templates are a static facility we will also provide dynamic versions (using inheritance), which will be slower but accessible from scripting languages and user interfaces.', 'To provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ) , and for the dynamic version we will use configuration classes .']","['However, because templates are a static facility we will also provide dynamic versions (using inheritance), which will be slower but accessible from scripting languages and user interfaces.', 'To provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ) , and for the dynamic version we will use configuration classes .']","['The infrastructure will be implemented in C/C++.', 'Templates will be used heavily to provide generality without significantly impacting on efficiency.', 'However, because templates are a static facility we will also provide dynamic versions (using inheritance), which will be slower but accessible from scripting languages and user interfaces.', 'To provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ) , and for the dynamic version we will use configuration classes .']","['To provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ) , and for the dynamic version we will use configuration classes .']"
CC1380,W03-0806,Blueprint for a high performance NLP infrastructure,the american national corpus more than the web can provide,"['Nancy Ide', 'Randi Reppen', 'Keith Suderman']",introduction,"The American National Corpus (ANC) project is developing a corpus comparable to the British National Corpus (BNC), covering American English. Recent interest in the web as a source of corpus materials has caused some in the language processing community to suggest that the development of a corpus of American English is unnecessary. However, we argue that far from being rendered superfluous by the availability of web materials, the ANC is likely to provide a resource for developing web acquisition techniques to support tasks such as genre and language detection and automatic annotation. This paper presents a comparison of the ANC in terms of both content and format with a test corpus compiled from web data, and a discussion of points of intersection and divergence.","For example , 10 million words of the American National Corpus ( #AUTHOR_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers .","['NLP is experiencing an explosion in the quantity of electronic text available.', 'Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( #AUTHOR_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations.']",0,"['However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003).', 'Recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .']","['Recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.']","['However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003).', 'Recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .']","['However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003).', 'Recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .']","['Recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.']","['However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003).', 'Recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.']","['However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003).', 'Recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.']","['However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003).', 'Recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .']","['Recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.', 'Other potential applications must process text online or in realtime.']"
CC1381,W03-0806,Blueprint for a high performance NLP infrastructure,lt ttt  a flexible tokenisation tool,"['Claire Grover', 'Colin Matheson', 'Andrei Mikheev', 'Marc Moens']",experiments,,"For example , the suite of LT tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly .","['A number of stand-alone tools have also been developed.', 'For example , the suite of LT tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', 'This gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'Other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998).', 'However, the source code for these tools is not freely available, so they cannot be extended.']",0,"['A number of stand-alone tools have also been developed.', 'For example , the suite of LT tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly .']","['For example , the suite of LT tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.']","['A number of stand-alone tools have also been developed.', 'For example , the suite of LT tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly .']","['A number of stand-alone tools have also been developed.', 'For example , the suite of LT tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly .']","['For example , the suite of LT tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', 'This gives a greater flexibility but the tradeoff is that these tools can run very slowly.']","['A number of stand-alone tools have also been developed.', 'For example , the suite of LT tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.']","['A number of stand-alone tools have also been developed.', 'For example , the suite of LT tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', 'This gives a greater flexibility but the tradeoff is that these tools can run very slowly.']","['A number of stand-alone tools have also been developed.', 'For example , the suite of LT tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly .']","['For example , the suite of LT tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', 'This gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'Other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998).']"
CC1382,W03-0806,Blueprint for a high performance NLP infrastructure,modern c design generic programming and design patterns applied c indepth series,['Andrei Alexandrescu'],experiments,"Modern C++ Designis an important book. Fundamentally, it demonstrates 'generic patterns' or 'pattern templates' as a powerful new way of creating extensible designs in C++i??a new way to combine templates and patterns that you may never have dreamt was possible, but is. If your work involves C++ design and coding, you should read this book. Highly recommended. i??Herb SutterWhat's left to say about C++ that hasn't already been said? Plenty, it turns out. i??From the Foreword by John VlissidesIn Modern C++ Design, Andrei Alexandrescu opens new vistas for C++ programmers. Displaying extraordinary creativity and programming virtuosity, Alexandrescu offers a cutting-edge approach to design that unites design patterns, generic programming, and C++, enabling programmers to achieve expressive, flexible, and highly reusable code.This book introduces the concept of generic componentsi??reusable design templates that produce boilerplate code for compiler consumptioni??all within C++. Generic components enable an easier and more seamless transition from design to application code, generate code that better expresses the original design intention, and support the reuse of design structures with minimal recoding.The author describes the specific C++ techniques and features that are used in building generic components and goes on to implement industrial strength generic components for real-world applications. Recurring issues that C++ developers face in their day-to-day activity are discussed in depth and implemented in a generic way. These include: Policy-based design for flexibility Partial template specialization Typelistsi??powerful type manipulation structures Patterns such as Visitor, Singleton, Command, and Factories Multi-method enginesFor each generic component, the book presents the fundamental problems and design options, and finally implements a generic solution.In addition, an accompanying Web site, http://www.awl.com/cseng/titles/0-201-70431-5, makes the code implementations available for the generic components in the book and provides a free, downloadable C++ library, called Loki, created by the author. Loki provides out-of-the-box functionality for virtually any C++ project.Get a value-added service! Try out all the examples from this book at www.codesaw.com. CodeSaw is a free online learning tool that allows you to experiment with live code from your book right in your browser. 0201704315B11102003","To provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ) , and for the dynamic version we will use configuration classes .","['The infrastructure will be implemented in C/C++.', 'Templates will be used heavily to provide generality without significantly impacting on efficiency.', 'However, because templates are a static facility we will also provide dynamic versions (using inheritance), which will be slower but accessible from scripting languages and user interfaces.', 'To provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ) , and for the dynamic version we will use configuration classes .']",5,"['Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #AUTHOR_TAG ) and Memory-based learning ( MBL ) ( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .']","['Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #AUTHOR_TAG ) and Memory-based learning ( MBL ) ( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .', 'We will base these components on the design of Weka (Witten and Frank, 1999).']","['Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #AUTHOR_TAG ) and Memory-based learning ( MBL ) ( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .', 'We will base these components on the design of Weka (Witten and Frank, 1999).']","['Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #AUTHOR_TAG ) and Memory-based learning ( MBL ) ( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .']","['Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #AUTHOR_TAG ) and Memory-based learning ( MBL ) ( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .', 'We will base these components on the design of Weka (Witten and Frank, 1999).']","['Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #AUTHOR_TAG ) and Memory-based learning ( MBL ) ( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .', 'We will base these components on the design of Weka (Witten and Frank, 1999).']","['Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #AUTHOR_TAG ) and Memory-based learning ( MBL ) ( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .', 'We will base these components on the design of Weka (Witten and Frank, 1999).']","['Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #AUTHOR_TAG ) and Memory-based learning ( MBL ) ( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .']","['Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #AUTHOR_TAG ) and Memory-based learning ( MBL ) ( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .', 'We will base these components on the design of Weka (Witten and Frank, 1999).']"
CC1383,W03-0806,Blueprint for a high performance NLP infrastructure,scaling to very very large corpora for natural language disambiguation,"['Michele Banko', 'Eric Brill']",introduction,"The amount of readily available online  text has reached hundreds of  billions of words and continues to  grow. Yet for most core natural  language tasks, algorithms continue  to be optimized, tested and compared  after training on corpora consisting  of only one million words or less. In  this paper, we evaluate the  performance of different learning  methods on a prototypical natural  language disambiguation task,  confusion set disambiguation, when  trained on orders of magnitude more  labeled data than has previously been  used. We are fortunate that for this  particular application, correctly  labeled training data is free. Since  this will often not be the case, we  examine methods for effectively  exploiting very large corpora when  labeled data comes at a cost","Recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .","['However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003).', 'Recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.', 'Other potential applications must process text online or in realtime.', 'For example, Google currently answers 250 million queries per day, thus processing time must be minimised.', 'Clearly, efficient NLP components will need to be developed.', 'At the same time, state-of-the-art performance will be needed for these systems to be of practical use.']",0,"['An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #AUTHOR_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) .']","['Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #AUTHOR_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) .', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']","['An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #AUTHOR_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) .', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']","['However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #AUTHOR_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) .']","['Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #AUTHOR_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) .', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']","['However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #AUTHOR_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) .', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']","['An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #AUTHOR_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) .', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']","['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #AUTHOR_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) .']","['Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #AUTHOR_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) .', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']"
CC1384,W03-0806,Blueprint for a high performance NLP infrastructure,xml tools and architecture for named entity recognition,"['Andrei Mikheev', 'Claire Grover', 'Marc Moens']",experiments,This paper reports on the development of a Named Entity recognition system developed fully within the xml paradigm,"For example , the suite of LT tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly .","['A number of stand-alone tools have also been developed.', 'For example , the suite of LT tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', 'This gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'Other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998).', 'However, the source code for these tools is not freely available, so they cannot be extended.']",0,"['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .']","['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'An example of using the Python tagger interface is shown in Figure 1.']","['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'An example of using the Python tagger interface is shown in Figure 1.']","['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .']","['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'An example of using the Python tagger interface is shown in Figure 1.']","['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'An example of using the Python tagger interface is shown in Figure 1.']","['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'An example of using the Python tagger interface is shown in Figure 1.']","['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .']","['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'An example of using the Python tagger interface is shown in Figure 1.']"
CC1385,W03-0806,Blueprint for a high performance NLP infrastructure,a corpusbased appreach to language learning,['Eric Brill'],,,"Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #AUTHOR_TAG ) and Memory-based learning ( MBL ) ( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .","['Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #AUTHOR_TAG ) and Memory-based learning ( MBL ) ( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .', 'We will base these components on the design of Weka (Witten and Frank, 1999).']",4,"['These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #AUTHOR_TAG ) .']","['These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #AUTHOR_TAG ) .', 'We expect even faster training times when we move to conjugate gradient methods.']","['These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #AUTHOR_TAG ) .', 'We expect even faster training times when we move to conjugate gradient methods.']","['We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #AUTHOR_TAG ) .']","['These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #AUTHOR_TAG ) .', 'We expect even faster training times when we move to conjugate gradient methods.']","['We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #AUTHOR_TAG ) .', 'We expect even faster training times when we move to conjugate gradient methods.']","['These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #AUTHOR_TAG ) .', 'We expect even faster training times when we move to conjugate gradient methods.']","['The implementation has been inspired by experience in extracting information from very large corpora (Curran and Moens, 2002) and performing experiments on maximum entropy sequence tagging (Curran and .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #AUTHOR_TAG ) .']","['These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #AUTHOR_TAG ) .', 'We expect even faster training times when we move to conjugate gradient methods.']"
CC1386,W03-0806,Blueprint for a high performance NLP infrastructure,transformationbased learning in the fast lane,"['Grace Ngai', 'Radu Florian']",experiments,,"Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #AUTHOR_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) .","['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #AUTHOR_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) .', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']",0,"['An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #AUTHOR_TAG ) .']","['Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #AUTHOR_TAG ) .', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']","['An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #AUTHOR_TAG ) .', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']","['However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #AUTHOR_TAG ) .']","['Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #AUTHOR_TAG ) .', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']","['However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #AUTHOR_TAG ) .', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']","['An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #AUTHOR_TAG ) .', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']","['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #AUTHOR_TAG ) .']","['Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #AUTHOR_TAG ) .', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']"
CC1387,W03-0806,Blueprint for a high performance NLP infrastructure,bootstrapping postaggers using unlabelled data,"['Stephen Clark', 'James R Curran', 'Miles Osborne']",,"This paper investigates booststrapping part-of-speech taggers using co-training, in which two taggers are iteratively re-trained on each other's output. Since the output of the taggers is noisy, there is a question of which newly labelled examples to add to the training set. We investigate selecting examples by directly maximising tagger agreement on unlabelled data, a method which has been theoretically and empirically motivated in the co-training literature. Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost.","The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .","['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'An example of using the Python tagger interface is shown in Figure 1.']",0,"['Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( #AUTHOR_TAG ) .']","['Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( #AUTHOR_TAG ) .', 'Python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple.']","['Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( #AUTHOR_TAG ) .', 'Python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple.']","['Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( #AUTHOR_TAG ) .']","['Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( #AUTHOR_TAG ) .', 'Python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple.']","['Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( #AUTHOR_TAG ) .', 'Python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple.']","['Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( #AUTHOR_TAG ) .', 'Python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple.']","['Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( #AUTHOR_TAG ) .']","['Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( #AUTHOR_TAG ) .', 'Python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple.']"
CC1388,W03-0806,Blueprint for a high performance NLP infrastructure,a gaussian prior for smoothing maximum entropy models,"['Stanley Chen', 'Ronald Rosenfeld']",experiments,"Abstract : In certain contexts, maximum entropy (ME) modeling can be viewed as maximum likelihood training for exponential models, and like other maximum likelihood methods is prone to overfitting of training data. Several smoothing methods for maximum entropy models have been proposed to address this problem, but previous results do not make it clear how these smoothing methods compare with smoothing methods for other types of related models. In this work, we survey previous work in maximum entropy smoothing and compare the performance of several of these algorithms with conventional techniques for smoothing n-gram language models. Because of the mature body of research in n-gram model smoothing and the close connection between maximum entropy and conventional n-gram models, this domain is well-suited to gauge the performance of maximum entropy smoothing methods. Over a large number of data sets, we find that an ME smoothing method proposed to us by Lafferty performs as well as or better than all other algorithms under consideration. This general and efficient method involves using a Gaussian prior on the parameters of the model and selecting maximum a posteriori instead of maximum likelihood parameter values. We contrast this method with previous n-gram smoothing methods to explain its superior performance.",These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #AUTHOR_TAG ) .,"['The implementation has been inspired by experience in extracting information from very large corpora (Curran and Moens, 2002) and performing experiments on maximum entropy sequence tagging (Curran and .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #AUTHOR_TAG ) .', 'We expect even faster training times when we move to conjugate gradient methods.']",5,"['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .']","['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.']","['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.']","['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .']","['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.']","['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.']","['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.']","['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .']","['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999).']"
CC1389,W03-0806,Blueprint for a high performance NLP infrastructure,deterministic partofspeech tagging with finitestate transducers,"['Emmanuel Roche', 'Yves Schabes']",experiments,,"Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #AUTHOR_TAG ) .","['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #AUTHOR_TAG ) .', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']",0,"['However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .']","['An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).']","['However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).']","['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .']","['An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']","['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).']","['However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']","['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .']","['An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']"
CC1390,W03-0806,Blueprint for a high performance NLP infrastructure,nltk the natural language toolkit,"['Edward Loper', 'Steven Bird']",experiments,,"Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( #AUTHOR_TAG ) .","['Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( #AUTHOR_TAG ) .', 'Python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple.']",0,"['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .']","['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'An example of using the Python tagger interface is shown in Figure 1.']","['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'An example of using the Python tagger interface is shown in Figure 1.']","['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .']","['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'An example of using the Python tagger interface is shown in Figure 1.']","['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'An example of using the Python tagger interface is shown in Figure 1.']","['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'An example of using the Python tagger interface is shown in Figure 1.']","['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .']","['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'An example of using the Python tagger interface is shown in Figure 1.']"
CC1391,W03-0806,Blueprint for a high performance NLP infrastructure,investigating gis and smoothing for maximum entropy taggers,"['James R Curran', 'Stephen Clark']",experiments,"This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (GIS) estimation algorithm, and techniques for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar.","The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .","['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999).', 'We expect even faster training times when we move to conjugate gradient methods.']",4,"['Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']","['The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']","['Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']","['An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']","['The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']","['An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']","['Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']","['However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']","['The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']"
CC1392,W03-0806,Blueprint for a high performance NLP infrastructure,a comparison of algorithms for maximum entropy parameter estimation,['Robert Malouf'],experiments,"Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing. However, the flexibility of ME models is not without cost. While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters. In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods. Sur-prisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limited-memory variable metric algorithm outperformed the other choices.","An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .","['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']",0,"['For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .']","['GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']","['For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']","['General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .']","['GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']","['General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']","['For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']","['Many provide graphical user interfaces (GUI) for manual annotation (e.g.', 'General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .']","['GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']"
CC1393,W03-0806,Blueprint for a high performance NLP infrastructure,investigating gis and smoothing for maximum entropy taggers,"['James R Curran', 'Stephen Clark']",,"This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (GIS) estimation algorithm, and techniques for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar.","The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .","['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'An example of using the Python tagger interface is shown in Figure 1.']",0,"['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['Thus for instance , ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['Thus for instance , ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['Thus for instance , ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']"
CC1394,W03-0806,Blueprint for a high performance NLP infrastructure,tnt  a statistical partofspeech tagger,['Thorsten Brants'],experiments,,"The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .","['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']",0,['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ) .'],"['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ) .', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.']","['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ) .', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.']",['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ) .'],"['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ) .', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity (Pereira et al., 1993;Lin, 1998).']","['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ) .', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.']","['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ) .', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity (Pereira et al., 1993;Lin, 1998).']",['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ) .'],"['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ) .', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity (Pereira et al., 1993;Lin, 1998).', 'techniques.']"
CC1395,W03-0806,Blueprint for a high performance NLP infrastructure,software architecture for language engineering,['Hamish Cunningham'],experiments,"Every building, and every computer program, has an architecture: structural and organisational principles that underpin its design and construction. The garden shed  once built by one of the authors had an ad hoc architecture, extracted (somewhat painfully) from the imagination during a slow and non-deterministic process that, luckily, resulted in a structure which keeps the rain on the outside and the mower on the inside (at least for the time being). As well as being ad hoc (i.e. not informed by analysis of similar practice or relevant science or engineering) this architecture is implicit: no explicit design was made, and no records or documentation kept of the construction process.  The pyramid in the courtyard of the Louvre, by contrast, was constructed in a process involving explicit design performed by qualified engineers with a wealth of theoretical and practical knowledge of the properties of materials, the relative merits and strengths of different construction techniques, et cetera.  So it is with software: sometimes it is thrown together by  enthusiastic amateurs; sometimes it is architected, built to last, and intended to be 'not something you finish, but something you start' (to paraphrase Brand (1994). A number of researchers argued in the early and middle 1990s that the field of computational infrastructure or architecture for human language computation merited an increase in attention. The reasoning was that the increasingly large-scale and technologically significant nature of language processing science was placing increasing burdens of an engineering nature on research and development workers seeking robust and practical methods (as was the increasingly collaborative nature of research in this field, which puts a large premium on software integration and interoperation). Over the intervening period a number of significant systems and practices have been developed in what we may call Software Architecture for Language Engineering (SALE).  This special issue represented an opportunity for practitioners in this area to report their work in a coordinated setting, and to present a snapshot of the state-ofthe-art in infrastructural work, which may indicate where further development and further take-up of these systems can be of benefit",GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .,"['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces (GUI) for manual annotation (e.g.', 'General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']",0,"['For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly , ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .']","['Similarly , ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']","['For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly , ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']","['Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly , ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .']","['Similarly , ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']","['Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly , ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']","['For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly , ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']","['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly , ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .']","['Similarly , ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']"
CC1396,W04-0910,Paraphrastic grammars,an open source grammar development environment and broadcoverage english grammar using hpsg,"['Ann Copestake', 'Dan Flickinger']",,"The LinGO (Linguistic Grammars Online) project's English Resource Grammar and the LKB grammar development environment are language resources which are freely available for download for any purpose, including commercial use (see http://lingo.stanford.edu). Executable programs and source code are both included. In this paper, we give an outline of the LinGO English grammar and LKB system, and discuss the ways in which they are currently being used. The grammar and processing system can be used independently or combined to give a central component which can be exploited in a variety of ways. Our intention in writing this paper is to encourage more people to use the technology, which supports collaborative development on many levels.","Thus for instance , ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']",0,"['Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'And ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .']",['And ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .'],"['Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'And ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .']","['For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'And ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .']",['And ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .'],"['For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'And ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .']","['Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'And ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .']","['Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'And ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .']",['And ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .']
CC1397,W04-0910,Paraphrastic grammars,modlisation et traitement informatique de la synonymi linguisticae investigationes,['S Ploux'],,,Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ) .,"['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ) .', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity (Pereira et al., 1993;Lin, 1998).', 'techniques.']",0,"['Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising.', 'To address this problem , we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG ) .']","['To address this problem , we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG ) .']","['Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising.', 'To address this problem , we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG ) .']","['For instance, as Figures 3, 4 and 5 show, the FTAG trees assigned on syntactic grounds by Anne Abeille FTAG to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above.', 'Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising.', 'To address this problem , we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG ) .']","['To address this problem , we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG ) .']","['For instance, as Figures 3, 4 and 5 show, the FTAG trees assigned on syntactic grounds by Anne Abeille FTAG to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above.', 'Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising.', 'To address this problem , we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG ) .']","['Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising.', 'To address this problem , we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG ) .']","[""A first investigation of Anne Abeille's TAG for French suggests that modeling the synonymic relations across syntactic constructs is reasonably straightforward."", 'For instance, as Figures 3, 4 and 5 show, the FTAG trees assigned on syntactic grounds by Anne Abeille FTAG to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above.', 'Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising.', 'To address this problem , we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG ) .']","['To address this problem , we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG ) .', 'This metagrammar allows us to factorise both syntactic and semantic information.', 'Syntactic information is factorised in the usual way.', 'For instance, there will be a class NOVN1 which groups together all the initial trees representing the possible syntactic configurations in which a transitive verb with two nominal arguments can occur.']"
CC1398,W04-0910,Paraphrastic grammars,automatic paraphrase acquisition from news articles,"['Y Shinyanma', 'S Sekine', 'K Sudo', 'R Grishman']",introduction,"Paraphrases play an important role in the variety and complexity of natural language documents. However, they add to the difficulty of natural language processing. Here we describe a procedure for ob-taining paraphrases from news articles. Articles derived from dif-ferent newspapers can contain paraphrases if they report the same event on the same day. We exploit this feature by using Named Entity recognition. Our approach is based on the assumption that Named Entities are preserved across paraphrases. We applied our method to articles of two domains and obtained notable examples. Although this is our initial attempt at automatically extracting para-phrases from a corpus, the results are promising. 1","Similarly , ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .","['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly , ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']",0,"['For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly , ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .']","['Similarly , ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']","['For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly , ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']","['Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly , ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .']","['Similarly , ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']","['Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly , ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']","['For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly , ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']","['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly , ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .']","['Similarly , ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']"
CC1399,W04-0910,Paraphrastic grammars,identifying lexical paraphrases from a single corpus a case study for verbs,"['O Glickman', 'I Dagan']",introduction,,And ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .,"['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'And ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .']",0,"['To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG ) .']","['To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG ) .', 'Johnson et al., 2002).']","['To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG ) .', 'Johnson et al., 2002).']","['To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG ) .']","['To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG ) .', 'Johnson et al., 2002).', 'FrameNet is an online lexical resource for English based on the principles of Frame Semantics.']","['To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG ) .', 'Johnson et al., 2002).']","['To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG ) .', 'Johnson et al., 2002).', 'FrameNet is an online lexical resource for English based on the principles of Frame Semantics.']","['To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG ) .']","['To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG ) .', 'Johnson et al., 2002).', 'FrameNet is an online lexical resource for English based on the principles of Frame Semantics.', 'In this approach, a word evokes a frame i.e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a given role in the frame.']"
CC1400,W04-0910,Paraphrastic grammars,un outil multilingue de generation de ltag  application au francais et a lâitalien,['M H Candito'],,,"To address this problem , we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG ) .","['Modeling intercategorial synonymic links.', ""A first investigation of Anne AbeillÃ©'s TAG for French suggests that modeling the synonymic relations across syntactic constructs is reasonably straightforward."", 'For instance, as Figures 3, 4 and 5 show, the FTAG trees assigned on syntactic grounds by Anne AbeillÃ© FTAG to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above.', 'Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising.', 'To address this problem , we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG ) .', 'This metagrammar allows us to factorise both syntactic and semantic information.', 'Syntactic information is factorised in the usual way.', 'For instance, there will be a class NOVN1 which groups together all the initial trees representing the possible syntactic configurations in which a transitive verb with two nominal arguments can occur.', 'But additionnally there will be semantic classes such as, ""binary predicate of semantic type X"" which will be associated with the relevant syntactic classes for instance, NOVN1 (the class of transitive verbs with nominal arguments), BINARY NPRED (the class of binary predicative nouns), NOVSUPNN1 , the class of support verb constructions taking two nominal arguments.', 'By further associating semantic units (e.g., ""cost"") with the appropriate semantic classes (e.g., ""binary predicate of semantic type X""), we can in this way capture both intra and intercategorial paraphrasing links in a general way.']",3,"['Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance , ( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .']","['For instance , ( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .', 'Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.']","['Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance , ( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .', 'Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.']","['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance , ( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .']","['For instance , ( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .', 'Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']","['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance , ( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .', 'Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.']","['Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance , ( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .', 'Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']","['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance , ( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .']","['For instance , ( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .', 'Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']"
CC1401,W04-0910,Paraphrastic grammars,learning to paraphrase an unsupervised approahc using mutliplesequence alignment,"['R Barzilay', 'L Lee']",introduction,,"Similarly , ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .","['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly , ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']",0,"['Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ) .']","['For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ) .', 'techniques.']","['Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ) .', 'techniques.']","['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997).', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ) .']","['For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ) .', 'techniques.']","['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997).', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ) .', 'techniques.']","['Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ) .', 'techniques.']","['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997).', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ) .']","['For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ) .', 'techniques.']"
CC1402,W04-0910,Paraphrastic grammars,framenet theory and practice,"['C Fillmore C Johnson', 'M Petruckand C Baker', 'M Ellsworth', 'J Ruppenhofer']",,,"To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG ) .","['To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG ) .', 'Johnson et al., 2002).', 'FrameNet is an online lexical resource for English based on the principles of Frame Semantics.', 'In this approach, a word evokes a frame i.e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a given role in the frame.', 'Finally each frame is associated with a set of target words, the words that evoke that frame.']",5,"['Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) .']","['For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) .']","['Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) .']","['While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) .']","['For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) .']","['While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) .']","['Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) .']","['and its degree of overgeneration (does it generate only the sentences of the described language?)', 'While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) .']","['For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) .', 'Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.', 'In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.']"
CC1403,W04-0910,Paraphrastic grammars,discovery of inference rules for question answering natural language engineering,"['Dekang Lin', 'Patrick Pantel']",introduction,,"For instance , ( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .","['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance , ( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .', 'Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']",0,"['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ) .']","['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.']","['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.']","['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ) .']","['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .']","['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.']","['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .']","['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ) .']","['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']"
CC1404,W04-0910,Paraphrastic grammars,automatic retrieval and clustering of similar words,['D Lin'],,Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.,"For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ) .","['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997).', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ) .', 'techniques.']",3,"['Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?) and its degree of overgeneration (does it generate only the sentences of the described language?)', 'While corpus driven efforts along the PARSEVAL lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .']","['While corpus driven efforts along the PARSEVAL lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.']","['Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?) and its degree of overgeneration (does it generate only the sentences of the described language?)', 'While corpus driven efforts along the PARSEVAL lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.']","['Based on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?) and its degree of overgeneration (does it generate only the sentences of the described language?)', 'While corpus driven efforts along the PARSEVAL lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .']","['While corpus driven efforts along the PARSEVAL lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english, there is for instance the 15 year old Hewlett- Packard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (Flickinger et al., 1987); and more recently, the much more sophisticated TSNLP (Test Suite for Natural Language Processing) which includes some 9500 test items for English, French and German, each of them being annotated with syntactic and application related information (Oepen and Flickinger, 1998).']","['Based on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?) and its degree of overgeneration (does it generate only the sentences of the described language?)', 'While corpus driven efforts along the PARSEVAL lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.']","['Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?) and its degree of overgeneration (does it generate only the sentences of the described language?)', 'While corpus driven efforts along the PARSEVAL lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english, there is for instance the 15 year old Hewlett- Packard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (Flickinger et al., 1987); and more recently, the much more sophisticated TSNLP (Test Suite for Natural Language Processing) which includes some 9500 test items for English, French and German, each of them being annotated with syntactic and application related information (Oepen and Flickinger, 1998).']","['Based on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?) and its degree of overgeneration (does it generate only the sentences of the described language?)', 'While corpus driven efforts along the PARSEVAL lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .']","['While corpus driven efforts along the PARSEVAL lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english, there is for instance the 15 year old Hewlett- Packard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (Flickinger et al., 1987); and more recently, the much more sophisticated TSNLP (Test Suite for Natural Language Processing) which includes some 9500 test items for English, French and German, each of them being annotated with syntactic and application related information (Oepen and Flickinger, 1998).']"
CC1405,W04-0910,Paraphrastic grammars,towards evaluation of nlp systems,"['D Flickinger', 'J Nerbonne', 'I Sag', 'T Wasow']",,,"For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) .","['Based on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?)', 'and its degree of overgeneration (does it generate only the sentences of the described language?)', 'While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) .', 'Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.', 'In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.', 'The construction and annotation of the paraphrases reflects the paraphrase typology.', 'In a first phase, we concentrate on simple, non-recursive predicate/argument structure.', 'Given such a structure, the construction and annotation of a test item proceeds as follows.']",1,"['The language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .']","['The language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.']","['The language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.']","['The language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .']","['The language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .']","['The language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.']","['The language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .']","['The language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .']","['The language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']"
CC1406,W04-0910,Paraphrastic grammars,minimal recursion semantics an introduction,"['A Copestake', 'D Flickinger', 'I Sag', 'C Pollard']",,"Minimal recursion semantics (MRS) is a framework for computational semantics that is suitable for parsing and generation and that can be implemented in typed feature structure formalisms. We discuss why, in general, a semantic representation with minimal structure is desirable and illustrate how a descriptively adequate representation with a nonrecursive structure may be achieved. MRS enables a simple formulation of the grammatical constraints on lexical and phrasal semantics, including the principles of semantic composition. We have integrated MRS with a broad-coverage HPSG grammar.","The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ) .","['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']",1,"['For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular , ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns .']","['In particular , ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns .']","['For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular , ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns .']","['For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular , ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns .']","['In particular , ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns .']","['For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular , ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns .']","['For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular , ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns .']","['For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular , ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns .']","['In particular , ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns .']"
CC1407,W04-0910,Paraphrastic grammars,a procedure for quantitatively comparing the syntactic coverage of english grammars,"['A Black', 'S Abney', 'D Flickinger', 'C Gdaniec', 'R Grishman', 'P Harrison', 'D Hindel', 'R INgria', 'F Jelinek', 'F Klaavans', 'M Liberman', 'M Marcus', 'S Roukos', 'B Santorini', 'T Strzalkowski']",,"The problem of quantitatively comparing the performance of different broad-coverage grammars of English has to date resisted solution. Prima facie, known English grammars appear to disagree strongly with each other as to the elements of even the simplest sentences. For instance, the grammars of Steve Abney (Bellcore), Ezra Black (IBM), Dan Flickinger (Hewlett Packard), Claudia Gdaniec (Logos), Ralph Grishman and Tomek Strzalkowski (NYU), Phil Harrison (Boeing), Don Hindle (AT&T), Bob Ingria (BBN), and Mitch Marcus (U. of Pennsylvania) recognize in common only the following constituents, when each grammarian provides the single parse which he/she would ideally want his/her grammar to specify for three sample Brown Corpus sentences:The famed Yankee Clipper, now retired, has been assisting (as (a batting coach)).One of those capital-gains ventures, in fact, has saddled him (with Gore Court).He said this constituted a (very serious) misuse (of the (Criminal court) processes).","While corpus driven efforts along the PARSEVAL lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .","['Based on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?) and its degree of overgeneration (does it generate only the sentences of the described language?)', 'While corpus driven efforts along the PARSEVAL lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english, there is for instance the 15 year old Hewlett- Packard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (Flickinger et al., 1987); and more recently, the much more sophisticated TSNLP (Test Suite for Natural Language Processing) which includes some 9500 test items for English, French and German, each of them being annotated with syntactic and application related information (Oepen and Flickinger, 1998).']",0,"['As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction .']","['As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction .', 'The metagrammar is an abstract specification of the linguistic properties (phrase structure, valency, realisation of grammatical functions etc.) encoded in the grammar basic units.']","['As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction .', 'The metagrammar is an abstract specification of the linguistic properties (phrase structure, valency, realisation of grammatical functions etc.) encoded in the grammar basic units.']","['As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction .']","['As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction .', 'The metagrammar is an abstract specification of the linguistic properties (phrase structure, valency, realisation of grammatical functions etc.) encoded in the grammar basic units.', 'This specification is then compiled to automatically produce a specific grammar.']","['As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction .', 'The metagrammar is an abstract specification of the linguistic properties (phrase structure, valency, realisation of grammatical functions etc.) encoded in the grammar basic units.']","['As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction .', 'The metagrammar is an abstract specification of the linguistic properties (phrase structure, valency, realisation of grammatical functions etc.) encoded in the grammar basic units.', 'This specification is then compiled to automatically produce a specific grammar.']","['As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction .']","['As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction .', 'The metagrammar is an abstract specification of the linguistic properties (phrase structure, valency, realisation of grammatical functions etc.) encoded in the grammar basic units.', 'This specification is then compiled to automatically produce a specific grammar.']"
CC1408,W04-0910,Paraphrastic grammars,predicate logic unplugged,['J Bos'],,"Die vorliegende Arbeit wurde im Rahmen des Verbundvorhabens Verbmobil vom Bundes-ministerium f ur Bildung, Wissenschaft, Forschung und Technologie (BMBF) unter dem FF orderkennzeichen 01 IV 101 R geff ordert. Die Verantwortung f ur den Inhalt dieser Arbeit liegt bei dem Autor.","The language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .","['The language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']",1,"['For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.', 'For complementing this database and for converse constructions , the LADL tables ( #AUTHOR_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .']","['For complementing this database and for converse constructions , the LADL tables ( #AUTHOR_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']","['For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.', 'For complementing this database and for converse constructions , the LADL tables ( #AUTHOR_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']","['For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.', 'For complementing this database and for converse constructions , the LADL tables ( #AUTHOR_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .']","['For complementing this database and for converse constructions , the LADL tables ( #AUTHOR_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']","['For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.', 'For complementing this database and for converse constructions , the LADL tables ( #AUTHOR_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']","['For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.', 'For complementing this database and for converse constructions , the LADL tables ( #AUTHOR_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']","['For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.', 'For complementing this database and for converse constructions , the LADL tables ( #AUTHOR_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .']","['For complementing this database and for converse constructions , the LADL tables ( #AUTHOR_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']"
CC1409,W04-0910,Paraphrastic grammars,les constructions converses du francais,['G Gross'],,,"In particular , ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns .","['For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular , ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns .']",3,"['Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #AUTHOR_TAG ) .']","['For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #AUTHOR_TAG ) .']","['Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #AUTHOR_TAG ) .']","['While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #AUTHOR_TAG ) .']","['For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #AUTHOR_TAG ) .']","['While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #AUTHOR_TAG ) .']","['Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #AUTHOR_TAG ) .']","['and its degree of overgeneration (does it generate only the sentences of the described language?)', 'While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #AUTHOR_TAG ) .']","['For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #AUTHOR_TAG ) .', 'Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.', 'In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.']"
CC1410,W04-0910,Paraphrastic grammars,un outil multilingue de generation de ltag  application au francais et a lâitalien,['M H Candito'],introduction,,"As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction .","['As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction .', 'The metagrammar is an abstract specification of the linguistic properties (phrase structure, valency, realisation of grammatical functions etc.) encoded in the grammar basic units.', 'This specification is then compiled to automatically produce a specific grammar.']",5,"['Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ) .']","['For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ) .', 'techniques.']","['Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ) .', 'techniques.']","['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997).', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ) .']","['For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ) .', 'techniques.']","['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997).', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ) .', 'techniques.']","['Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ) .', 'techniques.']","['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997).', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ) .']","['For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ) .', 'techniques.']"
CC1411,W04-0910,Paraphrastic grammars,mÂ´ethodes en syntase,['M Gross'],,,"For complementing this database and for converse constructions , the LADL tables ( #AUTHOR_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .","['For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.', 'For complementing this database and for converse constructions , the LADL tables ( #AUTHOR_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']",3,"['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']"
CC1412,W04-0910,Paraphrastic grammars,towards systematic grammar profiling test suite technology 10 years after computer speech and language,"['S Oepen', 'D Flickinger']",,,"For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #AUTHOR_TAG ) .","['Based on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?)', 'and its degree of overgeneration (does it generate only the sentences of the described language?)', 'While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #AUTHOR_TAG ) .', 'Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.', 'In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.', 'The construction and annotation of the paraphrases reflects the paraphrase typology.', 'In a first phase, we concentrate on simple, non-recursive predicate/argument structure.', 'Given such a structure, the construction and annotation of a test item proceeds as follows.']",0,['Semantic construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than -- as is more common in TAG -- from the derivation tree .'],"['Semantic construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than -- as is more common in TAG -- from the derivation tree .', 'This is done by associating each elementary tree with a semantic representation and by decorating relevant tree nodes with unification variables and constants occuring in associated semantic representation.']","['Semantic construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than -- as is more common in TAG -- from the derivation tree .', 'This is done by associating each elementary tree with a semantic representation and by decorating relevant tree nodes with unification variables and constants occuring in associated semantic representation.']",['Semantic construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than -- as is more common in TAG -- from the derivation tree .'],"['Semantic construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than -- as is more common in TAG -- from the derivation tree .', 'This is done by associating each elementary tree with a semantic representation and by decorating relevant tree nodes with unification variables and constants occuring in associated semantic representation.', 'The association between tree nodes and unification variables encodes the syntax/semantics interface -it specifies which node in the tree provides the value for which variable in the final semantic representation.']","['Semantic construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than -- as is more common in TAG -- from the derivation tree .', 'This is done by associating each elementary tree with a semantic representation and by decorating relevant tree nodes with unification variables and constants occuring in associated semantic representation.']","['Semantic construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than -- as is more common in TAG -- from the derivation tree .', 'This is done by associating each elementary tree with a semantic representation and by decorating relevant tree nodes with unification variables and constants occuring in associated semantic representation.', 'The association between tree nodes and unification variables encodes the syntax/semantics interface -it specifies which node in the tree provides the value for which variable in the final semantic representation.']",['Semantic construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than -- as is more common in TAG -- from the derivation tree .'],"['Semantic construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than -- as is more common in TAG -- from the derivation tree .', 'This is done by associating each elementary tree with a semantic representation and by decorating relevant tree nodes with unification variables and constants occuring in associated semantic representation.', 'The association between tree nodes and unification variables encodes the syntax/semantics interface -it specifies which node in the tree provides the value for which variable in the final semantic representation.']"
CC1413,W04-0910,Paraphrastic grammars,distributional clustering of english words,"['F Pereira', 'N Tishby', 'L Lee']",,"We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical ""soft"" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.","For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ) .","['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997).', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ) .', 'techniques.']",3,"['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']","['Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']"
CC1414,W04-0910,Paraphrastic grammars,semantics and syntax in lexical functional grammar,['M Dalrymple'],,,"Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']",0,"['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .']","['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.']","['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.']","['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .']","['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .']","['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.']","['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .']","['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .']","['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']"
CC1415,W04-0910,Paraphrastic grammars,semantic construction in ftag,"['C Gardent', 'L Kallmeyer']",,,Semantic construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than -- as is more common in TAG -- from the derivation tree .,"['Semantic construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than -- as is more common in TAG -- from the derivation tree .', 'This is done by associating each elementary tree with a semantic representation and by decorating relevant tree nodes with unification variables and constants occuring in associated semantic representation.', 'The association between tree nodes and unification variables encodes the syntax/semantics interface -it specifies which node in the tree provides the value for which variable in the final semantic representation.']",0,"['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .']","['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.']","['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.']","['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .']","['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']","['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.']","['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']","['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .']","['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']"
CC1416,W04-0910,Paraphrastic grammars,an algebra for semantic construction in constraintbased grammars,"['A Copestake', 'A Lascarides', 'D Flickinger']",,,"Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']",0,"['In this work, we use the Arabic root extraction technique in (El Kourdi, 2004).', 'It compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .']","['It compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', 'In the remainder of this paper, we will use the term ""root"" and ""term"" interchangeably to refer to canonical forms obtained through this root extraction process.']","['In this work, we use the Arabic root extraction technique in (El Kourdi, 2004).', 'It compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', 'In the remainder of this paper, we will use the term ""root"" and ""term"" interchangeably to refer to canonical forms obtained through this root extraction process.']","['This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root.', 'In this work, we use the Arabic root extraction technique in (El Kourdi, 2004).', 'It compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .']","['It compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', 'In the remainder of this paper, we will use the term ""root"" and ""term"" interchangeably to refer to canonical forms obtained through this root extraction process.']","['This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root.', 'In this work, we use the Arabic root extraction technique in (El Kourdi, 2004).', 'It compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', 'In the remainder of this paper, we will use the term ""root"" and ""term"" interchangeably to refer to canonical forms obtained through this root extraction process.']","['In this work, we use the Arabic root extraction technique in (El Kourdi, 2004).', 'It compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', 'In the remainder of this paper, we will use the term ""root"" and ""term"" interchangeably to refer to canonical forms obtained through this root extraction process.']","['The root extraction process is concerned with the transformation of all Arabic word derivatives to their single common root or canonical form.', 'This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root.', 'In this work, we use the Arabic root extraction technique in (El Kourdi, 2004).', 'It compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .']","['It compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', 'In the remainder of this paper, we will use the term ""root"" and ""term"" interchangeably to refer to canonical forms obtained through this root extraction process.']"
CC1417,W04-0910,Paraphrastic grammars,an algebra for semantic construction in constraintbased grammars,"['A Copestake', 'A Lascarides', 'D Flickinger']",,,"The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .","['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']",1,"['In this work, we use the Arabic root extraction technique in (El Kourdi, 2004).', 'It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .']","['It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', 'In the remainder of this paper, we will use the term ""root"" and ""term"" interchangeably to refer to canonical forms obtained through this root extraction process.']","['In this work, we use the Arabic root extraction technique in (El Kourdi, 2004).', 'It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', 'In the remainder of this paper, we will use the term ""root"" and ""term"" interchangeably to refer to canonical forms obtained through this root extraction process.']","['This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root.', 'In this work, we use the Arabic root extraction technique in (El Kourdi, 2004).', 'It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .']","['It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', 'In the remainder of this paper, we will use the term ""root"" and ""term"" interchangeably to refer to canonical forms obtained through this root extraction process.']","['This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root.', 'In this work, we use the Arabic root extraction technique in (El Kourdi, 2004).', 'It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', 'In the remainder of this paper, we will use the term ""root"" and ""term"" interchangeably to refer to canonical forms obtained through this root extraction process.']","['In this work, we use the Arabic root extraction technique in (El Kourdi, 2004).', 'It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', 'In the remainder of this paper, we will use the term ""root"" and ""term"" interchangeably to refer to canonical forms obtained through this root extraction process.']","['The root extraction process is concerned with the transformation of all Arabic word derivatives to their single common root or canonical form.', 'This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root.', 'In this work, we use the Arabic root extraction technique in (El Kourdi, 2004).', 'It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .']","['It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', 'In the remainder of this paper, we will use the term ""root"" and ""term"" interchangeably to refer to canonical forms obtained through this root extraction process.']"
CC1418,W04-0910,Paraphrastic grammars,alternations and verb semantic classes for french analysis and class formation chapter 5,['P Saint-Dizier'],,"In this paper, we show how alternations (called here contexts) can be defined for French, what their semantic properties are and how verb semantic classes can be constructed from syntactic criteria following (Levin 93). We then analyze the global quality of the results in terms of overlap with classes formed from the same verb-senses using WordNetlike classification criteria. Finally, we propose a method which combines these two approaches to form verb semantic classes better suited for natural language processing.","For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .","['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']",0,"['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #AUTHOR_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic .']","['Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #AUTHOR_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic .', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the kh 2 statistic for a term.']","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #AUTHOR_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic .']","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #AUTHOR_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic .']","['Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #AUTHOR_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic .', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the kh 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the kh 2 to produce best performance.']","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #AUTHOR_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic .', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the kh 2 statistic for a term.']","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #AUTHOR_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic .', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the kh 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the kh 2 to produce best performance.']","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #AUTHOR_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic .']","['Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #AUTHOR_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic .', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the kh 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the kh 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).']"
CC1419,W04-1610,Automatic Arabic document categorization based on the NaÃ¯ve Bayes algorithm,modern information retrieval,"['R B Yates', 'B R Neto']",,"Digital technologies can help scholars to navigate the vast quantities of musical data and source materials now available to them, but an imaginative leap is needed in order to conceptualize the kinds of musicological research questions we might ask of electronic corpora. In particular, our data-rich digital world offers enormous potential for the exploration of musical transmission and relatedness.  In this article, we explore the 16th- and 17th-century instrumental battaglia (battle piece), a genre with a very distinctive collective identity arising from the use of numerous shared ingredients (including melodic, motivic, textural, harmonic and rhythmic features). However, a battaglia is not defined by the presence of a core set of essential features, and exact concordance between these pieces is often remarkably low. This kind of musical 'family resemblance' (formulated after Wittgenstein) poses a serious challenge to both traditional musicological apparatus (for example, finding aids such as thematic catalogues) and Music Information Retrieval (which has often privileged melodic similarity at the expense of other kinds of musical relatedness).  This case study provides a stimulus for rethinking the complex nature of musical similarity. In doing this, we outline a set of requirements for digital tools that could support the discovery, exploration and representation of these kinds of relationships","It compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .","['The root extraction process is concerned with the transformation of all Arabic word derivatives to their single common root or canonical form.', 'This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root.', 'In this work, we use the Arabic root extraction technique in (El Kourdi, 2004).', 'It compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', 'In the remainder of this paper, we will use the term ""root"" and ""term"" interchangeably to refer to canonical forms obtained through this root extraction process.']",4,"['The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets.', 'This work is a continuation of that initiated in ( #AUTHOR_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .']","['This work is a continuation of that initiated in ( #AUTHOR_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .', 'A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories).']","['The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets.', 'This work is a continuation of that initiated in ( #AUTHOR_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .', 'A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories).']","['The present work evaluates the performance on Arabic documents of the Naive Bayes algorithm (NB) -one of the simplest algorithms applied to English document categorization (Mitchell, 1997).', 'The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets.', 'This work is a continuation of that initiated in ( #AUTHOR_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .']","['This work is a continuation of that initiated in ( #AUTHOR_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .', 'A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories).', 'The present work expands the work in (Yahyaoui, 2001) by experimenting with the use of a better root extraction algorithm (El Kourdi, 2004) for document preprocessing, and using a different data set, collected from the largest Arabic site on the web: aljazeera.net.']","['The present work evaluates the performance on Arabic documents of the Naive Bayes algorithm (NB) -one of the simplest algorithms applied to English document categorization (Mitchell, 1997).', 'The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets.', 'This work is a continuation of that initiated in ( #AUTHOR_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .', 'A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories).']","['The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets.', 'This work is a continuation of that initiated in ( #AUTHOR_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .', 'A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories).', 'The present work expands the work in (Yahyaoui, 2001) by experimenting with the use of a better root extraction algorithm (El Kourdi, 2004) for document preprocessing, and using a different data set, collected from the largest Arabic site on the web: aljazeera.net.']","[""Sakhr's marketing literature claims that this categorizer is based on Arabic morphology and some research that has been carried out on natural language processing."", 'The present work evaluates the performance on Arabic documents of the Naive Bayes algorithm (NB) -one of the simplest algorithms applied to English document categorization (Mitchell, 1997).', 'The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets.', 'This work is a continuation of that initiated in ( #AUTHOR_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .']","['This work is a continuation of that initiated in ( #AUTHOR_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .', 'A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories).', 'The present work expands the work in (Yahyaoui, 2001) by experimenting with the use of a better root extraction algorithm (El Kourdi, 2004) for document preprocessing, and using a different data set, collected from the largest Arabic site on the web: aljazeera.net.']"
CC1420,W04-1610,Automatic Arabic document categorization based on the NaÃ¯ve Bayes algorithm,towards an arabic information retrieval system,['Y Houmame'],,"Arabic, the mother tongue of over 300 million people around the world, is known as one of the most difficult languages in Automatic Natural Language processing (NLP) in general and information retrieval in particular. Hence, Arabic cannot trust any web information retrieval system as reliable and relevant as Google search engine. In this context, we dared to focus all our researches to implement an Arabic web-based information retrieval system entitled ARABIRS (ARABic Information Retrieval System). Therefore, to launch such a process so long and hard, we will start with Indexing as one of the crucial steps of the system","It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .","['The root extraction process is concerned with the transformation of all Arabic word derivatives to their single common root or canonical form.', 'This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root.', 'In this work, we use the Arabic root extraction technique in (El Kourdi, 2004).', 'It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', 'In the remainder of this paper, we will use the term ""root"" and ""term"" interchangeably to refer to canonical forms obtained through this root extraction process.']",4,"['To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #AUTHOR_TAG ) .']","['To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #AUTHOR_TAG ) .', 'In this work, the average accuracy over all categories is: 68.78% in cross validation and 62% in evaluation set experiments.']","['To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #AUTHOR_TAG ) .', 'In this work, the average accuracy over all categories is: 68.78% in cross validation and 62% in evaluation set experiments.']","['To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #AUTHOR_TAG ) .']","['To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #AUTHOR_TAG ) .', 'In this work, the average accuracy over all categories is: 68.78% in cross validation and 62% in evaluation set experiments.', 'The corresponding performances in (Yahyaoui, 2001) are 75.6% and 50%, respectively.']","['To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #AUTHOR_TAG ) .', 'In this work, the average accuracy over all categories is: 68.78% in cross validation and 62% in evaluation set experiments.']","['To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #AUTHOR_TAG ) .', 'In this work, the average accuracy over all categories is: 68.78% in cross validation and 62% in evaluation set experiments.', 'The corresponding performances in (Yahyaoui, 2001) are 75.6% and 50%, respectively.']","['To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #AUTHOR_TAG ) .']","['To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #AUTHOR_TAG ) .', 'In this work, the average accuracy over all categories is: 68.78% in cross validation and 62% in evaluation set experiments.', 'The corresponding performances in (Yahyaoui, 2001) are 75.6% and 50%, respectively.', 'Thus, the overall performance (including cross validation and evaluation set experiments) in this work is comparable to that in (Yahyaoui, 2001).']"
CC1421,W04-1610,Automatic Arabic document categorization based on the NaÃ¯ve Bayes algorithm,automatic indexing based on bayesian inference networksquot,"['K Tzeras', 'S Hartman']",experiments,"In this paper, a Bayesian inference network model for automatic indexing with index terms (descriptors) from a prescribed vocabulary is presented. It requires an indexing dictionary with rules mapping terms of the respective subject field onto descriptors and inverted lists for terms occuring in a set of documents of the subject field and descriptors manually assigned to these documents. The indexing dictionary can be derived automatically from a set of manually indexed documents. An application of the network model is described, followed by an indexing example and some experimental results about the indexing performance of the network model.","Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #AUTHOR_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic .","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #AUTHOR_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic .', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the Ï 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the Ï 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']",0,['A good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ) .'],"['A good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ) .', 'More recently, (Sebastiani, 2002) has performed a good survey of document categorization; recent works can also be found in (Joachims, 2002), (Crammer and Singer, 2003), and (Lewis et al., 2004).']","['A good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ) .', 'More recently, (Sebastiani, 2002) has performed a good survey of document categorization; recent works can also be found in (Joachims, 2002), (Crammer and Singer, 2003), and (Lewis et al., 2004).']",['A good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ) .'],"['A good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ) .', 'More recently, (Sebastiani, 2002) has performed a good survey of document categorization; recent works can also be found in (Joachims, 2002), (Crammer and Singer, 2003), and (Lewis et al., 2004).']","['A good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ) .', 'More recently, (Sebastiani, 2002) has performed a good survey of document categorization; recent works can also be found in (Joachims, 2002), (Crammer and Singer, 2003), and (Lewis et al., 2004).']","['A good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ) .', 'More recently, (Sebastiani, 2002) has performed a good survey of document categorization; recent works can also be found in (Joachims, 2002), (Crammer and Singer, 2003), and (Lewis et al., 2004).']",['A good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ) .'],"['A good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ) .', 'More recently, (Sebastiani, 2002) has performed a good survey of document categorization; recent works can also be found in (Joachims, 2002), (Crammer and Singer, 2003), and (Lewis et al., 2004).']"
CC1422,W04-1610,Automatic Arabic document categorization based on the NaÃ¯ve Bayes algorithm,toward an arabic web page classifierquot master project,['M Yahyaoui'],related work,,"This work is a continuation of that initiated in ( #AUTHOR_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .","['Concerning Arabic, one automatic categorizer has been reported to have been put under operational use to classify Arabic documents; it is referred to as ""Sakhr\'s categorizer"" (Sakhr, 2004).', 'Unfortunately, there is no technical documentation or specification concerning this Arabic categorizer.', ""Sakhr's marketing literature claims that this categorizer is based on Arabic morphology and some research that has been carried out on natural language processing."", 'The present work evaluates the performance on Arabic documents of the NaÃ¯ve Bayes algorithm (NB) -one of the simplest algorithms applied to English document categorization (Mitchell, 1997).', 'The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets.', 'This work is a continuation of that initiated in ( #AUTHOR_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .', 'A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories).', 'The present work expands the work in (Yahyaoui, 2001) by experimenting with the use of a better root extraction algorithm (El Kourdi, 2004) for document preprocessing, and using a different data set, collected from the largest Arabic site on the web: aljazeera.net.']",2,"['In Arabic, however, the use of stems will not yield satisfactory categorization.', 'This is mainly due to the fact that Arabic is a non-concatenative language ( #AUTHOR_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root .']","['This is mainly due to the fact that Arabic is a non-concatenative language ( #AUTHOR_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root .', 'The infix form (or stem) needs further to be processed in order to obtain the root.']","['In Arabic, however, the use of stems will not yield satisfactory categorization.', 'This is mainly due to the fact that Arabic is a non-concatenative language ( #AUTHOR_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root .']","['In Arabic, however, the use of stems will not yield satisfactory categorization.', 'This is mainly due to the fact that Arabic is a non-concatenative language ( #AUTHOR_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root .']","['This is mainly due to the fact that Arabic is a non-concatenative language ( #AUTHOR_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root .', 'The infix form (or stem) needs further to be processed in order to obtain the root.', 'This processing is not straightforward: it necessitates expert knowledge in Arabic language word morphology (Al-Shalabi and Evens, 1998).']","['In Arabic, however, the use of stems will not yield satisfactory categorization.', 'This is mainly due to the fact that Arabic is a non-concatenative language ( #AUTHOR_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root .', 'The infix form (or stem) needs further to be processed in order to obtain the root.']","['In Arabic, however, the use of stems will not yield satisfactory categorization.', 'This is mainly due to the fact that Arabic is a non-concatenative language ( #AUTHOR_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root .', 'The infix form (or stem) needs further to be processed in order to obtain the root.', 'This processing is not straightforward: it necessitates expert knowledge in Arabic language word morphology (Al-Shalabi and Evens, 1998).']","['In Arabic, however, the use of stems will not yield satisfactory categorization.', 'This is mainly due to the fact that Arabic is a non-concatenative language ( #AUTHOR_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root .']","['This is mainly due to the fact that Arabic is a non-concatenative language ( #AUTHOR_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root .', 'The infix form (or stem) needs further to be processed in order to obtain the root.', 'This processing is not straightforward: it necessitates expert knowledge in Arabic language word morphology (Al-Shalabi and Evens, 1998).', 'As an example, two close roots (i.e., roots made of the same letters), but semantically different, can yield the same infix form thus creating ambiguity.']"
CC1423,W04-1610,Automatic Arabic document categorization based on the NaÃ¯ve Bayes algorithm,toward an arabic web page classifierquot master project,['M Yahyaoui'],conclusion,,"To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #AUTHOR_TAG ) .","['To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #AUTHOR_TAG ) .', 'In this work, the average accuracy over all categories is: 68.78% in cross validation and 62% in evaluation set experiments.', 'The corresponding performances in (Yahyaoui, 2001) are 75.6% and 50%, respectively.', 'Thus, the overall performance (including cross validation and evaluation set experiments) in this work is comparable to that in (Yahyaoui, 2001).', 'This offers some indication that the performance of NB algorithm in classifying Arabic documents is not sensitive to the Arabic root extraction algorithm.', 'Future work will be directed at experimenting with other root extraction algorithms.', ""Further improvement of NB's performance may be effected by using unlabeled documents; e.g., EM has been used successfully for this purpose in (Nigam et al.,200), where EM has increased the classification accuracy by 30% for classifying English documents.""]",1,"['Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the kh 2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term .']","['( #AUTHOR_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term .']","['Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the kh 2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term .']","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the kh 2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term .']","['( #AUTHOR_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term .']","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the kh 2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term .']","['Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the kh 2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term .']","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the kh 2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term .']","['( #AUTHOR_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term .', 'On the other hand, (Rogati and Yang, 2002) reports the kh 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).']"
CC1424,W04-1610,Automatic Arabic document categorization based on the NaÃ¯ve Bayes algorithm,a reexamination of text categorization methodsâ,"['Y Yang', 'X Liu']",related work,,A good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ) .,"['A good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ) .', 'More recently, (Sebastiani, 2002) has performed a good survey of document categorization; recent works can also be found in (Joachims, 2002), (Crammer and Singer, 2003), and (Lewis et al., 2004).']",0,"['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']","['More recently , ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']","['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']","['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']","['More recently , ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']","['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']","['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']","['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']","['More recently , ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']"
CC1425,W04-1610,Automatic Arabic document categorization based on the NaÃ¯ve Bayes algorithm,a computational morphology system for arabicâ,"['R Al-Shalabi', 'M Evens']",,"This paper describes a new algorithm for morphological analysis of Arabic words, which has been tested on a corpus of 242 abstracts from the Saudi Arabian National Computer Conference . It runs an order of magnitude faster than other algorithms in the literature.","This is mainly due to the fact that Arabic is a non-concatenative language ( #AUTHOR_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root .","['In Arabic, however, the use of stems will not yield satisfactory categorization.', 'This is mainly due to the fact that Arabic is a non-concatenative language ( #AUTHOR_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root .', 'The infix form (or stem) needs further to be processed in order to obtain the root.', 'This processing is not straightforward: it necessitates expert knowledge in Arabic language word morphology (Al-Shalabi and Evens, 1998).', 'As an example, two close roots (i.e., roots made of the same letters), but semantically different, can yield the same infix form thus creating ambiguity.']",0,"['As such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre-specified subject themes.', 'Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #AUTHOR_TAG ) .']","['Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #AUTHOR_TAG ) .']","['As such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre-specified subject themes.', 'Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #AUTHOR_TAG ) .']","['It consists of assigning and labeling documents using a set of predefined categories based on document contents.', 'As such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre-specified subject themes.', 'Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #AUTHOR_TAG ) .']","['Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #AUTHOR_TAG ) .']","['It consists of assigning and labeling documents using a set of predefined categories based on document contents.', 'As such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre-specified subject themes.', 'Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #AUTHOR_TAG ) .']","['As such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre-specified subject themes.', 'Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #AUTHOR_TAG ) .']","['Automatic text (or document) categorization attempts to replace and save human effort required in performing manual categorization.', 'It consists of assigning and labeling documents using a set of predefined categories based on document contents.', 'As such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre-specified subject themes.', 'Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #AUTHOR_TAG ) .']","['Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #AUTHOR_TAG ) .', 'Such applications have included electronic email filtering, newsgroups classification, and survey data grouping.', 'Barq for instance uses automatic categorization to provide similar documents feature (Rachidi et al., 2003).', 'In this paper, NB which is a statistical machine learning algorithm is used to learn to classify non-vocalized 1 Arabic web text documents.']"
CC1426,W04-1610,Automatic Arabic document categorization based on the NaÃ¯ve Bayes algorithm,a comparative study on feature selection in text categorization,"['Y Yang', 'J P Pedersen']",experiments,This paper is a comparative study of feature selection methods in statistical learning of text categorization The focus is on aggres sive dimensionality reduction Five meth ods were evaluated including term selection based on document frequency DF informa tion gain IG mutual information MI a test CHI and term strength TS We found IG and CHI most e ective in our ex periments Using IG thresholding with a k nearest neighbor classi er on the Reuters cor pus removal of up to removal of unique terms actually yielded an improved classi cation accuracy measured by average preci sion DF thresholding performed similarly Indeed we found strong correlations between the DF IG and CHI values of a term This suggests that DF thresholding the simplest method with the lowest cost in computation can be reliably used instead of IG or CHI when the computation of these measures are too expensive TS compares favorably with the other methods with up to vocabulary reduction but is not competitive at higher vo cabulary reduction levels In contrast MI had relatively poor performance due to its bias towards favoring rare terms and its sen sitivity to probability estimation errors,"( #AUTHOR_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term .","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the Ï 2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term .', 'On the other hand, (Rogati and Yang, 2002) reports the Ï 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']",0,"['On the other hand, (Rogati and Yang, 2002) reports the kh 2 to produce best performance.', 'In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #AUTHOR_TAG ) .']","['In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #AUTHOR_TAG ) .', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).']","['On the other hand, (Rogati and Yang, 2002) reports the kh 2 to produce best performance.', 'In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #AUTHOR_TAG ) .', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).']","['(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the kh 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the kh 2 to produce best performance.', 'In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #AUTHOR_TAG ) .']","['In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #AUTHOR_TAG ) .', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']","['(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the kh 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the kh 2 to produce best performance.', 'In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #AUTHOR_TAG ) .', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).']","['On the other hand, (Rogati and Yang, 2002) reports the kh 2 to produce best performance.', 'In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #AUTHOR_TAG ) .', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']","['Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the kh 2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the kh 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the kh 2 to produce best performance.', 'In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #AUTHOR_TAG ) .']","['In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #AUTHOR_TAG ) .', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']"
CC1427,W04-1610,Automatic Arabic document categorization based on the NaÃ¯ve Bayes algorithm,machine learning in automated text categorizationâ,['F Sebastiani'],related work,"The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last ten years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert manpower, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely document representation, classifier construction, and classifier evaluation.Comment: Accepted for publication on ACM Computing Survey","More recently , ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .","['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']",0,"['The importance of each term is assumed to be inversely proportional to the number of documents that contain that term.', 'TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .']","['TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .', 'Thus, the weight of each term/root in a document is given by w D,t = TF D,t * IDF t .']","['The importance of each term is assumed to be inversely proportional to the number of documents that contain that term.', 'TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .', 'Thus, the weight of each term/root in a document is given by w D,t = TF D,t * IDF t .']","['While the TF measurement concerns the importance of a term in a given document, IDF seeks to measure the relative importance of a term in a collection of documents.', 'The importance of each term is assumed to be inversely proportional to the number of documents that contain that term.', 'TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .']","['TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .', 'Thus, the weight of each term/root in a document is given by w D,t = TF D,t * IDF t .']","['While the TF measurement concerns the importance of a term in a given document, IDF seeks to measure the relative importance of a term in a collection of documents.', 'The importance of each term is assumed to be inversely proportional to the number of documents that contain that term.', 'TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .', 'Thus, the weight of each term/root in a document is given by w D,t = TF D,t * IDF t .']","['The importance of each term is assumed to be inversely proportional to the number of documents that contain that term.', 'TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .', 'Thus, the weight of each term/root in a document is given by w D,t = TF D,t * IDF t .']","['While the TF measurement concerns the importance of a term in a given document, IDF seeks to measure the relative importance of a term in a collection of documents.', 'The importance of each term is assumed to be inversely proportional to the number of documents that contain that term.', 'TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .']","['TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .', 'Thus, the weight of each term/root in a document is given by w D,t = TF D,t * IDF t .']"
CC1428,W04-1610,Automatic Arabic document categorization based on the NaÃ¯ve Bayes algorithm,an evaluation of statistical approaches to text categorizationâ,['Y Yang'],introduction,"This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance; except for a Naive Bayes approach, the other learning algorithms also performed relatively well.","Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #AUTHOR_TAG ) .","['With the explosive growth of text documents on the web, relevant information retrieval has become a crucial task to satisfy the needs of different end users.', 'To this end, automatic text categorization has emerged as a way to cope with such a problem.', 'Automatic text (or document) categorization attempts to replace and save human effort required in performing manual categorization.', 'It consists of assigning and labeling documents using a set of predefined categories based on document contents.', 'As such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre-specified subject themes.', 'Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #AUTHOR_TAG ) .', 'Such applications have included electronic email filtering, newsgroups classification, and survey data grouping.', 'Barq for instance uses automatic categorization to provide similar documents feature (Rachidi et al., 2003).', 'In this paper, NB which is a statistical machine learning algorithm is used to learn to classify non-vocalized 1 Arabic web text documents.']",0,"['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( Tzeras and Hartman , 1993 ) , minimum description length principal ( #AUTHOR_TAG ) , and the X2 statistic .']","['Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( Tzeras and Hartman , 1993 ) , minimum description length principal ( #AUTHOR_TAG ) , and the X2 statistic .', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the kh 2 statistic for a term.']","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( Tzeras and Hartman , 1993 ) , minimum description length principal ( #AUTHOR_TAG ) , and the X2 statistic .']","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( Tzeras and Hartman , 1993 ) , minimum description length principal ( #AUTHOR_TAG ) , and the X2 statistic .']","['Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( Tzeras and Hartman , 1993 ) , minimum description length principal ( #AUTHOR_TAG ) , and the X2 statistic .', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the kh 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the kh 2 to produce best performance.']","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( Tzeras and Hartman , 1993 ) , minimum description length principal ( #AUTHOR_TAG ) , and the X2 statistic .', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the kh 2 statistic for a term.']","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( Tzeras and Hartman , 1993 ) , minimum description length principal ( #AUTHOR_TAG ) , and the X2 statistic .', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the kh 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the kh 2 to produce best performance.']","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( Tzeras and Hartman , 1993 ) , minimum description length principal ( #AUTHOR_TAG ) , and the X2 statistic .']","['Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( Tzeras and Hartman , 1993 ) , minimum description length principal ( #AUTHOR_TAG ) , and the X2 statistic .', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the kh 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the kh 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).']"
CC1429,W04-1610,Automatic Arabic document categorization based on the NaÃ¯ve Bayes algorithm,toward an arabic web page classifierquot master project,['M Yahyaoui'],experiments,,"In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #AUTHOR_TAG ) .","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the Ï 2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the Ï 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the Ï 2 to produce best performance.', 'In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #AUTHOR_TAG ) .', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']",1,"['Many machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in ( #AUTHOR_TAG ) , ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ) , respectively .']","['include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in ( #AUTHOR_TAG ) , ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ) , respectively .']","['Many machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in ( #AUTHOR_TAG ) , ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ) , respectively .']","['Many machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in ( #AUTHOR_TAG ) , ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ) , respectively .']","['include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in ( #AUTHOR_TAG ) , ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ) , respectively .']","['Many machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in ( #AUTHOR_TAG ) , ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ) , respectively .']","['Many machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in ( #AUTHOR_TAG ) , ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ) , respectively .']","['Many machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in ( #AUTHOR_TAG ) , ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ) , respectively .']","['include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in ( #AUTHOR_TAG ) , ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ) , respectively .']"
CC1430,W04-1610,Automatic Arabic document categorization based on the NaÃ¯ve Bayes algorithm,on the specification of term values in automatic indexingquot,"['G Salton', 'C S Yang']",experiments,,"TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .","['While the TF measurement concerns the importance of a term in a given document, IDF seeks to measure the relative importance of a term in a collection of documents.', 'The importance of each term is assumed to be inversely proportional to the number of documents that contain that term.', 'TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .', 'Thus, the weight of each term/root in a document is given by w D,t = TF D,t * IDF t .']",4,"['In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ) .']","['TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ) .', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']","['In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ) .', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']","['On the other hand, (Rogati and Yang, 2002) reports the kh 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ) .']","['TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ) .', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']","['On the other hand, (Rogati and Yang, 2002) reports the kh 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ) .', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']","['In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ) .', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']","['(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the kh 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the kh 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ) .']","['TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ) .', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']"
CC1431,W04-1610,Automatic Arabic document categorization based on the NaÃ¯ve Bayes algorithm,newsweeder learning to filter netnewsquot,['K Lang'],experiments,,"Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( Tzeras and Hartman , 1993 ) , minimum description length principal ( #AUTHOR_TAG ) , and the X2 statistic .","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( Tzeras and Hartman , 1993 ) , minimum description length principal ( #AUTHOR_TAG ) , and the X2 statistic .', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the Ï 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the Ï 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']",0,"['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']","['More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']","['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']","['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']","['More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']","['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']","['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']","['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']","['More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']"
CC1432,W04-1610,Automatic Arabic document categorization based on the NaÃ¯ve Bayes algorithm,comparison of two learning algorithms for text categorizationquot,"['D Lewis', 'M Ringnette']",related work,,"include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in ( #AUTHOR_TAG ) , ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ) , respectively .","['Many machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in ( #AUTHOR_TAG ) , ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ) , respectively .']",0,"['The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.', 'For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']","['For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']","['The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.', 'For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']","['The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.', 'For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']","['For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']","['The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.', 'For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']","['The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.', 'For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']","['The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.', 'For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']","['For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']"
CC1433,W04-1610,Automatic Arabic document categorization based on the NaÃ¯ve Bayes algorithm,modern information retrieval,"['R B Yates', 'B R Neto']",experiments,"Digital technologies can help scholars to navigate the vast quantities of musical data and source materials now available to them, but an imaginative leap is needed in order to conceptualize the kinds of musicological research questions we might ask of electronic corpora. In particular, our data-rich digital world offers enormous potential for the exploration of musical transmission and relatedness.  In this article, we explore the 16th- and 17th-century instrumental battaglia (battle piece), a genre with a very distinctive collective identity arising from the use of numerous shared ingredients (including melodic, motivic, textural, harmonic and rhythmic features). However, a battaglia is not defined by the presence of a core set of essential features, and exact concordance between these pieces is often remarkably low. This kind of musical 'family resemblance' (formulated after Wittgenstein) poses a serious challenge to both traditional musicological apparatus (for example, finding aids such as thematic catalogues) and Music Information Retrieval (which has often privileged melodic similarity at the expense of other kinds of musical relatedness).  This case study provides a stimulus for rethinking the complex nature of musical similarity. In doing this, we outline a set of requirements for digital tools that could support the discovery, exploration and representation of these kinds of relationships",TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ) .,"['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the Ï 2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the Ï 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the Ï 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ) .', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']",0,"['On the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( Oueslati , 1999 ) or , more frequently , on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG , for example ) .']","['Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( Oueslati , 1999 ) or , more frequently , on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG , for example ) .', 'It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs.']","['On the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( Oueslati , 1999 ) or , more frequently , on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG , for example ) .', 'It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs.']","['On the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( Oueslati , 1999 ) or , more frequently , on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG , for example ) .']","['Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( Oueslati , 1999 ) or , more frequently , on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG , for example ) .', 'It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs.', 'The original acquisition methodology we present in the next section will allow us to overcome this limitation.']","['On the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( Oueslati , 1999 ) or , more frequently , on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG , for example ) .', 'It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs.']","['On the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( Oueslati , 1999 ) or , more frequently , on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG , for example ) .', 'It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs.', 'The original acquisition methodology we present in the next section will allow us to overcome this limitation.']","['On the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( Oueslati , 1999 ) or , more frequently , on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG , for example ) .']","['Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( Oueslati , 1999 ) or , more frequently , on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG , for example ) .', 'It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs.', 'The original acquisition methodology we present in the next section will allow us to overcome this limitation.']"
CC1434,W04-1610,Automatic Arabic document categorization based on the NaÃ¯ve Bayes algorithm,learning to classify text using svm,['T Joachims'],related work,,"More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .","['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']",0,"['The terms have been identified as the most specific to our corpus by a program developed by Drouin (2003) and called TermoStat.', 'The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #AUTHOR_TAG ) .']","['The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #AUTHOR_TAG ) .', 'Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.']","['The terms have been identified as the most specific to our corpus by a program developed by Drouin (2003) and called TermoStat.', 'The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #AUTHOR_TAG ) .', 'Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.']","['To construct this test set, we have focused our attention on ten domain-specific terms: commande (command), configuration, fichier (file), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), systme (system), utilisateur (user ).', 'The terms have been identified as the most specific to our corpus by a program developed by Drouin (2003) and called TermoStat.', 'The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #AUTHOR_TAG ) .']","['The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #AUTHOR_TAG ) .', 'Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '(They were removed from the example set.)']","['To construct this test set, we have focused our attention on ten domain-specific terms: commande (command), configuration, fichier (file), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), systme (system), utilisateur (user ).', 'The terms have been identified as the most specific to our corpus by a program developed by Drouin (2003) and called TermoStat.', 'The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #AUTHOR_TAG ) .', 'Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.']","['The terms have been identified as the most specific to our corpus by a program developed by Drouin (2003) and called TermoStat.', 'The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #AUTHOR_TAG ) .', 'Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '(They were removed from the example set.)']","['To construct this test set, we have focused our attention on ten domain-specific terms: commande (command), configuration, fichier (file), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), systme (system), utilisateur (user ).', 'The terms have been identified as the most specific to our corpus by a program developed by Drouin (2003) and called TermoStat.', 'The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #AUTHOR_TAG ) .']","['The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #AUTHOR_TAG ) .', 'Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '(They were removed from the example set.)']"
CC1435,W04-1610,Automatic Arabic document categorization based on the NaÃ¯ve Bayes algorithm,using clustering to boost text classificationquot,"['Y C Fang', 'S Parthasarathy', 'F Schwartz']",related work,"In recent years we have seen a tremendous growth in the number of text document collections available on the Internet. Automatic text categorization, the process of assigning unseen documents to user-defined categories, is an important task that can help in the organization and querying of such collections. In this article we consider the problem of classifying online papers from a specific journal in the geological sciences, over a set of expert defined categories. We evaluate two general strategies and several variants thereof. The first strategy is based on Naive Bayes, a popular text classification algorithm. The second strategy is based on Principle Direction Divisive Partitioning, an unsupervised document clustering algorithm. While the performance of both approaches is quite good, some of the new variants that we propose including one, which involves a combination of these two approaches yield even better results.","For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .","['The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.', 'For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']",0,"['A number of applications have relied on distributional analysis (Harris, 1971) in order to build classes of semantically related terms.', 'This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG , for example ) , does not specify the relationship itself .']","['This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG , for example ) , does not specify the relationship itself .', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not di\x1berentiated.']","['A number of applications have relied on distributional analysis (Harris, 1971) in order to build classes of semantically related terms.', 'This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG , for example ) , does not specify the relationship itself .', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not di\x1berentiated.']","['A number of applications have relied on distributional analysis (Harris, 1971) in order to build classes of semantically related terms.', 'This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG , for example ) , does not specify the relationship itself .']","['This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG , for example ) , does not specify the relationship itself .', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not di\x1berentiated.']","['A number of applications have relied on distributional analysis (Harris, 1971) in order to build classes of semantically related terms.', 'This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG , for example ) , does not specify the relationship itself .', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not di\x1berentiated.']","['A number of applications have relied on distributional analysis (Harris, 1971) in order to build classes of semantically related terms.', 'This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG , for example ) , does not specify the relationship itself .', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not di\x1berentiated.']","['A number of applications have relied on distributional analysis (Harris, 1971) in order to build classes of semantically related terms.', 'This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG , for example ) , does not specify the relationship itself .']","['This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG , for example ) , does not specify the relationship itself .', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not di\x1berentiated.']"
CC1436,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,wordsketch extraction and display of significant collocations for lexicography,"['Adam Kilgarriff', 'David Tugwell']",related work,,"Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( Oueslati , 1999 ) or , more frequently , on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG , for example ) .","['On the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( Oueslati , 1999 ) or , more frequently , on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG , for example ) .', 'It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs.', 'The original acquisition methodology we present in the next section will allow us to overcome this limitation.']",1,"['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters .']","['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters .']","['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters .']","['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters .']","['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters .']","['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters .']","['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters .']","['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters .']","['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters .']"
CC1437,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,two methods for extracting quotspecificquot singleword terms from specialized corpora,"['Chantal Lemay', ""Marie-Claude L'Homme"", 'Patrick Drouin']",,,"The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #AUTHOR_TAG ) .","['To construct this test set, we have focused our attention on ten domain-specific terms: commande (command), configuration, fichier (file), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), systï¿½me (system), utilisateur (user ).', 'The terms have been identified as the most specific to our corpus by a program developed by Drouin (2003) and called TermoStat.', 'The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #AUTHOR_TAG ) .', 'Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '(They were removed from the example set.)']",5,"['To construct this test set, we have focused our attention on ten domain-speci\x1cc terms: commande (command), con\x1cguration, \x1cchier (\x1cle), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), systme (system), utilisateur (user ).', 'The terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called TER1vloSTAT .']","['The terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called TER1vloSTAT .', 'The ten most speci\x1cc nouns have been produced by comparing our corpus of computing to the French corpus Le Monde, composed of newspaper articles (Lemay et al., 2004).']","['To construct this test set, we have focused our attention on ten domain-speci\x1cc terms: commande (command), con\x1cguration, \x1cchier (\x1cle), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), systme (system), utilisateur (user ).', 'The terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called TER1vloSTAT .']","['To construct this test set, we have focused our attention on ten domain-speci\x1cc terms: commande (command), con\x1cguration, \x1cchier (\x1cle), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), systme (system), utilisateur (user ).', 'The terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called TER1vloSTAT .']","['The terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called TER1vloSTAT .', 'The ten most speci\x1cc nouns have been produced by comparing our corpus of computing to the French corpus Le Monde, composed of newspaper articles (Lemay et al., 2004).', 'Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.']","['To construct this test set, we have focused our attention on ten domain-speci\x1cc terms: commande (command), con\x1cguration, \x1cchier (\x1cle), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), systme (system), utilisateur (user ).', 'The terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called TER1vloSTAT .', 'The ten most speci\x1cc nouns have been produced by comparing our corpus of computing to the French corpus Le Monde, composed of newspaper articles (Lemay et al., 2004).']","['To construct this test set, we have focused our attention on ten domain-speci\x1cc terms: commande (command), con\x1cguration, \x1cchier (\x1cle), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), systme (system), utilisateur (user ).', 'The terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called TER1vloSTAT .', 'The ten most speci\x1cc nouns have been produced by comparing our corpus of computing to the French corpus Le Monde, composed of newspaper articles (Lemay et al., 2004).', 'Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.']","['To construct this test set, we have focused our attention on ten domain-speci\x1cc terms: commande (command), con\x1cguration, \x1cchier (\x1cle), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), systme (system), utilisateur (user ).', 'The terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called TER1vloSTAT .']","['The terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called TER1vloSTAT .', 'The ten most speci\x1cc nouns have been produced by comparing our corpus of computing to the French corpus Le Monde, composed of newspaper articles (Lemay et al., 2004).', 'Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '(They were removed from the example set.)']"
CC1438,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,symbolic word clustering for mediumsized corpora,"['Benoit Habert', 'Ellie Naulleau', 'Adeline Nazarenko']",related work,"When trying to identify essential concepts and relationships in a medium-size corpus, it is not always possible to rely on statistical methods, as the frequencies are too low. We present an alternative method, symbolic, based on the simplification of parse trees. We discuss the results on nominal phrases of two technical corpora, analyzed by two different robust parsers used for terminology updating in an industrial company. We compare our results with Hindle's scores of similarity.","This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG , for example ) , does not specify the relationship itself .","['A number of applications have relied on distributional analysis (Harris, 1971) in order to build classes of semantically related terms.', 'This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG , for example ) , does not specify the relationship itself .', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not di\x1berentiated.']",0,"['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use , as a starting point , a number of identical characters .']","['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use , as a starting point , a number of identical characters .']","['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use , as a starting point , a number of identical characters .']","['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use , as a starting point , a number of identical characters .']","['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use , as a starting point , a number of identical characters .']","['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use , as a starting point , a number of identical characters .']","['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use , as a starting point , a number of identical characters .']","['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use , as a starting point , a number of identical characters .']","['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use , as a starting point , a number of identical characters .']"
CC1439,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,conceptual structuring through term variation,['Beatrice Daille'],related work,"Term extraction systems are now an integral part of the compiling of specialized dictionaries and updating of term banks. In this paper, we present a term detection approach that discovers, structures, and infers conceptual relationships between terms for French. Conceptual relationships are deduced from specific types of term variations, morphological and syntagmatic, and are expressed through lexical functions. The linguistic precision of the conceptual structuring through morphological variations is of 95%.","More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters .","['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters .']",0,"['However , most strategies are based on `` internal  or `` external methods  ( #AUTHOR_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .']","['However , most strategies are based on `` internal  or `` external methods  ( #AUTHOR_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .', '(In some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identification process.)']","['However , most strategies are based on `` internal  or `` external methods  ( #AUTHOR_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .', '(In some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identification process.)']","['However , most strategies are based on `` internal  or `` external methods  ( #AUTHOR_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .']","['However , most strategies are based on `` internal  or `` external methods  ( #AUTHOR_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .', '(In some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identification process.)', 'The work reported here infers specific semantic relationships based on sets of examples and counterexamples.']","['However , most strategies are based on `` internal  or `` external methods  ( #AUTHOR_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .', '(In some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identification process.)']","['However , most strategies are based on `` internal  or `` external methods  ( #AUTHOR_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .', '(In some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identification process.)', 'The work reported here infers specific semantic relationships based on sets of examples and counterexamples.']","['However , most strategies are based on `` internal  or `` external methods  ( #AUTHOR_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .']","['However , most strategies are based on `` internal  or `` external methods  ( #AUTHOR_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .', '(In some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identification process.)', 'The work reported here infers specific semantic relationships based on sets of examples and counterexamples.']"
CC1440,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,termextraction using nontechnical corpora as a point of leverage,['Patrick Drouin'],,,The terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called TER1vloSTAT .,"['To construct this test set, we have focused our attention on ten domain-speci\x1cc terms: commande (command), con\x1cguration, \x1cchier (\x1cle), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), systï¿½me (system), utilisateur (user ).', 'The terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called TER1vloSTAT .', 'The ten most speci\x1cc nouns have been produced by comparing our corpus of computing to the French corpus Le Monde, composed of newspaper articles (Lemay et al., 2004).', 'Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '(They were removed from the example set.)']",5,"['In this paper, the method is applied to a French corpus on computing to and noun-verb combinations in which verbs convey a meaning of realization.', 'The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #AUTHOR_TAG ) .']",['The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #AUTHOR_TAG ) .'],"['In this paper, the method is applied to a French corpus on computing to and noun-verb combinations in which verbs convey a meaning of realization.', 'The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #AUTHOR_TAG ) .']","['In this paper, the method is applied to a French corpus on computing to and noun-verb combinations in which verbs convey a meaning of realization.', 'The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #AUTHOR_TAG ) .']",['The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #AUTHOR_TAG ) .'],"['In this paper, the method is applied to a French corpus on computing to and noun-verb combinations in which verbs convey a meaning of realization.', 'The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #AUTHOR_TAG ) .']","['In this paper, the method is applied to a French corpus on computing to and noun-verb combinations in which verbs convey a meaning of realization.', 'The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #AUTHOR_TAG ) .']","['In this paper, the method is applied to a French corpus on computing to and noun-verb combinations in which verbs convey a meaning of realization.', 'The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #AUTHOR_TAG ) .']",['The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #AUTHOR_TAG ) .']
CC1441,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,lexicallybased terminology structuring some inherent limits,"['Natalia Grabar', 'Pierre Zweigenbaum']",related work,"Terminology structuring has been the subject of much work in the context of terms extracted from corpora: given a set of terms, obtained from an existing resource or extracted from a corpus, identifying hierarchical (or other types of) relations between these terms. The present paper focusses on terminology structuring by lexical methods, which match terms on the basis on their content words, taking morphological variants into account. Experiments are done on a 'flat' list of terms obtained from an originally hierarchically-structured terminology: the French version of the US National Library of Medicine MeSH thesaurus. We compare the lexically-induced relations with the original MeSH relations: after a quantitative evaluation of their congruence through recall and precision metrics, we perform a qualitative, human analysis of the 'new' relations not present in the MeSH. This analysis shows, on the one hand, the limits of the lexical structuring method. On the other hand, it also reveals some specific structuring choices and naming conventions made by the MeSH designers, and emphasizes ontological commitments that cannot be left to automatic structuring.","More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use , as a starting point , a number of identical characters .","['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use , as a starting point , a number of identical characters .']",0,"['In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG ) .']","['In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG ) .']","['In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG ) .']","['In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG ) .']","['In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG ) .']","['In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG ) .']","['In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG ) .']","['In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG ) .']","['In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG ) .']"
CC1442,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,aide a lacquisition de connaissances a partir de corpus,['Rochdi Oueslati'],related work,"Le probleme d'identification des termes presente un interet particulier pour les applications du taln. En effet, la conception d'outils d'identification de termes et de relations entre termes est d'une aide considerable aux terminologues et aux cogniticiens qui veulent analyser un domaine nouveau. Les terminologues s'interessent surtout a l'etude des termes particulierement dans les domaines de specialite ou les termes designent des objets du domaine de facon la moins ambigue possible. Pour construire une terminologie on part souvent de textes et on applique un ensemble de methodes qui facilitent l'identification des termes. Les methodes classiques utilisent souvent des grammaires et des dictionnaires afin d'acquerir des concepts du domaine d'etude. L'approche que nous presentons dans cette these utilise une approche distributionnelle basee sur les travaux de z. Harris et utilise des algorithmes originaux pour la synthese automatique de contextes entre termes afin d'identifier des relations semantiques propres au domaine. Les resultats obtenus sont d'abord filtres puis valides par un linguiste avant d'etre structures sous forme hierarchique. Ils sont ensuite exploites afin d'acquerir d'autres connaissances en utilisant un processus iteratif et incremental base sur l'inference. L'utilisation d'un langage d'expression de contraintes entre termes du domaines permet de reperer un nombre fini de schemas morphosyntaxiques qui expriment des relations generiques notamment des definitions et des proprietes d'objets. Les resultats obtenus peuvent interesser d'autres travaux comme ceux lies a la construction de bases de connaissances terminologiques ou a la construction d'ontologies partielles propres au domaine.","Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990), on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist) ( #AUTHOR_TAG ) or, more frequently, on a combination of the two (Smadja, 1993; Kilgarri\x1b and Tugwell, 2001, for example).","['On the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990), on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist) ( #AUTHOR_TAG ) or, more frequently, on a combination of the two (Smadja, 1993; Kilgarri\x1b and Tugwell, 2001, for example).', 'It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs.', 'The original acquisition methodology we present in the next section will allow us to overcome this limitation.']",1,"['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ) .']","['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.']","['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.']","['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ) .']","['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.', 'However, the N-V combinations sought are more specific than those that were identified in these previous experiments.']","['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.']","['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.', 'However, the N-V combinations sought are more specific than those that were identified in these previous experiments.']","['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ) .']","['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.', 'However, the N-V combinations sought are more specific than those that were identified in these previous experiments.']"
CC1443,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,inductive logic programming theory and methods,"['Stephen Muggleton', 'Luc De-Raedt']",method,"AbstractInductive Logic Programming (ILP) is a new discipline which investigates the inductive construction of first-order clausal theories from examples and background knowledge. We survey the most important theories and methods of this new field. First, various problem specifications of ILP are formalized in semantic settings for ILP, yielding a ""model-theory"" for ILP. Second, a generic ILP algorithm is presented. Third, the inference rules and corresponding operators used in ILP are presented, resulting in a ""proof-theory"" for ILP. Fourth, since inductive inference does not produce statements which are assured to follow from what is given, inductive inferences require an alternative form of justification. This can take the form of either probabilistic support or logical constraints on the hypothesis language. Information compression techniques used within ILP are presented within a unifying Bayesian approach to confirmation and corroboration of hypotheses. Also, different ways to constrain the hypothesis language or specify the declarative bias are presented. Fifth, some advanced topics in ILP are addressed. These include aspects of computational learning theory as applied to ILP, and the issue of predicate invention. Finally, we survey some applications and implementations of ILP. ILP applications fall under two different categories: first, scientific discovery and knowledge acquisition, and second, programming assistants","ASARES is based on a Machine Learning technique , Inductive Logic Programming ( ILP ) ( #AUTHOR_TAG ) , which infers general morpho-syntactic patterns from a set of examples ( this set is noted E + hereafter ) and counter-examples ( E Ã¢\x88\x92 ) of the elements one wants to acquire and their context.","['ASARES is based on a Machine Learning technique , Inductive Logic Programming ( ILP ) ( #AUTHOR_TAG ) , which infers general morpho-syntactic patterns from a set of examples ( this set is noted E + hereafter ) and counter-examples ( E Ã¢\x88\x92 ) of the elements one wants to acquire and their context.', 'The contextual patterns produced can then be applied to the corpus in order to retrieve new elements.', 'The acquisition process can be summarized in 3 steps:']",0,"['Even though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval.', 'Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with WoRDNET relations ) .']","['Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with WoRDNET relations ) .']","['Even though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval.', 'Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with WoRDNET relations ) .']","['Even though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval.', 'Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with WoRDNET relations ) .']","['Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with WoRDNET relations ) .']","['Even though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval.', 'Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with WoRDNET relations ) .']","['Even though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval.', 'Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with WoRDNET relations ) .']","['Even though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval.', 'Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with WoRDNET relations ) .']","['Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with WoRDNET relations ) .']"
CC1444,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,lexicallybased terminology structuring some inherent limits,"['Natalia Grabar', 'Pierre Zweigenbaum']",introduction,"Terminology structuring has been the subject of much work in the context of terms extracted from corpora: given a set of terms, obtained from an existing resource or extracted from a corpus, identifying hierarchical (or other types of) relations between these terms. The present paper focusses on terminology structuring by lexical methods, which match terms on the basis on their content words, taking morphological variants into account. Experiments are done on a 'flat' list of terms obtained from an originally hierarchically-structured terminology: the French version of the US National Library of Medicine MeSH thesaurus. We compare the lexically-induced relations with the original MeSH relations: after a quantitative evaluation of their congruence through recall and precision metrics, we perform a qualitative, human analysis of the 'new' relations not present in the MeSH. This analysis shows, on the one hand, the limits of the lexical structuring method. On the other hand, it also reveals some specific structuring choices and naming conventions made by the MeSH designers, and emphasizes ontological commitments that cannot be left to automatic structuring.","However , most strategies are based on `` internal  or `` external methods  ( #AUTHOR_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .","['However , most strategies are based on `` internal  or `` external methods  ( #AUTHOR_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .', '(In some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identification process.)', 'The work reported here infers specific semantic relationships based on sets of examples and counterexamples.']",1,"['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ) .']","['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.']","['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.']","['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ) .']","['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.', 'However, the N-V combinations sought are more specific than those that were identified in these previous experiments.']","['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.']","['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.', 'However, the N-V combinations sought are more specific than those that were identified in these previous experiments.']","['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ) .']","['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.', 'However, the N-V combinations sought are more specific than those that were identified in these previous experiments.']"
CC1445,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,selection de termes dans un dictionnaire dinformatique  comparaison de corpus et criteres lexicosemantiques,"[""Marie-Claude L'Homme""]",introduction,"Resume Le present article propose une methode de selection des termes devant faire partie d'un dictionnaire specialise et, plus precisement, un dictionnaire fondamental d'informatique. La methode repose principalement sur un ensemble de criteres lexico-semantiques appliques a un corpus specialise. Elle tient egalement compte de la frequence et de la repartition des unites dans ce corpus. Dans ce travail, nous avons voulu savoir jusqu'a quel point des techniques de comparaison de corpus permettaient de ramener des termes coincidant avec la liste obtenue par l'application des criteres. L'examen de la liste generee automatiquement montre qu'un peu plus de 50 % des unites classees comme etant specifiques par la metrique sont egalement retenues par le terminographe. Les resultats revelent que la technique revet un interet dans la mesure ou elle permet d'aligner des choix sur des donnees extraites de corpus. Toutefois, la selection automatique recele un certain nombre d'imperfections qui doivent etre corrigees par une analyse terminographique.",The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #AUTHOR_TAG ) .,"['In this paper, the method is applied to a French corpus on computing to and noun-verb combinations in which verbs convey a meaning of realization.', 'The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #AUTHOR_TAG ) .']",4,"['The main benefits of this acquisition technique lie in the inferred patterns.', 'Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']","['Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']","['The main benefits of this acquisition technique lie in the inferred patterns.', 'Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']","['The main benefits of this acquisition technique lie in the inferred patterns.', 'Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']","['Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']","['The main benefits of this acquisition technique lie in the inferred patterns.', 'Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']","['The main benefits of this acquisition technique lie in the inferred patterns.', 'Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']","['The main benefits of this acquisition technique lie in the inferred patterns.', 'Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']","['Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']"
CC1446,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,acquisition of qualia elements from corpora â evaluation of a symbolic learning method,"['Pierrette Bouillon', 'Vincent Claveau', 'Cecile Fabre', 'Pascale Sebillot']",method,"This paper presents and evaluates a system extracting from a corpus noun-verb pairs whose components are related by a special kind of link: the qualia roles as defined in the Generative Lexicon. This system is based on a symbolic learning method that automatically learns, from noun-verb pairs that are or are not related by a qualia link, rules characterizing positive examples from negative ones in terms of their surrounding part-of-speech or semantic contexts. The qualia noun-verb pair extraction is thus performed by applying the learnt rules on a part-of-speech or semantically tagged text. Stress is put on the quality of the learning when compared with traditional statistical or syntactical-based approaches. The linguistic relevance of the rules is also evaluated through a comparison with manually acquired qualia patterns.","In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG ) .","['In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG ) .']",4,"['The method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool.', 'ASARES is presented in detail in ( #AUTHOR_TAG ) .']","['ASARES is presented in detail in ( #AUTHOR_TAG ) .', 'We simply give a short account of its basic principles herein.']","['The method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool.', 'ASARES is presented in detail in ( #AUTHOR_TAG ) .', 'We simply give a short account of its basic principles herein.']","['The method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool.', 'ASARES is presented in detail in ( #AUTHOR_TAG ) .']","['ASARES is presented in detail in ( #AUTHOR_TAG ) .', 'We simply give a short account of its basic principles herein.']","['The method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool.', 'ASARES is presented in detail in ( #AUTHOR_TAG ) .', 'We simply give a short account of its basic principles herein.']","['The method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool.', 'ASARES is presented in detail in ( #AUTHOR_TAG ) .', 'We simply give a short account of its basic principles herein.']","['The method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool.', 'ASARES is presented in detail in ( #AUTHOR_TAG ) .']","['ASARES is presented in detail in ( #AUTHOR_TAG ) .', 'We simply give a short account of its basic principles herein.']"
CC1447,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,the generative lexicon,['James Pustejovsky'],method,"In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole.","ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ) .","['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.', 'However, the N-V combinations sought are more specific than those that were identified in these previous experiments.']",0,['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .'],"['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996, for example), does not specify the relationship itself.']","['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996, for example), does not specify the relationship itself.']",['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .'],"['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996, for example), does not specify the relationship itself.', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not differentiated.']","['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996, for example), does not specify the relationship itself.']","['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996, for example), does not specify the relationship itself.', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not differentiated.']",['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .'],"['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996, for example), does not specify the relationship itself.', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not differentiated.']"
CC1448,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,query expansion using lexicalsemantic relations,['Ellen M Voorhees'],introduction,"Applications such as office automation, news filtering, help facilities in complex systems, and the like require the ability to retrieve documents from full-text databases where vocabulary problems can be particularly severe. Experiments performed on small collections with single-domain thesauri suggest that expanding query vectors with words that are lexically related to the original query words can ameliorate some of the problems of mismatched vocabularies. This paper examines the utility of lexical query expansion in the large, diverse TREC collection. Concepts are represented by WordNet synonym sets and are expanded by following the typed links included in WordNet. Experimental results show this query expansion technique makes little difference in retrieval effectiveness if the original queries are relatively complete descriptions of the information being sought even when the concepts to be expanded are selected by hand. Less well developed queries can be significantly improved by expansion of hand-chosen concepts. However, an automatic procedure that can approximate the set of hand picked synonym sets has yet to be devised, and expanding by the synonym sets that are automatically generated can degrade retrieval performance.","Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with WoRDNET relations ) .","['Even though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval.', 'Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with WoRDNET relations ) .']",1,"['In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 ) .']","['It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 ) .']","['In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 ) .']","['In fact, most Arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form Arabic surface forms (blank-delimited words).', 'In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 ) .']","['It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 ) .']","['In fact, most Arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form Arabic surface forms (blank-delimited words).', 'In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 ) .']","['In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 ) .']","['This is particularly true for Arabic where nominals and pronouns are also attached to the word they modify.', 'In fact, most Arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form Arabic surface forms (blank-delimited words).', 'In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 ) .']","['It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 ) .']"
CC1449,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,using partofspeech and semantic tagging for the corpusbased learning of qualia structure elements,"['Pierrette Bouillon', 'Vincent Claveau', 'Cecile Fabre', 'Pascale Sebillot']",method,This paper describes the im plementation and results of a machine learning method de veloped within the inductive logic programming ILP frame work Muggleton and De Raedt to automatically extract from a corpus tagged with parts of speech POS and semantic classes noun verb pairs whose components are bound by one of the relations de ned in the qualia structure in the Genera tive Lexicon Pustejovsky We demonstrate that the seman tic tagging of the corpus improves the quality of the learning both on a theoretical and an empiri cal point of view We also show that a set of the rules learnt by our ILP method have a linguistic signi cance regarding the detec tion of the clues that distinguish in terms of POS and seman tic surrounding context noun verb pairs that are linked by one qualia role from others that are not semantically related,"ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ) .","['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.', 'However, the N-V combinations sought are more specific than those that were identified in these previous experiments.']",0,"['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).']"
CC1450,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,a comparative evaluation of collocation extraction techniques,['Darren Pearce'],method,"This paper describes an experiment that attempts to compare a range of existing collocation extraction techniques as well as the implementation of a new technique based on tests for lexical substitutability. After a description of the experiment details, the techniques are discussed with particular emphasis on any adaptations that are required in order to evaluate it in the way proposed. This is followed by a discussion on the relative strengths and weaknesses of the techniques with reference to the results obtained. Since there is no general agreement on the exact nature of collocation, evaluating techniques with reference to any single standard is somewhat controversial. Departing from this point, part of the concluding discussion includes initial proposals for a common framework for evaluation of collocation extraction techniques.","Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :","['The main benefits of this acquisition technique lie in the inferred patterns.', 'Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']",0,"['Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) .']","['Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) .']","['Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) .']","[""The EDR has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the fo-cus of several recent investigations (Bikel et al., 1997;Miller et al., 1998;Borthwick, 1999;Mikheev et al., 1999;Soon et al., 2001;Ng and Cardie, 2002;Florian et al., 2004), and have been at the center of evaluations such as: MUC-6, MUC-7, and the CoNLL'02 and CoNLL'03 shared tasks."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) .']","['Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) .']","[""The EDR has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the fo-cus of several recent investigations (Bikel et al., 1997;Miller et al., 1998;Borthwick, 1999;Mikheev et al., 1999;Soon et al., 2001;Ng and Cardie, 2002;Florian et al., 2004), and have been at the center of evaluations such as: MUC-6, MUC-7, and the CoNLL'02 and CoNLL'03 shared tasks."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) .']","['Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) .']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the fo-cus of several recent investigations (Bikel et al., 1997;Miller et al., 1998;Borthwick, 1999;Mikheev et al., 1999;Soon et al., 2001;Ng and Cardie, 2002;Florian et al., 2004), and have been at the center of evaluations such as: MUC-6, MUC-7, and the CoNLL'02 and CoNLL'03 shared tasks."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) .']","['Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) .', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']"
CC1451,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,learning semantic lexicons from a partofspeech and semantically tagged corpus using inductive logic programming,"['Vincent Claveau', 'Pascale Sebillot', 'Cecile Fabre', 'Pierrette Bouillon']",method,"This paper describes an inductive logic programming learning method designed to acquire from a corpus specific Noun-Verb (N-V) pairs---relevant in information retrieval applications to perform index expansion---in order to build up semantic lexicons based on Pustejovsky's generative lexicon (GL) principles (Pustejovsky, 1995). In one of the components of this lexical model, called the qualia structure, words are described in terms of semantic roles. For example, the telic role indicates the purpose or function of an item (cut for knife), the agentive role its creation mode (build for house), etc. The qualia structure of a noun is mainly made up of verbal associations, encoding relational information. The learning method enables us to automatically extract, from a morpho-syntactically and semantically tagged corpus, N-V pairs whose elements are linked by one of the semantic relations defined in the qualia structure in GL. It also infers rules explaining what in the surrounding context distinguishes such pairs from others also found in sentences of the corpus but which are not relevant. Stress is put here on the learning efficiency that is required to be able to deal with all the available contextual information, and to produce linguistically meaningful rules.",ASARES is presented in detail in ( #AUTHOR_TAG ) .,"['The method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool.', 'ASARES is presented in detail in ( #AUTHOR_TAG ) .', 'We simply give a short account of its basic principles herein.']",5,"['In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #AUTHOR_TAG ) .']","['In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #AUTHOR_TAG ) .', 'For these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.']","['In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #AUTHOR_TAG ) .', 'For these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.']","['In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #AUTHOR_TAG ) .']","['In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #AUTHOR_TAG ) .', 'For these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.', 'Here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of their appearance in n-gram character contexts that appear in the training data.']","['In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #AUTHOR_TAG ) .', 'For these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.']","['In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #AUTHOR_TAG ) .', 'For these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.', 'Here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of their appearance in n-gram character contexts that appear in the training data.']","['In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #AUTHOR_TAG ) .']","['In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #AUTHOR_TAG ) .', 'For these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.', 'Here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of their appearance in n-gram character contexts that appear in the training data.', 'The character based model alone achieves a 94.5% exact match segmentation accuracy, considerably less accurate then the dictionary based model.']"
CC1452,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,structures mathematiques du langage,['Zellig Harris'],related work,,A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .,"['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996, for example), does not specify the relationship itself.', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not differentiated.']",0,"['These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #AUTHOR_TAG Arabic data .']","['These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #AUTHOR_TAG Arabic data .', 'The experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.']","['These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #AUTHOR_TAG Arabic data .', 'The experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.']","['These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #AUTHOR_TAG Arabic data .']","['These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #AUTHOR_TAG Arabic data .', 'The experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'In addition, we also report results on the official test data.']","['These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #AUTHOR_TAG Arabic data .', 'The experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.']","['These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #AUTHOR_TAG Arabic data .', 'The experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'In addition, we also report results on the official test data.']","['These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #AUTHOR_TAG Arabic data .']","['These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #AUTHOR_TAG Arabic data .', 'The experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'In addition, we also report results on the official test data.']"
CC1453,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,trec2001 crosslingual retrieval at bbn,"['J Xu', 'A Fraser', 'R Weischedel']",introduction,,"It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 ) .","['We separate the EDR task into two parts: a mention detection step, which identifies and classifies all the mentions in a text -and a coreference resolution step, which combinines the detected mentions into groups that refer to the same object.', 'In its entirety, the EDR task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non-named mentions (nominal and pronominal) and the requirement of grouping mentions into entities.', 'This is particularly true for Arabic where nominals and pronouns are also attached to the word they modify.', 'In fact, most Arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form Arabic surface forms (blank-delimited words).', 'In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 ) .']",0,"['Each punctuation symbol is considered a separate token.', 'Character classes , such as punctuation , are defined according to the Unicode Standard ( #AUTHOR_TAG ) .']","['Character classes , such as punctuation , are defined according to the Unicode Standard ( #AUTHOR_TAG ) .']","['Each punctuation symbol is considered a separate token.', 'Character classes , such as punctuation , are defined according to the Unicode Standard ( #AUTHOR_TAG ) .']","['We define a token and introduce whitespace boundaries between every span of one or more alphabetic or numeric characters.', 'Each punctuation symbol is considered a separate token.', 'Character classes , such as punctuation , are defined according to the Unicode Standard ( #AUTHOR_TAG ) .']","['Character classes , such as punctuation , are defined according to the Unicode Standard ( #AUTHOR_TAG ) .']","['We define a token and introduce whitespace boundaries between every span of one or more alphabetic or numeric characters.', 'Each punctuation symbol is considered a separate token.', 'Character classes , such as punctuation , are defined according to the Unicode Standard ( #AUTHOR_TAG ) .']","['Each punctuation symbol is considered a separate token.', 'Character classes , such as punctuation , are defined according to the Unicode Standard ( #AUTHOR_TAG ) .']","['(2) .', 'We define a token and introduce whitespace boundaries between every span of one or more alphabetic or numeric characters.', 'Each punctuation symbol is considered a separate token.', 'Character classes , such as punctuation , are defined according to the Unicode Standard ( #AUTHOR_TAG ) .']","['Character classes , such as punctuation , are defined according to the Unicode Standard ( #AUTHOR_TAG ) .']"
CC1454,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,improving machine learning approaches to coreference resolution,"['V Ng', 'C Cardie']",introduction,"Human speakers generally have no difficulty in determining which noun phrases in a text or dialogue refer to the same real-world entity. This task of identifying co-referring noun phrases --- noun phrase coreference resolution --- can present a serious challenge to a natural language processing system, however. Indeed, it is one of the critical problems that currently limits the performance of many practical natural language processing tasks.    State-of-the-art coreference resolution systems operate by relying on a set of hand-crafted heuristics that requires a lot of time and linguistic expertise to develop. Recently, machine learning techniques have been used to circumvent both of these problems by automating the acquisition of coreference resolution heuristics, yielding coreference systems that offer performance comparable to their heuristic-based counterparts. In this dissertation, we present a machine learning-based solution to noun phrase coreference that extends eariler work in the area and outperforms the best existing learning-based coreference engine on a suite of standard coreference data sets. Performance gains accrue from more effective use of the available training data via a set of linguistic and extra-linguistic extensions to the standard machine learning framework for coreference resolution","The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",0,"['In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG ) .']","['It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG ) .']","['In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG ) .']","['In fact, most Arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form Arabic surface forms (blank-delimited words).', 'In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG ) .']","['It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG ) .']","['In fact, most Arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form Arabic surface forms (blank-delimited words).', 'In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG ) .']","['In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG ) .']","['This is particularly true for Arabic where nominals and pronouns are also attached to the word they modify.', 'In fact, most Arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form Arabic surface forms (blank-delimited words).', 'In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG ) .']","['It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG ) .']"
CC1455,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,proceedings of ace evaluation and pi meeting,['NIST'],introduction,,"Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the fo-cus of several recent investigations (Bikel et al., 1997;Miller et al., 1998;Borthwick, 1999;Mikheev et al., 1999;Soon et al., 2001;Ng and Cardie, 2002;Florian et al., 2004), and have been at the center of evaluations such as: MUC-6, MUC-7, and the CoNLL'02 and CoNLL'03 shared tasks."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) .', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",5,['The coreference system system is similar to the Bell tree algorithm as described by ( #AUTHOR_TAG ) .'],['The coreference system system is similar to the Bell tree algorithm as described by ( #AUTHOR_TAG ) .'],['The coreference system system is similar to the Bell tree algorithm as described by ( #AUTHOR_TAG ) .'],['The coreference system system is similar to the Bell tree algorithm as described by ( #AUTHOR_TAG ) .'],['The coreference system system is similar to the Bell tree algorithm as described by ( #AUTHOR_TAG ) .'],['The coreference system system is similar to the Bell tree algorithm as described by ( #AUTHOR_TAG ) .'],['The coreference system system is similar to the Bell tree algorithm as described by ( #AUTHOR_TAG ) .'],['The coreference system system is similar to the Bell tree algorithm as described by ( #AUTHOR_TAG ) .'],['The coreference system system is similar to the Bell tree algorithm as described by ( #AUTHOR_TAG ) .']
CC1456,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a stochastic finitestate wordsegmentation algorithm for chinese,"['R Sproat', 'C Shih', 'W Gale', 'N Chang']",,,"In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #AUTHOR_TAG ) .","['In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #AUTHOR_TAG ) .', 'For these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.', 'Here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of their appearance in n-gram character contexts that appear in the training data.', 'The character based model alone achieves a 94.5% exact match segmentation accuracy, considerably less accurate then the dictionary based model.']",1,"['Arabic has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ) .']","['Arabic has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ) .', 'The formation of broken plurals is common, more complex and often irregular.']","['Arabic has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ) .', 'The formation of broken plurals is common, more complex and often irregular.']","['Arabic has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ) .']","['Arabic has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ) .', 'The formation of broken plurals is common, more complex and often irregular.', 'As an example, the plural form of the noun (man) is (men), which is formed by inserting the infix .']","['Arabic has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ) .', 'The formation of broken plurals is common, more complex and often irregular.']","['Arabic has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ) .', 'The formation of broken plurals is common, more complex and often irregular.', 'As an example, the plural form of the noun (man) is (men), which is formed by inserting the infix .']","['Arabic has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ) .']","['Arabic has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ) .', 'The formation of broken plurals is common, more complex and often irregular.', 'As an example, the plural form of the noun (man) is (men), which is formed by inserting the infix .', 'The plural form of the noun (book) is (books), which is formed by deleting the infix .']"
CC1457,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,automatic content extraction,['ACE'],conclusion,"This paper presents an integrated approach to automatically provide an overview of content on Thai websites based on tag cloud. This approach is intended to address the information overload issue by presenting the overview to users in order that they could assess whether the information meets their needs. The approach has incorporated Web content extraction, Thai word segmentation, and information presentation to generate a tag cloud in Thai language as an overview of the key content in the webpage. From the experimental study, the generated Thai Tag clouds are able to provide an overview of the tags which frequently appear in the title and body of the content. Moreover, the first few lines in the tag cloud offer an improved readability","These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #AUTHOR_TAG Arabic data .","['These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #AUTHOR_TAG Arabic data .', 'The experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'In addition, we also report results on the official test data.']",5,"['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).']"
CC1458,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,the unicode standard,"['Joan Aliprand', 'Julie Allen', 'Joe Becker', 'Mark Davis', 'Michael Everson']",,"The Unicode Standard is a global character set for worldwide computing covering the major modern scripts of the world as well as classical forms of Greek, Sanskrit, and Pali. The history and implications of Unicode Standard are discussed. The principles underpinning the design of the Unicode Standard are described with reference to those principles that also are present in USMARC and UNIMARC. Unicode give the potential to support every script. Expanding the character set would have consequences for transcription. Faithfulness of transcription has implications for retrieval. The addition of more characters to support more exact cataloging affects the economic cost of cataloging. The need for characters should be related not to the production of a surrogate for the physical item that has been cataloged, but to facilitating retrieval.","Character classes , such as punctuation , are defined according to the Unicode Standard ( #AUTHOR_TAG ) .","['(2) .', 'We define a token and introduce whitespace boundaries between every span of one or more alphabetic or numeric characters.', 'Each punctuation symbol is considered a separate token.', 'Character classes , such as punctuation , are defined according to the Unicode Standard ( #AUTHOR_TAG ) .']",5,"['We use a maximum entropy Markov model (MEMM) classifier.', 'The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ) .']","['The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ) .', 'One big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision.']","['We use a maximum entropy Markov model (MEMM) classifier.', 'The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ) .', 'One big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision.']","['We assign to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions.', 'We use a maximum entropy Markov model (MEMM) classifier.', 'The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ) .']","['The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ) .', 'One big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision.']","['We assign to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions.', 'We use a maximum entropy Markov model (MEMM) classifier.', 'The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ) .', 'One big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision.']","['We use a maximum entropy Markov model (MEMM) classifier.', 'The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ) .', 'One big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision.']","['We formulate the mention detection problem as a classification problem, which takes as input segmented Arabic text.', 'We assign to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions.', 'We use a maximum entropy Markov model (MEMM) classifier.', 'The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ) .']","['The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ) .', 'One big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision.']"
CC1459,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,empirical studies in strategies for arabic information retrieval,"['J Xu', 'A Fraser', 'R Weischedel']",introduction,,"It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG ) .","['We separate the EDR task into two parts: a mention detection step, which identifies and classifies all the mentions in a text -and a coreference resolution step, which combinines the detected mentions into groups that refer to the same object.', 'In its entirety, the EDR task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non-named mentions (nominal and pronominal) and the requirement of grouping mentions into entities.', 'This is particularly true for Arabic where nominals and pronouns are also attached to the word they modify.', 'In fact, most Arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form Arabic surface forms (blank-delimited words).', 'In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG ) .']",0,"['the mention sub-type , which is a sub-category of the mention type ( #AUTHOR_TAG ) ( e.g. OrgGovernmental , FacilityPath , etc. ) .']","['the mention sub-type , which is a sub-category of the mention type ( #AUTHOR_TAG ) ( e.g. OrgGovernmental , FacilityPath , etc. ) .']","['the mention sub-type , which is a sub-category of the mention type ( #AUTHOR_TAG ) ( e.g. OrgGovernmental , FacilityPath , etc. ) .']","['the mention sub-type , which is a sub-category of the mention type ( #AUTHOR_TAG ) ( e.g. OrgGovernmental , FacilityPath , etc. ) .']","['the mention sub-type , which is a sub-category of the mention type ( #AUTHOR_TAG ) ( e.g. OrgGovernmental , FacilityPath , etc. ) .']","['the mention sub-type , which is a sub-category of the mention type ( #AUTHOR_TAG ) ( e.g. OrgGovernmental , FacilityPath , etc. ) .']","['the mention sub-type , which is a sub-category of the mention type ( #AUTHOR_TAG ) ( e.g. OrgGovernmental , FacilityPath , etc. ) .']","['the mention sub-type , which is a sub-category of the mention type ( #AUTHOR_TAG ) ( e.g. OrgGovernmental , FacilityPath , etc. ) .']","['the mention sub-type , which is a sub-category of the mention type ( #AUTHOR_TAG ) ( e.g. OrgGovernmental , FacilityPath , etc. ) .']"
CC1460,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a mentionsynchronous coreference resolution algorithm based on the bell tree,"['Xiaoqiang Luo', 'Abe Ittycheriah', 'Hongyan Jing', 'Nanda Kambhatla', 'Salim Roukos']",,This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.,The coreference system system is similar to the Bell tree algorithm as described by ( #AUTHOR_TAG ) .,['The coreference system system is similar to the Bell tree algorithm as described by ( #AUTHOR_TAG ) .'],1,"['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).']"
CC1461,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,arabic verbs and essentials of grammar,"['J Wightwick', 'M Gaafar']",,"Your one-stop guide to mastering the basics of Arabic Can one book have all you need to communicate confidently in a new language? Yes, and that book is Arabic Verbs & Essentials of Grammar. It offers a solid foundation of major verbal and grammatical concepts of the language, from pronouns to idioms and expressions and from irregular verbs to expressions of time. Each unit is devoted to one topic, so you can find what you need right away and get focused instruction immediately. Concise yet thorough, the explanations are supported by numerous examples to help you master the different concepts. And for those tricky verbs, Arabic Verbs & Essentials of Grammar includes a Verb Index of the most common verbs, cross-referenced with the abundant verb tables appearing throughout the book. This book will give you: An excellent introduction to the basics of Arabic if you are a beginner or a quick, thorough reference if you already have experience in the language Contemporary usage of verbs, adjectives, pronouns, prepositions, conjunctions, and other grammar essentials Examples that reflect contemporary usage and real-life situations","Arabic has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ) .","['Arabic has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ) .', 'The formation of broken plurals is common, more complex and often irregular.', 'As an example, the plural form of the noun (man) is (men), which is formed by inserting the infix .', 'The plural form of the noun (book) is (books), which is formed by deleting the infix .', 'The plural form and the singular form may also be completely different (e.g. for woman, but for women).', 'The sound plurals are formed by adding plural suffixes to singular nouns (e.g., meaning researcher): the plural suffix is for feminine nouns in grammatical cases (e.g.,']",0,['#AUTHOR_TAG demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .'],"['#AUTHOR_TAG demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .', 'A trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules.']","['#AUTHOR_TAG demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .', 'A trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules.']",['#AUTHOR_TAG demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .'],"['#AUTHOR_TAG demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .', 'A trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules.']","['#AUTHOR_TAG demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .', 'A trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules.']","['#AUTHOR_TAG demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .', 'A trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules.']",['#AUTHOR_TAG demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .'],"['#AUTHOR_TAG demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .', 'A trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules.']"
CC1462,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a statistical model for multilingual entity detection and tracking,"['R Florian', 'H Hassan', 'A Ittycheriah', 'H Jing', 'N Kambhatla', 'X Luo', 'N Nicolov', 'S Roukos']",introduction,"Abstract : Entity detection and tracking is a relatively new addition to the repertoire of natural language tasks. In this paper, we present a statistical language-independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted text documents, and chaining them into clusters corresponding to each logical entity present in the text. Both the mention detection model and the novel entity tracking model can use arbitrary feature types, being able to integrate a wide array of lexical, syntactic and semantic features. In addition, the mention detection model crucially uses feature streams derived from different named entity classifiers. The proposed framework is evaluated with several experiments run in Arabic, Chinese and English texts; a system based on the approach described here and submitted to the latest Automatic Content Extraction (ACE) evaluation achieved top-tier results in all three evaluation languages.","The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",0,"['Both tasks are performed with a statistical framework: the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in .', 'Both systems are built around from the maximum-entropy technique ( #AUTHOR_TAG ) .']","['Both systems are built around from the maximum-entropy technique ( #AUTHOR_TAG ) .', 'We formulate the mention detection task as a sequence classification problem.']","['Both tasks are performed with a statistical framework: the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in .', 'Both systems are built around from the maximum-entropy technique ( #AUTHOR_TAG ) .']","['Both tasks are performed with a statistical framework: the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in .', 'Both systems are built around from the maximum-entropy technique ( #AUTHOR_TAG ) .']","['Both systems are built around from the maximum-entropy technique ( #AUTHOR_TAG ) .', 'We formulate the mention detection task as a sequence classification problem.', 'While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language.']","['Both tasks are performed with a statistical framework: the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in .', 'Both systems are built around from the maximum-entropy technique ( #AUTHOR_TAG ) .', 'We formulate the mention detection task as a sequence classification problem.']","['Both tasks are performed with a statistical framework: the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in .', 'Both systems are built around from the maximum-entropy technique ( #AUTHOR_TAG ) .', 'We formulate the mention detection task as a sequence classification problem.', 'While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language.']","['Both tasks are performed with a statistical framework: the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in .', 'Both systems are built around from the maximum-entropy technique ( #AUTHOR_TAG ) .']","['Both systems are built around from the maximum-entropy technique ( #AUTHOR_TAG ) .', 'We formulate the mention detection task as a sequence classification problem.', 'While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language.', 'The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.']"
CC1463,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a maximum entropy approach to natural language processing,"['A Berger', 'S Della Pietra', 'V Della Pietra']",,"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.","The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ) .","['We formulate the mention detection problem as a classification problem, which takes as input segmented Arabic text.', 'We assign to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions.', 'We use a maximum entropy Markov model (MEMM) classifier.', 'The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ) .', 'One big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision.']",0,"['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) .']","['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).']","['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).']","['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) .']","['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).', 'We formulate the mention detection task as a sequence classi cation problem.']","['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).']","['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).', 'We formulate the mention detection task as a sequence classi cation problem.']","['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) .']","['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).', 'We formulate the mention detection task as a sequence classi cation problem.', 'While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language.']"
CC1464,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,automatic content extraction,['ACE'],,"This paper presents an integrated approach to automatically provide an overview of content on Thai websites based on tag cloud. This approach is intended to address the information overload issue by presenting the overview to users in order that they could assess whether the information meets their needs. The approach has incorporated Web content extraction, Thai word segmentation, and information presentation to generate a tag cloud in Thai language as an overview of the key content in the webpage. From the experimental study, the generated Thai Tag clouds are able to provide an overview of the tags which frequently appear in the title and body of the content. Moreover, the first few lines in the tag cloud offer an improved readability","the mention sub-type , which is a sub-category of the mention type ( #AUTHOR_TAG ) ( e.g. OrgGovernmental , FacilityPath , etc. ) .","['the mention sub-type , which is a sub-category of the mention type ( #AUTHOR_TAG ) ( e.g. OrgGovernmental , FacilityPath , etc. ) .']",5,['The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ) .'],"['The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ) .', 'We denote these features as backward token tri-grams and forward token tri-grams for the previous and next context of t i respectively.']","['The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ) .', 'We denote these features as backward token tri-grams and forward token tri-grams for the previous and next context of t i respectively.']",['The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ) .'],"['The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ) .', 'We denote these features as backward token tri-grams and forward token tri-grams for the previous and next context of t i respectively.', 'For a token t i , the backward token n-gram feature will contains the previous n - 1 tokens in the history (t i-n+1 , . . .']","['The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ) .', 'We denote these features as backward token tri-grams and forward token tri-grams for the previous and next context of t i respectively.']","['The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ) .', 'We denote these features as backward token tri-grams and forward token tri-grams for the previous and next context of t i respectively.', 'For a token t i , the backward token n-gram feature will contains the previous n - 1 tokens in the history (t i-n+1 , . . .']",['The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ) .'],"['The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ) .', 'We denote these features as backward token tri-grams and forward token tri-grams for the previous and next context of t i respectively.', 'For a token t i , the backward token n-gram feature will contains the previous n - 1 tokens in the history (t i-n+1 , . . .', 't i-1 ) and the forward token n-gram feature will contains the next n - 1 tokens (t i+1 , . . .']"
CC1465,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a maximum entropy approach to named entity recognition,['A Borthwick'],introduction,"This thesis describes a novel statistical named-entity (i.e. ""proper name"") recognition system known as ""MENE"" (Maximum Entropy Named Entity). Named entity (N.E.) recognition is a form of information extraction in which we seek to classify every word in a document as being a person-name, organization, location, date, time, monetary value, percentage, or ""none of the above"". The task has particular significance for Internet search engines, machine translation, the automatic indexing of documents, and as a foundation for work on more complex information extraction tasks.  Two of the most significant problems facing the constructor of a named entity system are the questions of portability and system performance. A practical N.E. system will need to be ported frequently to new bodies of text and even to new languages. The challenge is to build a system which can be ported with minimal expense (in particular minimal programming by a computational linguist) while maintaining a high degree of accuracy in the new domains or languages.  MENE attempts to address these issues through the use of maximum entropy probabilistic modeling. It utilizes a very flexible object-based architecture which allows it to make use of a broad range of knowledge sources in making its tagging decisions. In the DARPA-sponsored MUC-7 named entity evaluation, the system displayed an accuracy rate which was well-above the median, demonstrating that it can achieve the performance goal. In addition, we demonstrate that the system can be used as a post-processing tool to enhance the output of a hand-coded named entity recognizer through experiments in which MENE improved on the performance of N.E. systems from three different sites. Furthermore, when all three external recognizers are combined under MENE, we are able to achieve very strong results which, in some cases, appear to be competitive with human performance.  Finally, we demonstrate the trans-lingual portability of the system. We ported the system to two Japanese-language named entity tasks, one of which involved a new named entity category, ""artifact"". Our results on these tasks were competitive with the best systems built by native Japanese speakers despite the fact that the author speaks no Japanese.","The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",0,['Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ) .'],"['Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ) .', 'For Arabic, since words are morphologically derived from a list of roots (stems), we expected that a feature based on the right and left stems would lead to improvement in system accuracy.']","['Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ) .', 'For Arabic, since words are morphologically derived from a list of roots (stems), we expected that a feature based on the right and left stems would lead to improvement in system accuracy.']",['Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ) .'],"['Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ) .', 'For Arabic, since words are morphologically derived from a list of roots (stems), we expected that a feature based on the right and left stems would lead to improvement in system accuracy.']","['Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ) .', 'For Arabic, since words are morphologically derived from a list of roots (stems), we expected that a feature based on the right and left stems would lead to improvement in system accuracy.']","['Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ) .', 'For Arabic, since words are morphologically derived from a list of roots (stems), we expected that a feature based on the right and left stems would lead to improvement in system accuracy.']",['Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ) .'],"['Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ) .', 'For Arabic, since words are morphologically derived from a list of roots (stems), we expected that a feature based on the right and left stems would lead to improvement in system accuracy.']"
CC1466,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,language model based arabic word segmentation,"['Y-S Lee', 'K Papineni', 'S Roukos', 'O Emam', 'H Hassan']",,"We approximate Arabic's rich morphology by a model that a word consists of a sequence of morphemes in the pattern prefix*-stem-suffix * (* denotes zero or more occurrences of a morpheme). Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus. The algorithm uses a trigram language model to determine the most probable morpheme sequence for a given input. The language model is initially estimated from a small manually segmented corpus of about 110,000 words. To improve the segmentation accuracy, we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus, and re-estimate the model parameters with the expanded vocabulary and training corpus. The resulting Arabic word segmentation system achieves around 97 % exact match accuracy on a test corpus containing 28,449 word tokens. We believe this is a state-of-the-art performance and the algorithm can be used for many highly inflected languages provided that one can create a small manually segmented corpus of the language of interest",#AUTHOR_TAG demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .,"['#AUTHOR_TAG demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .', 'A trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules.']",5,"['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).']"
CC1467,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a maximum entropy approach to natural language processing,"['A Berger', 'S Della Pietra', 'V Della Pietra']",introduction,"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.",Both systems are built around from the maximum-entropy technique ( #AUTHOR_TAG ) .,"['Both tasks are performed with a statistical framework: the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in .', 'Both systems are built around from the maximum-entropy technique ( #AUTHOR_TAG ) .', 'We formulate the mention detection task as a sequence classification problem.', 'While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language.', 'The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'We begin with a segmentation of the written text before starting the classification.', 'This segmentation process consists of separating the normal whitespace delimited words into (hypothesized) prefixes, stems, and suffixes, which become the subject of analysis (tokens).', 'The resulting granularity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label (for instance, in the case of nominal and pronominal mentions).', 'Additionally, because the prefixes and suffixes are quite frequent, directly processing unsegmented words results in significant data sparseness.']",5,"['We seeked to exploit this ability to generalize to improve the dictionary based model.', 'As in ( #AUTHOR_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .']","['As in ( #AUTHOR_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .', 'In our case, the character n-gram model is used to segment a portion of the Arabic Gigaword corpus.']","['We seeked to exploit this ability to generalize to improve the dictionary based model.', 'As in ( #AUTHOR_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .', 'In our case, the character n-gram model is used to segment a portion of the Arabic Gigaword corpus.']","['However, an analysis of the errors indicated that the character based model is more effective at segmenting words that do not appear in the training data.', 'We seeked to exploit this ability to generalize to improve the dictionary based model.', 'As in ( #AUTHOR_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .']","['As in ( #AUTHOR_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .', 'In our case, the character n-gram model is used to segment a portion of the Arabic Gigaword corpus.', 'From this, we create a vocabulary of stems and affixes by requiring that tokens appear more than twice in the supervised training data or more than ten times in the unsupervised, segmented corpus.']","['However, an analysis of the errors indicated that the character based model is more effective at segmenting words that do not appear in the training data.', 'We seeked to exploit this ability to generalize to improve the dictionary based model.', 'As in ( #AUTHOR_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .', 'In our case, the character n-gram model is used to segment a portion of the Arabic Gigaword corpus.']","['We seeked to exploit this ability to generalize to improve the dictionary based model.', 'As in ( #AUTHOR_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .', 'In our case, the character n-gram model is used to segment a portion of the Arabic Gigaword corpus.', 'From this, we create a vocabulary of stems and affixes by requiring that tokens appear more than twice in the supervised training data or more than ten times in the unsupervised, segmented corpus.']","['However, an analysis of the errors indicated that the character based model is more effective at segmenting words that do not appear in the training data.', 'We seeked to exploit this ability to generalize to improve the dictionary based model.', 'As in ( #AUTHOR_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .']","['As in ( #AUTHOR_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .', 'In our case, the character n-gram model is used to segment a portion of the Arabic Gigaword corpus.', 'From this, we create a vocabulary of stems and affixes by requiring that tokens appear more than twice in the supervised training data or more than ten times in the unsupervised, segmented corpus.']"
CC1468,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a statistical model for multilingual entity detection and tracking,"['R Florian', 'H Hassan', 'A Ittycheriah', 'H Jing', 'N Kambhatla', 'X Luo', 'N Nicolov', 'S Roukos']",introduction,"Abstract : Entity detection and tracking is a relatively new addition to the repertoire of natural language tasks. In this paper, we present a statistical language-independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted text documents, and chaining them into clusters corresponding to each logical entity present in the text. Both the mention detection model and the novel entity tracking model can use arbitrary feature types, being able to integrate a wide array of lexical, syntactic and semantic features. In addition, the mention detection model crucially uses feature streams derived from different named entity classifiers. The proposed framework is evaluated with several experiments run in Arabic, Chinese and English texts; a system based on the approach described here and submitted to the latest Automatic Content Extraction (ACE) evaluation achieved top-tier results in all three evaluation languages.","Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) .","['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).', 'We formulate the mention detection task as a sequence classi cation problem.', 'While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language.', 'The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'We begin with a segmentation of the written text before starting the classification.', 'This segmentation process consists of separating the normal whitespace delimited words into (hypothesized) prefixes, stems, and suffixes, which become the subject of analysis (tokens).', 'The resulting granulariity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label (for instance, in the case of nominal and pronominal mentions).', 'Additionally, because the pre xes and su_xes are quite frequent, directly processing unsegmented words results in signi cant data sparseness.']",1,"['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).']"
CC1469,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a statistical model for multilingual entity detection and tracking,"['R Florian', 'H Hassan', 'A Ittycheriah', 'H Jing', 'N Kambhatla', 'X Luo', 'N Nicolov', 'S Roukos']",,"Abstract : Entity detection and tracking is a relatively new addition to the repertoire of natural language tasks. In this paper, we present a statistical language-independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted text documents, and chaining them into clusters corresponding to each logical entity present in the text. Both the mention detection model and the novel entity tracking model can use arbitrary feature types, being able to integrate a wide array of lexical, syntactic and semantic features. In addition, the mention detection model crucially uses feature streams derived from different named entity classifiers. The proposed framework is evaluated with several experiments run in Arabic, Chinese and English texts; a system based on the approach described here and submitted to the latest Automatic Content Extraction (ACE) evaluation achieved top-tier results in all three evaluation languages.",The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ) .,"['The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ) .', 'We denote these features as backward token tri-grams and forward token tri-grams for the previous and next context of t i respectively.', 'For a token t i , the backward token n-gram feature will contains the previous n â 1 tokens in the history (t iân+1 , . . .', 't iâ1 ) and the forward token n-gram feature will contains the next n â 1 tokens (t i+1 , . . .', 't i+nâ1 ).']",0,"['The system is trained on the Arabic ACE 2003 and part of the 2004 data.', 'We introduce here a clearly defined and replicable split of the #AUTHOR_TAG data , so that future investigations can accurately and correctly compare against the results presented here .']","['We introduce here a clearly defined and replicable split of the #AUTHOR_TAG data , so that future investigations can accurately and correctly compare against the results presented here .']","['The system is trained on the Arabic ACE 2003 and part of the 2004 data.', 'We introduce here a clearly defined and replicable split of the #AUTHOR_TAG data , so that future investigations can accurately and correctly compare against the results presented here .']","['The system is trained on the Arabic ACE 2003 and part of the 2004 data.', 'We introduce here a clearly defined and replicable split of the #AUTHOR_TAG data , so that future investigations can accurately and correctly compare against the results presented here .']","['We introduce here a clearly defined and replicable split of the #AUTHOR_TAG data , so that future investigations can accurately and correctly compare against the results presented here .']","['The system is trained on the Arabic ACE 2003 and part of the 2004 data.', 'We introduce here a clearly defined and replicable split of the #AUTHOR_TAG data , so that future investigations can accurately and correctly compare against the results presented here .']","['The system is trained on the Arabic ACE 2003 and part of the 2004 data.', 'We introduce here a clearly defined and replicable split of the #AUTHOR_TAG data , so that future investigations can accurately and correctly compare against the results presented here .']","['The system is trained on the Arabic ACE 2003 and part of the 2004 data.', 'We introduce here a clearly defined and replicable split of the #AUTHOR_TAG data , so that future investigations can accurately and correctly compare against the results presented here .']","['We introduce here a clearly defined and replicable split of the #AUTHOR_TAG data , so that future investigations can accurately and correctly compare against the results presented here .']"
CC1470,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a mentionsynchronous coreference resolution algorithm based on the bell tree,"['Xiaoqiang Luo', 'Abe Ittycheriah', 'Hongyan Jing', 'Nanda Kambhatla', 'Salim Roukos']",,This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.,Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ) .,"['Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ) .', 'For Arabic, since words are morphologically derived from a list of roots (stems), we expected that a feature based on the right and left stems would lead to improvement in system accuracy.']",5,"['The second machine is a dictionary that accepts characters and produces identifiers corresponding to dictionary entries.', 'The final machine is a trigram language model , specifically a Kneser-Ney ( #AUTHOR_TAG ) based backoff language model .']","['The final machine is a trigram language model , specifically a Kneser-Ney ( #AUTHOR_TAG ) based backoff language model .', 'Differing from (Lee et al., 2003), we have also introduced an explicit model for un-known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty.']","['The second machine is a dictionary that accepts characters and produces identifiers corresponding to dictionary entries.', 'The final machine is a trigram language model , specifically a Kneser-Ney ( #AUTHOR_TAG ) based backoff language model .', 'Differing from (Lee et al., 2003), we have also introduced an explicit model for un-known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty.']","['The first machine, illustrated in Figure 1 encodes the prefix and suffix expansion rules, producing a lattice of possible segmentations.', 'The second machine is a dictionary that accepts characters and produces identifiers corresponding to dictionary entries.', 'The final machine is a trigram language model , specifically a Kneser-Ney ( #AUTHOR_TAG ) based backoff language model .']","['The final machine is a trigram language model , specifically a Kneser-Ney ( #AUTHOR_TAG ) based backoff language model .', 'Differing from (Lee et al., 2003), we have also introduced an explicit model for un-known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty.', 'Using 0.5M words from the combined Arabic Treebanks 1V2, 2V2 and 3V1, the dictionary based segmenter achieves a exact word match 97.8% correct segmentation.']","['The first machine, illustrated in Figure 1 encodes the prefix and suffix expansion rules, producing a lattice of possible segmentations.', 'The second machine is a dictionary that accepts characters and produces identifiers corresponding to dictionary entries.', 'The final machine is a trigram language model , specifically a Kneser-Ney ( #AUTHOR_TAG ) based backoff language model .', 'Differing from (Lee et al., 2003), we have also introduced an explicit model for un-known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty.']","['The second machine is a dictionary that accepts characters and produces identifiers corresponding to dictionary entries.', 'The final machine is a trigram language model , specifically a Kneser-Ney ( #AUTHOR_TAG ) based backoff language model .', 'Differing from (Lee et al., 2003), we have also introduced an explicit model for un-known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty.', 'Using 0.5M words from the combined Arabic Treebanks 1V2, 2V2 and 3V1, the dictionary based segmenter achieves a exact word match 97.8% correct segmentation.']","['In our latest implementation of this algorithm, we have recast this segmentation strategy as the composition of three distinct finite state machines.', 'The first machine, illustrated in Figure 1 encodes the prefix and suffix expansion rules, producing a lattice of possible segmentations.', 'The second machine is a dictionary that accepts characters and produces identifiers corresponding to dictionary entries.', 'The final machine is a trigram language model , specifically a Kneser-Ney ( #AUTHOR_TAG ) based backoff language model .']","['The final machine is a trigram language model , specifically a Kneser-Ney ( #AUTHOR_TAG ) based backoff language model .', 'Differing from (Lee et al., 2003), we have also introduced an explicit model for un-known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty.', 'Using 0.5M words from the combined Arabic Treebanks 1V2, 2V2 and 3V1, the dictionary based segmenter achieves a exact word match 97.8% correct segmentation.']"
CC1471,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a machine learning approach to coreference resolution of noun phrases,"['W M Soon', 'H T Ng', 'C Y Lim']",introduction,"this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of &quot;organization,&quot; &quot;person,&quot; or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data set","The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",0,"['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).']"
CC1472,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,language model based arabic word segmentation,"['Y-S Lee', 'K Papineni', 'S Roukos', 'O Emam', 'H Hassan']",,"We approximate Arabic's rich morphology by a model that a word consists of a sequence of morphemes in the pattern prefix*-stem-suffix * (* denotes zero or more occurrences of a morpheme). Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus. The algorithm uses a trigram language model to determine the most probable morpheme sequence for a given input. The language model is initially estimated from a small manually segmented corpus of about 110,000 words. To improve the segmentation accuracy, we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus, and re-estimate the model parameters with the expanded vocabulary and training corpus. The resulting Arabic word segmentation system achieves around 97 % exact match accuracy on a test corpus containing 28,449 word tokens. We believe this is a state-of-the-art performance and the algorithm can be used for many highly inflected languages provided that one can create a small manually segmented corpus of the language of interest","As in ( #AUTHOR_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .","['However, an analysis of the errors indicated that the character based model is more effective at segmenting words that do not appear in the training data.', 'We seeked to exploit this ability to generalize to improve the dictionary based model.', 'As in ( #AUTHOR_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .', 'In our case, the character n-gram model is used to segment a portion of the Arabic Gigaword corpus.', 'From this, we create a vocabulary of stems and affixes by requiring that tokens appear more than twice in the supervised training data or more than ten times in the unsupervised, segmented corpus.']",5,"['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.']","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]","[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).']"
CC1473,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,named entity recognition without gazetteers,"['A Mikheev', 'M Moens', 'C Grover']",introduction,"It is often claimed that Named Entity  recognition systems need extensive  gazetteers|lists of names of people, organisations,  locations, and other named  entities. Indeed, the compilation of such  gazetteers is sometimes mentioned as a  bottleneck in the design of Named Entity  recognition systems.  We report on a Named Entity recognition  system which combines rule-based  grammars with statistical (maximum entropy)  models. We report on the system  &apos;s performance with gazetteers of different  types and dierent sizes, using test  material from the muc{7 competition.  We show that, for the text type and task  of this competition, it is sucient to use  relatively small gazetteers of well-known  names, rather than large gazetteers of  low-frequency names. We conclude with  observations about the domain independence  of the competition and of our experiments.  1 Introduction  Named Entity recognition involves processing a text and identifying certain occurrences of words or expressions ..","The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",0,"['We report results with two metrics: ECM-F and ACE- Value.', 'ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #AUTHOR_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .']","['ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #AUTHOR_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .', 'The result is shown in Table 4: the baseline numbers without stem features are listed under \\Base,"" and the results of the coreference system with stem features are listed under \\Base+Stem.""']","['We report results with two metrics: ECM-F and ACE- Value.', 'ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #AUTHOR_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .', 'The result is shown in Table 4: the baseline numbers without stem features are listed under \\Base,"" and the results of the coreference system with stem features are listed under \\Base+Stem.""']","['True mentions mean that input to the coreference system are mentions marked by human, while system mentions are output from the mention detection system.', 'We report results with two metrics: ECM-F and ACE- Value.', 'ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #AUTHOR_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .']","['ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #AUTHOR_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .', 'The result is shown in Table 4: the baseline numbers without stem features are listed under \\Base,"" and the results of the coreference system with stem features are listed under \\Base+Stem.""']","['True mentions mean that input to the coreference system are mentions marked by human, while system mentions are output from the mention detection system.', 'We report results with two metrics: ECM-F and ACE- Value.', 'ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #AUTHOR_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .', 'The result is shown in Table 4: the baseline numbers without stem features are listed under \\Base,"" and the results of the coreference system with stem features are listed under \\Base+Stem.""']","['We report results with two metrics: ECM-F and ACE- Value.', 'ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #AUTHOR_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .', 'The result is shown in Table 4: the baseline numbers without stem features are listed under \\Base,"" and the results of the coreference system with stem features are listed under \\Base+Stem.""']","['We test the two systems on both ""true"" and system mentions of the devtest set.', 'True mentions mean that input to the coreference system are mentions marked by human, while system mentions are output from the mention detection system.', 'We report results with two metrics: ECM-F and ACE- Value.', 'ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #AUTHOR_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .']","['ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #AUTHOR_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .', 'The result is shown in Table 4: the baseline numbers without stem features are listed under \\Base,"" and the results of the coreference system with stem features are listed under \\Base+Stem.""']"
CC1474,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,automatic content extraction,['ACE'],experiments,"This paper presents an integrated approach to automatically provide an overview of content on Thai websites based on tag cloud. This approach is intended to address the information overload issue by presenting the overview to users in order that they could assess whether the information meets their needs. The approach has incorporated Web content extraction, Thai word segmentation, and information presentation to generate a tag cloud in Thai language as an overview of the key content in the webpage. From the experimental study, the generated Thai Tag clouds are able to provide an overview of the tags which frequently appear in the title and body of the content. Moreover, the first few lines in the tag cloud offer an improved readability","We introduce here a clearly defined and replicable split of the #AUTHOR_TAG data , so that future investigations can accurately and correctly compare against the results presented here .","['The system is trained on the Arabic ACE 2003 and part of the 2004 data.', 'We introduce here a clearly defined and replicable split of the #AUTHOR_TAG data , so that future investigations can accurately and correctly compare against the results presented here .']",5,"['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .']","['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .', 'The formation of broken plurals is common, more complex and often irregular.']","['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .', 'The formation of broken plurals is common, more complex and often irregular.']","['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .']","['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .', 'The formation of broken plurals is common, more complex and often irregular.', 'As an example, the plural form of the noun (man) is (men), which is formed by inserting the infix .']","['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .', 'The formation of broken plurals is common, more complex and often irregular.']","['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .', 'The formation of broken plurals is common, more complex and often irregular.', 'As an example, the plural form of the noun (man) is (men), which is formed by inserting the infix .']","['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .']","['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .', 'The formation of broken plurals is common, more complex and often irregular.', 'As an example, the plural form of the noun (man) is (men), which is formed by inserting the infix .', 'The plural form of the noun (book) is (books), which is formed by deleting the infix .']"
CC1475,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,an empirical study of smoothing techinques for language modeling,"['S F Chen', 'J Goodman']",,"We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including t...","The final machine is a trigram language model , specifically a Kneser-Ney ( #AUTHOR_TAG ) based backoff language model .","['In our latest implementation of this algorithm, we have recast this segmentation strategy as the composition of three distinct finite state machines.', 'The first machine, illustrated in Figure 1 encodes the prefix and suffix expansion rules, producing a lattice of possible segmentations.', 'The second machine is a dictionary that accepts characters and produces identifiers corresponding to dictionary entries.', 'The final machine is a trigram language model , specifically a Kneser-Ney ( #AUTHOR_TAG ) based backoff language model .', 'Differing from (Lee et al., 2003), we have also introduced an explicit model for un-known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty.', 'Using 0.5M words from the combined Arabic Treebanks 1V2, 2V2 and 3V1, the dictionary based segmenter achieves a exact word match 97.8% correct segmentation.']",5,"['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .']","['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).']","['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).']","['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .']","['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).', 'We formulate the mention detection task as a sequence classification problem.']","['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).']","['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).', 'We formulate the mention detection task as a sequence classification problem.']","['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .']","['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).', 'We formulate the mention detection task as a sequence classification problem.', 'While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language.']"
CC1476,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,bbn description of the sift system as used for muc7,"['S Miller', 'M Crystal', 'H Fox', 'L Ramshaw', 'R Schwarz', 'R Stone', 'R Weischedel']",introduction,,"The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",0,"['We want to investigate the usefulness of stem n- gram features in the mention detection system.', ""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) .""]","[""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."", 'Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system.']","['We want to investigate the usefulness of stem n- gram features in the mention detection system.', ""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) .""]","['We want to investigate the usefulness of stem n- gram features in the mention detection system.', ""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) .""]","[""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."", 'Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system.', 'The score that the ACE community uses (ACE value) attributes a higher importance (outlined by its weight) to the main type compared to other sub-tasks, such as the mention level and the class.']","['We want to investigate the usefulness of stem n- gram features in the mention detection system.', ""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."", 'Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system.']","['We want to investigate the usefulness of stem n- gram features in the mention detection system.', ""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."", 'Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system.', 'The score that the ACE community uses (ACE value) attributes a higher importance (outlined by its weight) to the main type compared to other sub-tasks, such as the mention level and the class.']","['We want to investigate the usefulness of stem n- gram features in the mention detection system.', ""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) .""]","[""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."", 'Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system.', 'The score that the ACE community uses (ACE value) attributes a higher importance (outlined by its weight) to the main type compared to other sub-tasks, such as the mention level and the class.', 'Hence, to build our mention detection system we spent a lot of effort in improving the rst step: detecting the mention boundary and their main type.']"
CC1477,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,nymble a highperformance learning namefinder,"['D M Bikel', 'S Miller', 'R Schwartz', 'R Weischedel']",introduction,,"The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",0,"['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']","['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']","['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']","['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']","['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']","['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']","['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']","['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']","['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']"
CC1478,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a mentionsynchronous coreference resolution algorithm based on the bell tree,"['Xiaoqiang Luo', 'Abe Ittycheriah', 'Hongyan Jing', 'Nanda Kambhatla', 'Salim Roukos']",experiments,This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.,"ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #AUTHOR_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .","['In this section, we present the coreference results on the devtest defined earlier.', 'First, to see the effect of stem matching features, we compare two coreference systems: one with the stem features, the other with- out.', 'We test the two systems on both ""true"" and system mentions of the devtest set.', 'True mentions mean that input to the coreference system are mentions marked by human, while system mentions are output from the mention detection system.', 'We report results with two metrics: ECM-F and ACE- Value.', 'ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #AUTHOR_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .', 'The result is shown in Table 4: the baseline numbers without stem features are listed under \\Base,"" and the results of the coreference system with stem features are listed under \\Base+Stem.""']",5,"['Thus, they were not supervised during the experiment.', '#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .']",['#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .'],"['Thus, they were not supervised during the experiment.', '#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .']","['Test subjects were invited via email to participate in the experiment.', 'Thus, they were not supervised during the experiment.', '#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .']",['#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .'],"['Test subjects were invited via email to participate in the experiment.', 'Thus, they were not supervised during the experiment.', '#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .']","['Thus, they were not supervised during the experiment.', '#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .']","['We developed a web-based interface to obtain human judgments of semantic relatedness for each automatically generated concept pair.', 'Test subjects were invited via email to participate in the experiment.', 'Thus, they were not supervised during the experiment.', '#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .']","['#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .', 'Their results differed particularly in cases of antonymy or distributionally related pairs.', 'We created a manual with a detailed introduction to SR stressing the crucial points.', 'The manual was presented to the subjects before the experiment and could be re-accessed at any time.']"
CC1479,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,building an arabic stemmer for information retrieval,"['Aitao Chen', 'Fredic Gey']",,"In TREC 2002 the Berkeley group participated only in the English-Arabic cross-language retrieval (CLIR) track. One Arabic monolingual run and three English-Arabic cross-language runs were submitted. Our approach to the crosslanguage retrieval was to translate the English topics into Arabic using online English-Arabic machine translation systems. The four official runs are named as BKYMON, BKYCL1, BKYCL2, and BKYCL3. The BKYMON is the Arabic monolingual run, and the other three runs are English-to-Arabic cross-language runs. This paper reports on the construction of an Arabic stoplist and two Arabic stemmers, and the experiments on Arabic monolingual retrieval, English-to-Arabic cross-language retrieval.","Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .","['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .', 'The formation of broken plurals is common, more complex and often irregular.', 'As an example, the plural form of the noun (man) is (men), which is formed by inserting the infix .', 'The plural form of the noun (book) is (books), which is formed by deleting the infix .', 'The plural form and the singular form may also be completely different (e.g. for woman, but for women).', 'The sound plurals are formed by adding plural suffixes to singular nouns (e.g., meaning researcher): the plural suffix is for feminine nouns in grammatical cases (e.g.,']",0,"['documents, parts of a document (e.g.', 'words and their surrounding context), words or concepts ( #AUTHOR_TAG ). 2 Linguistic distance between words is inverse to their semantic similarity or relatedness.']","['words and their surrounding context), words or concepts ( #AUTHOR_TAG ). 2 Linguistic distance between words is inverse to their semantic similarity or relatedness.']","['documents, parts of a document (e.g.', 'words and their surrounding context), words or concepts ( #AUTHOR_TAG ). 2 Linguistic distance between words is inverse to their semantic similarity or relatedness.']","['It is defined on different kinds of textual units, e.g.', 'documents, parts of a document (e.g.', 'words and their surrounding context), words or concepts ( #AUTHOR_TAG ). 2 Linguistic distance between words is inverse to their semantic similarity or relatedness.']","['words and their surrounding context), words or concepts ( #AUTHOR_TAG ). 2 Linguistic distance between words is inverse to their semantic similarity or relatedness.']","['It is defined on different kinds of textual units, e.g.', 'documents, parts of a document (e.g.', 'words and their surrounding context), words or concepts ( #AUTHOR_TAG ). 2 Linguistic distance between words is inverse to their semantic similarity or relatedness.']","['documents, parts of a document (e.g.', 'words and their surrounding context), words or concepts ( #AUTHOR_TAG ). 2 Linguistic distance between words is inverse to their semantic similarity or relatedness.']","['Linguistic distance plays an important role in many applications like information retrieval, word sense disambiguation, text summarization or spelling correction.', 'It is defined on different kinds of textual units, e.g.', 'documents, parts of a document (e.g.', 'words and their surrounding context), words or concepts ( #AUTHOR_TAG ). 2 Linguistic distance between words is inverse to their semantic similarity or relatedness.']","['words and their surrounding context), words or concepts ( #AUTHOR_TAG ). 2 Linguistic distance between words is inverse to their semantic similarity or relatedness.']"
CC1480,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a mentionsynchronous coreference resolution algorithm based on the bell tree,"['Xiaoqiang Luo', 'Abe Ittycheriah', 'Hongyan Jing', 'Nanda Kambhatla', 'Salim Roukos']",introduction,This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.,"Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .","['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).', 'We formulate the mention detection task as a sequence classification problem.', 'While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language.', 'The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'We begin with a segmentation of the written text before starting the classification.', 'This segmentation process consists of separating the normal whitespace delimited words into (hypothesized) prefixes, stems, and suffixes, which become the subject of analysis (tokens).', 'The resulting granularity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label (for instance, in the case of nominal and pronominal mentions).', 'Additionally, because the prefixes and suffixes are quite frequent, directly processing unsegmented words results in significant data sparseness.']",1,"['They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #AUTHOR_TAG ) . Table 1: Comparison of previous experiments.']","['In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #AUTHOR_TAG ) . Table 1: Comparison of previous experiments.']","['They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #AUTHOR_TAG ) . Table 1: Comparison of previous experiments.']","['Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #AUTHOR_TAG ) . Table 1: Comparison of previous experiments.']","['In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #AUTHOR_TAG ) . Table 1: Comparison of previous experiments.']","['Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #AUTHOR_TAG ) . Table 1: Comparison of previous experiments.']","['They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #AUTHOR_TAG ) . Table 1: Comparison of previous experiments.']","['This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #AUTHOR_TAG ) . Table 1: Comparison of previous experiments.']","['In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #AUTHOR_TAG ) . Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.', 'Therefore, we automatically create word pairs using a corpus-based approach.']"
CC1481,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,proceedings of ace evaluation and pi meeting,['NIST'],experiments,,"As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) .","['We want to investigate the usefulness of stem n- gram features in the mention detection system.', ""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."", 'Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system.', 'The score that the ACE community uses (ACE value) attributes a higher importance (outlined by its weight) to the main type compared to other sub-tasks, such as the mention level and the class.', 'Hence, to build our mention detection system we spent a lot of effort in improving the rst step: detecting the mention boundary and their main type.', 'In this paper, we report the results in terms of precision, recall, and F-measure3.']",5,"['According to #AUTHOR_TAG , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments .']","['According to #AUTHOR_TAG , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments .']","['According to #AUTHOR_TAG , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments .']","['According to #AUTHOR_TAG , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments .']","['According to #AUTHOR_TAG , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments .']","['According to #AUTHOR_TAG , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments .']","['According to #AUTHOR_TAG , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments .']","['According to #AUTHOR_TAG , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments .']","['According to #AUTHOR_TAG , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments .']"
CC1482,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a maximum entropy approach to natural language processing,"['A Berger', 'S Della Pietra', 'V Della Pietra']",,"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.","where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .","['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']",5,"['However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .']",['#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .'],"['However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .']","['To create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain- ing could be used.', 'However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .']",['#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .'],"['To create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain- ing could be used.', 'However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .']","['However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .']","['The distribution of averaged human judgments on the whole test set (see Figure 3) is almost balanced with a slight underrepresentation of highly related concepts.', 'To create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain- ing could be used.', 'However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .']","['#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .', 'This empty band is not observed here.', 'However, Figure 4 shows the distribution of averaged judgments with the highest agreement between annotators (standard deviation < 0.8).', 'The plot clearly shows an empty horizontal band with no judgments.']"
CC1483,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing semantic relatedness across parts of speech,['Iryna Gurevych'],experiments,,#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .,"['We developed a web-based interface to obtain human judgments of semantic relatedness for each automatically generated concept pair.', 'Test subjects were invited via email to participate in the experiment.', 'Thus, they were not supervised during the experiment.', '#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .', 'Their results differed particularly in cases of antonymy or distributionally related pairs.', 'We created a manual with a detailed introduction to SR stressing the crucial points.', 'The manual was presented to the subjects before the experiment and could be re-accessed at any time.', 'During the experiment, one concept pair at a time was presented to the test subjects in random ordering.', 'Subjects had to assign a discrete relatedness value {0,1,2,3,4} to each pair.', ""Figure 2 shows the system's GUI.""]",4,"['She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG .']",['This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG .'],"['She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG .']","['Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German.', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG .']",['This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG .'],"['Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German.', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG .']","['She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG .']","['Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', 'Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German.', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG .']","['This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG .', 'Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.']"
CC1484,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing similarity,"['Ludovic Lebart', 'Martin Rajman']",introduction,"In this paper we study similarity measures for moving curves which can, for example, model changing coastlines or retreating glacier termini. Points on a moving curve have two parameters, namely the position along the curve as well as time. We therefore focus on similarity measures for surfaces, specifically the Fr 'echet distance between surfaces. While the Fr 'echet distance between surfaces is not even known to be computable, we show for variants arising in the context of moving curves that they are polynomial-time solvable or NP-complete depending on the restrictions imposed on how the moving curves are matched. We achieve the polynomial-time solutions by a novel approach for computing a surface in the so-called free-space diagram based on max-flow min-cut duality","words and their surrounding context), words or concepts ( #AUTHOR_TAG ). 2 Linguistic distance between words is inverse to their semantic similarity or relatedness.","['Linguistic distance plays an important role in many applications like information retrieval, word sense disambiguation, text summarization or spelling correction.', 'It is defined on different kinds of textual units, e.g.', 'documents, parts of a document (e.g.', 'words and their surrounding context), words or concepts ( #AUTHOR_TAG ). 2 Linguistic distance between words is inverse to their semantic similarity or relatedness.']",0,"['Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'This experiment was again replicated by #AUTHOR_TAG with 10 subjects .']","['This experiment was again replicated by #AUTHOR_TAG with 10 subjects .', 'Table 1']","['Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'This experiment was again replicated by #AUTHOR_TAG with 10 subjects .', 'Table 1']","['Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.', 'Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'This experiment was again replicated by #AUTHOR_TAG with 10 subjects .']","['This experiment was again replicated by #AUTHOR_TAG with 10 subjects .', 'Table 1']","['Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.', 'Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'This experiment was again replicated by #AUTHOR_TAG with 10 subjects .', 'Table 1']","['Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'This experiment was again replicated by #AUTHOR_TAG with 10 subjects .', 'Table 1']","['In the seminal work by Rubenstein and Goodenough (1965), similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.', 'Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'This experiment was again replicated by #AUTHOR_TAG with 10 subjects .']","['This experiment was again replicated by #AUTHOR_TAG with 10 subjects .', 'Table 1']"
CC1485,W06-1104,Automatically creating datasets for measures of semantic relatedness,identifying semantic relations and functional properties of human verb associations,"['Sabine Schulte im Walde', 'Alissa Melinger']",related work,"This paper uses human verb associations as the basis for an investigation of verb properties, focusing on semantic verb relations and prominent nominal features. First, the lexical semantic taxonymy GermaNet is checked on the types of classic semantic relations in our data; verb-verb pairs not covered by GermaNet can help to detect missing links in the taxonomy, and provide a useful basis for defining non-classical relations. Second, a statistical grammar is used for determining the conceptual roles of the noun responses. We present prominent syntax-semantic roles and evidence for the usefulness of co-occurrence information in distributional verb descriptions.","In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #AUTHOR_TAG ) . Table 1: Comparison of previous experiments.","['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', 'Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German.', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #AUTHOR_TAG ) . Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.', 'Therefore, we automatically create word pairs using a corpus-based approach.']",0,"['This is motivated by the aim to enable research in information retrieval incorporating SR measures.', ""In particular , the `` Semantic Information Retrieval '' project ( SIR #AUTHOR_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .""]","[""In particular , the `` Semantic Information Retrieval '' project ( SIR #AUTHOR_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .""]","['This is motivated by the aim to enable research in information retrieval incorporating SR measures.', ""In particular , the `` Semantic Information Retrieval '' project ( SIR #AUTHOR_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .""]","['We extracted word pairs from three different domain-specific corpora (see Table 2).', 'This is motivated by the aim to enable research in information retrieval incorporating SR measures.', ""In particular , the `` Semantic Information Retrieval '' project ( SIR #AUTHOR_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .""]","[""In particular , the `` Semantic Information Retrieval '' project ( SIR #AUTHOR_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .""]","['We extracted word pairs from three different domain-specific corpora (see Table 2).', 'This is motivated by the aim to enable research in information retrieval incorporating SR measures.', ""In particular , the `` Semantic Information Retrieval '' project ( SIR #AUTHOR_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .""]","['This is motivated by the aim to enable research in information retrieval incorporating SR measures.', ""In particular , the `` Semantic Information Retrieval '' project ( SIR #AUTHOR_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .""]","['We extracted word pairs from three different domain-specific corpora (see Table 2).', 'This is motivated by the aim to enable research in information retrieval incorporating SR measures.', ""In particular , the `` Semantic Information Retrieval '' project ( SIR #AUTHOR_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .""]","[""In particular , the `` Semantic Information Retrieval '' project ( SIR #AUTHOR_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .""]"
CC1486,W06-1104,Automatically creating datasets for measures of semantic relatedness,evaluating wordnetbased measures of semantic distance,"['Alexander Budanitsky', 'Graeme Hirst']",,,"According to #AUTHOR_TAG , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments .","['According to #AUTHOR_TAG , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments .']",0,"['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ) .']","['dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ) .']","['dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ) .']","['dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']"
CC1487,W06-1104,Automatically creating datasets for measures of semantic relatedness,evaluating wordnetbased measures of semantic distance,"['Alexander Budanitsky', 'Graeme Hirst']",experiments,,#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .,"['The distribution of averaged human judgments on the whole test set (see Figure 3) is almost balanced with a slight underrepresentation of highly related concepts.', 'To create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain- ing could be used.', 'However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .', 'This empty band is not observed here.', 'However, Figure 4 shows the distribution of averaged judgments with the highest agreement between annotators (standard deviation < 0.8).', 'The plot clearly shows an empty horizontal band with no judgments.', 'The connection between averaged judgments and standard deviation is plotted in Figure 5.']",1,"['We used the following parameters:  were noun-noun pairs, 15% noun-verb pairs and so on.', 'Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #AUTHOR_TAG ) , the German equivalent to WordNet , as a sense inventory for each word .']","['Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #AUTHOR_TAG ) , the German equivalent to WordNet , as a sense inventory for each word .', 'It is the most complete resource of this type for German.']","['We used the following parameters:  were noun-noun pairs, 15% noun-verb pairs and so on.', 'Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #AUTHOR_TAG ) , the German equivalent to WordNet , as a sense inventory for each word .', 'It is the most complete resource of this type for German.']","['Another filter enforced a specified fraction of combinations of nouns (N), verbs (V) and adjectives (A) to be present in the result set.', 'We used the following parameters:  were noun-noun pairs, 15% noun-verb pairs and so on.', 'Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #AUTHOR_TAG ) , the German equivalent to WordNet , as a sense inventory for each word .']","['Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #AUTHOR_TAG ) , the German equivalent to WordNet , as a sense inventory for each word .', 'It is the most complete resource of this type for German.']","['Another filter enforced a specified fraction of combinations of nouns (N), verbs (V) and adjectives (A) to be present in the result set.', 'We used the following parameters:  were noun-noun pairs, 15% noun-verb pairs and so on.', 'Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #AUTHOR_TAG ) , the German equivalent to WordNet , as a sense inventory for each word .', 'It is the most complete resource of this type for German.']","['We used the following parameters:  were noun-noun pairs, 15% noun-verb pairs and so on.', 'Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #AUTHOR_TAG ) , the German equivalent to WordNet , as a sense inventory for each word .', 'It is the most complete resource of this type for German.']","['Word pairs are filtered if they contain at least one word that a) has less than three letters b) contains only uppercase letters (mostly acronyms) or c) can be found in a stoplist.', 'Another filter enforced a specified fraction of combinations of nouns (N), verbs (V) and adjectives (A) to be present in the result set.', 'We used the following parameters:  were noun-noun pairs, 15% noun-verb pairs and so on.', 'Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #AUTHOR_TAG ) , the German equivalent to WordNet , as a sense inventory for each word .']","['Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #AUTHOR_TAG ) , the German equivalent to WordNet , as a sense inventory for each word .', 'It is the most complete resource of this type for German.']"
CC1488,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing semantic relatedness across parts of speech,['Iryna Gurevych'],related work,,This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG .,"['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', 'Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German.', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG .', 'Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.', 'Therefore, we automatically create word pairs using a corpus-based approach.']",0,"['Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric ( #AUTHOR_TAG ) .4']","['Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric ( #AUTHOR_TAG ) .4', 'However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application.']","['Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric ( #AUTHOR_TAG ) .4', 'However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application.']","['Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric ( #AUTHOR_TAG ) .4']","['Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric ( #AUTHOR_TAG ) .4', 'However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application.']","['Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric ( #AUTHOR_TAG ) .4', 'However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application.']","['Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric ( #AUTHOR_TAG ) .4', 'However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application.']","['Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric ( #AUTHOR_TAG ) .4']","['Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric ( #AUTHOR_TAG ) .4', 'However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application.']"
CC1489,W06-1104,Automatically creating datasets for measures of semantic relatedness,using information content to evaluate semantic similarity,['Philip Resnik'],related work,"Evaluating Semantic similarity has a widely application areas range from Psychology, Linguistics, Cognitive Science to Artificial Intelligence. This paper proposes the merely use of HowNet to evaluate Information Content (IC) as the semantic similarity of two terms or word senses. While the conventional ways of measuring the IC of word senses must depend on both an ontology like WordNet and a large corpus, experiments of this paper prove that the semantic similarity measured in this method is easy to calculate and more approach to human judgments.",This experiment was again replicated by #AUTHOR_TAG with 10 subjects .,"['In the seminal work by Rubenstein and Goodenough (1965), similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.', 'Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'This experiment was again replicated by #AUTHOR_TAG with 10 subjects .', 'Table 1']",0,"['In our experiment, intra-subject correlation was r=.670 for the first and r=.623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=.647.', '#AUTHOR_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .']","['#AUTHOR_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .', 'The values may again not be compared directly.']","['In our experiment, intra-subject correlation was r=.670 for the first and r=.623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=.647.', '#AUTHOR_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .', 'The values may again not be compared directly.']","['In our experiment, intra-subject correlation was r=.670 for the first and r=.623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=.647.', '#AUTHOR_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .']","['#AUTHOR_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .', 'The values may again not be compared directly.', 'Furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low.']","['In our experiment, intra-subject correlation was r=.670 for the first and r=.623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=.647.', '#AUTHOR_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .', 'The values may again not be compared directly.']","['In our experiment, intra-subject correlation was r=.670 for the first and r=.623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=.647.', '#AUTHOR_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .', 'The values may again not be compared directly.', 'Furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low.']","['In our experiment, intra-subject correlation was r=.670 for the first and r=.623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=.647.', '#AUTHOR_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .']","['#AUTHOR_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .', 'The values may again not be compared directly.', 'Furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low.']"
CC1490,W06-1104,Automatically creating datasets for measures of semantic relatedness,project âsemantic information retrievalâ,['SIR Project'],experiments,"The aim of the SCHEMA Network of Excellence is to bring together a critical mass of universities, research centers, industrial partners and end users, in order to design a reference system for content-based semantic scene analysis, interpretation and understanding. Relevant research areas include: content-based multimedia analysis and automatic annotation of semantic multimedia content, combined textual and multimedia information retrieval, semantic -web, MPEG-7 and MPEG-21 standards, user interfaces and human factors. In this paper, recent advances in content-based analysis, indexing and retrieval of digital media within the SCHEMA Network are presented. These advances will be integrated in the SCHEMA module-based, expandable reference system","In particular , the `` Semantic Information Retrieval '' project ( SIR #AUTHOR_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .","['We extracted word pairs from three different domain-specific corpora (see Table 2).', 'This is motivated by the aim to enable research in information retrieval incorporating SR measures.', ""In particular , the `` Semantic Information Retrieval '' project ( SIR #AUTHOR_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .""]",4,"['In our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006).', 'We used the revised experimental setup ( #AUTHOR_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .']","['We used the revised experimental setup ( #AUTHOR_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .', 'We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.']","['In our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006).', 'We used the revised experimental setup ( #AUTHOR_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .', 'We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.']","['In our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006).', 'We used the revised experimental setup ( #AUTHOR_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .']","['We used the revised experimental setup ( #AUTHOR_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .', 'We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.', 'Additionally, our corpusbased approach includes domain-specific technical terms and enables evaluation of the robustness of a measure.']","['In our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006).', 'We used the revised experimental setup ( #AUTHOR_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .', 'We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.']","['In our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006).', 'We used the revised experimental setup ( #AUTHOR_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .', 'We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.', 'Additionally, our corpusbased approach includes domain-specific technical terms and enables evaluation of the robustness of a measure.']","['In our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006).', 'We used the revised experimental setup ( #AUTHOR_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .']","['We used the revised experimental setup ( #AUTHOR_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .', 'We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.', 'Additionally, our corpusbased approach includes domain-specific technical terms and enables evaluation of the robustness of a measure.']"
CC1491,W06-1104,Automatically creating datasets for measures of semantic relatedness,cooccurrence retrieval a flexible framework for lexical distributional similarity,"['Julie Weeds', 'David Weir']",,"Techniques that exploit knowledge of distributional similarity between words have been proposed in many areas of Natural Language Processing. For example, in language modeling, the sparse data problem can be alleviated by estimating the probabilities of unseen co-occurrences of events from the probabilities of seen co-occurrences of similar events. In other applications, distributional similarity is taken to be an approximation to semantic similarity. However, due to the wide range of potential applications and the lack of a strict definition of the concept of distributional similarity, many methods of calculating distributional similarity have been proposed or adopted. In this work, a flexible, parameterized framework for calculating distributional similarity is proposed. Within this framework, the problem of finding distributionally similar words is cast as one of co-occurrence retrieval (CR) for which precision and recall can be measured by analogy with the way they are measured in document retrieval. As will be shown, a number of popular existing measures of distributional similarity are simulated with parameter settings within the CR framework. In this article, the CR framework is then used to systematically investigate three fundamental questions concerning distributional similarity. First, is the relationship of lexical similarity necessarily symmetric, or are there advantages to be gained from considering it as an asymmetric relationship? Second, are some co-occurrences inherently more salient than others in the calculation of distributional similarity? Third, is it necessary to consider the difference in the extent to which each word occurs in each co-occurrence type? Two application-based tasks are used for evaluation: automatic thesaurus generation and pseudo-disambiguation. It is possible to achieve significantly better results on both these tasks by varying the parameters within the CR framework rather than using other existing distributional similarity measures; it will also be shown that any single unparameterized measure is unlikely to be able to do better on both tasks. This is due to an inherent asymmetry in lexical substitutability and therefore also in lexical distributional similarity.","dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ) .","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']",0,"['In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG .']","['In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG .', 'We used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.']","['In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG .', 'We used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.']","['In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG .']","['In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG .', 'We used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.']","['In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG .', 'We used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.']","['In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG .', 'We used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.']","['In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG .']","['In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG .', 'We used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.', 'Additionally, our corpusbased approach includes domain-specific technical terms and enables evaluation of the robustness of a measure.']"
CC1492,W06-1104,Automatically creating datasets for measures of semantic relatedness,lexikalischsemantische wortnetze chapter computerlinguistik und sprachtechnologie,['Claudia Kunze'],experiments,,"Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #AUTHOR_TAG ) , the German equivalent to WordNet , as a sense inventory for each word .","['We implemented a set of filters for word pairs.', 'One group of filters removed unwanted word pairs.', 'Word pairs are filtered if they contain at least one word that a) has less than three letters b) contains only uppercase letters (mostly acronyms) or c) can be found in a stoplist.', 'Another filter enforced a specified fraction of combinations of nouns (N), verbs (V) and adjectives (A) to be present in the result set.', 'We used the following parameters:  were noun-noun pairs, 15% noun-verb pairs and so on.', 'Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #AUTHOR_TAG ) , the German equivalent to WordNet , as a sense inventory for each word .', 'It is the most complete resource of this type for German.']",5,"['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']","['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']","['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']","['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']"
CC1493,W06-1104,Automatically creating datasets for measures of semantic relatedness,an informationtheoretic definition of similarity in,['Dekang Lin'],,,"Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric ( #AUTHOR_TAG ) .4","['Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric ( #AUTHOR_TAG ) .4', 'However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application.']",0,"['via functional relationships (night -dark) or when they are antonyms (high -low).', 'Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG ) .']",['Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG ) .'],"['via functional relationships (night -dark) or when they are antonyms (high -low).', 'Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG ) .']","['Dissimilar words can be semantically related, e.g.', 'via functional relationships (night -dark) or when they are antonyms (high -low).', 'Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG ) .']",['Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG ) .'],"['Dissimilar words can be semantically related, e.g.', 'via functional relationships (night -dark) or when they are antonyms (high -low).', 'Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG ) .']","['via functional relationships (night -dark) or when they are antonyms (high -low).', 'Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG ) .']","['Semantic similarity is typically defined via the lexical relations of synonymy (automobile -car) and hypernymy (vehicle -car), while semantic relatedness (SR) is defined to cover any kind of lexical or functional association that may exist be-tween two words (Gurevych, 2005). 3', 'Dissimilar words can be semantically related, e.g.', 'via functional relationships (night -dark) or when they are antonyms (high -low).', 'Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG ) .']",['Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG ) .']
CC1494,W06-1104,Automatically creating datasets for measures of semantic relatedness,contextual correlates of synonymy,"['Herbert Rubenstein', 'John B Goodenough']",experiments,"Experimentol corroboration was obtained for the hypothesis that the proportion of words common to the contexts of word A and to the contexts of word B is a function of the degree to which A and B are similar in meaning. The tests were carried out for variously defined contexts. The shapes of the functions, however, indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning.",#AUTHOR_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .,"['In our experiment, intra-subject correlation was r=.670 for the first and r=.623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=.647.', '#AUTHOR_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .', 'The values may again not be compared directly.', 'Furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low.']",1,"['In the seminal work by #AUTHOR_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .']","['In the seminal work by #AUTHOR_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .', 'Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.']","['In the seminal work by #AUTHOR_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .', 'Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.']","['In the seminal work by #AUTHOR_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .']","['In the seminal work by #AUTHOR_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .', 'Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.', 'Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.']","['In the seminal work by #AUTHOR_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .', 'Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.']","['In the seminal work by #AUTHOR_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .', 'Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.', 'Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.']","['In the seminal work by #AUTHOR_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .']","['In the seminal work by #AUTHOR_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .', 'Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.', 'Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'This experiment was again replicated by Resnik (1995) with 10 subjects.']"
CC1495,W06-1104,Automatically creating datasets for measures of semantic relatedness,using the structure of a conceptual network in computing semantic relatedness,['Iryna Gurevych'],related work,"Abstract. We present a new method for computing semantic relatedness of concepts. The method relies solely on the structure of a conceptual network and eliminates the need for performing additional corpus analysis. The network structure is employed to generate artificial conceptual glosses. They replace textual definitions proper written by humans and are processed by a dictionary based metric of semantic relatedness [1]. We implemented the metric on the basis of GermaNet, the German counterpart of WordNet, and evaluated the results on a German dataset of 57 word pairs rated by human subjects for their semantic relatedness. Our approach can be easily applied to compute semantic relatedness based on alternative conceptual networks, e.g. in the domain of life sciences.","We used the revised experimental setup ( #AUTHOR_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .","['In our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006).', 'We used the revised experimental setup ( #AUTHOR_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .', 'We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.', 'Additionally, our corpusbased approach includes domain-specific technical terms and enables evaluation of the robustness of a measure.']",5,"['The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task.']","['The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task.', 'But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.']","['The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task.', 'But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.']","['The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task.']","['The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task.', 'But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'A certain measure may work well in one application, but not in another.']","['The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task.', 'But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.']","['The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task.', 'But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'A certain measure may work well in one application, but not in another.']","['The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task.']","['The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task.', 'But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'A certain measure may work well in one application, but not in another.', 'Application-based evaluation can only state the fact, but give little explanation about the reasons.']"
CC1496,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing semantic relatedness across parts of speech,['Iryna Gurevych'],related work,,"In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG .","['In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG .', 'We used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.', 'Additionally, our corpusbased approach includes domain-specific technical terms and enables evaluation of the robustness of a measure.']",1,"['word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006).', '#AUTHOR_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task .']","['#AUTHOR_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task .']","['word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006).', '#AUTHOR_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task .']","['The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g.', 'word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006).', '#AUTHOR_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task .']","['#AUTHOR_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task .']","['The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g.', 'word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006).', '#AUTHOR_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task .']","['word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006).', '#AUTHOR_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task .']","['The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g.', 'word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006).', '#AUTHOR_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task .']","['#AUTHOR_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task .', 'But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'A certain measure may work well in one application, but not in another.', 'Application-based evaluation can only state the fact, but give little explanation about the reasons.']"
CC1497,W06-1104,Automatically creating datasets for measures of semantic relatedness,wordnet an electronic lexical database chapter combining local context and wordnet similarity for word sense identification,"['Claudia Leacock', 'Martin Chodorow']",,,"Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .","['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']",0,"['The three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger (Schmid, 1995).', ""The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #AUTHOR_TAG ) .""]","[""The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #AUTHOR_TAG ) .""]","['The three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger (Schmid, 1995).', ""The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #AUTHOR_TAG ) .""]","['The three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger (Schmid, 1995).', ""The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #AUTHOR_TAG ) .""]","[""The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #AUTHOR_TAG ) .""]","['The three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger (Schmid, 1995).', ""The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #AUTHOR_TAG ) .""]","['The three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger (Schmid, 1995).', ""The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #AUTHOR_TAG ) .""]","['The three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger (Schmid, 1995).', ""The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #AUTHOR_TAG ) .""]","[""The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #AUTHOR_TAG ) .""]"
CC1498,W06-1104,Automatically creating datasets for measures of semantic relatedness,evaluating wordnetbased measures of semantic distance,"['Alexander Budanitsky', 'Graeme Hirst']",introduction,,Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG ) .,"['Semantic similarity is typically defined via the lexical relations of synonymy (automobile -car) and hypernymy (vehicle -car), while semantic relatedness (SR) is defined to cover any kind of lexical or functional association that may exist be-tween two words (Gurevych, 2005). 3', 'Dissimilar words can be semantically related, e.g.', 'via functional relationships (night -dark) or when they are antonyms (high -low).', 'Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG ) .']",0,"['Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.', '#AUTHOR_TAG reported a correlation of r = .69 .']","['#AUTHOR_TAG reported a correlation of r = .69 .', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']","['Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.', '#AUTHOR_TAG reported a correlation of r = .69 .', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']","['The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', 'Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.', '#AUTHOR_TAG reported a correlation of r = .69 .']","['#AUTHOR_TAG reported a correlation of r = .69 .', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']","['The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', 'Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.', '#AUTHOR_TAG reported a correlation of r = .69 .', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']","['Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.', '#AUTHOR_TAG reported a correlation of r = .69 .', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']","['Resnik (1995) reported a correlation of r=.9026. 10', 'The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', 'Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.', '#AUTHOR_TAG reported a correlation of r = .69 .']","['#AUTHOR_TAG reported a correlation of r = .69 .', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']"
CC1499,W06-1104,Automatically creating datasets for measures of semantic relatedness,contextual correlates of synonymy,"['Herbert Rubenstein', 'John B Goodenough']",related work,"Experimentol corroboration was obtained for the hypothesis that the proportion of words common to the contexts of word A and to the contexts of word B is a function of the degree to which A and B are similar in meaning. The tests were carried out for variously defined contexts. The shapes of the functions, however, indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning.","In the seminal work by #AUTHOR_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .","['In the seminal work by #AUTHOR_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .', 'Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.', 'Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'This experiment was again replicated by Resnik (1995) with 10 subjects.', 'Table 1']",0,"['GermaNet contains only a few conceptual glosses.', 'As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #AUTHOR_TAG ) .']","['As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #AUTHOR_TAG ) .', 'We removed words which had more than three senses.']","['GermaNet contains only a few conceptual glosses.', 'As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #AUTHOR_TAG ) .', 'We removed words which had more than three senses.']","['GermaNet contains only a few conceptual glosses.', 'As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #AUTHOR_TAG ) .']","['As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #AUTHOR_TAG ) .', 'We removed words which had more than three senses.']","['GermaNet contains only a few conceptual glosses.', 'As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #AUTHOR_TAG ) .', 'We removed words which had more than three senses.']","['GermaNet contains only a few conceptual glosses.', 'As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #AUTHOR_TAG ) .', 'We removed words which had more than three senses.']","['GermaNet contains only a few conceptual glosses.', 'As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #AUTHOR_TAG ) .']","['As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #AUTHOR_TAG ) .', 'We removed words which had more than three senses.']"
CC1500,W06-1104,Automatically creating datasets for measures of semantic relatedness,using measures of semantic relatedness for word sense disambiguation,"['Siddharth Patwardhan', 'Satanjeev Banerjee', 'Ted Pedersen']",,This paper generalizes the Adapted Lesk Algorithm of Banerjee and Pedersen (2002) to a method of word sense disambiguation based on semantic relatedness. This is possible since Lesk&apos;s original algorithm (1986) is based on gloss overlaps which can be viewed as a measure of semantic relatedness. We evaluate a variety of measures of semantic relatedness when applied to word sense disambiguation by carrying out experiments using the English lexical sample data of Senseval-2. We find that the gloss overlaps of Adapted Lesk and the semantic distance measure of Jiang and Conrath (1997) result in the highest accuracy,"The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task.","['The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task.', 'But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'A certain measure may work well in one application, but not in another.', 'Application-based evaluation can only state the fact, but give little explanation about the reasons.']",0,"['Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', '#AUTHOR_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .']",['#AUTHOR_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .'],"['Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', '#AUTHOR_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .']","['Furthermore, manually selected word pairs are often biased towards highly related pairs (Gurevych, 2006), because human annotators tend to select only highly related pairs connected by relations they are aware of.', 'Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', '#AUTHOR_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .']",['#AUTHOR_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .'],"['Furthermore, manually selected word pairs are often biased towards highly related pairs (Gurevych, 2006), because human annotators tend to select only highly related pairs connected by relations they are aware of.', 'Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', '#AUTHOR_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .']","['Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', '#AUTHOR_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .']","['Furthermore, manually selected word pairs are often biased towards highly related pairs (Gurevych, 2006), because human annotators tend to select only highly related pairs connected by relations they are aware of.', 'Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', '#AUTHOR_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .']",['#AUTHOR_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .']
CC1501,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing similarity,"['Ludovic Lebart', 'Martin Rajman']",,"In this paper we study similarity measures for moving curves which can, for example, model changing coastlines or retreating glacier termini. Points on a moving curve have two parameters, namely the position along the curve as well as time. We therefore focus on similarity measures for surfaces, specifically the Fr 'echet distance between surfaces. While the Fr 'echet distance between surfaces is not even known to be computable, we show for variants arising in the context of moving curves that they are polynomial-time solvable or NP-complete depending on the restrictions imposed on how the moving curves are matched. We achieve the polynomial-time solutions by a novel approach for computing a surface in the so-called free-space diagram based on max-flow min-cut duality","#AUTHOR_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task .","['The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g.', 'word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006).', '#AUTHOR_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task .', 'But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'A certain measure may work well in one application, but not in another.', 'Application-based evaluation can only state the fact, but give little explanation about the reasons.']",0,"['This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ) , too .']","['#AUTHOR_TAG annotated a larger set of word pairs ( 353 ) , too .']","['This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ) , too .']","['She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ) , too .']","['#AUTHOR_TAG annotated a larger set of word pairs ( 353 ) , too .']","['She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ) , too .']","['This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ) , too .']","['Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German.', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ) , too .']","['#AUTHOR_TAG annotated a larger set of word pairs ( 353 ) , too .', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".']"
CC1502,W06-1104,Automatically creating datasets for measures of semantic relatedness,automatic text processing the transformation analysis and retrieval of information by computer,['Gerard Salton'],experiments,,The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #AUTHOR_TAG ) .,"['The three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger (Schmid, 1995).', ""The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #AUTHOR_TAG ) .""]",5,"['The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #AUTHOR_TAG ) .']","['The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #AUTHOR_TAG ) .', ""The resulting list of POS-tagged lemmas is weighted using the SMART 'ltc' 8 tf.idf-weighting scheme (Salton, 1989).""]","['The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #AUTHOR_TAG ) .', ""The resulting list of POS-tagged lemmas is weighted using the SMART 'ltc' 8 tf.idf-weighting scheme (Salton, 1989).""]","['The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #AUTHOR_TAG ) .']","['The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #AUTHOR_TAG ) .', ""The resulting list of POS-tagged lemmas is weighted using the SMART 'ltc' 8 tf.idf-weighting scheme (Salton, 1989).""]","['The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #AUTHOR_TAG ) .', ""The resulting list of POS-tagged lemmas is weighted using the SMART 'ltc' 8 tf.idf-weighting scheme (Salton, 1989).""]","['The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #AUTHOR_TAG ) .', ""The resulting list of POS-tagged lemmas is weighted using the SMART 'ltc' 8 tf.idf-weighting scheme (Salton, 1989).""]","['The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #AUTHOR_TAG ) .']","['The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #AUTHOR_TAG ) .', ""The resulting list of POS-tagged lemmas is weighted using the SMART 'ltc' 8 tf.idf-weighting scheme (Salton, 1989).""]"
CC1503,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing semantic relatedness across parts of speech,['Iryna Gurevych'],experiments,,#AUTHOR_TAG reported a correlation of r = .69 .,"['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', 'The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', 'Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.', '#AUTHOR_TAG reported a correlation of r = .69 .', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']",1,"['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']","['dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']","['dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']","['dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']"
CC1504,W06-1104,Automatically creating datasets for measures of semantic relatedness,using the structure of a conceptual network in computing semantic relatedness,['Iryna Gurevych'],experiments,"Abstract. We present a new method for computing semantic relatedness of concepts. The method relies solely on the structure of a conceptual network and eliminates the need for performing additional corpus analysis. The network structure is employed to generate artificial conceptual glosses. They replace textual definitions proper written by humans and are processed by a dictionary based metric of semantic relatedness [1]. We implemented the metric on the basis of GermaNet, the German counterpart of WordNet, and evaluated the results on a German dataset of 57 word pairs rated by human subjects for their semantic relatedness. Our approach can be easily applied to compute semantic relatedness based on alternative conceptual networks, e.g. in the domain of life sciences.","As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #AUTHOR_TAG ) .","['GermaNet contains only a few conceptual glosses.', 'As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #AUTHOR_TAG ) .', 'We removed words which had more than three senses.']",5,"['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']","['dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']","['dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']","['dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']"
CC1505,W06-1104,Automatically creating datasets for measures of semantic relatedness,nonclassical lexical semantic relations,"['Jane Morris', 'Graeme Hirst']",related work,,#AUTHOR_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .,"['Furthermore, manually selected word pairs are often biased towards highly related pairs (Gurevych, 2006), because human annotators tend to select only highly related pairs connected by relations they are aware of.', 'Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', '#AUTHOR_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .']",0,"['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6']","['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6', 'Pairs containing such words are not suitable for evaluation.']","['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6', 'Pairs containing such words are not suitable for evaluation.']","['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6']","['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6', 'Pairs containing such words are not suitable for evaluation.', 'To limit their impact on the experiment, a threshold for the maximal number of senses can be defined.']","['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6', 'Pairs containing such words are not suitable for evaluation.']","['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6', 'Pairs containing such words are not suitable for evaluation.', 'To limit their impact on the experiment, a threshold for the maximal number of senses can be defined.']","['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6']","['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6', 'Pairs containing such words are not suitable for evaluation.', 'To limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'Words with a number of senses above the threshold are removed from the list.']"
CC1506,W06-1104,Automatically creating datasets for measures of semantic relatedness,placing search in context the concept revisited,"['Lev Finkelstein', 'Evgeniy Gabrilovich', 'Yossi Matias', 'Ehud Rivlin', 'Zach Solan', 'Gadi Wolfman']",related work,"Keyword-based search engines are in widespread use today as a popular means for Web-based information retrieval. Although such systems seem deceptively simple, a considerable amount of skill is required in order to satisfy non-trivial information needs. This paper presents a new conceptual paradigm for performing search in context, that largely automates the search process, providing even non-professional users with highly relevant results. This paradigm is implemented in practice in the IntelliZap system, where search is initiated from a text query marked by the user in a document she views, and is guided by the text surrounding the marked query in that document (""the context""). The context-driven information retrieval process involves semantic keyword extraction and clustering to automatically generate new, augmented queries. The latter are submitted to a host of general and domain-specific search engines. Search results are then semantically reranked, using context. Experimental results testify that using context to guide search, effectively offers even inexperienced users an advanced search tool on the Web.","#AUTHOR_TAG annotated a larger set of word pairs ( 353 ) , too .","['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', 'Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German.', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ) , too .', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.', 'Therefore, we automatically create word pairs using a corpus-based approach.']",0,"['This is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car).', 'Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .']","['Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .']","['This is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car).', 'Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .']","['Previous studies only included general terms as opposed to domain-specific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domain-specific or technical terms.', 'This is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car).', 'Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .']","['Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .']","['Previous studies only included general terms as opposed to domain-specific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domain-specific or technical terms.', 'This is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car).', 'Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .']","['This is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car).', 'Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .']","['Resulting from our corpus-based approach, test sets will also contain domain-specific terms.', 'Previous studies only included general terms as opposed to domain-specific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domain-specific or technical terms.', 'This is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car).', 'Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .']","['Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .', 'Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', 'Morris and Hirst (2004) pointed out that many relations between words in a text are non-classical (i.e.', 'other than typical taxonomic relations like synonymy or hypernymy) and therefore not covered by semantic similarity.']"
CC1507,W06-1104,Automatically creating datasets for measures of semantic relatedness,probabilistic partofspeech tagging using decision trees,['Helmut Schmid'],experiments,"In this paper, a new probabilistic tagging method is presented which avoids problems that Markov Model based taggers face, when they have to estimate transition probabilities from sparse data. In this tagging method, transition probabilities are estimated using a decision tree. Based on this method, a part-of-speech tagger (called TreeTagger) has been implemented which achieves 96.36 % accuracy on Penn-Treebank data which is better than that of a trigram tagger (96.06 %) on the same data.","The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #AUTHOR_TAG ) .","['The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #AUTHOR_TAG ) .', ""The resulting list of POS-tagged lemmas is weighted using the SMART 'ltc' 8 tf.idf-weighting scheme (Salton, 1989).""]",5,"['The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', '#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .']","['#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .', 'Gurevych (2006) reported a correlation of r=.69.']","['The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', '#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .', 'Gurevych (2006) reported a correlation of r=.69.']","['Resnik (1995) reported a correlation of r=.9026. 10', 'The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', '#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .']","['#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .', 'Gurevych (2006) reported a correlation of r=.69.', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']","['Resnik (1995) reported a correlation of r=.9026. 10', 'The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', '#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .', 'Gurevych (2006) reported a correlation of r=.69.']","['The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', '#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .', 'Gurevych (2006) reported a correlation of r=.69.', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']","['This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', 'The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', '#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .']","['#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .', 'Gurevych (2006) reported a correlation of r=.69.', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']"
CC1508,W06-1104,Automatically creating datasets for measures of semantic relatedness,verb semantics and lexical selection,"['Zhibiao Wu', 'Martha Palmer']",,"This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT). Two groups of English and Chinese verbs are examined to show that lexical selection must be based on interpretation of the sentence as well as selection restrictions placed on the verb arguments. A novel representation scheme is suggested, and is compared to representations with selection restrictions used in transfer-based MT. We see our approach as closely aligned with knowledge-based MT approaches (KBMT), and as a separate component that could be incorporated into existing systems. Examples and experimental results will show that, using this scheme, inexact matches can achieve correct lexical selection.Comment: 6 pages, Figures and bib files are in part","dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']",0,"['Resnik (1995) reported a correlation of r=.9026. 10', '#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .']","['#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .']","['Resnik (1995) reported a correlation of r=.9026. 10', '#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .']","['This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', '#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .']","['#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .']","['This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', '#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .']","['Resnik (1995) reported a correlation of r=.9026. 10', '#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .']","['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', '#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .']","['#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .', 'Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.', 'Gurevych (2006) reported a correlation of r=.69.', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']"
CC1509,W06-1104,Automatically creating datasets for measures of semantic relatedness,using information content to evaluate semantic similarity,['Philip Resnik'],,"Evaluating Semantic similarity has a widely application areas range from Psychology, Linguistics, Cognitive Science to Artificial Intelligence. This paper proposes the merely use of HowNet to evaluate Information Content (IC) as the semantic similarity of two terms or word senses. While the conventional ways of measuring the IC of word senses must depend on both an ontology like WordNet and a large corpus, experiments of this paper prove that the semantic similarity measured in this method is easy to calculate and more approach to human judgments.","dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']",0,"['Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .']",['#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .'],"['Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .']","['However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .']",['#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .'],"['However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .']","['Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .']","['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .']","['#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.']"
CC1510,W06-1104,Automatically creating datasets for measures of semantic relatedness,using information content to evaluate semantic similarity,['Michael Lesk'],,"Evaluating Semantic similarity has a widely application areas range from Psychology, Linguistics, Cognitive Science to Artificial Intelligence. This paper proposes the merely use of HowNet to evaluate Information Content (IC) as the semantic similarity of two terms or word senses. While the conventional ways of measuring the IC of word senses must depend on both an ontology like WordNet and a large corpus, experiments of this paper prove that the semantic similarity measured in this method is easy to calculate and more approach to human judgments.","dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']",0,"['Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set.', 'Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']","['Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']","['Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set.', 'Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']","['that (car -vehicle) are highly related, a strong connection between (parts -speech) may only be established by a certain group.', 'Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set.', 'Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']","['Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']","['that (car -vehicle) are highly related, a strong connection between (parts -speech) may only be established by a certain group.', 'Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set.', 'Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']","['Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set.', 'Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']","['We computed the summarized correlation coefficient using a Fisher Z-value transformation.', 'that (car -vehicle) are highly related, a strong connection between (parts -speech) may only be established by a certain group.', 'Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set.', 'Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']","['Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']"
CC1511,W06-1104,Automatically creating datasets for measures of semantic relatedness,automatic generation of a coarse grained wordnet,"['Rada Mihalcea', 'Dan Moldovan']",experiments,,"If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6","['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6', 'Pairs containing such words are not suitable for evaluation.', 'To limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'Words with a number of senses above the threshold are removed from the list.']",0,"['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .']","['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .']","['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']","['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .']","['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']"
CC1512,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing semantic relatedness across parts of speech,['Iryna Gurevych'],related work,,"Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .","['We assume that due to lexical-semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'Resulting from our corpus-based approach, test sets will also contain domain-specific terms.', 'Previous studies only included general terms as opposed to domain-specific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domain-specific or technical terms.', 'This is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car).', 'Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .', 'Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', 'Morris and Hirst (2004) pointed out that many relations between words in a text are non-classical (i.e.', 'other than typical taxonomic relations like synonymy or hypernymy) and therefore not covered by semantic similarity.']",0,"['* Site based corpus annotation -in which the user can specify a web site to annotate * Domain based corpus annotation -in which the user specifies a content domain (with the use of keywords) to annotate * Crawler based corpus annotation -more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger).', 'Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #AUTHOR_TAGb ) and shingling techniques described by Chakrabarti ( 2002 ) .']","['Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #AUTHOR_TAGb ) and shingling techniques described by Chakrabarti ( 2002 ) .']","['* Site based corpus annotation -in which the user can specify a web site to annotate * Domain based corpus annotation -in which the user specifies a content domain (with the use of keywords) to annotate * Crawler based corpus annotation -more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger).', 'Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #AUTHOR_TAGb ) and shingling techniques described by Chakrabarti ( 2002 ) .']","['* Site based corpus annotation -in which the user can specify a web site to annotate * Domain based corpus annotation -in which the user specifies a content domain (with the use of keywords) to annotate * Crawler based corpus annotation -more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger).', 'Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #AUTHOR_TAGb ) and shingling techniques described by Chakrabarti ( 2002 ) .']","['Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #AUTHOR_TAGb ) and shingling techniques described by Chakrabarti ( 2002 ) .']","['* Site based corpus annotation -in which the user can specify a web site to annotate * Domain based corpus annotation -in which the user specifies a content domain (with the use of keywords) to annotate * Crawler based corpus annotation -more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger).', 'Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #AUTHOR_TAGb ) and shingling techniques described by Chakrabarti ( 2002 ) .']","['* Site based corpus annotation -in which the user can specify a web site to annotate * Domain based corpus annotation -in which the user specifies a content domain (with the use of keywords) to annotate * Crawler based corpus annotation -more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger).', 'Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #AUTHOR_TAGb ) and shingling techniques described by Chakrabarti ( 2002 ) .']","['* Site based corpus annotation -in which the user can specify a web site to annotate * Domain based corpus annotation -in which the user specifies a content domain (with the use of keywords) to annotate * Crawler based corpus annotation -more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger).', 'Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #AUTHOR_TAGb ) and shingling techniques described by Chakrabarti ( 2002 ) .']","['Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #AUTHOR_TAGb ) and shingling techniques described by Chakrabarti ( 2002 ) .']"
CC1513,W06-1104,Automatically creating datasets for measures of semantic relatedness,placing search in context the concept revisited,"['Lev Finkelstein', 'Evgeniy Gabrilovich', 'Yossi Matias', 'Ehud Rivlin', 'Zach Solan', 'Gadi Wolfman']",experiments,"Keyword-based search engines are in widespread use today as a popular means for Web-based information retrieval. Although such systems seem deceptively simple, a considerable amount of skill is required in order to satisfy non-trivial information needs. This paper presents a new conceptual paradigm for performing search in context, that largely automates the search process, providing even non-professional users with highly relevant results. This paradigm is implemented in practice in the IntelliZap system, where search is initiated from a text query marked by the user in a document she views, and is guided by the text surrounding the marked query in that document (""the context""). The context-driven information retrieval process involves semantic keyword extraction and clustering to automatically generate new, augmented queries. The latter are submitted to a host of general and domain-specific search engines. Search results are then semantically reranked, using context. Experimental results testify that using context to guide search, effectively offers even inexperienced users an advanced search tool on the Web.",#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .,"['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', 'The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', '#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .', 'Gurevych (2006) reported a correlation of r=.69.', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']",1,"['Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 ) .""]","[""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 ) .""]","['Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 ) .""]","['Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 ) .""]","[""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 ) .""]","['Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 ) .""]","['Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 ) .""]","['Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 ) .""]","[""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 ) .""]"
CC1514,W06-1104,Automatically creating datasets for measures of semantic relatedness,using information content to evaluate semantic similarity,['Philip Resnik'],experiments,"Evaluating Semantic similarity has a widely application areas range from Psychology, Linguistics, Cognitive Science to Artificial Intelligence. This paper proposes the merely use of HowNet to evaluate Information Content (IC) as the semantic similarity of two terms or word senses. While the conventional ways of measuring the IC of word senses must depend on both an ontology like WordNet and a large corpus, experiments of this paper prove that the semantic similarity measured in this method is easy to calculate and more approach to human judgments.","#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .","['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', '#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .', 'Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.', 'Gurevych (2006) reported a correlation of r=.69.', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']",1,"['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ) .']","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).']","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).']","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ) .']","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.']","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).']","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.']","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ) .']","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).']"
CC1515,W06-1104,Automatically creating datasets for measures of semantic relatedness,using the structure of a conceptual network in computing semantic relatedness,['Iryna Gurevych'],related work,"Abstract. We present a new method for computing semantic relatedness of concepts. The method relies solely on the structure of a conceptual network and eliminates the need for performing additional corpus analysis. The network structure is employed to generate artificial conceptual glosses. They replace textual definitions proper written by humans and are processed by a dictionary based metric of semantic relatedness [1]. We implemented the metric on the basis of GermaNet, the German counterpart of WordNet, and evaluated the results on a German dataset of 57 word pairs rated by human subjects for their semantic relatedness. Our approach can be easily applied to compute semantic relatedness based on alternative conceptual networks, e.g. in the domain of life sciences.",#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .,"['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.', 'Therefore, we automatically create word pairs using a corpus-based approach.']",0,"['In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #AUTHOR_TAG ; Hughes et al , 2004 ) .']","['In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #AUTHOR_TAG ; Hughes et al , 2004 ) .', 'While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach.']","['In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #AUTHOR_TAG ; Hughes et al , 2004 ) .', 'While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach.']","['In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #AUTHOR_TAG ; Hughes et al , 2004 ) .']","['In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #AUTHOR_TAG ; Hughes et al , 2004 ) .', 'While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach.', 'In simple terms, P2P is a technology that takes advantage of the resources and services available at the edge of the Internet (Shirky, 2001).']","['In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #AUTHOR_TAG ; Hughes et al , 2004 ) .', 'While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach.']","['In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #AUTHOR_TAG ; Hughes et al , 2004 ) .', 'While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach.', 'In simple terms, P2P is a technology that takes advantage of the resources and services available at the edge of the Internet (Shirky, 2001).']","['In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #AUTHOR_TAG ; Hughes et al , 2004 ) .']","['In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #AUTHOR_TAG ; Hughes et al , 2004 ) .', 'While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach.', 'In simple terms, P2P is a technology that takes advantage of the resources and services available at the edge of the Internet (Shirky, 2001).', 'Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems.']"
CC1516,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing semantic relatedness across parts of speech,['Iryna Gurevych'],experiments,,"Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .","['Correlation coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations (like cooccurrence of words) and is thus a more complicated task.', 'Judgments for such relations strongly depend on experience and cultural background of the test subjects.', 'While most people may agree 10 Note that Resnik used the averaged correlation coefficient.', 'We computed the summarized correlation coefficient using a Fisher Z-value transformation.', 'that (car -vehicle) are highly related, a strong connection between (parts -speech) may only be established by a certain group.', 'Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set.', 'Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']",1,"['Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; #AUTHOR_TAG ) .""]","[""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; #AUTHOR_TAG ) .""]","['Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; #AUTHOR_TAG ) .""]","['Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; #AUTHOR_TAG ) .""]","[""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; #AUTHOR_TAG ) .""]","['Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; #AUTHOR_TAG ) .""]","['Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; #AUTHOR_TAG ) .""]","['Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; #AUTHOR_TAG ) .""]","[""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; #AUTHOR_TAG ) .""]"
CC1517,W06-1104,Automatically creating datasets for measures of semantic relatedness,semantic similarity based on corpus statistics and lexical taxonomy,"['Jay J Jiang', 'David W Conrath']",,"This paper presents a new approach for measuring semantic similarity/distance between words and concepts. It combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data. Specifically, the proposed measure is a combined approach that inherits the edge-based approach of the edge counting scheme, which is then enhanced by the node-based approach of the information content calculation. When tested on a common data set of word pair similarity ratings, the proposed approach outperforms other computational models. It gives the highest correlation value (r = 0.828) with a benchmark based on human similarity judgements, whereas an upper bound (r = 0.885) is observed when human subjects replicate the same task.","Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .","['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']",0,"['Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( #AUTHOR_TAGa ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 ) .""]","[""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( #AUTHOR_TAGa ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 ) .""]","['Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( #AUTHOR_TAGa ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 ) .""]","['Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( #AUTHOR_TAGa ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 ) .""]","[""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( #AUTHOR_TAGa ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 ) .""]","['Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( #AUTHOR_TAGa ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 ) .""]","['Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( #AUTHOR_TAGa ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 ) .""]","['Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( #AUTHOR_TAGa ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 ) .""]","[""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( #AUTHOR_TAGa ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 ) .""]"
CC1518,W06-1705,Annotated web as corpus,facilitating the compilation and dissemination of adhoc web corpora,['W H Fletcher'],method,"Since the World Wide Web gained prominence in the mid-1990s it has tantalized language investigators and ins ructors as a virtually unlimited source of machine-readable texts for compiling corpora and developing teaching materials. The broad range of languages and content domains found online also offers translators enormous promise both for translation by-example and as a comprehensive supplement to published reference works This paper surveys the impediments which s ill prevent the Web from realizing its full potential as a linguistic resource and discusses tools to overcome the remaining hurdles. Identifying online documents which are both relevant and reliable presents a major challenge. As a partial solution the author's Web concordancer KWiCFinder au omates the process of seeking and retrieving webpages Enhancements which permit more focused queries than existing search engines and provide search results in an interactive explora ory environmen are described in detail. Despite the efficiency of automated downloading and excerpting, selecting Web documents still entails significant time and effort. To multiply the benefits of a search, an online forum for sharing annotated search reports and linguistically interesting texts with other users is outlined. Furthermore, the orien ation of commercial sea ch engines toward the general public makes them less beneficial for linguistic research. The author sketches plans for a specialized Search Engine for Applied Linguis s and a selective Web Corpus Archive which build on his experience with KWiCFinder. He compares his available and proposed solutions to existing resou ces, and su veys ways to exploi them in language teaching. Together these proposed services will enable language learners and professionals to tap into the Web effectively and efficiently for instruction research and translation. t","Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #AUTHOR_TAGb ) and shingling techniques described by Chakrabarti ( 2002 ) .","['â¢ Site based corpus annotation -in which the user can specify a web site to annotate â¢ Domain based corpus annotation -in which the user specifies a content domain (with the use of keywords) to annotate â¢ Crawler based corpus annotation -more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger).', 'Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #AUTHOR_TAGb ) and shingling techniques described by Chakrabarti ( 2002 ) .']",3,"['This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.']"
CC1519,W06-1705,Annotated web as corpus,linguistic search engine,['A Kilgarriff'],related work,"Users of search engines often have specific questions which they hope or believe a particular resource can answer. The problem, from the computer system's perspective, is cognitive understanding of the contents in the source and finding the desired answer. Most of the search engines, with Google on the top, able to retrieve most likely relevant information based on a query. But not capable of providing answer to a question due to lack of deduction capability. In order to find a specific answer to a question, the engine needs to understand the information content and able to do deductive reasoning. Conventional information representation models used in the search engines rely on an extensive use of keywords and their frequencies in storing and retrieving information and other characteristic data on specific body of information. It is believed that we need new approaches for the development of future search engines which will be more effective. Semantic model is an alternative to conventional approach. We have proposed logical-linguistic model where logic and linguistic formalism are used in providing mechanism for computer to understand the contents of the source and deduce answers to questions. The capability of deduction is much depended on the knowledge representation framework used. The approach applies semantic analysis in transforming and normalising information from natural language texts into a declarative knowledge based representation of first order predicate logic. Retrieval of relevant information can then be performed through plausible logical implication and answer to query is carried out using a theorem proving technique. This paper elaborates on the model and how it is used in search engine and question answering system as one unified model","Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 ) .""]",0,"['A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies.', 'Word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ) .']","['Word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ) .', 'Tools based on static corpora do not suffer from this problem, e.g.']","['A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies.', 'Word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ) .']","['A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies.', 'Word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ) .']","['Word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ) .', 'Tools based on static corpora do not suffer from this problem, e.g.', 'BNCweb 7 , developed at the University of Zurich, and View 8 (Variation in English Words and Phrases, developed at Brigham Young University) are both based on the British National Corpus.']","['A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies.', 'Word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ) .', 'Tools based on static corpora do not suffer from this problem, e.g.']","['A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies.', 'Word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ) .', 'Tools based on static corpora do not suffer from this problem, e.g.', 'BNCweb 7 , developed at the University of Zurich, and View 8 (Variation in English Words and Phrases, developed at Brigham Young University) are both based on the British National Corpus.']","['A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies.', 'Word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ) .']","['Word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ) .', 'Tools based on static corpora do not suffer from this problem, e.g.', 'BNCweb 7 , developed at the University of Zurich, and View 8 (Variation in English Words and Phrases, developed at Brigham Young University) are both based on the British National Corpus.', 'Both BNCweb and View enable access to annotated corpora and facilitate searching on part-ofspeech tags.']"
CC1520,W06-1705,Annotated web as corpus,introduction to the special issue on the web as corpus,"['A Kilgarriff', 'G Grefenstette']",introduction,"The Web, teeming as it is with language data, of all manner of varieties and languages, in vast quantity and freely available, is a fabulous linguists' playground. This special issue of Computational Linguistics explores ways in which this dream is being explored.","In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ) .","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']",0,"['This corpus annotation bot- tleneck becomes even more problematic for vo- luminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['This corpus annotation bot- tleneck becomes even more problematic for vo- luminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin- gle-server systems.', 'This corpus annotation bot- tleneck becomes even more problematic for vo- luminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin- gle-server systems.', 'This corpus annotation bot- tleneck becomes even more problematic for vo- luminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['This corpus annotation bot- tleneck becomes even more problematic for vo- luminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['Existing tagging systems are small scale and typically impose some limitation to prevent over- load (e.g. restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin- gle-server systems.', 'This corpus annotation bot- tleneck becomes even more problematic for vo- luminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.']"
CC1521,W06-1705,Annotated web as corpus,supporting text mining for escience the challenges for gridenabled natural language processing,"['J Carroll', 'R Evans', 'E Klein']",related work,"Over the last few years, language technology has moved rapidly from 'applied research' to 'engineering', and from small-scale to large-scale engineering. Applications such as advanced text mining systems are feasible, but very resource-intensive, while research seeking to address the underlying language processing questions faces very real practical and methodological limitations. The e-Science vision, and the creation of the e-Science Grid, promises the level of integrated large-scale technological support required to sustain this important and successful new technology area. In this paper, we discuss the foundations for the deployment of text mining and other language technology on the Grid - the protocols and tools required to build distributed large-scale language technology systems, meeting the needs of users, application builders and researchers.","In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #AUTHOR_TAG ; Hughes et al , 2004 ) .","['In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #AUTHOR_TAG ; Hughes et al , 2004 ) .', 'While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach.', 'In simple terms, P2P is a technology that takes advantage of the resources and services available at the edge of the Internet (Shirky, 2001).', 'Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems.', 'Examples include SETI@home (looking for radio evidence of extraterrestrial life), ClimatePrediction.net (studying climate change), Predictor@home (investigating protein-related diseases) and Einstein@home (searching for gravitational signals).']",0,"['This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG ) .']","['In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG ) .', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.']","['This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG ) .', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.']","['Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG ) .']","['In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG ) .', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']","['Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG ) .', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.']","['This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG ) .', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG ) .']","['In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG ) .', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']"
CC1522,W06-1705,Annotated web as corpus,the linguists search engine getting started guide,"['P Resnik', 'A Elkiss']",related work,,"Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; #AUTHOR_TAG ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; #AUTHOR_TAG ) .""]",0,"['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) .']","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).']","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).']","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) .']","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.']","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).']","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.']","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) .']","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).']"
CC1523,W06-1705,Annotated web as corpus,facilitating the compilation and dissemination of adhoc web corpora,['W H Fletcher'],related work,"Since the World Wide Web gained prominence in the mid-1990s it has tantalized language investigators and ins ructors as a virtually unlimited source of machine-readable texts for compiling corpora and developing teaching materials. The broad range of languages and content domains found online also offers translators enormous promise both for translation by-example and as a comprehensive supplement to published reference works This paper surveys the impediments which s ill prevent the Web from realizing its full potential as a linguistic resource and discusses tools to overcome the remaining hurdles. Identifying online documents which are both relevant and reliable presents a major challenge. As a partial solution the author's Web concordancer KWiCFinder au omates the process of seeking and retrieving webpages Enhancements which permit more focused queries than existing search engines and provide search results in an interactive explora ory environmen are described in detail. Despite the efficiency of automated downloading and excerpting, selecting Web documents still entails significant time and effort. To multiply the benefits of a search, an online forum for sharing annotated search reports and linguistically interesting texts with other users is outlined. Furthermore, the orien ation of commercial sea ch engines toward the general public makes them less beneficial for linguistic research. The author sketches plans for a specialized Search Engine for Applied Linguis s and a selective Web Corpus Archive which build on his experience with KWiCFinder. He compares his available and proposed solutions to existing resou ces, and su veys ways to exploi them in language teaching. Together these proposed services will enable language learners and professionals to tap into the Web effectively and efficiently for instruction research and translation. t","Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( #AUTHOR_TAGa ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( #AUTHOR_TAGa ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 ) .""]",0,"['We have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #AUTHOR_TAG ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ) .']","['This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #AUTHOR_TAG ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ) .']","['We have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #AUTHOR_TAG ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ) .']","[""We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun's P2P API)."", 'We have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #AUTHOR_TAG ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ) .']","['This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #AUTHOR_TAG ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ) .']","[""We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun's P2P API)."", 'We have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #AUTHOR_TAG ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ) .']","['We have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #AUTHOR_TAG ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ) .']","['The second stage of our work will involve im- plementing the framework within a P2P environment.', ""We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun's P2P API)."", 'We have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #AUTHOR_TAG ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ) .']","['This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #AUTHOR_TAG ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ) .', 'It is our intention to implement our distributed corpus annotation framework as a plug- in.', 'This will involve implementing new functionality and integrating this with our existing annotation tools (such as CLAWS11).', 'The development environment is also flexible enough to utilise the BOINC platform, and such support will be built into it.']"
CC1524,W06-1705,Annotated web as corpus,concordancing the web with kwicfinder third north american,['W H Fletcher'],related work,,"The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]",0,"['While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach.', 'In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #AUTHOR_TAG ) .']","['In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #AUTHOR_TAG ) .', 'Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems.']","['While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach.', 'In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #AUTHOR_TAG ) .', 'Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems.']","['In the areas of Natural Language Processing (NLP) and computational linguistics, proposals have been made for using the computational Grid for data-intensive NLP and text-mining for e-Science (Carroll et al., 2005;Hughes et al, 2004).', 'While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach.', 'In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #AUTHOR_TAG ) .']","['In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #AUTHOR_TAG ) .', 'Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems.', 'Examples include SETI@home (looking for radio evidence of extraterrestrial life), ClimatePrediction.net (studying climate change), Predictor@home (investigating protein-related diseases) and Einstein@home (searching for gravitational signals).']","['In the areas of Natural Language Processing (NLP) and computational linguistics, proposals have been made for using the computational Grid for data-intensive NLP and text-mining for e-Science (Carroll et al., 2005;Hughes et al, 2004).', 'While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach.', 'In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #AUTHOR_TAG ) .', 'Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems.']","['While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach.', 'In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #AUTHOR_TAG ) .', 'Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems.', 'Examples include SETI@home (looking for radio evidence of extraterrestrial life), ClimatePrediction.net (studying climate change), Predictor@home (investigating protein-related diseases) and Einstein@home (searching for gravitational signals).']","['In the areas of Natural Language Processing (NLP) and computational linguistics, proposals have been made for using the computational Grid for data-intensive NLP and text-mining for e-Science (Carroll et al., 2005;Hughes et al, 2004).', 'While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach.', 'In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #AUTHOR_TAG ) .']","['In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #AUTHOR_TAG ) .', 'Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems.', 'Examples include SETI@home (looking for radio evidence of extraterrestrial life), ClimatePrediction.net (studying climate change), Predictor@home (investigating protein-related diseases) and Einstein@home (searching for gravitational signals).']"
CC1525,W06-1705,Annotated web as corpus,web googles missing pages mystery solved httpaixtalblogspotcom200502webgooglesmissingpagesmysteryhtml accessed,['J Veronis'],related work,,Word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ) .,"['A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies.', 'Word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ) .', 'Tools based on static corpora do not suffer from this problem, e.g.', 'BNCweb 7 , developed at the University of Zurich, and View 8 (Variation in English Words and Phrases, developed at Brigham Young University) are both based on the British National Corpus.', 'Both BNCweb and View enable access to annotated corpora and facilitate searching on part-ofspeech tags.', 'In addition, PIE 9 (Phrases in English), developed at USNA, which performs searches on n-grams (based on words, parts-ofspeech and characters), is currently restricted to the British National Corpus as well, although other static corpora are being added to its database.', 'In contrast, little progress has been made toward annotating sizable sample corpora from the web.']",0,"['We have designed this environment so that specific application functionality can be captured within plug-ins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ) .']","['This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ) .']","['We have designed this environment so that specific application functionality can be captured within plug-ins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ) .']","[""We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun's P2P API)."", 'We have designed this environment so that specific application functionality can be captured within plug-ins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ) .']","['This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ) .']","[""We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun's P2P API)."", 'We have designed this environment so that specific application functionality can be captured within plug-ins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ) .']","['We have designed this environment so that specific application functionality can be captured within plug-ins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ) .']","['The second stage of our work will involve implementing the framework within a P2P environment.', ""We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun's P2P API)."", 'We have designed this environment so that specific application functionality can be captured within plug-ins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ) .']","['This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ) .', 'It is our intention to implement our distributed corpus annotation framework as a plugin.', 'This will involve implementing new functionality and integrating this with our existing annotation tools (such as CLAWS 11 ).', 'The development environment is also flexible enough to utilise the BOINC platform, and such support will be built into it.']"
CC1526,W06-1705,Annotated web as corpus,google as a corpus tool in,['T Robb'],related work,,"The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two ap- proaches e.g. automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS4, Amal- gam5, Connexor6) are available, for example, in the application of part-of-speech tags to corpora.', 'Existing tagging systems are ï¿½small scaleï¿½ and typically impose some limitation to prevent over- load (e.g. restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin- gle-server systems.', 'This corpus annotation bot- tleneck becomes even more problematic for vo- luminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', 'Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguistï¿½s Search En- gine (Kilgarriff, 2003; Resnik and Elkiss, 2003).']",0,"['This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.']"
CC1527,W06-1705,Annotated web as corpus,automatic profiling of learner texts,"['S Granger', 'P Rayson']",introduction,"In this chapter Crystal's (1991) notion of 'profiling', i.e. the identification of the most salient features in a particular person (clinical linguistics) or register (stylistics), is applied to the field of interlanguage studies. Starting from the assumption that every interlanguage is characterized by a 'unique matrix of frequencies of various linguistic forms' (Krzeszowski 1990: 212), we have submitted two similar-sized corpora of native and non-native writing to a lexical frequency software program to uncover some of the distinguishing features of learner writing. The non-native speaker corpus is taken from the International Corpus of Learner English (ICLE) database. It consists of argumentative essay writing by advanced French-speaking learners of English. The control corpus of similar writing is taken from the Louvain Corpus of Native English Essays (LOCNESS) database. Though limited to one specific type of interlanguage, the approach presented here is applicable to any learner variety and demonstrates a potential of automatic profiling for revealing the stylistic characteristics of EFL texts. In the present study, the learner data is shown to display many of the stylistic features of spoken, rather than written, English.","In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG ) .","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG ) .', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']",0,"['This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #AUTHOR_TAG ) .']","['The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #AUTHOR_TAG ) .']","['This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #AUTHOR_TAG ) .']","['Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #AUTHOR_TAG ) .']","['The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #AUTHOR_TAG ) .']","['Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #AUTHOR_TAG ) .']","['This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #AUTHOR_TAG ) .']","['restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #AUTHOR_TAG ) .']","['The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #AUTHOR_TAG ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.']"
CC1528,W06-1705,Annotated web as corpus,an introduction to corpus linguistics,['G Kennedy'],introduction,"On first looking into a corpus, teachers as well as students may well be blinded by the sheer scale of the resource and by the possibilities for research that it offers. As the author of this book puts it, ""The research topics in a machine-readable cor pus are potentially as various and wide ranging as are the facts about a language and the use of that language"" (274). The value of this book is that it provides practical examples of the range of research possibilities that a corpus offers, as well as indi cating how corpus-based research projects may be undertaken. It is informed throughout by the view that corpus linguistics is not a separate branch of linguistics, but rather ""descriptive linguistics aided by new technology"" (268). There is some theoretical discussion of the place of corpus linguistics in the wider field, but in general, the author's approach is to let the results speak for themselves. The author's declared aim is to whet the appetites of teachers and students, and in this he clearly succeeds. In the ""Introduction,"" the author suggests that some readers might usefully begin with chapter 3, ""Corpus-Based Descriptions of English."" This is the central part of the book and by far the most valuable in terms of whetting the appetite. It consists of a very comprehensive and wide-ranging review of previous corpus-based research, divided into the following sections: lexical description, grammatical studies cen tered on morphemes or words, grammatical studies centered on the sentence, pragmatics and spoken discourse, and studies of variation. The first section of the chapter investigates how computerized corpora are increasingly being used in lexi cography and continues with a review of collocational studies based on the LOB corpus, as well as work by Sinclair and Renouf on collocational frameworks. Under ""word-centered"" grammatical studies, previous work on modals, voice, aspect, the subjunctive, as well as prepositions and conjunctions are very comprehensively ex emplified and described. The list continues. The latter sections of this chapter re view work by Kuc*era and Francis on sentence length, Altenberg on verb complementation, Mair on nonfinite complementation, Meyer on apposition,","In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) .","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']",0,"['Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', '#AUTHOR_TAG built a corpus by iteratively searching Google for a small set of seed terms .']","['#AUTHOR_TAG built a corpus by iteratively searching Google for a small set of seed terms .', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]","['Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', '#AUTHOR_TAG built a corpus by iteratively searching Google for a small set of seed terms .', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]","['Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', '#AUTHOR_TAG built a corpus by iteratively searching Google for a small set of seed terms .']","['#AUTHOR_TAG built a corpus by iteratively searching Google for a small set of seed terms .', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]","['Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', '#AUTHOR_TAG built a corpus by iteratively searching Google for a small set of seed terms .', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]","['Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', '#AUTHOR_TAG built a corpus by iteratively searching Google for a small set of seed terms .', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]","['The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', '#AUTHOR_TAG built a corpus by iteratively searching Google for a small set of seed terms .']","['#AUTHOR_TAG built a corpus by iteratively searching Google for a small set of seed terms .', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]"
CC1529,W06-1705,Annotated web as corpus,distributed video encoding over a peertopeer network,"['D Hughes', 'J Walkerdine']",method,"How does the work advance the state-of-the-art?: Current video encoding technologies tend to focus on single machine solutions, while little or no work on distributed video encoding systems has been undertaken. Current work on distributed computation over peer-to-peer networks primarily focuses upon systems with heavily centralised control [1] [2]. The Distributed Video Encoder is a novel example of fully decentralized ad-hoc distributed computation.","This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #AUTHOR_TAG ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ) .","['The second stage of our work will involve im- plementing the framework within a P2P environment.', ""We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun's P2P API)."", 'We have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #AUTHOR_TAG ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ) .', 'It is our intention to implement our distributed corpus annotation framework as a plug- in.', 'This will involve implementing new functionality and integrating this with our existing annotation tools (such as CLAWS11).', 'The development environment is also flexible enough to utilise the BOINC platform, and such support will be built into it.']",0,"['A key aspect of our case study research will be to investigate extending corpus collection to new document types.', 'Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3 ( #AUTHOR_TAG ) .']","['Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3 ( #AUTHOR_TAG ) .']","['A key aspect of our case study research will be to investigate extending corpus collection to new document types.', 'Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3 ( #AUTHOR_TAG ) .']","['A key aspect of our case study research will be to investigate extending corpus collection to new document types.', 'Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3 ( #AUTHOR_TAG ) .']","['Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3 ( #AUTHOR_TAG ) .']","['A key aspect of our case study research will be to investigate extending corpus collection to new document types.', 'Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3 ( #AUTHOR_TAG ) .']","['A key aspect of our case study research will be to investigate extending corpus collection to new document types.', 'Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3 ( #AUTHOR_TAG ) .']","['A key aspect of our case study research will be to investigate extending corpus collection to new document types.', 'Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3 ( #AUTHOR_TAG ) .']","['Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3 ( #AUTHOR_TAG ) .']"
CC1530,W06-1705,Annotated web as corpus,listening to napster in peertopeer harnessing the power of disruptive technologies,['C Shirky'],related work,,"In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #AUTHOR_TAG ) .","['In the areas of Natural Language Processing (NLP) and computational linguistics, proposals have been made for using the computational Grid for data-intensive NLP and text-mining for e-Science (Carroll et al., 2005;Hughes et al, 2004).', 'While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach.', 'In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #AUTHOR_TAG ) .', 'Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems.', 'Examples include SETI@home (looking for radio evidence of extraterrestrial life), ClimatePrediction.net (studying climate change), Predictor@home (investigating protein-related diseases) and Einstein@home (searching for gravitational signals).']",0,"['In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #AUTHOR_TAG ) .']","['Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #AUTHOR_TAG ) .', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.']","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #AUTHOR_TAG ) .']","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #AUTHOR_TAG ) .']","['Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #AUTHOR_TAG ) .', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).']","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #AUTHOR_TAG ) .', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.']","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #AUTHOR_TAG ) .', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).']","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #AUTHOR_TAG ) .']","['Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #AUTHOR_TAG ) .', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.']"
CC1531,W06-1705,Annotated web as corpus,p2p4dl digital library over peertopeer,"['J Walkerdine', 'P Rayson']",method,,"This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ) .","['The second stage of our work will involve implementing the framework within a P2P environment.', ""We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun's P2P API)."", 'We have designed this environment so that specific application functionality can be captured within plug-ins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ) .', 'It is our intention to implement our distributed corpus annotation framework as a plugin.', 'This will involve implementing new functionality and integrating this with our existing annotation tools (such as CLAWS 11 ).', 'The development environment is also flexible enough to utilise the BOINC platform, and such support will be built into it.']",0,"['A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server.', 'The Gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .']",['The Gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .'],"['A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server.', 'The Gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .']","['The second method pushes the new collection onto a queue for the LSE annotator to analyse.', 'A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server.', 'The Gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .']",['The Gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .'],"['The second method pushes the new collection onto a queue for the LSE annotator to analyse.', 'A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server.', 'The Gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .']","['A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server.', 'The Gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .']","['or build their own collections from AltaVista search engine results.', 'The second method pushes the new collection onto a queue for the LSE annotator to analyse.', 'A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server.', 'The Gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .']","['The Gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .', 'Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up.', 'In contrast, the Sketch Engine system to assist lexicographers to construct dictionary entries requires large pre-annotated corpora.', ""A word sketch is an automatic one-page corpus-derived summary of a word's grammatical and collocational behaviour.""]"
CC1532,W06-1705,Annotated web as corpus,the biggest corpus of allquot,['M Rundell'],related work,,"The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]",0,"['Linguistic annotation of corpora contributes crucially to the study of language at several levels: morphology, syntax, semantics, and discourse.', 'Its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages .']","['Its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages .']","['Linguistic annotation of corpora contributes crucially to the study of language at several levels: morphology, syntax, semantics, and discourse.', 'Its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages .']","['Linguistic annotation of corpora contributes crucially to the study of language at several levels: morphology, syntax, semantics, and discourse.', 'Its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages .']","['Its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages .']","['Linguistic annotation of corpora contributes crucially to the study of language at several levels: morphology, syntax, semantics, and discourse.', 'Its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages .']","['Linguistic annotation of corpora contributes crucially to the study of language at several levels: morphology, syntax, semantics, and discourse.', 'Its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages .']","['Linguistic annotation of corpora contributes crucially to the study of language at several levels: morphology, syntax, semantics, and discourse.', 'Its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages .']","['Its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages .']"
CC1533,W06-1705,Annotated web as corpus,introduction to the special issue on the web as corpus,"['A Kilgarriff', 'G Grefenstette']",related work,"The Web, teeming as it is with language data, of all manner of varieties and languages, in vast quantity and freely available, is a fabulous linguists' playground. This special issue of Computational Linguistics explores ways in which this dream is being explored.","The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #AUTHOR_TAG ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #AUTHOR_TAG ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]",0,"['This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['Existing tagging systems are small scale and typically impose some limitation to prevent over- load (e.g. restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .']","['The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.']"
CC1534,W06-1705,Annotated web as corpus,bootcat bootstrapping corpora and terms from the web in,"['M Baroni', 'S Bernardini']",related work,"This paper introduces the BootCaT toolkit, a suite of perl programs implementing an iterative procedure to bootstrap specialized corpora and terms from the web. The procedure requires only a small set of seed terms as input. The seeds are used to build a corpus via automated Google queries, and more terms are extracted from this corpus. In turn, these new terms are used as seeds to build a larger corpus via automated queries, and so forth. The corpus and the unigram terms are then used to extract multi-word terms. We conducted an evaluation of the tools by applying them to the construction of English and Italian corpora and term lists from the domain of psychiatry. The results illustrate the potential usefulness of the tools.",#AUTHOR_TAG built a corpus by iteratively searching Google for a small set of seed terms .,"['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', '#AUTHOR_TAG built a corpus by iteratively searching Google for a small set of seed terms .', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]",0,"['Studies have used several different methods to mine web data.', '#AUTHOR_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .']","['#AUTHOR_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.']","['Studies have used several different methods to mine web data.', '#AUTHOR_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.']","['The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', '#AUTHOR_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .']","['#AUTHOR_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]","['The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', '#AUTHOR_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.']","['Studies have used several different methods to mine web data.', '#AUTHOR_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]","['This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', '#AUTHOR_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .']","['#AUTHOR_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]"
CC1535,W06-1705,Annotated web as corpus,creating specialized and general corpora using automated search engine queries web as corpus workshop,"['M Baroni', 'S Sharoff']",introduction,,"Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3 ( #AUTHOR_TAG ) .","['A key aspect of our case study research will be to investigate extending corpus collection to new document types.', 'Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3 ( #AUTHOR_TAG ) .']",0,"['Current practise elsewhere includes the distribution of URL lists, but given the dynamic nature of the web, this is not sufficiently robust.', 'Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #AUTHOR_TAGa ) .']","['Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #AUTHOR_TAGa ) .', 'Other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here.']","['Current practise elsewhere includes the distribution of URL lists, but given the dynamic nature of the web, this is not sufficiently robust.', 'Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #AUTHOR_TAGa ) .', 'Other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here.']","['We will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web-based corpus studies based in general.', 'Current practise elsewhere includes the distribution of URL lists, but given the dynamic nature of the web, this is not sufficiently robust.', 'Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #AUTHOR_TAGa ) .']","['Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #AUTHOR_TAGa ) .', 'Other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here.']","['We will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web-based corpus studies based in general.', 'Current practise elsewhere includes the distribution of URL lists, but given the dynamic nature of the web, this is not sufficiently robust.', 'Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #AUTHOR_TAGa ) .', 'Other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here.']","['Current practise elsewhere includes the distribution of URL lists, but given the dynamic nature of the web, this is not sufficiently robust.', 'Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #AUTHOR_TAGa ) .', 'Other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here.']","['The initial solution will be to store the resulting reference 11 http://www.comp.lancs.ac.uk/ucrel/claws/ corpus in the Sketch Engine.', 'We will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web-based corpus studies based in general.', 'Current practise elsewhere includes the distribution of URL lists, but given the dynamic nature of the web, this is not sufficiently robust.', 'Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #AUTHOR_TAGa ) .']","['Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #AUTHOR_TAGa ) .', 'Other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here.']"
CC1536,W06-1705,Annotated web as corpus,using the web to overcome data sparseness,"['F Keller', 'M Lapata', 'O Ourioupina']",introduction,"This paper shows that the web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the web by querying a search engine. We evaluate this method by demonstrating that web frequencies and correlate with frequencies obtained from a carefully edited, balanced corpus. We also perform a task-based evaluation, showing that web frequencies can reliably predict human plausibility judgments.","Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #AUTHOR_TAG ) .","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #AUTHOR_TAG ) .', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']",0,"['This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .']","['In addition , the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.']","['This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.']","['Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .']","['In addition , the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']","['Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.']","['This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .']","['In addition , the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']"
CC1537,W06-1705,Annotated web as corpus,finding syntactic structure in unparsed corpora the gsearch corpus query system computers and the humanities,"['S Corley', 'M Corley', 'F Keller', 'M Crocker', 'S Trewin']",related work,,The Gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .,"['""Real-time"" linguistic analysis of web data at the syntactic level has been piloted by the Linguist\'s Search Engine (LSE).', 'Using this tool, linguists can either perform syntactic searches via parse trees on a pre-analysed web collection of around three million sentences from the Internet Archive (www.archive.org)', 'or build their own collections from AltaVista search engine results.', 'The second method pushes the new collection onto a queue for the LSE annotator to analyse.', 'A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server.', 'The Gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .', 'Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up.', 'In contrast, the Sketch Engine system to assist lexicographers to construct dictionary entries requires large pre-annotated corpora.', ""A word sketch is an automatic one-page corpus-derived summary of a word's grammatical and collocational behaviour."", 'Word Sketches were first used to prepare the Macmillan English Dictionary for Advanced Learners (2002, edited by Michael Rundell).', 'They have also served as the starting point for high-accuracy Word Sense Disambiguation.', 'More recently, the Sketch Engine was used to develop the new edition of the Oxford Thesaurus of English (2004, edited by Maurice Waite).']",0,['#AUTHOR_TAG'],['#AUTHOR_TAG'],['#AUTHOR_TAG'],['#AUTHOR_TAG'],['#AUTHOR_TAG'],['#AUTHOR_TAG'],['#AUTHOR_TAG'],['#AUTHOR_TAG'],['#AUTHOR_TAG']
CC1538,W06-1705,Annotated web as corpus,introduction to the special issue on evaluating word sense disambiguation systems,"['P Edmonds', 'A Kilgarriff']",introduction,"Has system performance on Word Sense Disambiguation (WSD) reached a limit? Automatic systems don't perform nearly as well as humans on the task, and from the results of the SENSEVAL exercises, recent improvements in system performance appear negligible or even negative. Still, systems do perform much better than the baselines, so something is being done right. System evaluation is crucial to explain these results and to show the way forward. Indeed, the success of any project in WSD is tied to the evaluation methodology used, and especially to the formalization of the task that the systems perform. The evaluation of WSD has turned out to be as difficult as designing the systems in the first place.","Its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages .","['Linguistic annotation of corpora contributes crucially to the study of language at several levels: morphology, syntax, semantics, and discourse.', 'Its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages .']",0,"['We believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work.', 'We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ) .']","['We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ) .', 'Narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture (Stallman, 2001).']","['We believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work.', 'We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ) .', 'Narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture (Stallman, 2001).']","['We believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work.', 'We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ) .']","['We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ) .', 'Narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture (Stallman, 2001).']","['We believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work.', 'We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ) .', 'Narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture (Stallman, 2001).']","['We believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work.', 'We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ) .', 'Narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture (Stallman, 2001).']","['We believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work.', 'We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ) .']","['We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ) .', 'Narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture (Stallman, 2001).']"
CC1539,W06-1705,Annotated web as corpus,web as corpus,['A Kilgarriff'],related work,,"The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g. automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS4, Amal- gam5, Connexor6) are available, for example, in the application of part-of-speech tags to corpora.', 'Existing tagging systems are ï¿½small scaleï¿½ and typically impose some limitation to prevent over- load (e.g. restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', 'Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguistï¿½s Search En- gine (Kilgarriff, 2003; Resnik and Elkiss, 2003).']",0,"['In fact, they were used expecially in academic writing with some success.', 'Intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ) .']",['Intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ) .'],"['In fact, they were used expecially in academic writing with some success.', 'Intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ) .']","['Apart from wikis, blogs, and cognitive mapping, we were also inspired by the experiences of early hypertext writing tools, in particular Intermedia and Storyspace.', 'In fact, they were used expecially in academic writing with some success.', 'Intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ) .']",['Intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ) .'],"['Apart from wikis, blogs, and cognitive mapping, we were also inspired by the experiences of early hypertext writing tools, in particular Intermedia and Storyspace.', 'In fact, they were used expecially in academic writing with some success.', 'Intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ) .']","['In fact, they were used expecially in academic writing with some success.', 'Intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ) .']","['Apart from wikis, blogs, and cognitive mapping, we were also inspired by the experiences of early hypertext writing tools, in particular Intermedia and Storyspace.', 'In fact, they were used expecially in academic writing with some success.', 'Intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ) .']","['Intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ) .', 'Storyspace is currently distributed by Eastgate ( 2005), and we have used it for a time.', ""However, in our opinion Storyspace is a product of its time and in fact it isn't a web application."", 'Although it is possible to label links, it lacks a lot of features we need.']"
CC1540,W06-1705,Annotated web as corpus,word sense disambiguation by web mining for word cooccurrence probabilities,['P Turney'],related work,"This paper describes the National Research Council (NRC) Word Sense Disambiguation (WSD) system, as applied to the English Lexical Sample (ELS) task in Senseval-3. The NRC system approaches WSD as a classical supervised machine learning problem, using familiar tools such as the Weka machine learning software and Brill's rule-based part-of-speech tagger. Head words are represented as feature vectors with several hundred features. Approximately half of the features are syntactic and the other half are semantic. The main novelty in the system is the method for generating the semantic features, based on word co-occurrence probabilities. The probabilities are estimated using the Waterloo MultiText System with a corpus of about one terabyte of unlabeled text, collected by a web crawler.",#AUTHOR_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .,"['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', '#AUTHOR_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]",0,"[""Following the example of #AUTHOR_TAG , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by Roland Barthes ( 1970 ) .""]","[""Following the example of #AUTHOR_TAG , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by Roland Barthes ( 1970 ) ."", 'Consequently, a hypertext is a set of lexias.']","[""Following the example of #AUTHOR_TAG , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by Roland Barthes ( 1970 ) ."", 'Consequently, a hypertext is a set of lexias.']","[""Following the example of #AUTHOR_TAG , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by Roland Barthes ( 1970 ) .""]","[""Following the example of #AUTHOR_TAG , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by Roland Barthes ( 1970 ) ."", 'Consequently, a hypertext is a set of lexias.', 'In hypertexts transitions from one lexia to another are not necessarily sequential, but navigational.']","[""Following the example of #AUTHOR_TAG , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by Roland Barthes ( 1970 ) ."", 'Consequently, a hypertext is a set of lexias.']","[""Following the example of #AUTHOR_TAG , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by Roland Barthes ( 1970 ) ."", 'Consequently, a hypertext is a set of lexias.', 'In hypertexts transitions from one lexia to another are not necessarily sequential, but navigational.']","[""Following the example of #AUTHOR_TAG , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by Roland Barthes ( 1970 ) .""]","[""Following the example of #AUTHOR_TAG , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by Roland Barthes ( 1970 ) ."", 'Consequently, a hypertext is a set of lexias.', 'In hypertexts transitions from one lexia to another are not necessarily sequential, but navigational.', 'The main problems of hypertexts, acknowledged since the beginning, have been traced as follows (Nelson, 1992):']"
CC1541,W06-1705,Annotated web as corpus,facilitating the compilation and dissemination of adhoc web corpora,['W H Fletcher'],,"Since the World Wide Web gained prominence in the mid-1990s it has tantalized language investigators and ins ructors as a virtually unlimited source of machine-readable texts for compiling corpora and developing teaching materials. The broad range of languages and content domains found online also offers translators enormous promise both for translation by-example and as a comprehensive supplement to published reference works This paper surveys the impediments which s ill prevent the Web from realizing its full potential as a linguistic resource and discusses tools to overcome the remaining hurdles. Identifying online documents which are both relevant and reliable presents a major challenge. As a partial solution the author's Web concordancer KWiCFinder au omates the process of seeking and retrieving webpages Enhancements which permit more focused queries than existing search engines and provide search results in an interactive explora ory environmen are described in detail. Despite the efficiency of automated downloading and excerpting, selecting Web documents still entails significant time and effort. To multiply the benefits of a search, an online forum for sharing annotated search reports and linguistically interesting texts with other users is outlined. Furthermore, the orien ation of commercial sea ch engines toward the general public makes them less beneficial for linguistic research. The author sketches plans for a specialized Search Engine for Applied Linguis s and a selective Web Corpus Archive which build on his experience with KWiCFinder. He compares his available and proposed solutions to existing resou ces, and su veys ways to exploi them in language teaching. Together these proposed services will enable language learners and professionals to tap into the Web effectively and efficiently for instruction research and translation. t","Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #AUTHOR_TAGa ) .","['We will explore techniques to make the resulting annotated web corpus data available in static form to enable replication and verification of corpus studies based on such data.', 'The initial solution will be to store the resulting reference 11 http://www.comp.lancs.ac.uk/ucrel/claws/ corpus in the Sketch Engine.', 'We will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web-based corpus studies based in general.', 'Current practise elsewhere includes the distribution of URL lists, but given the dynamic nature of the web, this is not sufficiently robust.', 'Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #AUTHOR_TAGa ) .', 'Other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here.']",0,"['Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', ""For example , a ` web page ' is more similar to an infinite canvas than a written page ( #AUTHOR_TAG ) .""]","[""For example , a ` web page ' is more similar to an infinite canvas than a written page ( #AUTHOR_TAG ) .""]","['Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', ""For example , a ` web page ' is more similar to an infinite canvas than a written page ( #AUTHOR_TAG ) .""]","['literary criticism -unlike in the previous times (Eisenstein, 1983).', 'Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', ""For example , a ` web page ' is more similar to an infinite canvas than a written page ( #AUTHOR_TAG ) .""]","[""For example , a ` web page ' is more similar to an infinite canvas than a written page ( #AUTHOR_TAG ) .""]","['literary criticism -unlike in the previous times (Eisenstein, 1983).', 'Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', ""For example , a ` web page ' is more similar to an infinite canvas than a written page ( #AUTHOR_TAG ) .""]","['Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', ""For example , a ` web page ' is more similar to an infinite canvas than a written page ( #AUTHOR_TAG ) .""]","[""For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e."", 'literary criticism -unlike in the previous times (Eisenstein, 1983).', 'Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', ""For example , a ` web page ' is more similar to an infinite canvas than a written page ( #AUTHOR_TAG ) .""]","[""For example , a ` web page ' is more similar to an infinite canvas than a written page ( #AUTHOR_TAG ) ."", 'Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an 'opera aperta' (open work), as Eco would define it (1962)."", 'From a more pessimistic one, an author may feel to have lost power in this openness.']"
CC1542,W06-1705,Annotated web as corpus,the corpusbased study of language change in progress the extra value of tagged corpora,['C Mair'],introduction,,"In addition , the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']",0,"['We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve (Lessig, 2004).', 'Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG ) .']","['Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG ) .']","['We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve (Lessig, 2004).', 'Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG ) .']","['We believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work.', 'We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve (Lessig, 2004).', 'Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG ) .']","['Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG ) .']","['We believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work.', 'We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve (Lessig, 2004).', 'Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG ) .']","['We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve (Lessig, 2004).', 'Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG ) .']","['We believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work.', 'We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve (Lessig, 2004).', 'Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG ) .']","['Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG ) .']"
CC1543,W06-1705,Annotated web as corpus,blueprint for a high performance nlp infrastructure,['J R Curran'],related work,"Natural Language Processing (NLP) system de-velopers face a number of new challenges. In-terest is increasing for real-world systems that use NLP tools and techniques. The quantity of text now available for training and processing is increasing dramatically. Also, the range of languages and tasks being researched contin-ues to grow rapidly. Thus it is an ideal time to consider the development of new experimen-tal frameworks. We describe the requirements, initial design and exploratory implementation of a high performance NLP infrastructure.",#AUTHOR_TAG,['#AUTHOR_TAG'],0,"[""Now they try to collect themselves in so-called 'blogspheres'."", 'On the other side , wikis started as collective works where each entry is not owned by a single author e.g. #AUTHOR_TAG .']","['On the other side , wikis started as collective works where each entry is not owned by a single author e.g. #AUTHOR_TAG .']","[""Now they try to collect themselves in so-called 'blogspheres'."", 'On the other side , wikis started as collective works where each entry is not owned by a single author e.g. #AUTHOR_TAG .']","['So blogs are a literary metagenre which started as authored personal diaries or journals.', ""Now they try to collect themselves in so-called 'blogspheres'."", 'On the other side , wikis started as collective works where each entry is not owned by a single author e.g. #AUTHOR_TAG .']","['On the other side , wikis started as collective works where each entry is not owned by a single author e.g. #AUTHOR_TAG .']","['So blogs are a literary metagenre which started as authored personal diaries or journals.', ""Now they try to collect themselves in so-called 'blogspheres'."", 'On the other side , wikis started as collective works where each entry is not owned by a single author e.g. #AUTHOR_TAG .']","[""Now they try to collect themselves in so-called 'blogspheres'."", 'On the other side , wikis started as collective works where each entry is not owned by a single author e.g. #AUTHOR_TAG .']","['Furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', 'So blogs are a literary metagenre which started as authored personal diaries or journals.', ""Now they try to collect themselves in so-called 'blogspheres'."", 'On the other side , wikis started as collective works where each entry is not owned by a single author e.g. #AUTHOR_TAG .']","['On the other side , wikis started as collective works where each entry is not owned by a single author e.g. #AUTHOR_TAG .', 'Wikipedia (2005).', 'Now personal wiki tools are arising for brainstorming and mind mapping.', 'See Section 4 for further aspects.']"
CC1544,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,free culture how big media uses technology and the law to lock down culture and control creativity,['Lawrence Lessig'],,"espanolEl presente texto publicado en la Zona REMIX de la revista Communiars se corresponde con la introduccion del afamado libro Cultura Libre, del profesor Lawrence Lessig, presidente de la organizacion Creative Commons, dedicada a promover el acceso e intercambio culturales. El texto > (en espanol Cultura Libre. Como los grandes medios usan la tecnologia y la ley para bloquear la cultura y controlar la creatividad) es un libro publicado en 2004 y centrado en presentar otra manera de organizar la cultura y el conocimiento, abriendo las restricciones del obsoleto paradigma del copyright, y apoyandose en el modelo copyleft promovido desde el software libre. La introduccion que aqui se presenta traduce el espiritu abierto de un texto clave para la comprension y evolucion de la actualidad cultural. La version que se publica procede la version PDF de Free Culture, licenciada bajo Creative Commons en su variante BY-NC 1.0. EnglishThe present text published in the REMIX Zone of the Communiars Journal corresponds to the introduction of the famous book Free Culture, by Professor Lawrence Lessig, president of the Creative Commons organization, dedicated to promoting cultural access and exchange. The text Free Culture. How big media uses technology and the law to lock down culture and control creativity is a book published in 2004 and focused on presenting another way of organizing culture and knowledge, opening the restrictions of the obsolete paradigm of copyright, and relying on the copyleft model promoted by free software. The introduction presented here translates the open spirit of a key text for the understanding and evolution of current cultural reality. The version that is published is part of PDF version of Free Culture, licensed under Creative Commons in its variant BY-NC 1.0.",We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ) .,"['We believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work.', 'We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ) .', 'Narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture (Stallman, 2001).']",5,"['The lexia will be marked as a derivative work from the original one, and a new document history timeline will start (see Figure 2).', 'Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #AUTHOR_TAG ) .']",['Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #AUTHOR_TAG ) .'],"['The lexia will be marked as a derivative work from the original one, and a new document history timeline will start (see Figure 2).', 'Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #AUTHOR_TAG ) .']","['In the second one, the last user, who has edited the lexia, may claim the attribution for himself.', 'The lexia will be marked as a derivative work from the original one, and a new document history timeline will start (see Figure 2).', 'Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #AUTHOR_TAG ) .']",['Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #AUTHOR_TAG ) .'],"['In the second one, the last user, who has edited the lexia, may claim the attribution for himself.', 'The lexia will be marked as a derivative work from the original one, and a new document history timeline will start (see Figure 2).', 'Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #AUTHOR_TAG ) .']","['The lexia will be marked as a derivative work from the original one, and a new document history timeline will start (see Figure 2).', 'Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #AUTHOR_TAG ) .']","['In the first instance, the edited version simply moves ahead the document history.', 'In the second one, the last user, who has edited the lexia, may claim the attribution for himself.', 'The lexia will be marked as a derivative work from the original one, and a new document history timeline will start (see Figure 2).', 'Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #AUTHOR_TAG ) .']","['Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #AUTHOR_TAG ) .', 'If nobody claims the document for himself, it will fall in the public domain.', 'The set of lexias in the public domain will form a special document, owned by a special user, called Public Domain.', ""If the author refuses the permission to create derivative works, i.e. to edit his own lexias, users still have the right to comment the author's work.""]"
CC1545,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,hypertext 20 the convergence of contemporary critical theory and technology the johns hopkins,['George P Landow'],related work,,Intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ) .,"['Apart from wikis, blogs, and cognitive mapping, we were also inspired by the experiences of early hypertext writing tools, in particular Intermedia and Storyspace.', 'In fact, they were used expecially in academic writing with some success.', 'Intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ) .', 'Storyspace is currently distributed by Eastgate ( 2005), and we have used it for a time.', ""However, in our opinion Storyspace is a product of its time and in fact it isn't a web application."", 'Although it is possible to label links, it lacks a lot of features we need.', 'Moreover, no hypertext writing tool available is released under an open source licence.', 'We hope that Novelle will bridge this gap -we will choose the exact licence when our first public release is ready.']",0,"['The main source of Novelle are wikis and blogs.', 'While wikis have spread from a detailed design ( #AUTHOR_TAG ) , unfortunately blogs have not been designed under a model .']","['While wikis have spread from a detailed design ( #AUTHOR_TAG ) , unfortunately blogs have not been designed under a model .', 'So we have tested and compared the most used tools available for blogging: Bloggers, WordPress, MovableType and LiveJournal.']","['The main source of Novelle are wikis and blogs.', 'While wikis have spread from a detailed design ( #AUTHOR_TAG ) , unfortunately blogs have not been designed under a model .', 'So we have tested and compared the most used tools available for blogging: Bloggers, WordPress, MovableType and LiveJournal.']","['The main source of Novelle are wikis and blogs.', 'While wikis have spread from a detailed design ( #AUTHOR_TAG ) , unfortunately blogs have not been designed under a model .']","['While wikis have spread from a detailed design ( #AUTHOR_TAG ) , unfortunately blogs have not been designed under a model .', 'So we have tested and compared the most used tools available for blogging: Bloggers, WordPress, MovableType and LiveJournal.']","['The main source of Novelle are wikis and blogs.', 'While wikis have spread from a detailed design ( #AUTHOR_TAG ) , unfortunately blogs have not been designed under a model .', 'So we have tested and compared the most used tools available for blogging: Bloggers, WordPress, MovableType and LiveJournal.']","['The main source of Novelle are wikis and blogs.', 'While wikis have spread from a detailed design ( #AUTHOR_TAG ) , unfortunately blogs have not been designed under a model .', 'So we have tested and compared the most used tools available for blogging: Bloggers, WordPress, MovableType and LiveJournal.']","['The main source of Novelle are wikis and blogs.', 'While wikis have spread from a detailed design ( #AUTHOR_TAG ) , unfortunately blogs have not been designed under a model .']","['While wikis have spread from a detailed design ( #AUTHOR_TAG ) , unfortunately blogs have not been designed under a model .', 'So we have tested and compared the most used tools available for blogging: Bloggers, WordPress, MovableType and LiveJournal.']"
