token_context,word_context,seg_context,sent_cotext,label
"['employed  #TAUTHOR_TAG : 460 ff. )', 'however, only two of these criteria are explicit in the coding system']","['employed  #TAUTHOR_TAG : 460 ff. )', 'however, only two of these criteria are explicit in the coding system']","[' #TAUTHOR_TAG : 460 ff. )', 'however, only two of these criteria are explicit in the coding system']","['', 'thirdly, it seems clear that the object raising rule is straining the limits of what can be reliably extracted from the ldoce coding system.', 'ideally, to distinguish between raising and equi verbs, a number of syntactic criteria should be employed  #TAUTHOR_TAG : 460 ff. )', 'however, only two of these criteria are explicit in the coding system']",3
"['the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information  #TAUTHOR_TAG.', 'independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample']","['the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information  #TAUTHOR_TAG.', 'independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample']","['the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information  #TAUTHOR_TAG.', 'independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample']","['the master ldoce file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'in addition to headwords, dictionary search through the pronunciation field is available ;  #AUTHOR_TAG has merged information from the pronunciation and hyphenation fields, creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure  #AUTHOR_TAG.', 'in addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information  #TAUTHOR_TAG.', 'independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample']",0
"['tested the classification of verbs into semantic types using a verb list of 139 pre - classified items drawn from the lists published in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'figure 16 gives the number of verbs classified under each category by these authors and the number successfully classified into the same categories by the system']","['tested the classification of verbs into semantic types using a verb list of 139 pre - classified items drawn from the lists published in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'figure 16 gives the number of verbs classified under each category by these authors and the number successfully classified into the same categories by the system']","['tested the classification of verbs into semantic types using a verb list of 139 pre - classified items drawn from the lists published in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'figure 16 gives the number of verbs classified under each category by these authors and the number successfully classified into the same categories by the system']","['tested the classification of verbs into semantic types using a verb list of 139 pre - classified items drawn from the lists published in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'figure 16 gives the number of verbs classified under each category by these authors and the number successfully classified into the same categories by the system']",5
"[' #TAUTHOR_TAG points out, given that no situations were envisaged']","[' #TAUTHOR_TAG points out, given that no situations were envisaged']","[' #TAUTHOR_TAG points out, given that no situations were envisaged']","['series of systems in cambridge are implemented in lisp running under unixtm.', 'they all make use of an efficient dictionary access system which services requests for s - expression entries made by client pro - grams.', 'a dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved.', 'as  #TAUTHOR_TAG points out, given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.', 'the use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation.', 'for reasons of efficiency and flexibility of customisation, namely the use of ldoce by different client programs and from different lisp and / or prolog systems, the dictionary access system is implemented in the programming language c and makes use of the inter - process communication facilities provided by the unix operating system.', 'to the lisp programmer, the creation of a dictionary process and subsequent requests for information from the dictionary appear simply as lisp function calls']",0
"['context of three collaborative projects  #TAUTHOR_TAG to develop a general - purpose, wide coverage morphological and syntactic analyse']","['context of three collaborative projects  #TAUTHOR_TAG to develop a general - purpose, wide coverage morphological and syntactic analyser for english.', 'one']","['frequent and rapid access to the information in the dictionary via the dictionary head words.', 'the research described below is taking place in the context of three collaborative projects  #TAUTHOR_TAG to develop a general - purpose, wide coverage morphological and syntactic analyser for english.', 'one motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by']","['developments in linguistics, and especially on grammatical theory - - for example, generalised phrase structure grammar ( gpsg )  #AUTHOR_TAG, lexical functional grammar ( lfg )  #AUTHOR_TAG - - and on natural language parsing frameworks for example, functional unification grammar ( fug )  #AUTHOR_TAG a ), patr - ii  #AUTHOR_TAG - - make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'these developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'real - time parsing imposes stringent requirements on a dictionary support environment ; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'the research described below is taking place in the context of three collaborative projects  #TAUTHOR_TAG to develop a general - purpose, wide coverage morphological and syntactic analyser for english.', 'one motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser']",0
['. g.  #TAUTHOR_TAG ;'],"['comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'many investigators ( e. g.', 'many investigators ( e. g.  #TAUTHOR_TAG ; elowitz']",['. g.  #TAUTHOR_TAG ;'],"['text - to - speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'many investigators ( e. g.', 'many investigators ( e. g.  #TAUTHOR_TAG ; elowitz et al. 1976 ; luce et al. 1983 ; cahn 1988 ) have suggested that the poor prosody of synthetic speech, in comparison with natural speech, is the primary factor leading to difficulties in the comprehension of fluent synthetic speech.', ""and while researchers in text - tospeech synthesis have adopted a variety of approaches to prosodic phrase generation - - from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in  #AUTHOR_TAG and o' #AUTHOR_TAG - - - the generation of appropriate prosodic phrasing in unres ~ tricted text has remained a problem."", 'as we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', '']",4
"[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']","['previous work ( bachenko et al. 1986 ), we described an experimental text - to - speech system that determined prosodic phrasing for the olive - - liberman synthesizer  #TAUTHOR_TAG.', 'the system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'while we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'this paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'like the earlier version, it has been implemented in a text - to - speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries']",0
[';  #TAUTHOR_TAG have suggested'],[';  #TAUTHOR_TAG have suggested'],['. 1983 ;  #TAUTHOR_TAG have suggested'],[' #TAUTHOR_TAG'],4
"[' #TAUTHOR_TAG.', 'two concerns']","[' #TAUTHOR_TAG.', 'two concerns']","[' #TAUTHOR_TAG.', 'two concerns']","['have built an experimental text - to - speech system that uses our analysis of prosody to generate phrase boundaries for the olive - - liberman synthesizer  #TAUTHOR_TAG.', 'two concerns motivated our implementation.', 'first, we hoped the system would provide us with a research tool for testing our ideas about syntax and phrasing against a large unrestricted collection of sentences.', 'second, we wished to investigate how well our approach would work for determining prosodic phrasing in a text - to - speech synthesizer']",5
"['status in the discourse.', ' #TAUTHOR_TAG and  #AUTHOR_TAG also examine the']","['status in the discourse.', ' #TAUTHOR_TAG and  #AUTHOR_TAG also examine the']","['status in the discourse.', ' #TAUTHOR_TAG and  #AUTHOR_TAG also examine the relation between discourse and prosodic phrasing.', 'their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from']","['', ' #AUTHOR_TAG and  #AUTHOR_TAG take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in  #AUTHOR_TAG also assume that phrasing is dependent on predicate - argument structure.', 'the problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'in 17a - f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'all of the sentences have the same x ( vy ) pattern even though y is a complement in the first case ( thefirst serious attempt ) and an adjunct in the others.', '( the complement in 17a and the adjuncts in 17b - f are italicized. )', 'the relation between discourse and prosodic phrasing has been examined in some detail by  #AUTHOR_TAG, who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'this may be the reason that word count and syllable count play a prominent role in prosodic phrasing ( see section 2. 1. 3. ).', 'to our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', ' #TAUTHOR_TAG and  #AUTHOR_TAG also examine the relation between discourse and prosodic phrasing.', 'their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing']",0
"['often refers to examples of embedded sentences.', 'sentences like 12, from  #TAUTHOR_TAG, are frequently cited.', '( square brackets mark off the np constituents that contain embed - ded sentences.']","['often refers to examples of embedded sentences.', 'sentences like 12, from  #TAUTHOR_TAG, are frequently cited.', '( square brackets mark off the np constituents that contain embed - ded sentences.']","['often refers to examples of embedded sentences.', 'sentences like 12, from  #TAUTHOR_TAG, are frequently cited.', '( square brackets mark off the np constituents that contain embed - ded sentences.']","['it comes to sentence - level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'this observation has led some researchers, e. g., cooper and paccia -  #AUTHOR_TAG, to claim a direct mapping between the syntactic phrase and the prosodic phra, ; e.', ""however, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'for example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'sentences like 12, from  #TAUTHOR_TAG, are frequently cited.', '( square brackets mark off the np constituents that contain embed - ded sentences.']",0
"[' #TAUTHOR_TAG, we described an experimental text - to - speech system that']","[' #TAUTHOR_TAG, we described an experimental text - to - speech system that']","['previous work  #TAUTHOR_TAG, we described an experimental text - to - speech system that']","['previous work  #TAUTHOR_TAG, we described an experimental text - to - speech system that determined prosodic phrasing for the olive - - liberman synthesizer ( olive and liberman 1985 ).', 'the system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'while we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'this paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'like the earlier version, it has been implemented in a text - to - speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries']",2
"['', ' #TAUTHOR_TAG proposes readjust']","['proper.', ' #TAUTHOR_TAG proposes readjustment rules similar']","['', ' #TAUTHOR_TAG proposes readjustment rules similar']","['', 'rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12 ] "" ( p.', '372 ).', 'thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', ' #TAUTHOR_TAG proposes readjustment rules similar to those of chomsky and halle, but he claims that the readjustment of structure is part of the grammar, not part of the performance model.', 'he thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2 - - that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface, ; with prosodic phonology']",0
"['psycholinguistic studies of  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG, responding to the idea of readjust']","['psycholinguistic studies of  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG, responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if']","['psycholinguistic studies of  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG, responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if read']","['psycholinguistic studies of  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG, responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'for example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i. e., the data reveal both x ( vy ) and ( xv ) y groupings.', ' #AUTHOR_TAG claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length ; for example, the main prosodic phrase break corresponds to the subject - predicate boundary in waiters who remember well ] [ serve orders correctly.', 'discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in chickens were eating i [ the remaining green vegetables, where the subject - predicate boundary finds no prosodic correspondent.']",0
"['in  #TAUTHOR_TAG,']","['in  #TAUTHOR_TAG,']","['in  #TAUTHOR_TAG,']","['work on the prosodic phrase status of clause final prepositional phrases, which we discuss below, suggests the existence of a discourse - neutral phrasing that depends on syntactic constituency mediated by string adjacency and length of a potential prosodic phrase.', '3 such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in  #TAUTHOR_TAG, which "" are often overwhelmed by the chiaroscuro of highlight and background in discourse, but retain the status of null - hypothesis patterns that emerge when there is no good reason to take some other option "" ( p. 251 ).', 'this approach to prosodic phrase boundary determination brings us closer to a framework in which phonological, syntactic, and discourse features all contribute to prosodic phrasing']",1
"['##tened structure that more accurately reflects the prosodic phrasing.', 'in  #TAUTHOR_TAG,']","['flattened structure that more accurately reflects the prosodic phrasing.', 'in  #TAUTHOR_TAG,']","['', 'the result is a flattened structure that more accurately reflects the prosodic phrasing.', 'in  #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],0
"['whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'an alternative representation based on  #TAUTHOR_TAG is presented in  #AUTHOR_TAG, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'although a grid may be more descriptively suitable for some aspects of prosody ( for example,  #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing']","['whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'an alternative representation based on  #TAUTHOR_TAG is presented in  #AUTHOR_TAG, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'although a grid may be more descriptively suitable for some aspects of prosody ( for example,  #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing']","['g & g, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'an alternative representation based on  #TAUTHOR_TAG is presented in  #AUTHOR_TAG, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'although a grid may be more descriptively suitable for some aspects of prosody ( for example,  #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing']","['g & g, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'an alternative representation based on  #TAUTHOR_TAG is presented in  #AUTHOR_TAG, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'although a grid may be more descriptively suitable for some aspects of prosody ( for example,  #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing']",1
"['to prosodic phrasing.', ' #TAUTHOR_TAG claims that prosodic phrase boundaries']","['to prosodic phrasing.', ' #TAUTHOR_TAG claims that prosodic phrase boundaries']","['possible input to prosodic phrasing.', ' #TAUTHOR_TAG claims that prosodic phrase boundaries will co - occur with grammatical functions']","['syntax / prosody misalignment may be viewed as resulting in part from semantic considerations.', 'both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', ' #TAUTHOR_TAG']",0
"['psycholinguistic studies of  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, responding to the idea of readjust']","['psycholinguistic studies of  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if']","['psycholinguistic studies of  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if read']","['psycholinguistic studies of  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'for example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i. e., the data reveal both x ( vy ) and ( xv ) y groupings.', ' #AUTHOR_TAG claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length ; for example, the main prosodic phrase break corresponds to the subject - predicate boundary in waiters who remember well ] [ serve orders correctly.', 'discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in chickens were eating i [ the remaining green vegetables, where the subject - predicate boundary finds no prosodic correspondent.']",0
[';  #TAUTHOR_TAG ; cahn'],[';  #TAUTHOR_TAG ; cahn'],[';  #TAUTHOR_TAG ; cahn 1988 ) have suggested'],"['text - to - speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'many investigators ( e. g.', 'many investigators ( e. g. allen 1976 ; elowitz et al. 1976 ;  #TAUTHOR_TAG ; cahn 1988 ) have suggested that the poor prosody of synthetic speech, in comparison with natural speech, is the primary factor leading to difficulties in the comprehension of fluent synthetic speech.', ""and while researchers in text - tospeech synthesis have adopted a variety of approaches to prosodic phrase generation - - from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in  #AUTHOR_TAG and o' #AUTHOR_TAG - - - the generation of appropriate prosodic phrasing in unres ~ tricted text has remained a problem."", 'as we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', '']",4
"['g & g, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'an alternative representation based on  #AUTHOR_TAG is presented in  #AUTHOR_TAG, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'although a grid may be more descriptively suitable for some aspects of prosody ( for example,  #TAUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing']","['g & g, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'an alternative representation based on  #AUTHOR_TAG is presented in  #AUTHOR_TAG, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'although a grid may be more descriptively suitable for some aspects of prosody ( for example,  #TAUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing']","['g & g, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'an alternative representation based on  #AUTHOR_TAG is presented in  #AUTHOR_TAG, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'although a grid may be more descriptively suitable for some aspects of prosody ( for example,  #TAUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing']","['g & g, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'an alternative representation based on  #AUTHOR_TAG is presented in  #AUTHOR_TAG, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'although a grid may be more descriptively suitable for some aspects of prosody ( for example,  #TAUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing']",1
['described in  #TAUTHOR_TAG'],['described in  #TAUTHOR_TAG'],['described in  #TAUTHOR_TAG'],"['', ' #AUTHOR_TAG and  #AUTHOR_TAG take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in  #TAUTHOR_TAG also assume that phrasing is dependent on predicate - argument structure.', 'the problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'in 17a - f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'all of the sentences have the same x ( vy ) pattern even though y is a complement in the first case ( thefirst serious attempt ) and an adjunct in the others.', '( the complement in 17a and the adjuncts in 17b - f are italicized. )', 'the relation between discourse and prosodic phrasing has been examined in some detail by  #AUTHOR_TAG, who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'this may be the reason that word count and syllable count play a prominent role in prosodic phrasing ( see section 2. 1. 3. ).', 'to our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', ' #AUTHOR_TAG and  #AUTHOR_TAG also examine the relation between discourse and prosodic phrasing.', 'their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing']",2
"[',  #TAUTHOR_TAG,']","[',  #TAUTHOR_TAG,']","[',  #TAUTHOR_TAG,']","['it comes to sentence - level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'this observation has led some researchers, e. g.,  #TAUTHOR_TAG, to claim a direct mapping between the syntactic phrase and the prosodic phrase.', ""however, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'for example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'sentences like 12, from  #AUTHOR_TAG to account for such mismatches, "" readjustment rules "" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'the result is a flattened structure that more accurately reflects the prosodic phrasing.', '']",0
['is the analysis presented in  #TAUTHOR_TAG ('],['is the analysis presented in  #TAUTHOR_TAG ( henceforth g & g )'],['predicate boundary finds no prosodic correspondent. 4 the most explicit version of this approach is the analysis presented in  #TAUTHOR_TAG ('],"['psycholinguistic studies of  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'for example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i. e., the data reveal both x ( vy ) and ( xv ) y groupings.', ' #AUTHOR_TAG claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length ; for example, the main prosodic phrase break corresponds to the subject - predicate boundary in waiters who remember well ] [ serve orders correctly.', 'discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in chickens were eating ii the remaining green vegetables, where the subject - predicate boundary finds no prosodic correspondent. 4 the most explicit version of this approach is the analysis presented in  #TAUTHOR_TAG ( henceforth g & g )']",1
"['discourse and prosodic phrasing has been examined in some detail by  #TAUTHOR_TAG, who argues']","['discourse and prosodic phrasing has been examined in some detail by  #TAUTHOR_TAG, who argues']","['discourse and prosodic phrasing has been examined in some detail by  #TAUTHOR_TAG, who argues']","['', ' #AUTHOR_TAG and  #AUTHOR_TAG take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in  #AUTHOR_TAG also assume that phrasing is dependent on predicate - argument structure.', 'the problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'in 17a - f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'all of the sentences have the same x ( vy ) pattern even though y is a complement in the first case ( thefirst serious attempt ) and an adjunct in the others.', '( the complement in 17a and the adjuncts in 17b - f are italicized. )', 'the relation between discourse and prosodic phrasing has been examined in some detail by  #TAUTHOR_TAG, who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'this may be the reason that word count and syllable count play a prominent role in prosodic phrasing ( see section 2. 1. 3. ).', 'to our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', ' #AUTHOR_TAG and  #AUTHOR_TAG also examine the relation between discourse and prosodic phrasing.', 'their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing']",0
"['formation are adopted, for the most part, from g & g,  #TAUTHOR_TAG, and the account of monosyllabic destressing in  #AUTHOR_TAG.', 'thus in our analysis, rules of phonological']","['formation are adopted, for the most part, from g & g,  #TAUTHOR_TAG, and the account of monosyllabic destressing in  #AUTHOR_TAG.', 'thus in our analysis, rules of phonological']","['rules for phonological word formation are adopted, for the most part, from g & g,  #TAUTHOR_TAG, and the account of monosyllabic destressing in  #AUTHOR_TAG.', 'thus in our analysis, rules of']","['rules for phonological word formation are adopted, for the most part, from g & g,  #TAUTHOR_TAG, and the account of monosyllabic destressing in  #AUTHOR_TAG.', '']",5
"['impossible such a construction in real time.', 'secondly, the cooperative principle of  #TAUTHOR_TAG, 1978 ), under']","['impossible such a construction in real time.', 'secondly, the cooperative principle of  #TAUTHOR_TAG, 1978 ), under']","['impossible such a construction in real time.', 'secondly, the cooperative principle of  #TAUTHOR_TAG, 1978 ), under the assumption that referential levels']","[', there are at least three arguments against iterating pt.', 'first of all, iteration would increase the complexity of building a model of a paragraph ; infinite iteration would almost certainly make impossible such a construction in real time.', 'secondly, the cooperative principle of  #TAUTHOR_TAG, 1978 ), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader.', 'finally, it has been shown by  #AUTHOR_TAG that the ratio of derived to explicit information necessary for understanding a piece of text is about 8 : 1 ; furthermore, our reading of the analysis of five paragraphs by  #AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.', 'thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh - questions']",4
"['j.  #TAUTHOR_TAG, 1982 )']","['j.  #TAUTHOR_TAG, 1982 )']","['j.  #TAUTHOR_TAG, 1982 )']","['this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others.', 'we are going to make such a comparison with the theories proposed by j.  #TAUTHOR_TAG, 1982 ) that represent a more computationally oriented approach to coherence, and those of t. a. van dijk and w.  #AUTHOR_TAG, who are more interested in addressing psychological and cognitive aspects of discourse coherence.', 'the quoted works seem to be good representatives for each of the directions ; they also point to related literature']",1
"['( cf.  #TAUTHOR_TAG, and their kin.', 'similarly, the notion of']","['of "" unique - name assumption "" ( genesereth and nilsson 1987 ), "" domain closure as - sumption "" ( ibid. ), "" domain circumscription "" ( cf.  #TAUTHOR_TAG, and their kin.', 'similarly, the notion of r + m - abduction is spiritually']","['domain closure as - sumption "" ( ibid. ), "" domain circumscription "" ( cf.  #TAUTHOR_TAG, and their kin.', 'similarly, the notion of']","['techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of "" unique - name assumption "" ( genesereth and nilsson 1987 ), "" domain closure as - sumption "" ( ibid. ), "" domain circumscription "" ( cf.  #TAUTHOR_TAG, and their kin.', 'similarly, the notion of r + m - abduction is spiritually related to the "" abduc - tive inference "" of  #AUTHOR_TAG, the "" diagnosis from first principles "" of  #AUTHOR_TAG, "" explainability "" of  #AUTHOR_TAG, and the subset principle of  #AUTHOR_TAG.', 'but, ob - viously, trying to establish precise connections for the metarules or the provability and the r + m - abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'these connections are being examined elsewhere ( zadrozny forthcoming )']",1
['.  #TAUTHOR_TAG discussed sentences of the form * this is a chair but you can sit on it'],['.  #TAUTHOR_TAG discussed sentences of the form * this is a chair but you can sit on it'],['.  #TAUTHOR_TAG discussed sentences of the form * this is a chair but you can sit on it'],['.  #TAUTHOR_TAG discussed sentences of the form * this is a chair but you can sit on it'],0
"[' #TAUTHOR_TAG, p. 672 )']","[' #TAUTHOR_TAG, p. 672 )']","['and hasan 1976 ; cf. also  #TAUTHOR_TAG, p. 672 )']","['##s are function words - - like conjunctions and some adverbs - - that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '"" and, "" "" or, "" and "" but "" are the three main coordinating connectives in english.', 'however, "" but "" does not behave quite like the other two - - semantically, "" but "" signals a contradiction, and in this role it seems to have three subfunctions :..', 'opposition ( called "" adversative "" or "" contrary - to - expectation "" by halliday and hasan 1976 ; cf. also  #TAUTHOR_TAG, p. 672 )']",0
"[',  #TAUTHOR_TAG, longacre']","[',  #TAUTHOR_TAG, longacre']","['g. chafe 1979,  #TAUTHOR_TAG, longacre 1979, hab']","['there are other discussions of the paragraph as a central element of discourse ( e. g. chafe 1979,  #TAUTHOR_TAG, longacre 1979, haberlandt et al. 1980 ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']",1
"["". g., ` ` colorless green ideas...'' ), while before the advent of chomskyan formalisms, a sentence was defined as the smallest meaningful collection of words ;  #TAUTHOR_TAG, p. 546 ) gives 10 definitions of a sentence""]","[""current formal grammars allow nonsensical but parsable collections of words ( e. g., ` ` colorless green ideas...'' ), while before the advent of chomskyan formalisms, a sentence was defined as the smallest meaningful collection of words ;  #TAUTHOR_TAG, p. 546 ) gives 10 definitions of a sentence""]","["". g., ` ` colorless green ideas...'' ), while before the advent of chomskyan formalisms, a sentence was defined as the smallest meaningful collection of words ;  #TAUTHOR_TAG, p. 546 ) gives 10 definitions of a sentence""]","[', our definition of coherence may not be restrictive enough : two collections of sentences, one referring to "" black "" ( about black pencils, black pullovers, and black poodles ), the other one about "" death "" ( war, cancer, etc. ), connected by a sentence referring to both of these, could be interpreted as one paragraph about the new, broader topic "" black + death. ""', ""this problem may be similar to the situation in which current formal grammars allow nonsensical but parsable collections of words ( e. g., ` ` colorless green ideas...'' ), while before the advent of chomskyan formalisms, a sentence was defined as the smallest meaningful collection of words ;  #TAUTHOR_TAG, p. 546 ) gives 10 definitions of a sentence""]",0
"[' #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and implicitly or explicitly']","[' #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and implicitly or explicitly']","[' #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and implicitly or explicitly']","['logical notions that we are going to consider, such as theory or model, will be finitary.', 'for example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'therefore these notions, and all other constructs we are going to define ( axioms, metarules, definitions etc. ) are computational, although usually we will not provide explicit algorithms for computing them.', 'the issues of control are not so important for us at this point ; we restrict ourselves to describing the logic.', 'this principle of finitism is also assumed by  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'mycielski 1981 )']",1
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['', ""but, obviously, there are other possibilities - - for instance, the discourse representation structures ( drs's ) of  #TAUTHOR_TAG, which have been used to translate a subset of english into logical formulas, to model text ( identified with a list of sentences ), to analyze a fragment of english, and to deal with anaphora."", 'the logical notation of  #AUTHOR_TAG is more sophisticated, and may be considered another possibility.', ' #AUTHOR_TAG formalism is richer and resembles more closely an english grammar.', ' #AUTHOR_TAG writes "" it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys. ""', 'therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'it will also be a model for our simplified logical notation ( cf.', 'section 5 ).', 'we can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences']",1
"[') or quantifier scoping  #TAUTHOR_TAG must play a role, too']","[') or quantifier scoping  #TAUTHOR_TAG must play a role, too']","['the role of focus ( grosz 1977, 1978 ; sidner 1983 ) or quantifier scoping  #TAUTHOR_TAG must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( grosz 1977, 1978 ; sidner 1983 ) or quantifier scoping  #TAUTHOR_TAG must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']",0
"['to  #TAUTHOR_TAG, p. 67 ), these']","['to  #TAUTHOR_TAG, p. 67 ), these']","['to  #TAUTHOR_TAG, p. 67 ), these two sentences are incoherent.', 'however,']","['to  #TAUTHOR_TAG, p. 67 ), these two sentences are incoherent.', 'however, the same fragment, augmented with the third sentence mary told him yesterday that the french spinach crop failed and turkey is the only country... ( ibid. )', 'suddenly ( for hobbs ) becomes coherent.', ""it seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change ; after all, the first two sentences didn't change when the third one was added."", 'on the other hand, this change is easily explained when we treat the first two sentences as a paragraph : if the third sentence is not a part of the background knowledge, the paragraph is incoherent.', 'and the paragraph obtained by adding the third sentence is coherent.', 'moreover, coherence here is clearly the result of the existence of the topic "" john likes spinach.']",1
[' #TAUTHOR_TAG p. 195 ; za'],['( cfxxx  #TAUTHOR_TAG p. 195 ; zadrozny 1987a ) : we bought the boys apples because'],['( cfxxx  #TAUTHOR_TAG p. 195 ; za'],"['adopt the three - level semantics as a formal tool for the analysis of paragraphs.', 'this semantics was constructed ( zadrozny 1987a, 1987b ) as a formal framework for default and commonsense reasoning.', 'it should not come as a surprise that we can now use this apparatus for text / discourse analysis ; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.', 'for instance, relating "" they "" to "" apples "" in the sentence ( cfxxx  #TAUTHOR_TAG p. 195 ; zadrozny 1987a ) : we bought the boys apples because they were so']",0
['of paragraph units can be found in  #AUTHOR_TAG and  #TAUTHOR_TAG'],['of paragraph units can be found in  #AUTHOR_TAG and  #TAUTHOR_TAG'],"['by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in  #AUTHOR_TAG and  #TAUTHOR_TAG']","['example of psycholinguistically oriented research work can be found in  #AUTHOR_TAG.', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that are used, by readers, to identify a paragraph : ( 1 ) the repetition of content words ( nouns, verbs, adjectives, adverbs ) ; ( 2 ) pronoun reference ; and ( 3 ) paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in  #AUTHOR_TAG and  #TAUTHOR_TAG']",0
['of five paragraphs by  #TAUTHOR_TAG strongly suggests that only'],['of five paragraphs by  #TAUTHOR_TAG strongly suggests that only'],"[': 1 ; furthermore, our reading of the analysis of five paragraphs by  #TAUTHOR_TAG strongly suggests that only']","[', there are at least three arguments against iterating pt.', 'first of all, iteration would increase the complexity of building a model of a paragraph ; infinite iteration would almost certainly make impossible such a construction in real time.', 'secondly, the cooperative principle of  #AUTHOR_TAG grice (, 1978, under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader.', 'finally, it has been shown by  #AUTHOR_TAG that the ratio of derived to explicit information necessary for understanding a piece of text is about 8 : 1 ; furthermore, our reading of the analysis of five paragraphs by  #TAUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.', 'thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh - questions']",4
"['theory and background knowledge ( cfxxx  #TAUTHOR_TAG.', 'brachman']","['theory and background knowledge ( cfxxx  #TAUTHOR_TAG.', 'brachman']","['also distinguishes between an object theory and background knowledge ( cfxxx  #TAUTHOR_TAG.', 'brachman']","['last point may be seen better if we look at some differences between our system and krypton, which also distinguishes between an object theory and background knowledge ( cfxxx  #TAUTHOR_TAG.', 'brachman et al. 1985 ).', ""krypton's a - box, encoding the object theory as a set of assertions, uses standard first order logic ; the t - box contains information expressed in a frame - based language equivalent to a fragment of fol."", ""however, the distinction between the two parts is purely functional - - that is, characterized in terms of the system's behavior."", 'from the logical point of view, the knowledge base is the union of the two boxes, i. e. a theory, and the entailment is standard.', 'in our system, we also distinguish between the "" definitional "" and factual information, but the "" definitional "" part contains collections of mutually excluding theories, not just of formulas describing a semantic network.', 'moreover, in addition to proposing this structure of r, we have described the two mechanisms for exploiting it, "" coherence "" and "" dominance, "" which are not variants of the standard first order entailment, but abduction']",1
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],1
['##n 1987 ;  #TAUTHOR_TAG to see'],['1987 ;  #TAUTHOR_TAG to see'],['##n 1987 ;  #TAUTHOR_TAG to see'],"['text concerns events happening in time.', 'naturally, we will use a logical notation in which formulas may have temporal and event compo - nents.', 'we assume that any formal interpretation of time will agree with the intuitive one.', 'so it is not necessary now to present a formal semantics here.', 'the reader may consult recent papers on this subject ( e. g. moens and steedman 1987 ;  #TAUTHOR_TAG to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ;  #AUTHOR_TAG describes how first order logic can be augmented with such an operator.', 'extending and revising  #AUTHOR_TAG formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram - matical constraint ( that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon - - - ibid. )']",0
"['""', 'according to  #TAUTHOR_TAG, paragraphs']","['', 'according to  #TAUTHOR_TAG, paragraphs']","['""', 'according to  #TAUTHOR_TAG, paragraphs']",[' #TAUTHOR_TAG'],0
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],1
[';  #TAUTHOR_TAG ; patel - schneider'],[';  #TAUTHOR_TAG ; patel - schneider'],"[';  #TAUTHOR_TAG ; patel - schneider 1985 ).', 'levesque 1984 ; frisch']","['in this paper we take modus ponens as the main rule of inference, in general one can consider deductive closures with respect to weaker, nonstandard logics, ( cfxxx levesque 1984 ;  #TAUTHOR_TAG ; patel - schneider 1985 ).', 'levesque 1984 ; frisch 1987 ; patel - schneider 1985 ).', ""but we won't pursue this topic further here""]",1
"[""a is the same as b,'' etc. are not directly represented by logical equality ; similarly, ` ` not'' is often not treated as logical negation ; cfxxx  #TAUTHOR_TAG."", ' #AUTHOR_TAG']","[""as ` ` a is b,'' ` ` a is the same as b,'' etc. are not directly represented by logical equality ; similarly, ` ` not'' is often not treated as logical negation ; cfxxx  #TAUTHOR_TAG."", ' #AUTHOR_TAG']","[""a is the same as b,'' etc. are not directly represented by logical equality ; similarly, ` ` not'' is often not treated as logical negation ; cfxxx  #TAUTHOR_TAG."", ' #AUTHOR_TAG']","['explicitly stated otherwise, we assume that formulas are expressed in a certain ( formal ) language l without equality ; the extension l ( = ) of l is going to be used only in section 5 for dealing with noun phrase references.', ""this means that natural language expressions such as ` ` a is b,'' ` ` a is the same as b,'' etc. are not directly represented by logical equality ; similarly, ` ` not'' is often not treated as logical negation ; cfxxx  #TAUTHOR_TAG."", ' #AUTHOR_TAG']",1
"['all "" in the above definitions ( cf.  #TAUTHOR_TAG b ).', 'we will have, however, no need for']","[': the notions of strong provability and strong r + m - abduction can be in - troduced by replacing "" there exists "" by "" all "" in the above definitions ( cf.  #TAUTHOR_TAG b ).', 'we will have, however, no need for "" strong "" notions in this paper.', 'also, in a practical system,']","['all "" in the above definitions ( cf.  #TAUTHOR_TAG b ).', 'we will have, however, no need for']","[': the notions of strong provability and strong r + m - abduction can be in - troduced by replacing "" there exists "" by "" all "" in the above definitions ( cf.  #TAUTHOR_TAG b ).', 'we will have, however, no need for "" strong "" notions in this paper.', 'also, in a practical system, "" satisfies "" should be probably replaced by "" violates fewest.']",1
"['selectional restrictions ( semantic feature information,  #TAUTHOR_TAG does not solve the problem,']","['selectional restrictions ( semantic feature information,  #TAUTHOR_TAG does not solve the problem,']","['selectional restrictions ( semantic feature information,  #TAUTHOR_TAG does not solve the problem,']","['selectional restrictions ( semantic feature information,  #TAUTHOR_TAG does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ' #AUTHOR_TAG hobbs (, 1982 proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using "" salience "" in choosing facts from this knowledge base']",0
[' #TAUTHOR_TAG ; webber 1987 ) to see'],[' #TAUTHOR_TAG ; webber 1987 ) to see'],['.  #TAUTHOR_TAG ; webber 1987 ) to see'],"['text concerns events happening in time.', 'naturally, we will use a logical notation in which formulas may have temporal and event components.', 'we assume that any formal interpretation of time will agree with the intuitive one.', 'so it is not necessary now to present a formal semantics here.', 'the reader may consult recent papers on this subject ( e. g.  #TAUTHOR_TAG ; webber 1987 ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ;  #AUTHOR_TAG describes how first order logic can be augmented with such an operator.', 'extending and revising  #AUTHOR_TAG formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram - matical constraint ( that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon - - - ibid. )']",0
"['will be chosen as a best interpretation of s, ( cfxxx  #TAUTHOR_TAG a, 1987b ).', 'zadrozny 1987']","['will be chosen as a best interpretation of s, ( cfxxx  #TAUTHOR_TAG a, 1987b ).', 'zadrozny 1987a  #AUTHOR_TAG b.', 'another theory, consisting of']","['r ) interpretation of the words that appear in the sentence.', 'because it is also consistent, it will be chosen as a best interpretation of s, ( cfxxx  #TAUTHOR_TAG a, 1987b ).', 'zadrozny 1987']","['it is the "" highest "" path, fint is the most plausible ( relative to r ) interpretation of the words that appear in the sentence.', 'because it is also consistent, it will be chosen as a best interpretation of s, ( cfxxx  #TAUTHOR_TAG a, 1987b ).', 'zadrozny 1987a  #AUTHOR_TAG b.', 'another theory, consisting of f ~ = { el, sh2, pl, b2 ~ dl } and s, saying that a space vehicle came into the harbor and caused a disease ~ illness is less plausible according to that ordering.', 'as it turns out, f ~ is never constructed in the process of building an interpretation of a paragraph containing the sentence s, unless assuming fint would lead to a contradiction, for instance within the higher level context of a science fiction story']",0
"[',  #TAUTHOR_TAG, hab']","[',  #TAUTHOR_TAG, haberlandt et al.']","['. g. chafe 1979, halliday and hasan 1976,  #TAUTHOR_TAG, hab']","['there are other discussions of the paragraph as a central element of discourse ( e. g. chafe 1979, halliday and hasan 1976,  #TAUTHOR_TAG, haberlandt et al. 1980 ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']",1
"['to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cfxxx  #TAUTHOR_TAG.', 'mycielski']","['to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cfxxx  #TAUTHOR_TAG.', 'mycielski 1981 )']","['in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cfxxx  #TAUTHOR_TAG.', 'mycielski 1981 )']","['logical notions that we are going to consider, such as theory or model, will be finitary.', 'for example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'therefore these notions, and all other constructs we are going to define ( axioms, metarules, definitions etc. ) are computational, although usually we will not provide explicit algorithms for computing them.', 'the issues of control are not so important for us at this point ; we restrict ourselves to describing the logic.', 'this principle of finitism is also assumed by johnson -  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cfxxx  #TAUTHOR_TAG.', 'mycielski 1981 )']",0
"['have shown elsewhere  #TAUTHOR_TAG ; zadrozny 1987a, 1987b ) that natural language programs, such as on - line grammars and dictionaries,']","['have shown elsewhere  #TAUTHOR_TAG ; zadrozny 1987a, 1987b ) that natural language programs, such as on - line grammars and dictionaries,']","['have shown elsewhere  #TAUTHOR_TAG ; zadrozny 1987a, 1987b ) that natural language programs, such as on - line grammars and dictionaries,']","['have shown elsewhere  #TAUTHOR_TAG ; zadrozny 1987a, 1987b ) that natural language programs, such as on - line grammars and dictionaries, can be used as referential levels for commonsense reasoning - - for example, to disambiguate pp attachment.', 'this means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object - level theory']",2
['arguments has been recognized before :  #TAUTHOR_TAG'],['arguments has been recognized before :  #TAUTHOR_TAG'],['arguments has been recognized before :  #TAUTHOR_TAG call'],"['necessity of this kind of merging of arguments has been recognized before :  #TAUTHOR_TAG call it abductive unification / matching,  #AUTHOR_TAG, 1979 ) refers to such operations using the terms knitting or petty conversational implicature']",0
"[' #TAUTHOR_TAG is more sophisticated, and may be']","[' #TAUTHOR_TAG is more sophisticated, and may be']","[' #TAUTHOR_TAG is more sophisticated, and may be considered another possibility.', ' #AUTHOR_TAG formalism is richer']","['', ""but, obviously, there are other possibilities - - for instance, the discourse representation structures ( drs's ) of  #AUTHOR_TAG, which have been used to translate a subset of english into logical formulas, to model text ( identified with a list of sentences ), to analyze a fragment of english, and to deal with anaphora."", 'the logical notation of  #TAUTHOR_TAG is more sophisticated, and may be considered another possibility.', ' #AUTHOR_TAG formalism is richer and resembles more closely an english grammar.', ' #AUTHOR_TAG writes "" it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys. ""', 'therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'it will also be a model for our simplified logical notation ( cf.', 'section 5 ).', 'we can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences']",1
"['johnson -  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and implicitly or explicitly']","['johnson -  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and implicitly or explicitly']","['johnson -  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and implicitly or explicitly']","['logical notions that we are going to consider, such as theory or model, will be finitary.', 'for example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'therefore these notions, and all other constructs we are going to define ( axioms, metarules, definitions etc. ) are computational, although usually we will not provide explicit algorithms for computing them.', 'the issues of control are not so important for us at this point ; we restrict ourselves to describing the logic.', 'this principle of finitism is also assumed by johnson -  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'mycielski 1981 )']",1
['consequent and presupposition  #TAUTHOR_TAG : 112 ) have collected convincing evidence of'],['consequent and presupposition  #TAUTHOR_TAG : 112 ) have collected convincing evidence of'],['consequent and presupposition  #TAUTHOR_TAG : 112 ) have collected convincing evidence of the existence'],"['textualist approach to paragraph analysis is exemplified by e. j. crothers.', 'his work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'he lists, classifies, and discusses various types of inference, by which he means, generally, ` ` the linguistic - logical notions of consequent and presupposition  #TAUTHOR_TAG : 112 ) have collected convincing evidence of the existence of language chunks - - real structures, not just orthographic conventions - - that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature ( like sentences ).', '']",0
"['in logical theories.', 'however, it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfxxx  #TAUTHOR_TAG.', 'woods 1987 )']","['in logical theories.', 'however, it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfxxx  #TAUTHOR_TAG.', 'woods 1987 )']","['in logical theories.', 'however, it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfxxx  #TAUTHOR_TAG.', 'woods 1987 )']","['the word "" up "" is given its meaning relative to our experience with gravity, it is not free to "" slip "" into its opposite.', '"" up "" means up and not down....', 'we have a model that men and women couple to produce offspring who are similar to their parents, and this model is grounded in genetics, and the semantics of kinship metaphor is grounded in this model.', 'mothers have a different role than fathers in this model, and thus there is a reason why "" death is the father of beauty "" fails poetically while "" death is the mother of beauty "" succeeds....', 'it is precisely this "" grounding "" of logical predicates in other conceptual structures that we would like to capture.', 'we investigate here only the "" grounding "" in logical theories.', 'however, it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfxxx  #TAUTHOR_TAG.', 'woods 1987 )']",0
"['english grammar.', ' #TAUTHOR_TAG, p. 14 ) writes ` ` it would be pervers']","['english grammar.', ' #TAUTHOR_TAG, p. 14 ) writes ` ` it would be perverse']","['resembles more closely an english grammar.', ' #TAUTHOR_TAG, p. 14 ) writes ` ` it would be pervers']","['assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'its details are not important for our aim of giving a semantic interpretation of paragraphs ; the main theses of our theory do not depend on a logical notation.', 'so we will use a very simple formalism, like the one above, resembling the standard first order language.', ""but, obviously, there are other possibilities - - for instance, the discourse representation structures ( drs's ) of  #AUTHOR_TAG, which have been used to translate a subset of english into logical formulas, to model text ( identified with a list of sentences ), to analyze a fragment of english, and to deal with anaphora."", 'the logical notation of  #AUTHOR_TAG is more sophisticated, and may be considered another possibility.', ' #AUTHOR_TAG formalism is richer and resembles more closely an english grammar.', "" #TAUTHOR_TAG, p. 14 ) writes ` ` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.''"", 'therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'it will also be a model for our simplified logical notation ( cf.', 'section 5 ).', 'we can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences']",1
"['research work can be found in  #TAUTHOR_TAG.', 'these authors take the position that a paragraph is a psychologically real']","['research work can be found in  #TAUTHOR_TAG.', 'these authors take the position that a paragraph is a psychologically real']","['research work can be found in  #TAUTHOR_TAG.', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that']","['example of psycholinguistically oriented research work can be found in  #TAUTHOR_TAG.', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that are used, by readers, to identify a paragraph : ( 1 ) the repetition of content words ( nouns, verbs, adjectives, adverbs ) ; ( 2 ) pronoun reference ; and ( 3 ) paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholinguistic studies that confirm the validity of paragraph units can be found in  #AUTHOR_TAG and  #AUTHOR_TAG']",0
"['have been interested in the paragraph as a unit.', ' #TAUTHOR_TAG, p']","['have been interested in the paragraph as a unit.', ' #TAUTHOR_TAG, p. 112 ), for example, bemoans the fact that his']","['have been interested in the paragraph as a unit.', ' #TAUTHOR_TAG, p']","['demonstrates that information needed to identify and resolve anaphoric references can be found, to an interesting extent, in standard dictionaries and thesauri.', '( other reference works could be treated as additional sources of world knowledge. )', 'this type of consultation uses existing natural language texts as a referential level for processing purposes.', 'it is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit.', "" #TAUTHOR_TAG, p. 112 ), for example, bemoans the fact that his ` ` theory lacks a world knowledge component, a mental ` encyclopedia,'which could be invoked to generate inferences...''."", 'with respect to that independent source of knowledge, our main contributions are two.', 'first, we identify its possible structure ( a collection of partially ordered theories ) and make formal the choice of a most plausible interpretation.', 'in other words, we recognize it as a separate logical level - - the referential level.', 'second, we suggest that natural language reference works, like dictionaries and thesauri, can quite often fill the role of the referential level']",0
"['##ture ""  #TAUTHOR_TAG, or the']","['by using only "" petty conversational implicature ""  #TAUTHOR_TAG, or the']","['by using only "" petty conversational implicature ""  #TAUTHOR_TAG, or']","['. 1. 1 was the use of a gricean maxim necessary?', 'can one deal effectivelywith the problem of reference without axiomatized gricean maxims, for instance by using only "" petty conversational implicature ""  #TAUTHOR_TAG, or the metarules of section 5. 2?', 'it seems to us that the answer is no']",0
"['of paragraphs.', 'this semantics was constructed  #TAUTHOR_TAG a, 1987b ) as a formal']","['of paragraphs.', 'this semantics was constructed  #TAUTHOR_TAG a, 1987b ) as a formal']","['the analysis of paragraphs.', 'this semantics was constructed  #TAUTHOR_TAG a, 1987b ) as a formal framework for default and commonsense reasoning.', 'it should not']","['adopt the three - level semantics as a formal tool for the analysis of paragraphs.', 'this semantics was constructed  #TAUTHOR_TAG a, 1987b ) as a formal framework for default and commonsense reasoning.', 'it should not come as a surprise that we can now use this apparatus for text / discourse analysis ; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.', 'for instance, relating "" they "" to "" apples "" in the sentence ( cf.', 'haugeland 1985 p. 195 ; zadrozny 1987a )']",5
"[' #TAUTHOR_TAG.', 'but, obviously, trying to establish precise connections']","[' #TAUTHOR_TAG.', 'but, obviously, trying to establish precise connections']","[' #TAUTHOR_TAG.', 'but, obviously, trying to establish precise connections']",[' #TAUTHOR_TAG'],1
"[',  #TAUTHOR_TAG,']","[',  #TAUTHOR_TAG,']","['. g. chafe 1979, halliday and hasan 1976, longacre 1979,  #TAUTHOR_TAG,']","['there are other discussions of the paragraph as a central element of discourse ( e. g. chafe 1979, halliday and hasan 1976, longacre 1979,  #TAUTHOR_TAG, all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']",1
"['##7, 1978 ;  #TAUTHOR_TAG or quantifier scoping ( webber']","['role of focus ( grosz 1977, 1978 ;  #TAUTHOR_TAG or quantifier scoping ( webber']","['the role of focus ( grosz 1977, 1978 ;  #TAUTHOR_TAG or quantifier scoping ( webber 1983 ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( grosz 1977, 1978 ;  #TAUTHOR_TAG or quantifier scoping ( webber 1983 ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']",0
"['referential structures we are going to use are collections of logical theories, but the concept of reference is more general.', 'some of the intuitions we associate with this notion have been very well expressed by  #TAUTHOR_TAG, pp. 7 - 8 ) :... semantics is constrained by our models of ourselves and our worlds.', 'we have models of up and down that are based by the way our bodies actually function']","['referential structures we are going to use are collections of logical theories, but the concept of reference is more general.', 'some of the intuitions we associate with this notion have been very well expressed by  #TAUTHOR_TAG, pp. 7 - 8 ) :... semantics is constrained by our models of ourselves and our worlds.', 'we have models of up and down that are based by the way our bodies actually function']","['referential structures we are going to use are collections of logical theories, but the concept of reference is more general.', 'some of the intuitions we associate with this notion have been very well expressed by  #TAUTHOR_TAG, pp. 7 - 8 ) :... semantics is constrained by our models of ourselves and our worlds.', 'we have models of up and down that are based by the way our bodies actually function']","['referential structures we are going to use are collections of logical theories, but the concept of reference is more general.', 'some of the intuitions we associate with this notion have been very well expressed by  #TAUTHOR_TAG, pp. 7 - 8 ) :... semantics is constrained by our models of ourselves and our worlds.', 'we have models of up and down that are based by the way our bodies actually function']",0
"['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing ( hirst 1987 ;  #TAUTHOR_TAG, with one difference : the activation spreads to theories that share a predicate, not through the""]","['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing ( hirst 1987 ;  #TAUTHOR_TAG, with one difference : the activation spreads to theories that share a predicate, not through the""]","['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing ( hirst 1987 ;  #TAUTHOR_TAG, with one difference : the activation spreads to theories that share a predicate, not through""]","['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing ( hirst 1987 ;  #TAUTHOR_TAG, with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text""]",1
"['to weaker, nonstandard logics, ( cfxxx  #TAUTHOR_TAG ; frisch']","['to weaker, nonstandard logics, ( cfxxx  #TAUTHOR_TAG ; frisch']","['to weaker, nonstandard logics, ( cfxxx  #TAUTHOR_TAG ; frisch 1987 ; patel - schneider 1985 ).', 'levesque 1984 ; frisch 1987 ; patel - schneider 1985 ).', ""but we won't pursue this topic further here""]","['in this paper we take modus ponens as the main rule of inference, in general one can consider deductive closures with respect to weaker, nonstandard logics, ( cfxxx  #TAUTHOR_TAG ; frisch 1987 ; patel - schneider 1985 ).', 'levesque 1984 ; frisch 1987 ; patel - schneider 1985 ).', ""but we won't pursue this topic further here""]",1
"['of focus  #TAUTHOR_TAG, 1978 ; sidner']","['of focus  #TAUTHOR_TAG, 1978 ; sidner']","['of focus  #TAUTHOR_TAG, 1978 ; sidner 1983 ) or quantifier scoping ( webber 1983 ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus  #TAUTHOR_TAG, 1978 ; sidner 1983 ) or quantifier scoping ( webber 1983 ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']",0
"['of reference in english prose  #TAUTHOR_TAG, p. 329 )']","['of reference in english prose  #TAUTHOR_TAG, p. 329 )']","['is always the more typical direction of reference in english prose  #TAUTHOR_TAG, p. 329 )']","[': in our translation from english to logic we are assuming that "" it "" is anaphoric ( with the pronoun following the element that it refers to ), not cataphoric ( the other way around ).', 'this means that the "" it "" that brought the disease in p1 will not be considered to refer to the infection "" i "" or the death "" d "" in p3.', 'this strategy is certainly the right one to start out with, since anaphora is always the more typical direction of reference in english prose  #TAUTHOR_TAG, p. 329 )']",4
"['. g.', ' #TAUTHOR_TAG ; the number of unlikely parses is severely reduced']","['is taken in computational syntactic grammars ( e. g.', ' #TAUTHOR_TAG ; the number of unlikely parses is severely reduced']","['. g.', ' #TAUTHOR_TAG ; the number of unlikely parses is severely reduced']","['can also hope for some fine - tuning of the notion of topic, which would prevent many offensive examples.', 'this approach is taken in computational syntactic grammars ( e. g.', ' #TAUTHOR_TAG ; the number of unlikely parses is severely reduced whenever possible, but no attempt is made to define only the so - called grammatical strings of a language']",0
"['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing  #TAUTHOR_TAG ; charniak 1983 ), with one difference : the activation spreads to theories that share a predicate, not through the""]","['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing  #TAUTHOR_TAG ; charniak 1983 ), with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text""]","['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing  #TAUTHOR_TAG ; charniak 1983 ), with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text""]","['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing  #TAUTHOR_TAG ; charniak 1983 ), with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text""]",1
"['.', 'we shall see this in the next example : two sentences, regarded as a fragment of paragraph, are a variation on a theme by  #TAUTHOR_TAG']","['writer did not say too much. ""', 'rather, we stress the possibility that one can axiomatize and productively use such a rule.', 'we shall see this in the next example : two sentences, regarded as a fragment of paragraph, are a variation on a theme by  #TAUTHOR_TAG']","['do not claim that gla is the best or unique way of expressing the rule "" assume that the writer did not say too much. ""', 'rather, we stress the possibility that one can axiomatize and productively use such a rule.', 'we shall see this in the next example : two sentences, regarded as a fragment of paragraph, are a variation on a theme by  #TAUTHOR_TAG']","['do not claim that gla is the best or unique way of expressing the rule "" assume that the writer did not say too much. ""', 'rather, we stress the possibility that one can axiomatize and productively use such a rule.', 'we shall see this in the next example : two sentences, regarded as a fragment of paragraph, are a variation on a theme by  #TAUTHOR_TAG']",2
['by  #TAUTHOR_TAG ; cfxxx also quirk'],['by  #TAUTHOR_TAG ; cfxxx also quirk'],['by  #TAUTHOR_TAG ; cfxxx also quirk'],"['##s are function words - - like conjunctions and some adverbs - - that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '"" and, "" "" or, "" and "" but "" are the three main coordinating connectives in english.', 'however, "" but "" does not behave quite like the other two - - semantically, "" but "" signals a contradiction, and in this role it seems to have three subfunctions :..', 'opposition ( called "" adversative "" or "" contrary - to - expectation "" by  #TAUTHOR_TAG ; cfxxx also quirk et al. 1972, p. 672 )']",0
"['.', ' #TAUTHOR_TAG, hall']","['', ' #TAUTHOR_TAG, halliday and hasan']","['.', ' #TAUTHOR_TAG, hall']","['there are other discussions of the paragraph as a central element of discourse ( e. g.', ' #TAUTHOR_TAG, halliday and hasan 1976, longacre 1979, haberlandt et al. 1980 ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']",1
"['johnson -  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and implicitly or explicitly']","['johnson -  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and implicitly or explicitly']","['johnson -  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and implicitly or explicitly']","['logical notions that we are going to consider, such as theory or model, will be finitary.', 'for example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'therefore these notions, and all other constructs we are going to define ( axioms, metarules, definitions etc. ) are computational, although usually we will not provide explicit algorithms for computing them.', 'the issues of control are not so important for us at this point ; we restrict ourselves to describing the logic.', 'this principle of finitism is also assumed by johnson -  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'mycielski 1981 )']",1
['of paragraph units can be found in  #TAUTHOR_TAG and  #AUTHOR_TAG'],['of paragraph units can be found in  #TAUTHOR_TAG and  #AUTHOR_TAG'],"['by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in  #TAUTHOR_TAG and  #AUTHOR_TAG']","['example of psycholinguistically oriented research work can be found in  #AUTHOR_TAG.', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that are used, by readers, to identify a paragraph : ( 1 ) the repetition of content words ( nouns, verbs, adjectives, adverbs ) ; ( 2 ) pronoun reference ; and ( 3 ) paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in  #TAUTHOR_TAG and  #AUTHOR_TAG']",0
"['have shown elsewhere ( jensen and binot 1988 ;  #TAUTHOR_TAG a, 1987b ) that natural language programs, such as on - line grammars and dictionaries,']","['have shown elsewhere ( jensen and binot 1988 ;  #TAUTHOR_TAG a, 1987b ) that natural language programs, such as on - line grammars and dictionaries,']","['have shown elsewhere ( jensen and binot 1988 ;  #TAUTHOR_TAG a, 1987b ) that natural language programs, such as on - line grammars and dictionaries, can be used as referential levels']","['have shown elsewhere ( jensen and binot 1988 ;  #TAUTHOR_TAG a, 1987b ) that natural language programs, such as on - line grammars and dictionaries, can be used as referential levels for commonsense reasoning - - for example, to disambiguate pp attachment.', 'this means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object - level theory']",2
"['that the iteration of the pt operation to form a closure is needed ( cfxxx  #TAUTHOR_TAG b ).', 'zadrozny 1987b )']","['that the iteration of the pt operation to form a closure is needed ( cfxxx  #TAUTHOR_TAG b ).', 'zadrozny 1987b )']","['important information about a formula.', 'this immediate information may be insufficient to decide the truth of certain predicates.', 'it would seem therefore that the iteration of the pt operation to form a closure is needed ( cfxxx  #TAUTHOR_TAG b ).', 'zadrozny 1987b )']","['an interpretation of a paragraph does not mean finding all of its possible meanings ; the implausible ones should not be computed at all.', 'this viewpoint has been reflected in the definition of a partial theory as a most plausible interpretation of a sequence of predicates.', 'now we want to restrict the notion of a partial theory by introducing the formal notions of topic and coherence.', 'we can then later ( section 5. 2 ) define p - models - - a category of models corresponding to paragraphs - - as models of coherent theories that satisfy all metalevel conditions.', 'the partial theories pick up from the referential level the most obvious or the most important information about a formula.', 'this immediate information may be insufficient to decide the truth of certain predicates.', 'it would seem therefore that the iteration of the pt operation to form a closure is needed ( cfxxx  #TAUTHOR_TAG b ).', 'zadrozny 1987b )']",1
"['.', 'later,  #TAUTHOR_TAG, 1982 ) proposed a']","['isolated features offer only part of the background knowledge necessary for reference disambiguation.', 'later,  #TAUTHOR_TAG, 1982 ) proposed a']","['isolated features offer only part of the background knowledge necessary for reference disambiguation.', 'later,  #TAUTHOR_TAG, 1982 ) proposed']","['selectional restrictions ( semantic feature information, hobbs 1977 ) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""later,  #TAUTHOR_TAG, 1982 ) proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using ` ` salience'' in choosing facts from this knowledge base""]",0
"['matching,  #TAUTHOR_TAG, 1979 ) refers to such operations using the terms knitting or petty conversational impl']","['unification / matching,  #TAUTHOR_TAG, 1979 ) refers to such operations using the terms knitting or petty conversational implicature']","[',  #TAUTHOR_TAG, 1979 ) refers to such operations using the terms knitting or petty conversational implicature']","['necessity of this kind of merging of arguments has been recognized before : charniak and mc  #AUTHOR_TAG call it abductive unification / matching,  #TAUTHOR_TAG, 1979 ) refers to such operations using the terms knitting or petty conversational implicature']",0
['quotation operator ;  #TAUTHOR_TAG describes how first order logic'],['quotation operator ;  #TAUTHOR_TAG describes how first order logic'],['a quotation operator ;  #TAUTHOR_TAG describes how first order logic'],"['', 'the text concerns events happening in time.', 'naturally, we will use a logical notation in which formulas may have temporal and event components.', 'we assume that any formal interpretation of time will agree with the intuitive one.', 'so it is not necessary now to present a formal semantics here.', 'the reader may consult recent papers on this subject ( e. g.', 'moens and steedman 1987 ; webber 1987 ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ;  #TAUTHOR_TAG describes how first order logic can be augmented with such an operator.', 'extending and revising  #AUTHOR_TAG formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint ( "" that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon "" - - - ibid. ).', 'however, as noted before, we will use a simplified version of such a logical notation ; we will have only time, event, result, and property as primitives.', 'after these remarks we can begin constructing the model of the example paragraph.', 'we assume that constants are introduced by nps.', 'we have then ( i ) constants s, m, d, i, b, 1347 satisfying : ship ( s ), messina ( m ), disease ( d ), infection ( i ), death ( b ), year ( 1347 )']",0
"[',  #TAUTHOR_TAG, p. 8 ) says that the']","[',  #TAUTHOR_TAG, p. 8 ) says that the']","[',  #TAUTHOR_TAG, p. 8 ) says that the sentence']",[' #TAUTHOR_TAG'],4
"['', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG']","['proper response generation.', 'current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG']","[', in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG']","['the past few years, there has been a gradual paradigm shift in speech recognition research both in the u. s. and in europe.', 'in addition to continued research on the transcription problem, i. e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 this shift is at least partly brought on by the realization that many of the applications involving human / machine interface using speech require an "" understanding "" of the intended message.', 'in fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG']",0
['formula for the test set perplexity  #TAUTHOR_TAG is :'],['formula for the test set perplexity  #TAUTHOR_TAG is :'],['formula for the test set perplexity  #TAUTHOR_TAG is :'],['formula for the test set perplexity  #TAUTHOR_TAG is :'],0
"['- end  #TAUTHOR_TAG.', 'the']","['and with a functioning database back - end  #TAUTHOR_TAG.', 'the']","['- end  #TAUTHOR_TAG.', '']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( lamel et al. 1986 ).', 'the second version ( rm ) concerns the resource management task ( pallett 1989 ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end  #TAUTHOR_TAG.', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']",5
['automatically by the computer  #TAUTHOR_TAG'],['automatically by the computer  #TAUTHOR_TAG'],['response generation components was done automatically by the computer  #TAUTHOR_TAG'],"['obtain training and test data for this task, we had a number of naive subjects use the system as if they were trying to obtain actual information.', 'their speech was recorded in a simulation mode in which the speech recognition component was excluded.', 'instead, an experimenter in a separate room typed in the utterances as spoken by the subject.', 'subsequent processing by the natural language and response generation components was done automatically by the computer  #TAUTHOR_TAG']",5
[''],[''],[''],[''],0
"['by  #AUTHOR_TAG and  #TAUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass']","['by  #AUTHOR_TAG and  #TAUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass']","['by  #AUTHOR_TAG and  #TAUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass']","['far, we have added all semantic filters by hand, and they are implemented in a hard - fail mode, i. e., if the semantic restrictions fail, the node dies.', 'this strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains.', 'in principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.', 'then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals.', 'this approach resembles the work by  #AUTHOR_TAG and  #TAUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', 'there is obviously a great deal more work to be done in this important area']",1
"['a number of phonological rules.', 'the search algorithm is the standard viterbi search  #TAUTHOR_TAG, except that the match involves a network - to - network alignment problem rather than sequence - to - sequence']","['a number of phonological rules.', 'the search algorithm is the standard viterbi search  #TAUTHOR_TAG, except that the match involves a network - to - network alignment problem rather than sequence - to - sequence']","['a number of phonological rules.', 'the search algorithm is the standard viterbi search  #TAUTHOR_TAG, except that the match involves a network - to - network alignment problem rather than sequence - to - sequence']","['', 'this aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'the recognizer for these systems is the summit system ( zue et al. 1989 ), which uses a segmental - based framework and includes an auditory model in the front - end processing.', 'the lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'the search algorithm is the standard viterbi search  #TAUTHOR_TAG, except that the match involves a network - to - network alignment problem rather than sequence - to - sequence']",5
[' #TAUTHOR_TAG showing up as complements'],[' #TAUTHOR_TAG showing up as complements'],"['the same case frame  #TAUTHOR_TAG showing up as complements.', 'for instance, the set']","['filters can also be used to prevent multiple versions of the same case frame  #TAUTHOR_TAG showing up as complements.', 'for instance, the set of complements [ from - place ], [ to - place ], and [ at - time ] are freely ordered following a movement verb such as "" leave. ""', '']",5
"['', 'representative systems are described in  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['sundial project.', 'representative systems are described in  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['participants of the esprit sundial project.', 'representative systems are described in  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['the past few years, there has been a gradual paradigm shift in speech recognition research both in the u. s. and in europe.', 'in addition to continued research on the transcription problem, i. e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 this shift is at least partly brought on by the realization that many of the applications involving human / machine interface using speech require an "" understanding "" of the intended message.', 'in fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']",0
"[' #TAUTHOR_TAG.', 'ultimately']","[' #TAUTHOR_TAG.', 'ultimately']","['- best outputs, giving a significant improvement in performance  #TAUTHOR_TAG.', 'ultimately']","['', ""we have not yet made use of tina's probabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort n - best outputs, giving a significant improvement in performance  #TAUTHOR_TAG."", ""ultimately we want to incorporate tina's probabilities directly into the a * search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model""]",5
"['( timit ) was developed for the 450 phonetically rich sentences of the timit database  #TAUTHOR_TAG.', '']","['( timit ) was developed for the 450 phonetically rich sentences of the timit database  #TAUTHOR_TAG.', '']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database  #TAUTHOR_TAG.', '']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database  #TAUTHOR_TAG.', 'the second version ( rm ) concerns the resource management task ( pallett 1989 ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( zue et al. 1990 ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']",5
['atis domain  #TAUTHOR_TAG represents our most promising approach to this problem'],['atis domain  #TAUTHOR_TAG represents our most promising approach to this problem'],"['', 'however, the method we are currently using in the atis domain  #TAUTHOR_TAG represents our most promising approach to this problem.', 'we have decided to limit semantic frame types to a small set of choices']","['how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'however, the method we are currently using in the atis domain  #TAUTHOR_TAG represents our most promising approach to this problem.', '']",3
"[', atis  #TAUTHOR_TAG et al']","['vicinity of mit and harvard university.', 'the second one, atis  #TAUTHOR_TAG et al. 1991 ), is']","['our case, the vicinity of mit and harvard university.', 'the second one, atis  #TAUTHOR_TAG et al']","['currently have two application domains that can carry on a spoken dialog with a user.', 'one, the voyager domain ( zue et al. 1990 ), answers questions about places of interest in an urban area, in our case, the vicinity of mit and harvard university.', 'the second one, atis  #TAUTHOR_TAG et al. 1991 ), is a system for accessing data in the official 80 stephanie seneff tina : a natural language system for spoken language applications airline guide and booking flights.', 'work continues on improving all aspects of these domains. our current research is directed at a number of different remaining issues']",5
"['by  #TAUTHOR_TAG and  #AUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass']","['by  #TAUTHOR_TAG and  #AUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass']","['on whether to accept the proposals.', 'this approach resembles the work by  #TAUTHOR_TAG and  #AUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass']","['far, we have added all semantic filters by hand, and they are implemented in a hard - fail mode, i. e., if the semantic restrictions fail, the node dies.', 'this strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains.', 'in principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.', 'then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals.', 'this approach resembles the work by  #TAUTHOR_TAG and  #AUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', 'there is obviously a great deal more work to be done in this important area']",1
"[', an n - gram back - off model  #TAUTHOR_TAG']","[', an n - gram back - off model  #TAUTHOR_TAG']","[', an n - gram back - off model  #TAUTHOR_TAG']","['appendix walks through a pedagogical example to parse spoken digit sequences up to three long, as in "" three hundred and sixteen. ""', 'included is a set of initial contextfree rules, a set of training sentences, an illustration of how to compute the path probabilities from the training sentences, and an illustration of both parsing and perplexity computation for a test sentence.', 'since there are only five training sentences, a number of the arcs of the original grammar are lost after training.', 'this is a problem to be aware of in building grammars from example sentences.', 'in the absence of a sufficient amount of training data, some arcs will inevitably be zeroed out.', 'unless it is desired to intentionally filter these out as being outside of the new domain, one can insert some arbitrarily small probability for these arcs, using, for example, an n - gram back - off model  #TAUTHOR_TAG']",0
[' #TAUTHOR_TAG by its generator'],"['[ end ] node is encountered.', 'to a first approximation, a current - focus reaches only nodes that are c - commanded  #TAUTHOR_TAG by its generator']","['', 'to a first approximation, a current - focus reaches only nodes that are c - commanded  #TAUTHOR_TAG by its generator.', '']","['', 'that is to say, the current - focus is a feature, like verb - mode, that is blocked when an [ end ] node is encountered.', 'to a first approximation, a current - focus reaches only nodes that are c - commanded  #TAUTHOR_TAG by its generator.', 'finally, certain blocker nodes block the transfer of the float - object to their children']",0
"['voyager domain  #TAUTHOR_TAG, answers questions']","['voyager domain  #TAUTHOR_TAG, answers questions']","[', the voyager domain  #TAUTHOR_TAG, answers questions']","['_ currently have two application domains that can carry on a spoken dialog with a user.', 'one, the voyager domain  #TAUTHOR_TAG, answers questions about places of interest in an urban area, in our case, the vicinity of mit and harvard university.', 'the second one, atis ( seneff et al. 1991 ), is a system for accessing data in the official airline guide and booking flights.', 'work continues on improving all aspects of these domains']",5
"['##ns  #TAUTHOR_TAG,']","['atns  #TAUTHOR_TAG,']","['##ns  #TAUTHOR_TAG,']","[""example used to illustrate the power of atns  #TAUTHOR_TAG, ` ` john was believed to have been shot,'' also parses correctly, because the [ object ] node following the verb ` ` believed'' acts as both an absorber and a ( re ) generator."", '']",1
"['', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['proper response generation.', 'current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","[', in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['the past few years, there has been a gradual paradigm shift in speech recognition research both in the u. s. and in europe.', 'in addition to continued research on the transcription problem, i. e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 this shift is at least partly brought on by the realization that many of the applications involving human / machine interface using speech require an "" understanding "" of the intended message.', 'in fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']",0
['resource management task  #TAUTHOR_TAG that has been popular within'],['resource management task  #TAUTHOR_TAG that has been popular within'],['( rm ) concerns the resource management task  #TAUTHOR_TAG that has been popular within'],"['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( lamel et al. 1986 ).', 'the second version ( rm ) concerns the resource management task  #TAUTHOR_TAG that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( zue et al. 1990 ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']",5
"['the a * algorithm  #TAUTHOR_TAG as applied to speech recognition, the actual path score is']","['the a * algorithm  #TAUTHOR_TAG as applied to speech recognition, the actual path score is']","['the a * algorithm  #TAUTHOR_TAG as applied to speech recognition, the actual path score is typically augmented with an estimated score for the unseen portion.', 'unless some kind of normalization is done, the short theories have an unfair advantage, simply']","['modification of this scheme is necessary when the input stream is not deterministic.', 'for the a * algorithm  #TAUTHOR_TAG as applied to speech recognition, the actual path score is typically augmented with an estimated score for the unseen portion.', 'unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied.', 'with a deterministic word sequence it seems reasonable to assume probability 1. 0 for what has been found']",5
"['summit system  #TAUTHOR_TAG, which uses a segmental - based']","['summit system  #TAUTHOR_TAG, which uses a segmental - based']","['will not be covered in detail.', 'the recognizer for these systems is the summit system  #TAUTHOR_TAG, which uses a segmental - based framework']","['', 'this aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'the recognizer for these systems is the summit system  #TAUTHOR_TAG, which uses a segmental - based framework and includes an auditory model in the front - end processing.', 'the lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'the search algorithm is the standard viterbi search ( viterbi 1967 ), except that the match involves a network - to - network alignment problem rather than sequence - to - sequence']",5
['##ns  #TAUTHOR_TAG and'],['atns  #TAUTHOR_TAG and'],"['##ns  #TAUTHOR_TAG and the treatment of bounded domination metavariables in lexical functional grammars ( lfgs ) ( bresnan 1982, p. 235 ff. ), but it is different from these in that the process of filling the hold register equivalent involves two steps separately initiated by two independent nodes']","['section describes how tina handles several issues that are often considered to be part of the task of a parser.', 'these include agreement constraints, semantic restrictions, subject - tagging for verbs, and long distance movement ( often referred to as gaps, or the trace, as in "" ( which article ) / do you think i should read ( ti )? "" ) ( chomsky 1977 ).', 'the gap mechanism resembles the hold register idea of atns  #TAUTHOR_TAG and the treatment of bounded domination metavariables in lexical functional grammars ( lfgs ) ( bresnan 1982, p. 235 ff. ), but it is different from these in that the process of filling the hold register equivalent involves two steps separately initiated by two independent nodes']",1
"['', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG']","['proper response generation.', 'current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG']","[', in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG']","['the past few years, there has been a gradual paradigm shift in speech recognition research both in the u. s. and in europe.', 'in addition to continued research on the transcription problem, i. e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 this shift is at least partly brought on by the realization that many of the applications involving human / machine interface using speech require an "" understanding "" of the intended message.', 'in fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['we first integrated this recognizer with tina, we used a "" wire "" connection, in that the recognizer produced a single best output, which was then passed to tina for parsing.', 'a simple word - pair grammar constrained the search space.', 'if the parse failed, then the sentence was rejected.', ""we have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse  #TAUTHOR_TAG to produce these ` ` n - best'' alternatives, we make use of a standard a * search algorithm ( hart 1968, jelinek 1976 )."", 'both the a * and the viterbi search are left - to - right search algorithms.', 'however, the a * search is contrasted with the viterbi search in that the set of active hypotheses take up unequal segments of time.', 'that is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold']",5
"['system the parser acts as a filter only on completed candidate solutions  #TAUTHOR_TAG, the tightly coupled system allows the']","['system the parser acts as a filter only on completed candidate solutions  #TAUTHOR_TAG, the tightly coupled system allows the']","['in the loosely coupled system the parser acts as a filter only on completed candidate solutions  #TAUTHOR_TAG, the tightly coupled system allows the parser']","['in the loosely coupled system the parser acts as a filter only on completed candidate solutions  #TAUTHOR_TAG, the tightly coupled system allows the parser to discard partial theories that have no way of continuing.', 'following the viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""we have not yet made use of tina'sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort n - best outputs, giving a significant improvement in performance ( goodine et al. 1991 )."", ""ultimately we want to incorporate tina'sprobabilities directly into the a * search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model""]",5
['##rs provided by  #TAUTHOR_TAG defines a formal lexical'],['dlrs provided by  #TAUTHOR_TAG defines a formal lexical'],['##rs provided by  #TAUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : a rewrite system'],"['thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'the formalization of dlrs provided by  #TAUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : a rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in figure 1.', 'this description can then be given the standard set - theoretical interpretation of  #AUTHOR_TAG, 1994 )']",0
['to  #TAUTHOR_TAG for a more detailed discussion of our use of constraint propagation'],['to  #TAUTHOR_TAG for a more detailed discussion of our use of constraint propagation'],"['more specific.', 'this technique closely resembles the off - line constraint propagation technique described by  #AUTHOR_TAG.', 'the reader is referred to  #TAUTHOR_TAG for a more detailed discussion of our use of constraint propagation']","['', 'when c is the common information, and d1,..., dk are the definitions of the interaction predicate called, we use distributivity to factor out c in ( c a d1 ) v -.. v ( c a dk ) : we compute c a ( d1 v... v dk ), where the r ) are assumed to contain no further common factors.', 'once we have computed c, we use it to make the extended lexical entry more specific.', 'this technique closely resembles the off - line constraint propagation technique described by  #AUTHOR_TAG.', 'the reader is referred to  #TAUTHOR_TAG for a more detailed discussion of our use of constraint propagation']",0
"['##lexical rules  #TAUTHOR_TAG, 31 ).', 'a similar method is included in']","['and the parser makes no distinction between lexical and nonlexical rules  #TAUTHOR_TAG, 31 ).', 'a similar method is included in']","['##lexical rules  #TAUTHOR_TAG, 31 ).', 'a similar method is included in']","['common approach to lexical rules is to encode them as unary phrase structure rules.', 'this approach is taken, for example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules  #TAUTHOR_TAG, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the cuf system ( dbrre and eisele 1991 ; d6rre and dorna 1993b ) or the tfs system ( emele and zajac 1990 ; emele 1994 ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']",1
"['19.', '27  #TAUTHOR_TAG argue that semi - productivity of lexical rules, which can be understood as a']","['19.', '27  #TAUTHOR_TAG argue that semi - productivity of lexical rules, which can be understood as a']","['way these predicates interconnect is represented in figure 19.', '27  #TAUTHOR_TAG argue that semi - productivity of lexical rules, which can be understood as a generalization of exceptions to lexical rules, can be integrated with our']","['way these predicates interconnect is represented in figure 19.', '27  #TAUTHOR_TAG argue that semi - productivity of lexical rules, which can be understood as a generalization of exceptions to lexical rules, can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry.', '28 in order to distinguish the different interaction predicates for the different classes of lexical entries, the compiler indexes the names of the interaction predicates.', 'since for expository reasons we will only discuss one kind of lexical entry in this paper, we will not show those indices in the examples given']",0
[' #TAUTHOR_TAG to'],[' #TAUTHOR_TAG to'],['sag 1994 ) or the complement extraction lexical rule  #TAUTHOR_TAG to'],"['the authors are listed alphabetically.', 'sfb 340, kleine wilhelmstr.', '113, d - 72074 tiibingen, germany.', 'email : { dm, minnen } @ sfs. nphil. uni - tuebingen. de', 'url : http : / / www. sfs. nphil. uni - tuebingen. de / sfb', '/ b4home. html 1 this is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( hinrichs and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement extraction lexical rule  #TAUTHOR_TAG to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']",0
"['lkb system  #TAUTHOR_TAG.', 'both the input and output of a lexical']","['lkb system  #TAUTHOR_TAG.', 'both the input and output of a lexical rule,']","[', adopted in the lkb system  #TAUTHOR_TAG.', 'both the input and output of a lexical rule,']","['', 'this conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as, for example, adopted in the lkb system  #TAUTHOR_TAG.', 'both the input and output of a lexical rule, i. e., the mother and the daughter of a phrase structure rule, are available during a generation or parsing process.', 'as a result, in addition to the information present in the lexical entry, syntactic information can be accessed to execute the constraints on the input of a lexical rule.', 'the computational treatment of lexical rules that we propose in this paper is essentially a domain - specific refinement of such an approach to lexical rules.', '']",0
"[' #TAUTHOR_TAG.', 'given a']","[' #TAUTHOR_TAG.', 'given a']","['of clauses with a finitely failed body  #TAUTHOR_TAG.', 'given']","['', 'we thereby eliminate the redundant nondeterminism resulting from multiply defined frame predicates.', 'the elimination of redundant nondeterminism is based on unfold / fold transformation techniques ( tamaki and sato 1984 ).', '29 the unfolding transformation is also referred to as partial execution, for example, by  #AUTHOR_TAG.', 'intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile - time.', 'as a result, the literal can be removed from the body of 29 this improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body  #TAUTHOR_TAG.', '']",1
"[';  #TAUTHOR_TAG can be used to circumvent constraint propagation.', 'encoding the disjunc']","[';  #TAUTHOR_TAG can be used to circumvent constraint propagation.', 'encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments,']","[';  #TAUTHOR_TAG can be used to circumvent constraint propagation.', 'encoding the disjunctive possibilities for lexical rule application in this']","['in certain cases an extension of the constraint language with named disjunctions or contexted constraints ( maxwell and kaplan 1989 ; eisele and dorre 1990 ;  #TAUTHOR_TAG can be used to circumvent constraint propagation.', 'encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments, makes all relevant lexical information available at lexical lookup.', 'for analyses proposing infinite lexica, though, a definite clause encoding of disjunctive possibilities is still necessary and constraint propagation is indispensable for efficient processing']",0
"['specification language introduced in  #TAUTHOR_TAG.', 'the reader interested in that']","['specification language introduced in  #TAUTHOR_TAG.', 'the reader interested in that']","['into a discussion of the full lexical rule specification language introduced in  #TAUTHOR_TAG.', 'the reader interested in that language and']","['translation of the lexical rule into a predicate is trivial.', 'the result is displayed description language.', '12 in order to focus on the computational aspects of the covariation approach, in this paper we will not go into a discussion of the full lexical rule specification language introduced in  #TAUTHOR_TAG.', 'the reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 a more detailed presentation can be found in minnen ( in preparation ).', '14 we use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15  #AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention.', 'we here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form ( g6tz 1994 ).', 'computationally, a subsumption test could equally well be used in our compiler']",0
"[' #TAUTHOR_TAG, 10 ) is a linguistic example.', 'the in - specification of this lexical']","[' #TAUTHOR_TAG, 10 ) is a linguistic example.', 'the in - specification of this lexical']","[' #TAUTHOR_TAG, 10 ) is a linguistic example.', 'the in - specification of this lexical rule']","['terminology used in the literature varies.', 'types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'to avoid confusion, we will only use the terminology introduced in the text.', '4 this interpretation of the signature is sometimes referred to as closed world ( gerdemann and king 1994 ; gerdemann 1995 ).', '5 an in - depth discussion including a comparison of both approaches is provided in calcagno, meurers, and pollard ( in preparation ).', '6 the partial - vp topicalization lexical rule proposed by  #TAUTHOR_TAG, 10 ) is a linguistic example.', 'the in - specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'in the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement']",0
"['##demann and king 1994 ;  #TAUTHOR_TAG.', '5 an in - depth discussion including a comparison of both approaches is provided in calcagn']","['to as closed world ( gerdemann and king 1994 ;  #TAUTHOR_TAG.', '5 an in - depth discussion including a comparison of both approaches is provided in calcagno,']","['##demann and king 1994 ;  #TAUTHOR_TAG.', '5 an in - depth discussion including a comparison of both approaches is provided in calcagn']","['terminology used in the literature varies.', 'types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'to avoid confusion, we will only use the terminology introduced in the text.', '4 this interpretation of the signature is sometimes referred to as closed world ( gerdemann and king 1994 ;  #TAUTHOR_TAG.', '5 an in - depth discussion including a comparison of both approaches is provided in calcagno, meurers, and pollard ( in preparation ).', '6 the partial - vp topicalization lexical rule proposed by  #AUTHOR_TAG, 10 ) is a linguistic example.', 'the in - specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'in the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement']",0
['more compact representation of lexical rules where explicit framing would require the rules to be split up  #TAUTHOR_TAG'],['more compact representation of lexical rules where explicit framing would require the rules to be split up  #TAUTHOR_TAG'],['more compact representation of lexical rules where explicit framing would require the rules to be split up  #TAUTHOR_TAG'],"['the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.', 'this is so since the lexical rule in figure 2 "" ( like all lexical rules in hpsg ) preserves all properties of the input not mentioned in the rule. ""', '( pollard and sag [ 1994, 314 ], following flickinger [ 1987 ] ).', 'this idea of preserving properties can be considered an instance of the well - known frame problem in ai ( mccarthy and hayes 1969 ), and we will therefore refer to the specifications left implicit by the linguist as the frame specification, or simply frame, of a lexical rule.', 'not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up  #TAUTHOR_TAG']",0
"['mentioned.', 'as shown in  #TAUTHOR_TAG this is a well - motivated convention since it avoids splitting up lexical rules to transfer the specifications that']","['mentioned.', 'as shown in  #TAUTHOR_TAG this is a well - motivated convention since it avoids splitting up lexical rules to transfer the specifications that']","['in hpsg that only the properties changed by a lexical rule need be mentioned.', 'as shown in  #TAUTHOR_TAG this is a well - motivated convention since it avoids splitting up lexical rules to transfer the specifications that']","['computational treatment expanding out the lexicon cannot be used for the increasing number of hpsg analyses that propose lexical rules that would result in an infinite lexicon.', 'most current hpsg analyses of dutch, german, italian, and french fall into that category.', '1 furthermore, since lexical rules in such an approach only serve in a precompilation step, the generalizations captured by the lexical rules cannot be used at run - time.', 'finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry.', 'this conflicts with the standard assumption made in hpsg that only the properties changed by a lexical rule need be mentioned.', 'as shown in  #TAUTHOR_TAG this is a well - motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries']",4
"['fold transformation techniques  #TAUTHOR_TAG.', '29']","['transformation techniques  #TAUTHOR_TAG.', '29']","['fold transformation techniques  #TAUTHOR_TAG.', '29 the unfolding transformation is also referred to']","['elimination of redundant nondeterminism is based on unfold / fold transformation techniques  #TAUTHOR_TAG.', '29 the unfolding transformation is also referred to as partial execution, for example, by  #AUTHOR_TAG.', 'intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile - time.', 'as a result, the literal can be removed from the body of the clause.', 'whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation']",5
[';  #TAUTHOR_TAG ; san'],['captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ;  #TAUTHOR_TAG ; sanfilippo'],"['##nged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ;  #TAUTHOR_TAG ; san']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ;  #TAUTHOR_TAG ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
['- ii  #TAUTHOR_TAG and can be used to encode lexical rules as binary relations in'],"['( copestake 1993, 31 ).', 'a similar method is included in patr - ii  #TAUTHOR_TAG and can be used to encode lexical rules as binary relations in']",['- ii  #TAUTHOR_TAG and can be used to encode lexical rules as binary relations in the cuf system ( dor'],"['common approach to lexical rules is to encode them as unary phrase structure rules.', 'this approach is taken, for example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii  #TAUTHOR_TAG and can be used to encode lexical rules as binary relations in the cuf system ( dorre and eisele 1991 ; done and dorna 1993b ) or the tfs system ( emele and zajac 1990 ; emele 1994 ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']",1
"[';  #TAUTHOR_TAG.', 'the covariation']","[';  #TAUTHOR_TAG.', 'the covariation']","['##na 1993b ) or the tfs system ( emele and zajac 1990 ;  #TAUTHOR_TAG.', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']","['common approach to lexical rules is to encode them as unary phrase structure rules.', 'this approach is taken, for example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the cuf system ( dorre and eisele 1991 ; done and dorna 1993b ) or the tfs system ( emele and zajac 1990 ;  #TAUTHOR_TAG.', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']",1
[';  #TAUTHOR_TAG and'],[';  #TAUTHOR_TAG and'],['; calcagno 1995 ;  #TAUTHOR_TAG and'],"['the setup of king provides a clear formal basis for basic hpsg grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'two formalizations of lexical rules as used by hpsg linguists have been proposed, the meta - level lexical rules ( mlrs ; calcagno 1995 ;  #TAUTHOR_TAG and the.', '']",0
['##demann and  #TAUTHOR_TAG ; gotz and me'],['was implemented in prolog by the authors in cooperation with dieter martini for the controll system ( gerdemann and  #TAUTHOR_TAG ; gotz and'],['##demann and  #TAUTHOR_TAG ; gotz and me'],"['computational treatment of lexical rules as covariation in lexical entries was implemented in prolog by the authors in cooperation with dieter martini for the controll system ( gerdemann and  #TAUTHOR_TAG ; gotz and meurers 1997a ).', 'we tested the covariation approach with a complex grammar implementing an hpsg analysis covering the so - called aux - flip phenomenon, and partial - vp topicalization in the three clause types of german ( hinrichs, meurers, and nakazawa 1994 ).', 'this test grammar includes eight lexical rules ; some serve syntactic purposes, like the partial - vp topicalization lexical rule, others are of morphological nature as, for example, an inflectional lexical rule that relates nonfinite verbs to their finite form.', 'our compiler distinguished seven word classes.', 'some nouns and most verbal lexical entries fed lexical rules, and a single base lexical entry resulted in up to 12 derivations']",5
[';  #TAUTHOR_TAG ; calcagno and pollard'],[';  #TAUTHOR_TAG ; calcagno and pollard'],[';  #TAUTHOR_TAG ; calcagno and pollard 1995 ) and'],"['the setup of king provides a clear formal basis for basic hpsg grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'two formalizations of lexical rules as used by hpsg linguists have been proposed, the meta - level lexical rules ( mlrs ;  #TAUTHOR_TAG ; calcagno and pollard 1995 ) and the.', '']",0
['verbal complement  #TAUTHOR_TAG'],['verbal complement  #TAUTHOR_TAG'],['the arguments of a verbal complement  #TAUTHOR_TAG'],"['the authors are listed alphabetically.', 'sfb 340, kleine wilhelmstr.', '113, d - 72074 tiibingen, germany.', 'email : { dm, minnen } @ sfs. nphil. uni - tuebingen. de', 'url : http : / / www. sfs. nphil. uni - tuebingen. de / sfb', '/ b4home. html 1 this is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement  #TAUTHOR_TAG that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement cliticization lexical rule ( miller and sag 1993 ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']",0
[' #TAUTHOR_TAG consists of computing the'],[' #TAUTHOR_TAG consists of computing the'],[' #TAUTHOR_TAG consists of computing the transitive closure'],"['common computational treatment of lexical rules adopted, for example, in the ale system  #TAUTHOR_TAG consists of computing the transitive closure of the base lexical entries under lexical rule application at compile - time.', 'while this provides a front - end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation.', 'we mentioned in section 2. 2 that eliminating lexical rules in a precompilation step makes it impossible to process lexical rules or lexical entries that impose constraints that can only be properly executed once information from syntactic processing is available.', 'a related problem is that for analyses resulting in infinite lexica, the number of lexical rule applications needs to be limited.', 'in the ale system, for example, a depth bound can be specified for this purpose.', 'finally, as shown in section 6, using an expanded out lexicon can be less time and space efficient than using a lexicon encoding that makes computational use of generalizations over lexical information, as, for example, the covariation encoding']",1
"['logic that provides the formal architecture required by  #AUTHOR_TAG was defined by  #TAUTHOR_TAG, 1994 ).', 'the formal language of king allows the']","['logic that provides the formal architecture required by  #AUTHOR_TAG was defined by  #TAUTHOR_TAG, 1994 ).', 'the formal language of king allows the']","['logic that provides the formal architecture required by  #AUTHOR_TAG was defined by  #TAUTHOR_TAG, 1994 ).', 'the formal language of king allows the expression']","['logic that provides the formal architecture required by  #AUTHOR_TAG was defined by  #TAUTHOR_TAG, 1994 ).', 'the formal language of king allows the expression of grammatical principles using type assignments to refer to the type of an object and path equalities to require the ( token ) identity of objects.', 'these atomic expressions can be combined using conjunction, disjunction, and negation.', 'the expressions are interpreted by a set - theoretical semantics']",0
"[', for example, by  #TAUTHOR_TAG.', 'intuitively understood, unfolding comprises']","[', for example, by  #TAUTHOR_TAG.', 'intuitively understood, unfolding comprises']","['fold transformation techniques ( tamaki and sato 1984 ). 29', 'the unfolding transformation is also referred to as partial execution, for example, by  #TAUTHOR_TAG.', 'intuitively understood, unfolding comprises']","['elimination of redundant nondeterminism is based on unfold / fold transformation techniques ( tamaki and sato 1984 ). 29', 'the unfolding transformation is also referred to as partial execution, for example, by  #TAUTHOR_TAG.', 'intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile - time.', 'as a result, the literal can be removed from the body of the clause.', 'whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation']",0
"['1993b ) or the tfs system  #TAUTHOR_TAG ; emele 1994 ).', 'the covariation']","['1993b ) or the tfs system  #TAUTHOR_TAG ; emele 1994 ).', 'the covariation']","['##na 1993b ) or the tfs system  #TAUTHOR_TAG ; emele 1994 ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']","['common approach to lexical rules is to encode them as unary phrase structure rules.', 'this approach is taken, for example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the cuf system ( dorre and eisele 1991 ; done and dorna 1993b ) or the tfs system  #TAUTHOR_TAG ; emele 1994 ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']",1
"[""be given the standard set - theoretical interpretation of  #TAUTHOR_TAG, 1994 ). '"", '11']","[""be given the standard set - theoretical interpretation of  #TAUTHOR_TAG, 1994 ). '"", '11']","[""be given the standard set - theoretical interpretation of  #TAUTHOR_TAG, 1994 ). '"", '11 10 note that the passivization lexical rule in figure 2 is only']","['', ""this description can then be given the standard set - theoretical interpretation of  #TAUTHOR_TAG, 1994 ). '"", '11 10 note that the passivization lexical rule in figure 2 is only intended to illustrate the mechanism.', 'we do not make the linguistic claim that passives should be analyzed using such a lexical rule.', 'for space reasons, the synsem feature is abbreviated by its first letter.', 'the traditional ( first i rest ) list notation is used, and the operator  stands for the append relation in the usual way.', '1l  #AUTHOR_TAG proposes to unify these two steps by including an update operator in the the computational treatment we discuss in the rest of the paper follows this setup in that it automatically computes, for each lexical rule specification, the frames necessary to preserve the properties not changed by it.', '12 we will show that the detection and specification of frames and the use of program transformation to advance their integration into the lexicon encoding is one of the key ingredients of the covariation approach to hpsg lexical rules']",0
[';  #TAUTHOR_TAG ; frank'],[';  #TAUTHOR_TAG ; frank'],"['##nged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ;  #TAUTHOR_TAG ; frank 1994 ; opalka 1995 ; san']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ;  #TAUTHOR_TAG ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
[' #TAUTHOR_TAG proposes to uni'],[' #TAUTHOR_TAG proposes to unify these two steps by including an update operator in the description language'],[' #TAUTHOR_TAG proposes to uni'],[' #TAUTHOR_TAG proposes to unify these two steps by including an update operator in the description language'],0
['the constraint language with named disjunctions or contexted constraints  #TAUTHOR_TAG ; eisele and dor'],"['the constraint language with named disjunctions or contexted constraints  #TAUTHOR_TAG ; eisele and dorre 1990 ; griffith 1996 ) can be used to circumvent constraint propagation.', 'encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments,']","['the constraint language with named disjunctions or contexted constraints  #TAUTHOR_TAG ; eisele and dorre 1990 ; griffith 1996 ) can be used to circumvent constraint propagation.', 'encoding the disjunctive possibilities for lexical rule application in this']","['in certain cases an extension of the constraint language with named disjunctions or contexted constraints  #TAUTHOR_TAG ; eisele and dorre 1990 ; griffith 1996 ) can be used to circumvent constraint propagation.', 'encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments, makes all relevant lexical information available at lexical lookup.', 'for analyses proposing infinite lexica, though, a definite clause encoding of disjunctive possibilities is still necessary and constraint propagation is indispensable for efficient processing']",0
['of lexical rules  #TAUTHOR_TAG has been used in many natural language'],['of lexical rules  #TAUTHOR_TAG has been used in many natural language'],"['powerful mechanism of lexical rules  #TAUTHOR_TAG has been used in many natural language processing systems.', 'in this section we briefly discuss']","['powerful mechanism of lexical rules  #TAUTHOR_TAG has been used in many natural language processing systems.', 'in this section we briefly discuss some of the more prominent approaches and compare them with the treatment proposed in this paper']",0
"['problem in at  #TAUTHOR_TAG, and we will therefore']","['problem in at  #TAUTHOR_TAG, and we will therefore']","['##er [ 1987 ] ).', 'this idea of preserving properties can be considered an instance of the well - known frame problem in at  #TAUTHOR_TAG, and we will therefore']","['the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.', 'this is so since the lexical rule in figure 2 "" ( like all lexical rules in hpsg ) preserves all properties of the input not mentioned in the rule. ""', '( pollard and sag [ 1994, 314 ], following flickinger [ 1987 ] ).', 'this idea of preserving properties can be considered an instance of the well - known frame problem in at  #TAUTHOR_TAG, and we will therefore refer to the specifications left implicit by the linguist as the frame specification, or simply frame, of a lexical rule.', 'not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( meurers 1994 )']",1
"['passing technique  #TAUTHOR_TAG, we ensure that upon']","['by a unit clause.', 'using an accumulator passing technique  #TAUTHOR_TAG, we ensure that upon']","['is encoded by a unit clause.', 'using an accumulator passing technique  #TAUTHOR_TAG, we ensure that upon execution']","['', 'using an accumulator passing technique  #TAUTHOR_TAG, we ensure that upon execution of a call to the interaction predicate q _ 1 a new lexical entry is derived as the result of successive application of a number of lexical rules.', 'because of the word class specialization step discussed in section 3. 3, the execution avoids trying out many lexical rule applications that are guaranteed to fail']",5
"['can be found in minnen ( in preparation ).', '14 we use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15  #TAUTHOR_TAG show that the']","['can be found in minnen ( in preparation ).', '14 we use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15  #TAUTHOR_TAG show that the']","['the relevant details in that paper.', '13 a more detailed presentation can be found in minnen ( in preparation ).', '14 we use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15  #TAUTHOR_TAG show that the question of']","['translation of the lexical rule into a predicate is trivial.', 'the result is displayed description language.', '12 in order to focus on the computational aspects of the covariation approach, in this paper we will not go into a discussion of the full lexical rule specification language introduced in  #AUTHOR_TAG.', 'the reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 a more detailed presentation can be found in minnen ( in preparation ).', '14 we use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15  #TAUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention.', 'we here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form ( g6tz 1994 ).', 'computationally, a subsumption test could equally well be used in our compiler']",0
[';  #TAUTHOR_TAG ; oliv'],['captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ;  #TAUTHOR_TAG ; oliva'],"['##nged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ;  #TAUTHOR_TAG ; oliv']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ;  #TAUTHOR_TAG ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
[';  #TAUTHOR_TAG ;'],[';  #TAUTHOR_TAG ; opalka'],[';  #TAUTHOR_TAG ;'],"['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ;  #TAUTHOR_TAG ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
['##demann and  #TAUTHOR_TAG ;'],['to as closed world ( gerdemann and  #TAUTHOR_TAG ;'],['##demann and  #TAUTHOR_TAG ;'],"['terminology used in the literature varies.', 'types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'to avoid confusion, we will only use the terminology introduced in the text.', '4 this interpretation of the signature is sometimes referred to as closed world ( gerdemann and  #TAUTHOR_TAG ; gerdemann 1995 ).', '5 an in - depth discussion including a comparison of both approaches is provided in calcagno, meurers, and pollard ( in preparation ).', '6 the partial - vp topicalization lexical rule proposed by  #AUTHOR_TAG, 10 ) is a linguistic example.', 'the in - specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'in the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement']",0
['( pollard and sag 1994 ) or the complement cliticization lexical rule  #TAUTHOR_TAG to'],['( pollard and sag 1994 ) or the complement cliticization lexical rule  #TAUTHOR_TAG to'],['the complement extraction lexical rule ( pollard and sag 1994 ) or the complement cliticization lexical rule  #TAUTHOR_TAG to'],"['the authors are listed alphabetically.', 'sfb 340, kleine wilhelmstr.', '113, d - 72074 tiibingen, germany.', 'email : { dm, minnen } @ sfs. nphil. uni - tuebingen. de', 'url : http : / / www. sfs. nphil. uni - tuebingen. de / sfb', 'de url : http : / / www. sfs. nphil. uni - tuebingen. de / sfb / b4home. html 1 this is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( hinrichs and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement cliticization lexical rule  #TAUTHOR_TAG to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']",0
[' #TAUTHOR_TAG would be'],[' #TAUTHOR_TAG would be'],[' #TAUTHOR_TAG would be a lexical rule deriving predicative signs from'],"['a linguistic example based on the signature given by  #TAUTHOR_TAG would be a lexical rule deriving predicative signs from nonpredicative ones, i. e., changing the prd value of substantive signs from - to +, much like the lexical rule for nps given by  #AUTHOR_TAG, fn. 20 ).', 'in such a predicative lexical rule ( which we only note as an example and not as a linguistic proposal ) the subtype of the head object undergoing the rule as well as the value of the features only appropriate for the subtypes of substantive either is lost or must be specified by a separate rule for each of the subtypes']",0
"['', 'the computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by gotz and  #TAUTHOR_TAG, 1996, 1997b )']","['time after lexical lookup.', 'the computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by gotz and  #TAUTHOR_TAG, 1996, 1997b )']","['some time after lexical lookup.', 'the computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by gotz and  #TAUTHOR_TAG, 1996, 1997b )']","['relations are a convenient way of encoding the interaction of lexical rules, as they readily support various program transformations to improve the encoding : we show that the definite relations produced by the compiler can be refined by program transformation techniques to increase efficiency.', 'the resulting encoding allows the execution of lexical rules on - the - fly, i. e., coroutined with other constraints at some time after lexical lookup.', 'the computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by gotz and  #TAUTHOR_TAG, 1996, 1997b ) for encoding the main building block of hpsg grammars - - the implicative constraints - - as a logic program']",2
['captured using lexical underspecification  #TAUTHOR_TAG ; krie'],['captured using lexical underspecification  #TAUTHOR_TAG ; krieger and nerbonne'],"['##nged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification  #TAUTHOR_TAG ; krie']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification  #TAUTHOR_TAG ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
"[';  #TAUTHOR_TAG.', 'the lexical entries are only partially specified,']","[';  #TAUTHOR_TAG.', 'the lexical entries are only partially specified,']","['; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ;  #TAUTHOR_TAG.', 'the lexical entries are only partially specified,']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ;  #TAUTHOR_TAG.', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
[' #TAUTHOR_TAG or'],[' #TAUTHOR_TAG or'],['the complement extraction lexical rule  #TAUTHOR_TAG or the complement cliticization lexical rule ( miller and sag 1993 ) to'],"['the authors are listed alphabetically.', 'sfb 340, kleine wilhelmstr.', '113, d - 72074 tiibingen, germany.', 'email : { dm, minnen } @ sfs. nphil. uni - tuebingen. de', 'url : http : / / www. sfs. nphil. uni - tuebingen. de / sfb', '/ b4home. html 1 this is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( hinrichs and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule  #TAUTHOR_TAG or the complement cliticization lexical rule ( miller and sag 1993 ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']",0
"['on the research results reported in  #TAUTHOR_TAG, 1996 ), we propose a new computational treatment of lexical rules that overcomes these short - comings']","['on the research results reported in  #TAUTHOR_TAG, 1996 ), we propose a new computational treatment of lexical rules that overcomes these short - comings']","['on the research results reported in  #TAUTHOR_TAG, 1996 ), we propose a new computational treatment of lexical rules that overcomes these short - comings']","['on the research results reported in  #TAUTHOR_TAG, 1996 ), we propose a new computational treatment of lexical rules that overcomes these short - comings and results in a more efficient processing of lexical rules as used in hpsg.', 'we developed a compiler that takes as its input a set of lexical rules, deduces the nec - essary transfer of properties not changed by the individual lexical rules, and encodes the set of lexical rules and their interaction into definite relations constraining lexical entries.', 'each lexical entry is automatically extended with a definite clause encoding of the lexical rule applications which the entry can undergo.', 'the definite clauses thereby introduce what we refer to as systematic covariationinlexicalentries']",4
"[', in lkb  #TAUTHOR_TAG where lexical rules']","[', in lkb  #TAUTHOR_TAG where lexical rules']","[', in lkb  #TAUTHOR_TAG where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and']","['common approach to lexical rules is to encode them as unary phrase structure rules.', 'this approach is taken, for example, in lkb  #TAUTHOR_TAG where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the cuf system ( dbrre and eisele 1991 ; d6rre and dorna 1993b ) or the tfs system ( emele and zajac 1990 ; emele 1994 ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']",1
"['latter case, we can also take care of transferring the value of z.', 'however, as discussed by  #TAUTHOR_TAG, creating several instances of lexical']","['latter case, we can also take care of transferring the value of z.', 'however, as discussed by  #TAUTHOR_TAG, creating several instances of lexical']","['those with t2 as their c value.', 'in the latter case, we can also take care of transferring the value of z.', 'however, as discussed by  #TAUTHOR_TAG, creating several instances of lexical rules can be avoided.', 'instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.', 'this is accomplished by having each lexical rule predicate']","['ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'in the above example, this would result in two lexical rules : one for words with tl as their c value and one for those with t2 as their c value.', 'in the latter case, we can also take care of transferring the value of z.', 'however, as discussed by  #TAUTHOR_TAG, creating several instances of lexical rules can be avoided.', 'instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.', 'this is accomplished by having each lexical rule predicate call a so - called framepredicate, which can have multiple defining clauses.', 'so for the lexical rule 1, the frame specification is taken care of by extending the predicate in figure 6 with a call to a frame predicate, as shown in figure 8.']",4
"[' #AUTHOR_TAG, 215 ) in terms of the setup of  #TAUTHOR_TAG, ch.', 'this lexical']","[' #AUTHOR_TAG, 215 ) in terms of the setup of  #TAUTHOR_TAG, ch.', 'this lexical']","[' #AUTHOR_TAG, 215 ) in terms of the setup of  #TAUTHOR_TAG, ch.', 'this lexical rule could be used in a grammar of english to']","['as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'consider, for example, the lexical rule in figure 2, which encodes a passive lexical rule like the one presented by  #AUTHOR_TAG, 215 ) in terms of the setup of  #TAUTHOR_TAG, ch.', 'this lexical rule could be used in a grammar of english to relate past participle forms of verbs to their passive form2  the rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'the index that the subject bore in the input is assigned to an optional prepositional complement in the output']",0
[';  #TAUTHOR_TAG ; riehemann'],[';  #TAUTHOR_TAG ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo'],"['##nged as a mechanism for expressing generalizations over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ;  #TAUTHOR_TAG ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ; san']","['rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ;  #TAUTHOR_TAG ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
"['presented by  #TAUTHOR_TAG, 215 ) in terms of the setup of  #AUTHOR_TAG, ch.', 'this lexical']","['presented by  #TAUTHOR_TAG, 215 ) in terms of the setup of  #AUTHOR_TAG, ch.', 'this lexical']","['presented by  #TAUTHOR_TAG, 215 ) in terms of the setup of  #AUTHOR_TAG, ch.', 'this lexical rule could be used in a grammar of english to']","['as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'consider, for example, the lexical rule in figure 2, which encodes a passive lexical rule like the one presented by  #TAUTHOR_TAG, 215 ) in terms of the setup of  #AUTHOR_TAG, ch.', 'this lexical rule could be used in a grammar of english to relate past participle forms of verbs to their passive form2  the rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'the index that the subject bore in the input is assigned to an optional prepositional complement in the output']",1
['##rs ;'],['( dlrs ;'],['##rs ;'],"['the setup of king provides a clear formal basis for basic hpsg grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'two formalizations of lexical rules as used by hpsg linguists have been proposed, the meta - level lexical rules ( mlrs ; calcagno 1995 ; calcagno and pollard 1995 ) and the description - level lexical rules ( dlrs ;']",0
"['', ' #TAUTHOR_TAG present detailed']","['', ' #TAUTHOR_TAG present detailed']","['', ' #TAUTHOR_TAG present detailed studies on']","['', ' #TAUTHOR_TAG present detailed studies on the task of named entity recognition, which discusses and compares different methods on multiple aspects including chunk representation, inference method, utility of non - local features, and integration of external knowledge.', 'ner can be regarded as a sequential labeling problem, which can be modeled by several proposed models, e. g.', 'hidden markov model  #AUTHOR_TAG or conditional random fields  #AUTHOR_TAG.', 'the typical bio representation was introduced in  #AUTHOR_TAG ; oc representations were introduced in  #AUTHOR_TAG, while  #AUTHOR_TAG further study nested named entity recognition, which employs a tree structure as a representation of identifying named entities within other named entities']",0
['of features used in  #TAUTHOR_TAG'],['of features used in  #TAUTHOR_TAG'],['of features used in  #TAUTHOR_TAG'],"[', y u, v = 1 iff mentions u, v are directly linked.', 'thus, we can construct a forest and the mentions in the same connected component ( i. e., in the same tree ) are co - referred.', 'for this mention - pair coreference model i ( u, v ), we use the same set of features used in  #TAUTHOR_TAG']",5
['solely on mention heads  #TAUTHOR_TAG'],['solely on mention heads  #TAUTHOR_TAG'],['solely on mention heads  #TAUTHOR_TAG'],"[', phrases in the brackets are mentions and the underlined simple phrases are mention heads.', 'moreover, mention boundaries can be nested ( the boundary of a mention is inside the boundary of another mention ), but mention heads never overlap.', 'this property also simplifies the problem of mention head candidate generation.', 'in the example above, the first "" they "" refers to "" multinational companies investing in china "" and the second "" they "" refers to "" domestic manufacturers, who are also suffering "".', 'in both cases, the mention heads are sufficient to support the decisions : "" they "" refers to "" companies "", and "" they "" refers to "" manufacturers "".', 'in fact, most of the features3 implemented in existing coreference resolution systems rely solely on mention heads  #TAUTHOR_TAG']",0
"['in bjorkelund and  #AUTHOR_TAG.', 'for berkeley system, we use the reported results from  #TAUTHOR_TAG']","['in bjorkelund and  #AUTHOR_TAG.', 'for berkeley system, we use the reported results from  #TAUTHOR_TAG']","['in bjorkelund and  #AUTHOR_TAG.', 'for berkeley system, we use the reported results from  #TAUTHOR_TAG']","['latest scorer is version v8. 01, but muc, b, ceaf e and conll average scores are not changed.', 'for evaluation on ace - 2004, we convert the system output and gold annotations into conll format. 12', 'we do not provide results from berkeley and hotcoref on ace - 2004 dataset as they do not directly support ace input.', 'results for hotcoref are slightly different from the results reported in bjorkelund and  #AUTHOR_TAG.', 'for berkeley system, we use the reported results from  #TAUTHOR_TAG']",1
['system  #TAUTHOR_TAG'],['system  #TAUTHOR_TAG'],['berkeley system  #TAUTHOR_TAG'],"['ace - 2004 dataset is annotated with both mention and mention heads, while the ontonotes - 5. 0', 'dataset only has mention annotations.', 'therefore, we preprocess ontonote - 5. 0 to derive mention heads using collins head rules  #AUTHOR_TAG with gold constituency parsing information and gold named entity information.', 'the parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'we set these extracted heads as gold, which enables us to train the two layer bilou - classifier described in sec.', '3. 1. 1.', 'the nonoverlapping mention head assumption in sec.', '3. 1. 1', 'can be verified empirically on both ace - 2004 and ontonotes - 5. 0', 'datasets.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system  #AUTHOR_TAG, berkeley system  #TAUTHOR_TAG and hotcoref system ( bj [UNK] orkelund and  #AUTHOR_TAG.', 'developed systems our developed system is built on the work by  #AUTHOR_TAG, using constrained latent left - linking model ( cl 3 m ) as our mention - pair coreference model in the joint framework 10.', '']",1
"[', and 106 testing documents  #TAUTHOR_TAG.', 'the ontonotes - 5']","[', and 106 testing documents  #TAUTHOR_TAG.', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task  #AUTHOR_TAG contains 3, 145 annotated']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents  #TAUTHOR_TAG.', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task  #AUTHOR_TAG contains 3']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents  #TAUTHOR_TAG.', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task  #AUTHOR_TAG contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as  = 0. 9,  = 0. 8,  1 = 0. 2 and  2 = 0. 3']",5
"['collins head rules  #TAUTHOR_TAG to identify their heads.', 'when these']","['collins head rules  #TAUTHOR_TAG to identify their heads.', 'when these']","['more noun phrases from the text and employ collins head rules  #TAUTHOR_TAG to identify their heads.', 'when these']","['of the head mentions proposed by the algorithms described in sec. 3 are positive examples.', 'we ensure a balanced training of the mention head detection model by adding sub - sampled invalid mention head candidates as negative examples.', 'specifically, after mention head candidate generation ( described in sec.', '3 ), we train on a set of candidates with precision larger than 50 %.', 'we then use illinois chunker  #AUTHOR_TAG 6 to extract more noun phrases from the text and employ collins head rules  #TAUTHOR_TAG to identify their heads.', 'when these extracted heads do not overlap with gold mention heads, we treat them as negative examples']",5
"['system  #TAUTHOR_TAG bj [UNK] orkelund and  #AUTHOR_TAG.', 'however, the lack of emphasis is not']","['system  #TAUTHOR_TAG bj [UNK] orkelund and  #AUTHOR_TAG.', 'however, the lack of emphasis is not']","['detection is rarely studied as a stand - alone research problem (  #AUTHOR_TAG is one key exception ).', 'most coreference resolution work simply mentions it in passing as a module in the pipelined system  #TAUTHOR_TAG bj [UNK] orkelund and  #AUTHOR_TAG.', 'however, the lack of emphasis is not']","['detection is rarely studied as a stand - alone research problem (  #AUTHOR_TAG is one key exception ).', 'most coreference resolution work simply mentions it in passing as a module in the pipelined system  #TAUTHOR_TAG bj [UNK] orkelund and  #AUTHOR_TAG.', 'however, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty.', 'indeed, many papers report results in terms of gold mentions versus system generated mentions, as shown in table 1.', 'current state - of - the - art systems show a very significant drop in performance when running on system generated mentions.', 'these performance gaps are worrisome, since the real goal of nlp systems is to process raw data.', '1 : performance gaps between using gold mentions and predicted mentions for three state - of - the - art coreference resolution systems.', 'performance gaps are always larger than 10 %.', ""illinois's system  #AUTHOR_TAG is evaluated on conll ( 2012conll (, 2011 ) shared task and ace - 2004 datasets."", 'it reports an average f1 score of muc, b and ceaf e metrics using conll v7. 0 scorer.', '']",0
"['it has advantages over traditional bio - representation, as shown, e. g. in  #TAUTHOR_TAG.', 'the bilourepr']","['it has advantages over traditional bio - representation, as shown, e. g. in  #TAUTHOR_TAG.', 'the bilourepresentation suggests learning classifiers that']","['it has advantages over traditional bio - representation, as shown, e. g. in  #TAUTHOR_TAG.', 'the bilourepresentation suggests learning classifiers that']","['on this assumption, the problem of identifying mention heads is a sequential phrase identification problem, and we choose to employ the bilou - representation as it has advantages over traditional bio - representation, as shown, e. g. in  #TAUTHOR_TAG.', 'the bilourepresentation suggests learning classifiers that identify the beginning, inside and last tokens of multi - token chunks as well as unit - length chunks.', 'the problem is then transformed into a simple, but constrained, 5 - class classification problem']",4
"['##ll - 2012 shared task  #TAUTHOR_TAG, contains 3, 145 annotated']","['standard split of 268 training documents, 68 development documents, and 106 testing documents  #AUTHOR_TAG.', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012 shared task  #TAUTHOR_TAG, contains 3, 145 annotated']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents  #AUTHOR_TAG.', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012 shared task  #TAUTHOR_TAG, contains 3, 145 annotated documents.', 'these documents come']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents  #AUTHOR_TAG.', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012 shared task  #TAUTHOR_TAG, contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as  = 0. 9,  = 0. 8,  1 = 0. 2 and  2 = 0. 3']",5
"['on the work by  #TAUTHOR_TAG, using constrained latent left - linking']","['on the work by  #TAUTHOR_TAG, using constrained latent left - linking']","['hotcoref system ( bjorkelund and  #AUTHOR_TAG.', 'developed systems our developed system is built on the work by  #TAUTHOR_TAG, using constrained latent left - linking model ( cl']","['', 'datasets.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system  #AUTHOR_TAG, berkeley system  #AUTHOR_TAG and hotcoref system ( bjorkelund and  #AUTHOR_TAG.', 'developed systems our developed system is built on the work by  #TAUTHOR_TAG, using constrained latent left - linking model ( cl3m ) as our mention - pair coreference model in the joint framework10.', '']",5
"['area too  #AUTHOR_TAG.', 'in this paper, we use the constrained latent left - linking model ( cl3m ) described in  #TAUTHOR_TAG in our experiments']","['the coreference area too  #AUTHOR_TAG.', 'in this paper, we use the constrained latent left - linking model ( cl3m ) described in  #TAUTHOR_TAG in our experiments']","[' #AUTHOR_TAG gained considerable popularity.', 'the early designs were easy to understand and the rules were designed manually.', 'machine learning approaches were introduced in many works  #AUTHOR_TAG.', 'the introduction of ilp methods has influenced the coreference area too  #AUTHOR_TAG.', 'in this paper, we use the constrained latent left - linking model ( cl3m ) described in  #TAUTHOR_TAG in our experiments']","['##ference resolution has been extensively studied, with several state - of - the - art approaches addressing this task  #AUTHOR_TAG bjorkelund and  #AUTHOR_TAG.', 'many of the early rule - based systems like  #AUTHOR_TAG and  #AUTHOR_TAG gained considerable popularity.', 'the early designs were easy to understand and the rules were designed manually.', 'machine learning approaches were introduced in many works  #AUTHOR_TAG.', 'the introduction of ilp methods has influenced the coreference area too  #AUTHOR_TAG.', 'in this paper, we use the constrained latent left - linking model ( cl3m ) described in  #TAUTHOR_TAG in our experiments']",5
"[', and 106 testing documents  #TAUTHOR_TAG.', 'the ontonotes - 5']","[', and 106 testing documents  #TAUTHOR_TAG.', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task  #AUTHOR_TAG contains 3, 145 annotated']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents  #TAUTHOR_TAG.', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task  #AUTHOR_TAG contains 3']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents  #TAUTHOR_TAG.', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task  #AUTHOR_TAG contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as  = 0. 9,  = 0. 8,  1 = 0. 2 and  2 = 0. 3']",5
"['details can be found in  #TAUTHOR_TAG et al. ( 2013 ).', 'the difference here is that we also consider the validity of mention heads using ( u ), ( m']","['details can be found in  #TAUTHOR_TAG et al. ( 2013 ).', 'the difference here is that we also consider the validity of mention heads using ( u ), ( m']","['details can be found in  #TAUTHOR_TAG et al. ( 2013 ).', 'the difference here is that we also consider the validity of mention heads using ( u ), ( m']","['details can be found in  #TAUTHOR_TAG et al. ( 2013 ).', 'the difference here is that we also consider the validity of mention heads using ( u ), ( m']",0
"['recent works suggest studying coreference jointly with other tasks.', ' #AUTHOR_TAG model entity coreference and event coreference jointly ;  #TAUTHOR_TAG consider joint coreference and entity - linking.', 'the']","['recent works suggest studying coreference jointly with other tasks.', ' #AUTHOR_TAG model entity coreference and event coreference jointly ;  #TAUTHOR_TAG consider joint coreference and entity - linking.', 'the']","['recent works suggest studying coreference jointly with other tasks.', ' #AUTHOR_TAG model entity coreference and event coreference jointly ;  #TAUTHOR_TAG consider joint coreference and entity - linking.', 'the work closest to ours is']","['recent works suggest studying coreference jointly with other tasks.', ' #AUTHOR_TAG model entity coreference and event coreference jointly ;  #TAUTHOR_TAG consider joint coreference and entity - linking.', 'the work closest to ours is that of  #AUTHOR_TAG, which studies a joint anaphoricity detection and coreference resolution framework.', 'while their inference objective is similar, their work assumes gold mentions are given and thus their modeling is very different']",0
"['derive mention heads using collins head rules  #TAUTHOR_TAG with gold constituency parsing information and gold named entity information.', 'the parsing']","['derive mention heads using collins head rules  #TAUTHOR_TAG with gold constituency parsing information and gold named entity information.', 'the parsing']","['derive mention heads using collins head rules  #TAUTHOR_TAG with gold constituency parsing information and gold named entity information.', 'the parsing information 9 is only needed']","['ace - 2004 dataset is annotated with both mention and mention heads, while the ontonotes - 5. 0', 'dataset only has mention annotations.', 'therefore, we preprocess ontonote - 5. 0 to derive mention heads using collins head rules  #TAUTHOR_TAG with gold constituency parsing information and gold named entity information.', 'the parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'we set these extracted heads as gold, which enables us to train the two layer bilou - classifier described in sec.', '3. 1. 1.', 'the nonoverlapping mention head assumption in sec.', '3. 1. 1', 'can be verified empirically on both ace - 2004 and ontonotes - 5. 0', 'datasets.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system  #AUTHOR_TAG, berkeley system  #AUTHOR_TAG and hotcoref system ( bjorkelund and  #AUTHOR_TAG.', 'developed systems our developed system is built on the work by  #AUTHOR_TAG, using constrained latent left - linking model ( cl 3 m ) as our mention - pair coreference model in the joint framework 10.', '']",5
['is inspired by the latent left - linking model in  #TAUTHOR_TAG and the ilp formulation from  #AUTHOR_TAG'],['is inspired by the latent left - linking model in  #TAUTHOR_TAG and the ilp formulation from  #AUTHOR_TAG'],"['is inspired by the latent left - linking model in  #TAUTHOR_TAG and the ilp formulation from  #AUTHOR_TAG.', 'the joint learning']","['', 'our work is inspired by the latent left - linking model in  #TAUTHOR_TAG and the ilp formulation from  #AUTHOR_TAG.', 'the joint learning and inference model takes as input mention head candidates ( sec.', '3 ) and jointly ( 1 ) determines if they are indeed mention heads and ( 2 ) learns a similarity metric between mentions.', 'this is done by simultaneously learning a binary mention head detection classifier and a mention - pair coreference classifier.', 'the mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones.', 'by learning and making decisions jointly, it also serves as a singleton mention head classifier, building on insights from  #AUTHOR_TAG.', 'this joint framework aims to improve performance on both mention head detection and on coreference']",5
"['0  #TAUTHOR_TAG.', ' #AUTHOR_TAG']","['( nist, 2004 ) and ontonotes - 5. 0  #TAUTHOR_TAG.', ' #AUTHOR_TAG']","['2004 ( nist, 2004 ) and ontonotes - 5. 0  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'our approach results in']","['present experiments on the two standard coreference resolution datasets, ace - 2004 ( nist, 2004 ) and ontonotes - 5. 0  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat - of - the - art results on coreference resolution ; in addition, it achieves significant performance improvement on md for both datasets']",5
"['p cat.', 'we can define pcat using a probabilistic grammar  #TAUTHOR_TAG.', 'the grammar may first generate a start or end category ( s, e ) with']","['p cat.', 'we can define pcat using a probabilistic grammar  #TAUTHOR_TAG.', 'the grammar may first generate a start or end category ( s, e ) with']","['we will call p cat.', 'we can define pcat using a probabilistic grammar  #TAUTHOR_TAG.', 'the grammar may first generate a start or end category ( s, e ) with probability p se or a special tokendeletion category (']","['the root, binary, and unary parameters, we want to choose prior means that encode our bias toward cross - linguistically - plausible categories.', 'to formalize the notion of what it means for a category to be more "" plausible "", we extend the category generator of our previous work, which we will call p cat.', 'we can define pcat using a probabilistic grammar  #TAUTHOR_TAG.', 'the grammar may first generate a start or end category ( s, e ) with probability p se or a special tokendeletion category ( d ; explained in  5 ) with probability p del, or a standard ccg category c']",0
"[""' s ccm is an unlabeled bracketing model""]","[""' s ccm is an unlabeled bracketing model""]","[""' s ccm is an unlabeled bracketing model""]","[""' s ccm is an unlabeled bracketing model that generates the span of part - of - speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non - constituent )."", 'they found that modeling constituent context aids in parser learning because it is able to capture the observation that the same contexts tend to appear repeatedly in a corpus, even with different constituents.', 'while ccm is designed to learn which tag pairs make for likely contexts, without regard for the constituents themselves, our model attempts to learn the relationships between context categories and the types of the constituents, allowing us to take advantage of the natural a priori knowledge about which contexts fit with which constituent labels']",0
"['words to sets of potential supertags.', 'in order to estimate the parameters of our model, we develop a blocked sampler based on that of  #TAUTHOR_TAG to sample parse']","['words to sets of potential supertags.', 'in order to estimate the parameters of our model, we develop a blocked sampler based on that of  #TAUTHOR_TAG to sample parse']","['this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags.', 'in order to estimate the parameters of our model, we develop a blocked sampler based on that of  #TAUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to']","['this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags.', 'in order to estimate the parameters of our model, we develop a blocked sampler based on that of  #TAUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities.', 'however, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a metropolis - hastings step that allows us to sample efficiently from the correct posterior.', 'our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability - preferring priors on those parameters yields further gains in many scenarios']",5
[' #TAUTHOR_TAG ;'],[' #TAUTHOR_TAG ;'],[' #TAUTHOR_TAG ; the ctbccg  #AUTHOR_TAG transformation'],"['our evaluation we compared our supertagcontext approach to ( our reimplementation of ) the best - performing model of our previous work  #AUTHOR_TAG, which scm extends.', 'we evaluated on the english ccgbank  #AUTHOR_TAG, which is a transformation of the penn treebank  #TAUTHOR_TAG ; the ctbccg  #AUTHOR_TAG transformation of the penn chinese treebank  #AUTHOR_TAG ; and the ccg - tut corpus  #AUTHOR_TAG, built from the tut corpus of italian text  #AUTHOR_TAG']",5
"['the hmm supertagger of  #TAUTHOR_TAG.', 'thus, the']","['the hmm supertagger of  #TAUTHOR_TAG.', 'thus, the']","['the hmm supertagger of  #TAUTHOR_TAG.', 'thus, the right - side context prior mean  rctx - 0 t can be biased in']","[""right - side context of a non - terminal category - - the probability of generating a category to the right of the current constituent's category - - corresponds directly to the category transitions used for the hmm supertagger of  #TAUTHOR_TAG."", ""thus, the right - side context prior mean  rctx - 0 t can be biased in exactly the same way as the hmm supertagger's transitions : toward context supertags that connect to the constituent label""]",1
"['splits as  #TAUTHOR_TAG.', '']","['splits as  #TAUTHOR_TAG.', '']","['a test set.', 'we use the same splits as  #TAUTHOR_TAG.', '']","['corpus was divided into four distinct data sets : a set from which we extract the tag dictionaries, a set of raw ( unannotated ) sentences, a development set, and a test set.', 'we use the same splits as  #TAUTHOR_TAG.', 'since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form ( x \\ x ) / x rather than introducing special conjunction rules.', 'in order to increase the amount of raw data available to the sampler, we supplemented the english data with raw, unannotated newswire sentences from the nyt giga - word 5 corpus  #AUTHOR_TAG and supplemented italian with the out - of - domain wacky corpus  #AUTHOR_TAG.', 'for english and italian, this allowed us to use 100k raw tokens for training ( chinese uses 62k ).', 'for chinese and italian, for training efficiency, we used only raw sentences that were 50 words or fewer ( note that we did not drop tag dictionary set or test set sentences )']",5
"[' #TAUTHOR_TAG, but we do it directly in the grammar.', 'we add una']","[' #TAUTHOR_TAG, but we do it directly in the grammar.', 'we add unary']","[' #TAUTHOR_TAG, but we do it directly in the grammar.', 'we add una']","['', 'this is similar to the "" deletion "" strategy employed by  #TAUTHOR_TAG, but we do it directly in the grammar.', 'we add unary rules of the form d u for every potential supertag u in the tree.', 'then, at each node spanning exactly two tokens ( but no higher in the tree ), we allow rules t d, v and t v, d.', 'recall that in  3. 1, we stated that d is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary.', 'additionally, since u will still remain as the preterminal, it will be the category examined as the context by adjacent constituents']",1
"['be combinable.', 'we previously showed that incorporating this intuition into a bayesian prior can help train a ccg supertagger  #TAUTHOR_TAG']","['be combinable.', 'we previously showed that incorporating this intuition into a bayesian prior can help train a ccg supertagger  #TAUTHOR_TAG']","['be combinable.', 'we previously showed that incorporating this intuition into a bayesian prior can help train a ccg supertagger  #TAUTHOR_TAG']","['##ridge observed is that, cross - linguistically, grammars prefer simpler syntactic structures when possible, and that due to the natural correspondence of categories and syntactic structure, biasing toward simpler categories encourages simpler structures.', 'in previous work, we were able to incorporate this preference into a bayesian parsing model, biasing pcfg productions toward sim - pler categories by encoding a notion of category simplicity into a prior  #AUTHOR_TAG.', 'baldridge further notes that due to the natural associativity of ccg, adjacent categories tend to be combinable.', 'we previously showed that incorporating this intuition into a bayesian prior can help train a ccg supertagger  #TAUTHOR_TAG']",2
"['- given - category relationships from the weak supervision : the tag dictionary and raw corpus  #TAUTHOR_TAG. 4', 'this procedure attempts to automatically estimate the frequency of']","['setting the terminal production prior distributions _ term - 0 ( w ) by estimating word - given - category relationships from the weak supervision : the tag dictionary and raw corpus  #TAUTHOR_TAG. 4', 'this procedure attempts to automatically estimate the frequency of']","['setting the terminal production prior distributions _ term - 0 ( w ) by estimating word - given - category relationships from the weak supervision : the tag dictionary and raw corpus  #TAUTHOR_TAG. 4', 'this procedure attempts to automatically estimate the frequency of each word / tag combination by dividing the number of raw - corpus occurrences of each word in the dictionary evenly']","['employ the same procedure as our previous work for setting the terminal production prior distributions _ term - 0 ( w ) by estimating word - given - category relationships from the weak supervision : the tag dictionary and raw corpus  #TAUTHOR_TAG. 4', 'this procedure attempts to automatically estimate the frequency of each word / tag combination by dividing the number of raw - corpus occurrences of each word in the dictionary evenly across all of its associated tags.', 'these counts are then combined with estimates of the openness of each tag in order to assess its likelihood of appearing with new words']",5
"['not valid.', 'since we are not generating from the model, this does not introduce difficulties  #TAUTHOR_TAG']","['not valid.', 'since we are not generating from the model, this does not introduce difficulties  #TAUTHOR_TAG']","['parses with conflicting supertags are not valid.', 'since we are not generating from the model, this does not introduce difficulties  #TAUTHOR_TAG']","['ccm, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid.', 'since we are not generating from the model, this does not introduce difficulties  #TAUTHOR_TAG']",4
['x a x x of  #TAUTHOR_TAG'],['x a x x of  #TAUTHOR_TAG'],['the merge rule x a x x of  #TAUTHOR_TAG'],"['of adjacent constituents can be combined using one of a set of combination rules to form categories of higher - level constituents, as seen in figure 1.', 'the direction of the slash operator gives the behavior of the function.', 'a category ( s \\ np ) / pp might describe an intransitive verb with a prepositional phrase complement ; it combines on the right ( / ) with a constituent with category pp, and then on the left ( \\ ) with a noun phrase ( np ) that serves as its subject.', 'we follow  #AUTHOR_TAG in allowing a small set of generic, linguistically - plausible unary and binary grammar rules.', 'we further add rules for combining with punctuation to the left and right and allow for the merge rule x a x x of  #TAUTHOR_TAG']",5
"['by  #AUTHOR_TAG and used by  #TAUTHOR_TAG that samples entire parse trees.', 'for a sentence w, the strategy is to use the inside algorithm  #AUTHOR_TAG to inductively compute, for']","['by  #AUTHOR_TAG and used by  #TAUTHOR_TAG that samples entire parse trees.', 'for a sentence w, the strategy is to use the inside algorithm  #AUTHOR_TAG to inductively compute, for']","['by  #AUTHOR_TAG and used by  #TAUTHOR_TAG that samples entire parse trees.', 'for a sentence w, the strategy is to use the inside algorithm  #AUTHOR_TAG to inductively compute, for each potential non - terminal position spanning words w i through']","['sample from our proposal distribution, we use a blocked gibbs sampler based on the one proposed by  #AUTHOR_TAG and used by  #TAUTHOR_TAG that samples entire parse trees.', 'for a sentence w, the strategy is to use the inside algorithm  #AUTHOR_TAG to inductively compute, for each potential non - terminal position spanning words w i through w j1 and category t, going "" up "" the tree, the probability of generating w i,...', ', w j1 via any arrangement of productions that is rooted by y ij = t']",5
"['presented by  #TAUTHOR_TAG.', 'at a high level, we alternate between resampling model parameters']","['presented by  #TAUTHOR_TAG.', 'at a high level, we alternate between resampling model parameters ( _ root, _ bin, _, _, _, _, _ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences.', 'to efficiently sample new model parameters, we exploit dirichlet - multinomial conjugacy.', 'by repeating these alternating steps and accumu - lating the productions, we']","['just described and a corpus of sentences.', 'since there is no way to analytically compute these modes, we resort to gibbs sampling to find an approximate solution.', 'our strategy is based on the approach presented by  #TAUTHOR_TAG.', 'at a high level, we alternate between resampling model parameters']","['wish to infer the distribution over ccg parses, given the model we just described and a corpus of sentences.', 'since there is no way to analytically compute these modes, we resort to gibbs sampling to find an approximate solution.', 'our strategy is based on the approach presented by  #TAUTHOR_TAG.', 'at a high level, we alternate between resampling model parameters ( _ root, _ bin, _, _, _, _, _ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences.', 'to efficiently sample new model parameters, we exploit dirichlet - multinomial conjugacy.', 'by repeating these alternating steps and accumu - lating the productions, we obtain an approximation of the required posterior quantities']",5
"['##context model ( ccm ) of  #TAUTHOR_TAG,']","['important example is the constituentcontext model ( ccm ) of  #TAUTHOR_TAG,']","['important example is the constituentcontext model ( ccm ) of  #TAUTHOR_TAG,']","['important example is the constituentcontext model ( ccm ) of  #TAUTHOR_TAG, which was specifically designed to capture the linguistic observation made by  #AUTHOR_TAG that there are regularities to the contexts in which constituents appear.', 'this phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts.', 'for example, the part - of - speech ( pos ) sequence adj noun frequently occurs between the tags det and verb.', '']",0
"['subject.', 'we follow  #TAUTHOR_TAG in allowing a small set of generic, linguistically - plausible una']","['subject.', 'we follow  #TAUTHOR_TAG in allowing a small set of generic, linguistically - plausible unary and binary grammar rules.', 'we further add rules']","['subject.', 'we follow  #TAUTHOR_TAG in allowing a small set of generic, linguistically - plausible una']","['of adjacent constituents can be combined using one of a set of combination rules to form categories of higher - level constituents, as seen in figure 1.', 'the direction of the slash operator gives the behavior of the function.', 'a category ( s \\ np ) / pp might describe an intransitive verb with a prepositional phrase complement ; it combines on the right ( / ) with a constituent with category pp, and then on the left ( \\ ) with a noun phrase ( np ) that serves as its subject.', 'we follow  #TAUTHOR_TAG in allowing a small set of generic, linguistically - plausible unary and binary grammar rules.', 'we further add rules for combining with punctuation to the left and right and allow for the merge rule x  x x of  #AUTHOR_TAG']",5
"['similar to that used by  #TAUTHOR_TAG.', 'the motivation for the features was to capture declaratively decisions']","['similar to that used by  #TAUTHOR_TAG.', 'the motivation for the features was to capture declaratively decisions']","['291 features in training the spr.', 'features were discovered from the actual sentence plan trees that the spg generated through the feature derivation process described below, in a manner similar to that used by  #TAUTHOR_TAG.', 'the motivation for the features was to capture declaratively decisions']","['##boost, like other machine learning programs of the boosting family, can handle a very large number of features.', 'therefore, instead of carefully choosing a small number of features by hand which may be useful, we generated a very large number of features and let rank - boost choose the relevant ones.', 'in total, we used 3, 291 features in training the spr.', 'features were discovered from the actual sentence plan trees that the spg generated through the feature derivation process described below, in a manner similar to that used by  #TAUTHOR_TAG.', 'the motivation for the features was to capture declaratively decisions made by the randomized spg.', 'we avoided features specific to particular text plans by discarding those that occurred fewer than 10 times']",1
[') community uses hand - written rules to approximate the distribution of linguistic phenomena in a corpus ( see  #TAUTHOR_TAG'],['natural language generation ( nlg ) community uses hand - written rules to approximate the distribution of linguistic phenomena in a corpus ( see  #TAUTHOR_TAG'],[') community uses hand - written rules to approximate the distribution of linguistic phenomena in a corpus ( see  #TAUTHOR_TAG'],"['work in sentence planning in the natural language generation ( nlg ) community uses hand - written rules to approximate the distribution of linguistic phenomena in a corpus ( see  #TAUTHOR_TAG for a recent example with further references ).', 'this approach is difficult to scale due to the nonrobustness of rules and unexpected interactions  #AUTHOR_TAG, and it is difficult to develop new applications quickly.', 'presumably, this is the reason why dialog systems to date have not used this kind of sentence planning']",0
"['from training data, using techniques similar to  #TAUTHOR_TAG']","['the spr uses rules automatically learned from training data, using techniques similar to  #TAUTHOR_TAG']","['primary contribution is a method for training the', 'spr. the spr uses rules automatically learned from training data, using techniques similar to  #TAUTHOR_TAG']","['', 'number : 1 ) request ( depart - time ) in this paper, we present spot, for "" sentence planner, trainable "". we also present a new methodology for automatically training spot on the basis of feedback provided by human judges. in order to train spot, we reconceptualize its task as consisting of two distinct phases. in the first phase, the', 'sentence - plan - generator ( spg ) generates a potentially large sample of possible sentence plans for a', 'given text - plan input. in the second phase, the sentence - plan - ranker ( spr', ') ranks the sample sentence plans, and then selects the top - ranked output to input to the surface realizer. our primary contribution is a method for training the', 'spr. the spr uses rules automatically learned from training data, using techniques similar to  #TAUTHOR_TAG']",1
"['of previous aggregation components  #TAUTHOR_TAG, although the various merge operations are, to our knowledge, novel in this form']","['of previous aggregation components  #TAUTHOR_TAG, although the various merge operations are, to our knowledge, novel in this form']","['of previous aggregation components  #TAUTHOR_TAG, although the various merge operations are, to our knowledge, novel in this form']","['', 'a strength of our approach is the ability to use a very simple spg, as we explain below.', 'the basis of our spg is a set of clausecombining operations that incrementally transform a list of elementary predicate - argument representations ( the dsyntss corresponding to elementary speech acts, in our case ) into a single lexico - structural representation, by combining these representations using the following combining operations.', 'examples can be found in figure adjective.', 'this transforms a predicative use of an adjective into an adnominal construction.', 'period.', 'joins two complete clauses with a period.', 'these operations are not domain - specific and are similar to those of previous aggregation components  #TAUTHOR_TAG, although the various merge operations are, to our knowledge, novel in this form']",0
"['by  #AUTHOR_TAG,  #AUTHOR_TAG, or  #TAUTHOR_TAG are similar, but do not ( always ) explicitly represent']","['by  #AUTHOR_TAG,  #AUTHOR_TAG, or  #TAUTHOR_TAG are similar, but do not ( always ) explicitly represent']","['by  #AUTHOR_TAG,  #AUTHOR_TAG, or  #TAUTHOR_TAG are similar, but do not ( always ) explicitly represent the clause combining operations as labeled nodes.', '4 shows the result of applying the soft - merge operation when args 1 and 2 are implicit confirmations of']","['', 'figure 2 shows some of the realizations of alternative sentence plans generated by our spg for utterance sys - 3 the sp - tree is inspired by  #AUTHOR_TAG.', 'the representations used by  #AUTHOR_TAG,  #AUTHOR_TAG, or  #TAUTHOR_TAG are similar, but do not ( always ) explicitly represent the clause combining operations as labeled nodes.', '4 shows the result of applying the soft - merge operation when args 1 and 2 are implicit confirmations of the origin and destination cities. figure 8 illustrates the relationship between the sp - tree and the dsynts for alternative 8.', 'the labels and arrows show the dsyntss associated with each node in the sp - tree ( in figure 7 ), and the diagram also shows how structures are composed into larger structures by the clause combining operations.', 'the complexity of most sentence planners arises from the attempt to encode constraints on the application of, and ordering of, the operations, in order to generate a single high quality sentence plan.', 'in our approach, we do not need to encode such constraints.', 'rather, we generate a random sample of possible sentence plans for each text plan, up to a pre - specified maximum number of sentence plans, by randomly selecting among the operations according to some probability distribution']",0
"['##the algorithm was implemented by the the authors, following the description in  #TAUTHOR_TAG']","['##the algorithm was implemented by the the authors, following the description in  #TAUTHOR_TAG']","['##the algorithm was implemented by the the authors, following the description in  #TAUTHOR_TAG']","['##the algorithm was implemented by the the authors, following the description in  #TAUTHOR_TAG']",5
['ica system  #TAUTHOR_TAG'],['ica system  #TAUTHOR_TAG'],"['paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation - based learner, and the ica system  #TAUTHOR_TAG']","['paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation - based learner, and the ica system  #TAUTHOR_TAG']",1
"['data used in the experiment was selected from the penn treebank wall street journal, and is the same used by  #TAUTHOR_TAG']","['data used in the experiment was selected from the penn treebank wall street journal, and is the same used by  #TAUTHOR_TAG']","['data used in the experiment was selected from the penn treebank wall street journal, and is the same used by  #TAUTHOR_TAG']","['data used in the experiment was selected from the penn treebank wall street journal, and is the same used by  #TAUTHOR_TAG']",5
"["", which makes extensive use of indexes to speed up the rules'update ; a the fasttbl algorithm ; a the ica algorithm  #TAUTHOR_TAG""]","[""described in section 2 ; a an improved version of tbl, which makes extensive use of indexes to speed up the rules'update ; a the fasttbl algorithm ; a the ica algorithm  #TAUTHOR_TAG""]","["", which makes extensive use of indexes to speed up the rules'update ; a the fasttbl algorithm ; a the ica algorithm  #TAUTHOR_TAG""]","[""## the regular tbl, as described in section 2 ; a an improved version of tbl, which makes extensive use of indexes to speed up the rules'update ; a the fasttbl algorithm ; a the ica algorithm  #TAUTHOR_TAG""]",1
['ica system  #TAUTHOR_TAG aims to reduce the training time by introducing independence assumptions on the training samples that dramatically'],['ica system  #TAUTHOR_TAG aims to reduce the training time by introducing independence assumptions on the training samples that dramatically'],['ica system  #TAUTHOR_TAG aims to reduce the training time by introducing independence assumptions on the training samples that dramatically'],['ica system  #TAUTHOR_TAG aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance'],0
"['the tree - cut technique described above, our previous work  #TAUTHOR_TAG extracted systematic polysemy from wordnet.', 'in this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method']","['the tree - cut technique described above, our previous work  #TAUTHOR_TAG extracted systematic polysemy from wordnet.', 'in this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method']","['the tree - cut technique described above, our previous work  #TAUTHOR_TAG extracted systematic polysemy from wordnet.', 'in this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method']","['the tree - cut technique described above, our previous work  #TAUTHOR_TAG extracted systematic polysemy from wordnet.', 'in this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method']",2
"[' #TAUTHOR_TAG, we']","[' #TAUTHOR_TAG, we']","[' #TAUTHOR_TAG, we applied this method to a small subset']","['this paper, we describes a lexicon organized around systematic polysemy.', 'the lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree - cut  #AUTHOR_TAG.', 'in our previous work  #TAUTHOR_TAG, we applied this method to a small subset of wordnet nouns and showed potential applicability.', 'in the current work, we applied the method to all nouns and verbs in wordnet, and built a lexicon in which word senses are partitioned by systematic polysemy.', 'we report results of comparing our lexicon with the wordnet cousins as well as the inter - annotator disagreement observed between two semantically annotated corpora : wordnet semcor  #AUTHOR_TAG and dso  #AUTHOR_TAG.', 'the results are quite promising : our extraction method discovered 89 % of the wordnet cousins, and the sense partitions in our lexicon yielded better values  #AUTHOR_TAG than arbitrary sense groupings on the agreement data']",2
"['our previous strategy  #TAUTHOR_TAG.', 'rather than using a termdocument matrix,']","['our previous strategy  #TAUTHOR_TAG.', 'rather than using a termdocument matrix,']","['order to obtain semantic representations of each word, we apply our previous strategy  #TAUTHOR_TAG.', 'rather than using a termdocument matrix, we had followed an approach akin to that']","['order to obtain semantic representations of each word, we apply our previous strategy  #TAUTHOR_TAG.', 'rather than using a termdocument matrix, we had followed an approach akin to that of schutze ( 1993 ), who performed svd on a nx2n term - term matrix.', 'the n here represents the n - 1 most - frequent words as well as a glob position to account for all other words not in the top n - 1.', ""the matrix is structured such that for a given word w's row, the first n columns denote words that - ncs ( [UNK], 1""]",2
"['( forbes -  #AUTHOR_TAG a ;  #TAUTHOR_TAG,']","['( forbes -  #AUTHOR_TAG a ;  #TAUTHOR_TAG,']","['prior uncertainty detection experiments ( forbes -  #AUTHOR_TAG a ;  #TAUTHOR_TAG,']","['train our dise model, we first extracted the set of speech and dialogue features shown in figure 2 from the user turns in our corpus.', 'as shown, the acoustic - prosodic features represent duration, pausing, pitch, and energy, and were normalized by the first user turn, as well as totaled and averaged over each dialogue.', 'the lexical and dialogue features consist of the current dialogue name ( i. e., one of the six physics problems ) and turn number, the current itspoke question\'s name ( e. g., t 3 in figure 1 has a unique identifier ) and depth in the discourse structure ( e. g., an itspoke remediation question after an incorrect user answer would be at one greater depth than the prior question ), a word occurrence vector for the automatically recognized text of the user turn, an automatic ( in ) correctness label, and lastly, the number of user turns since the last correct turn ( "" incorrect runs "" ).', 'we also included two user - based features, gender and pretest score.', 'note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( forbes -  #AUTHOR_TAG a ;  #TAUTHOR_TAG, we have also experimented with other features, including state - of - theart acoustic - prosodic features used in the last interspeech challenges  #AUTHOR_TAG b ) and made freely available in the opensmile toolkit  #AUTHOR_TAG.', 'to date, however, these features have only decreased the crossvalidation performance of our models.', '8 while some of our features are tutoring - specific, these have similar counterparts in other applications ( i. e., answer ( in ) correctness corresponds to a more general notion of "" response appropriateness "" in other domains, while pretest score corresponds to the general notion of domain expertise ).', 'moreover, all of our features are fully automatic and available in real - time, so that the model can be directly implemented and deployed.', 'to that end, we now describe the results of our intrinsic and extrinsic evaluations of our dise model, aimed at determining whether it is ready to be evaluated with real users']",2
['bilingual stochastic grammars  #TAUTHOR_TAG'],['bilingual stochastic grammars  #TAUTHOR_TAG'],"['automatic translation  #AUTHOR_TAG, as have been bilingual stochastic grammars  #TAUTHOR_TAG']","['state transducers, which can be learned from bilingual corpora, have been proposed for automatic translation  #AUTHOR_TAG, as have been bilingual stochastic grammars  #TAUTHOR_TAG']",0
['also shows the structural identity to bilingual grammars as used in  #TAUTHOR_TAG'],['also shows the structural identity to bilingual grammars as used in  #TAUTHOR_TAG'],['also shows the structural identity to bilingual grammars as used in  #TAUTHOR_TAG'],['also shows the structural identity to bilingual grammars as used in  #TAUTHOR_TAG'],5
['can be shown  #TAUTHOR_TAG that the use of this model with maximum likelihood parameter estimation is justified on information - theoretic grounds'],['can be shown  #TAUTHOR_TAG that the use of this model with maximum likelihood parameter estimation is justified on information - theoretic grounds'],['can be shown  #TAUTHOR_TAG that the use of this model with maximum likelihood parameter estimation is justified on information - theoretic grounds'],['can be shown  #TAUTHOR_TAG that the use of this model with maximum likelihood parameter estimation is justified on information - theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values. 3 there is no requirement that the components of f represent disjoint or statistically independent events'],4
['describe an efficient algorithm for accomplish'],['describe an efficient algorithm for accomplishing this in which approximations to pst ( tis ) are'],['describe an efficient algorithm for accomplish'],['describe an efficient algorithm for accomplishing this in which approximations to pst ( tis ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t'],0
[') modeling  #TAUTHOR_TAG'],['memd ) modeling  #TAUTHOR_TAG'],[') modeling  #TAUTHOR_TAG'],['statistical technique which has recently become popular for nlp is maximum entropy / minimum divergence ( memd ) modeling  #TAUTHOR_TAG'],5
"[' #TAUTHOR_TAG show, lexical information improves on np and vp chunking as well']","[' #TAUTHOR_TAG show, lexical information improves on np and vp chunking as well']","[' #TAUTHOR_TAG show, lexical information improves on np and vp chunking as well']","[' #TAUTHOR_TAG show, lexical information improves on np and vp chunking as well']",3
"['tested on section 23 ( table 1 ), same as used by  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, and became a common testbed']","['tested on section 23 ( table 1 ), same as used by  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, and became a common testbed']","['tested on section 23 ( table 1 ), same as used by  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, and became a common testbed']","['system was trained on the penn treebank  #AUTHOR_TAG wsj sections 221 and tested on section 23 ( table 1 ), same as used by  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, and became a common testbed']",1
"[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG )']","['learning to produce a full parse of free - text sentences ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG )']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG )']",0
"['is not aimed at handling dependencies, which require heavy use of lexical information  #TAUTHOR_TAG, for pp attachment )']","['is not aimed at handling dependencies, which require heavy use of lexical information  #TAUTHOR_TAG, for pp attachment )']","['is not aimed at handling dependencies, which require heavy use of lexical information  #TAUTHOR_TAG, for pp attachment )']","['is not aimed at handling dependencies, which require heavy use of lexical information  #TAUTHOR_TAG, for pp attachment )']",1
['approach for partial parsing was presented by  #TAUTHOR_TAG'],['approach for partial parsing was presented by  #TAUTHOR_TAG'],['approach for partial parsing was presented by  #TAUTHOR_TAG'],['approach for partial parsing was presented by  #TAUTHOR_TAG'],0
"[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG )']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG )']",0
"['of full parsers, e. g.,  #TAUTHOR_TAG as']","['of full parsers, e. g.,  #TAUTHOR_TAG as']","['of full parsers, e. g.,  #TAUTHOR_TAG as']","['results are lower than those of full parsers, e. g.,  #TAUTHOR_TAG as might be expected since much less structural data, and no lexical data are being used']",1
"['a similar vain to  #AUTHOR_TAG and  #TAUTHOR_TAG, the method extends an existing flat shallow - parsing method to handle composite structures']","['a similar vain to  #AUTHOR_TAG and  #TAUTHOR_TAG, the method extends an existing flat shallow - parsing method to handle composite structures']","['a similar vain to  #AUTHOR_TAG and  #TAUTHOR_TAG, the method extends an existing flat shallow - parsing method to handle composite structures']","['a similar vain to  #AUTHOR_TAG and  #TAUTHOR_TAG, the method extends an existing flat shallow - parsing method to handle composite structures']",3
"[',  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g.,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG )']","[',  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g.,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG )']",0
"['tested on section 23 ( table 1 ), same as used by  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG, and became a common testbed']","['tested on section 23 ( table 1 ), same as used by  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG, and became a common testbed']","['tested on section 23 ( table 1 ), same as used by  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG, and became a common testbed']","['system was trained on the penn treebank  #AUTHOR_TAG wsj sections 221 and tested on section 23 ( table 1 ), same as used by  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG, and became a common testbed']",1
"['tested on section 23 ( table 1 ), same as used by  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG, and became a common testbed']","['tested on section 23 ( table 1 ), same as used by  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG, and became a common testbed']","['tested on section 23 ( table 1 ), same as used by  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG, and became a common testbed']","['system was trained on the penn treebank  #AUTHOR_TAG wsj sections 221 and tested on section 23 ( table 1 ), same as used by  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG, and became a common testbed']",1
"['approach to partial parsing was presented by  #TAUTHOR_TAG, who extended a shallow - parsing technique to partial parsing']","['approach to partial parsing was presented by  #TAUTHOR_TAG, who extended a shallow - parsing technique to partial parsing']","['approach to partial parsing was presented by  #TAUTHOR_TAG, who extended a shallow - parsing technique to partial parsing']","['approach to partial parsing was presented by  #TAUTHOR_TAG, who extended a shallow - parsing technique to partial parsing']",0
['a similar vain to  #TAUTHOR_TAG'],['a similar vain to  #TAUTHOR_TAG'],['a similar vain to  #TAUTHOR_TAG'],"['a similar vain to  #TAUTHOR_TAG and  #AUTHOR_TAG, the method extends an existing flat shallow - parsing method to handle composite structures']",3
"['individual adjectives involved.', 'to quantify the relative strengths of these transitive inferences,  #TAUTHOR_TAG propose to']","['individual adjectives involved.', 'to quantify the relative strengths of these transitive inferences,  #TAUTHOR_TAG propose to']","['the individual adjectives involved.', 'to quantify the relative strengths of these transitive inferences,  #TAUTHOR_TAG propose to']","[', the evidence for the first order is quite a bit stronger than the evidence for the second.', 'the first ordered pairs are more frequent, as are the individual adjectives involved.', 'to quantify the relative strengths of these transitive inferences,  #TAUTHOR_TAG propose to assign a weight to each link.', 'say the order a, b occurs m times and the pair { a, b } occurs n times in total.', 'then the weight of the pair a  b is']",0
"['', 'more generally, distributional clustering techniques ( sch [UNK] utze, 1992 ;  #TAUTHOR_TAG could be applied to extract semantic classes from the corpus itself']","['limited vocabulary for which semantic information would be available.', 'more generally, distributional clustering techniques ( sch [UNK] utze, 1992 ;  #TAUTHOR_TAG could be applied to extract semantic classes from the corpus itself']","['', 'more generally, distributional clustering techniques ( sch [UNK] utze, 1992 ;  #TAUTHOR_TAG could be applied to extract semantic classes from the corpus itself.', 'since the constraints on adjective ordering in english depend largely on semantic classes, the addition of semantic information to the model ought to improve the results']","['', 'first, while semantic information is not available for all adjectives, it is clearly available for some.', 'furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available.', 'more generally, distributional clustering techniques ( sch [UNK] utze, 1992 ;  #TAUTHOR_TAG could be applied to extract semantic classes from the corpus itself.', 'since the constraints on adjective ordering in english depend largely on semantic classes, the addition of semantic information to the model ought to improve the results']",3
"[' #TAUTHOR_TAG call the direct evidence method.', 'to order the pair { a, b }, count how many times the ordered sequences']","[' #TAUTHOR_TAG call the direct evidence method.', 'to order the pair { a, b }, count how many times the ordered sequences a, b and b, a appear in the training data and output the pair in the order which occurred more often']","[' #TAUTHOR_TAG call the direct evidence method.', 'to order the pair { a, b }, count how many times the ordered sequences']","['simplest strategy for ordering adjectives is what  #TAUTHOR_TAG call the direct evidence method.', 'to order the pair { a, b }, count how many times the ordered sequences a, b and b, a appear in the training data and output the pair in the order which occurred more often']",0
"[', boosting  #TAUTHOR_TAG offers the possibility of achieving high']","[', boosting  #TAUTHOR_TAG offers the possibility of achieving high']","[', boosting  #TAUTHOR_TAG offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly']","['second area where the methods described here could be improved is in the way that multiple information sources are integrated.', 'the technique method described in section 3. 7 is a fairly crude method for combining frequency information with symbolic data.', 'it would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature  #AUTHOR_TAG.', 'in particular, boosting  #TAUTHOR_TAG offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly']",3
"['randomly assigned.', ' #TAUTHOR_TAG propose to generalize the direct evidence method so that']","['randomly assigned.', ' #TAUTHOR_TAG propose to generalize the direct evidence method so that']","['assigned.', ' #TAUTHOR_TAG propose to generalize the direct evidence method so that']","['way to think of the direct evidence method is to see that it defines a relation  on the set of english adjectives.', 'given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a, then a  b.', 'if the re - verse is true, and b, a is found more often than a, b, then b  a.', 'if neither order appears in the training data, then neither a  b nor b  a and an order must be randomly assigned.', ' #TAUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation.', '']",0
"[""by the ` nitrogen'generator  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ), takes advantage of standard statistical techniques by generating a lattice of all possible strings given""]","[""by the ` nitrogen'generator  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ), takes advantage of standard statistical techniques by generating a lattice of all possible strings given""]","[""by the ` nitrogen'generator  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ), takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as""]","['problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""one approach to this more general problem, taken by the ` nitrogen'generator  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ), takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model."", 'langkilde and knight report that this strategy yields good results for problems like generating verb / object collocations and for selecting the correct morphological form of a word.', 'it also should be straightforwardly applicable to the more specific problem we are addressing here.', 'to determine the correct order for a sequence of prenominal adjectives, we can simply generate all possible orderings and choose the one with the highest probability.', 'this has the advantage of reducing the problem of adjective ordering to the problem of estimating n - gram probabilities, something which is relatively well understood']",5
"['speech recognition  #AUTHOR_TAG and machine translation  #TAUTHOR_TAG.', 'moreover, once the models']","['speech recognition  #AUTHOR_TAG and machine translation  #TAUTHOR_TAG.', 'moreover, once the models']","['speech recognition  #AUTHOR_TAG and machine translation  #TAUTHOR_TAG.', 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply']","['availability of toolkits for this weighted case  #AUTHOR_TAG van  #AUTHOR_TAG promises to unify much of statistical nlp.', ""such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding,'including classic models for speech recognition  #AUTHOR_TAG and machine translation  #TAUTHOR_TAG."", 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']",0
"['weights, but this paper uses "" weight "" to mean something else.', '[UNK] on demand  #TAUTHOR_TAG can pay off here, since only part of [UNK] may be needed subsequently.']","['weights, but this paper uses "" weight "" to mean something else.', '[UNK] on demand  #TAUTHOR_TAG can pay off here, since only part of [UNK] may be needed subsequently.']","['weights, but this paper uses "" weight "" to mean something else.', '[UNK] on demand  #TAUTHOR_TAG can pay off here, since only part of [UNK] may be needed subsequently.']","['traditionally log ( strength ) values are called weights, but this paper uses "" weight "" to mean something else.', '[UNK] on demand  #TAUTHOR_TAG can pay off here, since only part of [UNK] may be needed subsequently.']",0
"['(  p (  ),  p (  ) val (  ) )  r 0  v, from which ( 1 ) is trivial to compute.', ' #TAUTHOR_TAG give a sufficiently general finite - state']","['(  p (  ),  p (  ) val (  ) )  r 0  v, from which ( 1 ) is trivial to compute.', ' #TAUTHOR_TAG give a sufficiently general finite - state']","['(  p (  ),  p (  ) val (  ) )  r 0  v, from which ( 1 ) is trivial to compute.', ' #TAUTHOR_TAG give a sufficiently general finite - state framework to allow this : weights may fall in any set k ( instead of r ).', 'multiplication and addition are replaced by binary operations  and  on k. thus  is used to combine arc weights into a path weight and  is used to combine the weights of alternative paths.', 'to sum over infinite sets of cyclic paths we also need a closure operation *, interpreted as k * =  i = 0 k i.', 'the usual finite - state algorithms work if ( k, , , * ) has the structure']","['denominator of equation ( 1 ) is the total probability of all accepting paths in x i  f  y i.', 'but while computing this, we will also compute the numerator.', 'the idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability.', 'we will enforce an invariant : the weight of any pathset  must be (  p (  ),  p (  ) val (  ) )  r 0  v, from which ( 1 ) is trivial to compute.', ' #TAUTHOR_TAG give a sufficiently general finite - state framework to allow this : weights may fall in any set k ( instead of r ).', 'multiplication and addition are replaced by binary operations  and  on k. thus  is used to combine arc weights into a path weight and  is used to combine the weights of alternative paths.', 'to sum over infinite sets of cyclic paths we also need a closure operation *, interpreted as k * =  i = 0 k i.', 'the usual finite - state algorithms work if ( k, , , * ) has the structure of a closed semiring. 15', 'rdinary probabilities fall in the semiring ( r 0, +, , * ). 16', 'our novel weights fall in a novel 14 formal derivation of ( 1 )']",5
"['algorithm  #TAUTHOR_TAG.', 'but notice that it has no backward pass.', 'in place of pushing cumulative probabilities backward to']","['algorithm  #TAUTHOR_TAG.', 'but notice that it has no backward pass.', 'in place of pushing cumulative probabilities backward to']",[' #TAUTHOR_TAG'],"['in many cases of interest, t i is an acyclic graph. 20', ""hen tarjan's method computes w 0j for each j in topologically sorted order, thereby finding t i in a linear number of  and  operations."", 'for hmms ( footnote 11 ), ti is the familiar trellis, and we would like this computation of ti to reduce to the forwardbackward algorithm  #TAUTHOR_TAG.', 'but notice that it has no backward pass.', 'in place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs ( more generally, values in v ) forward to the probabilities.', 'this is slower because our  and  are vector operations, and the vectors rapidly lose sparsity as they are added together']",0
['availability of toolkits for this weighted case  #TAUTHOR_TAG ; van  #AUTHOR_TAG promises to'],['availability of toolkits for this weighted case  #TAUTHOR_TAG ; van  #AUTHOR_TAG promises to'],['availability of toolkits for this weighted case  #TAUTHOR_TAG ; van  #AUTHOR_TAG promises to'],"['availability of toolkits for this weighted case  #TAUTHOR_TAG ; van  #AUTHOR_TAG promises to unify much of statistical nlp.', 'such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding, 1 including classic models for speech recognition  #AUTHOR_TAG and machine translation ( knight and al -  #AUTHOR_TAG.', 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']",0
"['- backward algorithm  #AUTHOR_TAG trains only hidden markov models, while  #TAUTHOR_TAG trains only stochastic edit distance']","['are limited to particular kinds of models and training regimens.', 'for example, the forward - backward algorithm  #AUTHOR_TAG trains only hidden markov models, while  #TAUTHOR_TAG trains only stochastic edit distance']","['they are limited to particular kinds of models and training regimens.', 'for example, the forward - backward algorithm  #AUTHOR_TAG trains only hidden markov models, while  #TAUTHOR_TAG trains only stochastic edit distance']","[', there is a stumbling block : where do the weights come from?', 'after all, statistical models require supervised or unsupervised training.', 'currently, finite - state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'for example, the forward - backward algorithm  #AUTHOR_TAG trains only hidden markov models, while  #TAUTHOR_TAG trains only stochastic edit distance']",0
"['nm ) time, and faster approximations  #TAUTHOR_TAG']","['nm ) time, and faster approximations  #TAUTHOR_TAG']","['nm ) time, and faster approximations  #TAUTHOR_TAG']","['therefore reintroduce a backward pass that lets us avoid  and  when computing t i ( so they are needed only to construct t i ).', 'this speedup also works for cyclic graphs and for any v.', 'write w jk as ( p jk, v jk ), and let w 1 jk = ( p 1 jk, v 1 jk ) denote the weight of the edge from j to k. 19 then it can be shown that w 0n = ( p 0n, j, k p 0j v 1 jk p kn ).', 'the forward and backward probabilities, p0j and pkn, can be computed using single - source algebraic path for the simpler semiring ( r, +, x, a ) - - or equivalently, by solving a sparse linear system of equations over r, a much - studied problem at o ( n ) space, o ( nm ) time, and faster approximations  #TAUTHOR_TAG']",0
"['the well - known kleene - sch [UNK] utzenberger construction  #TAUTHOR_TAG, taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'a full proof is straightforward, as are']","['the well - known kleene - sch [UNK] utzenberger construction  #TAUTHOR_TAG, taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'a full proof is straightforward, as are']","['the well - known kleene - sch [UNK] utzenberger construction  #TAUTHOR_TAG, taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'a full proof is straightforward, as are']","['##to prove ( 1 ) a ( 3 ), express f as an fst and apply the well - known kleene - sch [UNK] utzenberger construction  #TAUTHOR_TAG, taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'a full proof is straightforward, as are proofs of ( 3 ) _ ( 2 ), ( 2 ) _ ( 1 )']",5
"['1 vp 1 ).', 'division is commonly used in defining f  ( for normalization ). 19', 'multiple edges from j to k are summed into a single edge.', ' #AUTHOR_TAG.', 'efficient hardware implementation is also possible via chip - level parallelism  #TAUTHOR_TAG']","['and subtraction are also possible :  ( p, v ) = ( p, v ) and ( p, v ) 1 = ( p 1, p 1 vp 1 ).', 'division is commonly used in defining f  ( for normalization ). 19', 'multiple edges from j to k are summed into a single edge.', ' #AUTHOR_TAG.', 'efficient hardware implementation is also possible via chip - level parallelism  #TAUTHOR_TAG']","[', v ) and ( p, v ) 1 = ( p 1, p 1 vp 1 ).', 'division is commonly used in defining f  ( for normalization ). 19', 'multiple edges from j to k are summed into a single edge.', ' #AUTHOR_TAG.', 'efficient hardware implementation is also possible via chip - level parallelism  #TAUTHOR_TAG']","['and subtraction are also possible :  ( p, v ) = ( p, v ) and ( p, v ) 1 = ( p 1, p 1 vp 1 ).', 'division is commonly used in defining f  ( for normalization ). 19', 'multiple edges from j to k are summed into a single edge.', ' #AUTHOR_TAG.', 'efficient hardware implementation is also possible via chip - level parallelism  #TAUTHOR_TAG']",3
"[').', 'per - state joint normalization  #TAUTHOR_TAG b, a  8. 2 ) is similar but drops the dependence on a.', 'the difficult case is global conditional normalization.', 'it arises, for example, when training a joint model of the']","['chosen arc in dj, a ).', 'per - state joint normalization  #TAUTHOR_TAG b, a  8. 2 ) is similar but drops the dependence on a.', 'the difficult case is global conditional normalization.', 'it arises, for example, when training a joint model of the']","[').', 'per - state joint normalization  #TAUTHOR_TAG b, a  8. 2 ) is similar but drops the dependence on a.', 'the difficult case is global conditional normalization.', 'it arises, for example, when training a joint model of the']","['for per - state conditional normalization, let dj, a be the set of arcs from state j with input symbol a   ; their weights are normalized to sum to 1.', 'besides computing c, the e step must count the expected number dj, a of traversals of arcs in each dj, a.', 'then the predicted vector given  is j, a dj, a  ( expected feature counts on a randomly chosen arc in dj, a ).', 'per - state joint normalization  #TAUTHOR_TAG b, a  8. 2 ) is similar but drops the dependence on a.', 'the difficult case is global conditional normalization.', 'it arises, for example, when training a joint model of the']",1
"['em algorithm  #TAUTHOR_TAG can maximize these functions.', 'roughly,']","['em algorithm  #TAUTHOR_TAG can maximize these functions.', 'roughly,']","['em algorithm  #TAUTHOR_TAG can maximize these functions.', 'roughly, the e step guesses hidden information : if ( x i, y i ) was generated from the current f ,']","['em algorithm  #TAUTHOR_TAG can maximize these functions.', 'roughly, the e step guesses hidden information : if ( x i, y i ) was generated from the current f , which fst paths stand a chance of having been the path used?', '( guessing the path also guesses the exact input and output. )', 'the m step updates  to make those paths more likely.', 'em alternates these steps and converges to a local optimum.', ""the m step's form depends on the parameterization and the e step serves the m step's needs""]",5
"[' #TAUTHOR_TAG, ( 4']","[' #TAUTHOR_TAG, ( 4']","[' #TAUTHOR_TAG, ( 4']","['defining conditional relations, a good regexp language is unknown to us, but they can be defined in several other ways : ( 1 ) via fsts as in fig. 1c, ( 2 ) by compilation of weighted rewrite rules  #AUTHOR_TAG, ( 3 ) by compilation of decision trees  #TAUTHOR_TAG, ( 4 ) as a relation that performs contextual left - to - right replacement of input substrings by a smaller conditional relation ( gerdemann and van  #AUTHOR_TAG, 5 ( 5 ) by conditionalization of a joint relation as discussed below']",0
"['- known algebraic path problem  #TAUTHOR_TAG ; tar an,']","['for some important remarks on efficiency : a computing ti is an instance of the well - known algebraic path problem  #TAUTHOR_TAG ; tar an, 1981a ). then ti is']","['for some important remarks on efficiency : a computing ti is an instance of the well - known algebraic path problem  #TAUTHOR_TAG ; tar an,']","['for some important remarks on efficiency : a computing ti is an instance of the well - known algebraic path problem  #TAUTHOR_TAG ; tar an, 1981a ). then ti is the total semiring weight w0n of paths in ti from initial state 0 to final state n ( assumed wlog to be unique and un - weighted ).', 'it is wasteful to compute ti as suggested earlier, by minimizing ( _ _ xi ) _ f _ ( yi _ _ ), since then the real work is done by an _ - closure step  #AUTHOR_TAG that implements the all - pairs version of algebraic path, whereas all we need is the single - source version.', 'if n and m are the number of states and edges, 19 then both problems are o ( n3 ) in the worst case, but the single - source version can be solved in essentially o ( m ) time for acyclic graphs and other reducible flow graphs  #AUTHOR_TAG b ).', 'for a general graph  #AUTHOR_TAG b ) shows how to partition into hard subgraphs that localize the cyclicity or irreducibility, then run the o ( n3 ) algorithm on each subgraph ( thereby reducing n to as little as 1 ), and recombine the results.', 'the overhead of partitioning and recombining is essentially only o ( m )']",0
"['desired.', 'such approaches have been tried recently in restricted cases ( mc  #AUTHOR_TAG b ;  #TAUTHOR_TAG']","['desired.', 'such approaches have been tried recently in restricted cases ( mc  #AUTHOR_TAG b ;  #TAUTHOR_TAG']","['desired.', 'such approaches have been tried recently in restricted cases ( mc  #AUTHOR_TAG b ;  #TAUTHOR_TAG']","[': e , and a : ae  share a contextual "" vowel - fronting "" feature, then their weights rise and fall together with the strength of that feature.', 'the resulting machine must be normalized, either per - state or globally, to obtain a joint or a conditional distribution as desired.', 'such approaches have been tried recently in restricted cases ( mc  #AUTHOR_TAG b ;  #TAUTHOR_TAG']",0
['feature selection and avoid overfitting  #TAUTHOR_TAG'],['feature selection and avoid overfitting  #TAUTHOR_TAG'],['feature selection and avoid overfitting  #TAUTHOR_TAG'],"['- posterior estimation tries to maximize p (  )  i f  ( x i, y i ) where p (  ) is a prior probability.', 'in a log - linear parameterization, for example, a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting  #TAUTHOR_TAG']",5
"['_ ( x, y ) ; the goal is to recover the true _.', 'samples need not be fully observed ( partly supervised training ) : thus xi _ _ _, yi _ _ may be given as regular sets in which input and output were observed to fall.', 'for example, in ordinary hmm training, xi = e * and represents a completely hidden state sequence ( cfxxx  #TAUTHOR_TAG, who allows any regular set ), while yi is a single string representing a completely observed emission sequence.']","['training data we are given a set of observed ( input, output ) pairs, ( xi, yi ).', 'these are assumed to be independent random samples from a joint dis - tribution of the form f _ ( x, y ) ; the goal is to recover the true _.', 'samples need not be fully observed ( partly supervised training ) : thus xi _ _ _, yi _ _ may be given as regular sets in which input and output were observed to fall.', 'for example, in ordinary hmm training, xi = e * and represents a completely hidden state sequence ( cfxxx  #TAUTHOR_TAG, who allows any regular set ), while yi is a single string representing a completely observed emission sequence.']","['_ ( x, y ) ; the goal is to recover the true _.', 'samples need not be fully observed ( partly supervised training ) : thus xi _ _ _, yi _ _ may be given as regular sets in which input and output were observed to fall.', 'for example, in ordinary hmm training, xi = e * and represents a completely hidden state sequence ( cfxxx  #TAUTHOR_TAG, who allows any regular set ), while yi is a single string representing a completely observed emission sequence.']","['training data we are given a set of observed ( input, output ) pairs, ( xi, yi ).', 'these are assumed to be independent random samples from a joint dis - tribution of the form f _ ( x, y ) ; the goal is to recover the true _.', 'samples need not be fully observed ( partly supervised training ) : thus xi _ _ _, yi _ _ may be given as regular sets in which input and output were observed to fall.', 'for example, in ordinary hmm training, xi = e * and represents a completely hidden state sequence ( cfxxx  #TAUTHOR_TAG, who allows any regular set ), while yi is a single string representing a completely observed emission sequence.']",0
['speech recognition  #TAUTHOR_TAG and machine'],['speech recognition  #TAUTHOR_TAG and machine'],"['speech recognition  #TAUTHOR_TAG and machine translation ( knight and al -  #AUTHOR_TAG.', 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply']","['availability of toolkits for this weighted case  #AUTHOR_TAG van  #AUTHOR_TAG promises to unify much of statistical nlp.', ""such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding,'including classic models for speech recognition  #TAUTHOR_TAG and machine translation ( knight and al -  #AUTHOR_TAG."", 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']",0
['by an c - closure step  #TAUTHOR_TAG that implements'],['by an c - closure step  #TAUTHOR_TAG that implements'],"['ti as suggested earlier, by minimizing ( cxxi ) of o ( yixe ), since then the real work is done by an c - closure step  #TAUTHOR_TAG that implements the all - pairs version']","['', 'then t i is the total semiring weight w 0n of paths in t i from initial state 0 to final state n ( assumed wlog to be unique and unweighted ).', 'it is wasteful to compute ti as suggested earlier, by minimizing ( cxxi ) of o ( yixe ), since then the real work is done by an c - closure step  #TAUTHOR_TAG that implements the all - pairs version of algebraic path, whereas all we need is the single - source version.', 'if n and m are the number of states and edges, 19 then both problems are o ( n 3 ) in the worst case, but the single - source version can be solved in essentially o ( m ) time for acyclic graphs and other reducible flow graphs  #AUTHOR_TAG b ).', 'for a general graph t i,  #AUTHOR_TAG b ) shows how to partition into "" hard "" subgraphs that localize the cyclicity or irreducibility, then run the o ( n 3 ) algorithm on each subgraph ( thereby reducing n to as little as 1 ), and recombine the results.', 'the overhead of partitioning and recombining is essentially only o ( m )']",0
"['weighted relation.', 'undesirable consequences of this fact have been termed "" label bias ""  #TAUTHOR_TAG.', 'also, in']","['weighted relation.', 'undesirable consequences of this fact have been termed "" label bias ""  #TAUTHOR_TAG.', 'also, in']","['weighted relation.', 'undesirable consequences of this fact have been termed "" label bias ""  #TAUTHOR_TAG.', 'also, in the conditional case such per - state normalization is only correct if all states accept all input suffixes ( since']","['an easy approach is to normalize the options at each state to make the fst markovian.', 'unfortunately, the result may differ for equivalent fsts that express the same weighted relation.', 'undesirable consequences of this fact have been termed "" label bias ""  #TAUTHOR_TAG.', 'also, in the conditional case such per - state normalization is only correct if all states accept all input suffixes ( since "" dead ends "" leak probability mass ). 8', 'a better - founded approach is global normalization, which simply divides each f ( x, y )']",0
"['brief version of this work, with some additional material, first appeared as  #TAUTHOR_TAG a ).', 'a leisurely journal - length version with more details has been prepared and is available']","['brief version of this work, with some additional material, first appeared as  #TAUTHOR_TAG a ).', 'a leisurely journal - length version with more details has been prepared and is available']","['brief version of this work, with some additional material, first appeared as  #TAUTHOR_TAG a ).', 'a leisurely journal - length version with more details has been prepared and is available']","['brief version of this work, with some additional material, first appeared as  #TAUTHOR_TAG a ).', 'a leisurely journal - length version with more details has been prepared and is available']",2
"['##s  #TAUTHOR_TAG, or to extend the idea, weighted fsts']","['approximate pcfgs  #TAUTHOR_TAG, or to extend the idea, weighted fsts']","['approximate pcfgs  #TAUTHOR_TAG, or to extend the idea, weighted fsts']","[', x, y p ( v | w ) p ( w, x ) p ( y | x ) p ( z | y ), implemented by composing 4 machines. 6, 7', 'here are also procedures for defining weighted fsts that are not probabilistic  #AUTHOR_TAG.', 'arbitrary weights such as 2. 7 may be assigned to arcs or sprinkled through a regexp ( to be compiled into : / 2. 7  arcs ).', 'a more subtle example is weighted fsas that approximate pcfgs  #TAUTHOR_TAG, or to extend the idea, weighted fsts that approximate joint or conditional synchronous pcfgs built for translation.', ""these are parameterized by the pcfg's parameters, but add or remove strings of the pcfg to leave an improper probability distribution""]",0
"[' #TAUTHOR_TAG, ( 3']","[' #TAUTHOR_TAG, ( 3']","[' #TAUTHOR_TAG, ( 3']","['defining conditional relations, a good regexp language is unknown to us, but they can be defined in several other ways : ( 1 ) via fsts as in fig. 1c, ( 2 ) by compilation of weighted rewrite rules  #TAUTHOR_TAG, ( 3 ) by compilation of decision trees  #AUTHOR_TAG, ( 4 ) as a relation that performs contextual left - to - right replacement of input substrings by a smaller conditional relation ( gerdemann and van  #AUTHOR_TAG, 5 ( 5 ) by conditionalization of a joint relation as discussed below']",0
"['desired.', 'such approaches have been tried recently in restricted cases  #TAUTHOR_TAG b ;  #AUTHOR_TAG']","['desired.', 'such approaches have been tried recently in restricted cases  #TAUTHOR_TAG b ;  #AUTHOR_TAG']","['desired.', 'such approaches have been tried recently in restricted cases  #TAUTHOR_TAG b ;  #AUTHOR_TAG']","[': e , and a : ae  share a contextual "" vowel - fronting "" feature, then their weights rise and fall together with the strength of that feature.', 'the resulting machine must be normalized, either per - state or globally, to obtain a joint or a conditional distribution as desired.', 'such approaches have been tried recently in restricted cases  #TAUTHOR_TAG b ;  #AUTHOR_TAG']",0
"['##s  #TAUTHOR_TAG, or to extend the idea, weighted fsts']","['approximate pcfgs  #TAUTHOR_TAG, or to extend the idea, weighted fsts']","['approximate pcfgs  #TAUTHOR_TAG, or to extend the idea, weighted fsts']","[', x, y p ( v | w ) p ( w, x ) p ( y | x ) p ( z | y ), implemented by composing 4 machines. 6, 7', 'here are also procedures for defining weighted fsts that are not probabilistic  #AUTHOR_TAG.', 'arbitrary weights such as 2. 7 may be assigned to arcs or sprinkled through a regexp ( to be compiled into : / 2. 7  arcs ).', 'a more subtle example is weighted fsas that approximate pcfgs  #TAUTHOR_TAG, or to extend the idea, weighted fsts that approximate joint or conditional synchronous pcfgs built for translation.', ""these are parameterized by the pcfg's parameters, but add or remove strings of the pcfg to leave an improper probability distribution""]",0
"['- backward algorithm  #TAUTHOR_TAG trains only hidden markov models, while  #AUTHOR_TAG trains only stochastic edit distance']","['are limited to particular kinds of models and training regimens.', 'for example, the forward - backward algorithm  #TAUTHOR_TAG trains only hidden markov models, while  #AUTHOR_TAG trains only stochastic edit distance']","['they are limited to particular kinds of models and training regimens.', 'for example, the forward - backward algorithm  #TAUTHOR_TAG trains only hidden markov models, while  #AUTHOR_TAG trains only stochastic edit distance']","[', there is a stumbling block : where do the weights come from?', 'after all, statistical models require supervised or unsupervised training.', 'currently, finite - state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'for example, the forward - backward algorithm  #TAUTHOR_TAG trains only hidden markov models, while  #AUTHOR_TAG trains only stochastic edit distance']",0
"['desired.', 'such approaches have been tried recently in restricted cases  #TAUTHOR_TAG b ;  #AUTHOR_TAG']","['desired.', 'such approaches have been tried recently in restricted cases  #TAUTHOR_TAG b ;  #AUTHOR_TAG']","['desired.', 'such approaches have been tried recently in restricted cases  #TAUTHOR_TAG b ;  #AUTHOR_TAG']","[': e , and a : ae  share a contextual "" vowel - fronting "" feature, then their weights rise and fall together with the strength of that feature.', 'the resulting machine must be normalized, either per - state or globally, to obtain a joint or a conditional distribution as desired.', 'such approaches have been tried recently in restricted cases  #TAUTHOR_TAG b ;  #AUTHOR_TAG']",0
"['of total feature counts equals c, using improved iterative scaling ( della  #TAUTHOR_TAG ; chen and', 'for globally normalized, joint models, the predicted vector is ec f (  *,  * ).', 'if the log - linear probabilities are conditioned on the state and / or the input, the predicted vector is harder to describe ( though usually much easier to compute ).']","['of total feature counts equals c, using improved iterative scaling ( della  #TAUTHOR_TAG ; chen and', 'for globally normalized, joint models, the predicted vector is ec f (  *,  * ).', 'if the log - linear probabilities are conditioned on the state and / or the input, the predicted vector is harder to describe ( though usually much easier to compute ).']","['the predicted vector of total feature counts equals c, using improved iterative scaling ( della  #TAUTHOR_TAG ; chen and', 'for globally normalized, joint models, the predicted vector is ec f (  *,  * ).', 'if the log - linear probabilities are conditioned on the state and / or the input, the predicted vector is harder to describe ( though usually much easier to compute ).']","['if arc probabilities ( or even , , [UNK],  ) have loglinear parameterization, then the e step must compute c = i ec f ( x i, y i ), where ec ( x, y ) denotes the expected vector of total feature counts along a random path in f  whose ( input, output ) matches ( x, y ).', 'the m step then treats c as fixed, observed data and adjusts 0 until the predicted vector of total feature counts equals c, using improved iterative scaling ( della  #TAUTHOR_TAG ; chen and', 'for globally normalized, joint models, the predicted vector is ec f (  *,  * ).', 'if the log - linear probabilities are conditioned on the state and / or the input, the predicted vector is harder to describe ( though usually much easier to compute ).']",5
"['of one or more conditional relations as in fig. 1  #TAUTHOR_TAG.', 'the general form is illustrated by 3 conceptually, the parameters']","['of one or more conditional relations as in fig. 1  #TAUTHOR_TAG.', 'the general form is illustrated by 3 conceptually, the parameters']","['central technique is to define a joint relation as a noisy - channel model, by composing a joint relation with a cascade of one or more conditional relations as in fig. 1  #TAUTHOR_TAG.', 'the general form is illustrated by 3 conceptually, the parameters']","['central technique is to define a joint relation as a noisy - channel model, by composing a joint relation with a cascade of one or more conditional relations as in fig. 1  #TAUTHOR_TAG.', 'the general form is illustrated by 3 conceptually, the parameters represent the probabilities of reading another a (  ) ; reading another b (  ) ; transducing b to p rather than q ( [UNK] ) ; starting to transduce p to rather than x (  ). 4 to prove ( 1 )  ( 3 ), express f as an fst and apply the well - known kleene - schutzenberger construction  #AUTHOR_TAG, taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'a full proof is straightforward, as are proofs of ( 3 )  ( 2 ), ( 2 )  ( 1 )']",0
"[' #TAUTHOR_TAG,']","['11 ], refseq  #TAUTHOR_TAG,']","['[ 11 ], refseq  #TAUTHOR_TAG,']","['system utilizes several large size biological databases including three ncbi databases ( genpept [ 11 ], refseq  #TAUTHOR_TAG, and entrez gene [ 13 ] ), psd database from protein information resources ( pir ) [ 14 ], and', 'additionally, several model organism databases or nomenclature databases were used.', 'correspondences among records from these databases are identified using the rich cross - reference information provided by the iproclass database of pir [ 14 ].', 'the following provides a brief description of each of the database']",5
"[')  #TAUTHOR_TAG,']","['( sgd )  #TAUTHOR_TAG,']","[')  #TAUTHOR_TAG,']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase [ 19 ], yeast saccharomyces genome database ( sgd )  #TAUTHOR_TAG, rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
"[')  #TAUTHOR_TAG, fly']","['( mgd )  #TAUTHOR_TAG, fly flybase [ 19 ], yeast saccharomyces genome']","[')  #TAUTHOR_TAG, fly flybase [ 19 ], yeast saccharomyces genome database ( sg']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd )  #TAUTHOR_TAG, fly flybase [ 19 ], yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
"['worm - - wormbase  #TAUTHOR_TAG, human nomenclature']","['worm - - wormbase  #TAUTHOR_TAG, human nomenclature']","['worm - - wormbase  #TAUTHOR_TAG, human nomenclature database ( hugo ) [ 23 ],']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase [ 19 ], yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase  #TAUTHOR_TAG, human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
"['nlm )  #TAUTHOR_TAG.', '']","['nlm )  #TAUTHOR_TAG.', '']","['nlm )  #TAUTHOR_TAG.', 'it contains three knowledge']","['umls - - the unified medical language system ( umls ) has been developed and maintained by national library of medicine ( nlm )  #TAUTHOR_TAG.', '']",0
"['the task of managing information recorded in free text more feasible  #TAUTHOR_TAG.', 'one requirement for nlp is the']","['the task of managing information recorded in free text more feasible  #TAUTHOR_TAG.', 'one requirement for nlp is the']","['the task of managing information recorded in free text more feasible  #TAUTHOR_TAG.', 'one requirement for nlp is the ability to']","['the use of computers in storing the explosive amount of biological information, natural language processing ( nlp ) approaches have been explored to make the task of managing information recorded in free text more feasible  #TAUTHOR_TAG.', 'one requirement for nlp is the ability to accurately recognize terms that represent biological entities in free text.', 'another requirement is the ability to associate these terms with corresponding biological entities ( i. e., records in biological databases ) in order to be used by other automated systems for literature mining.', 'such task is called biological entity tagging.', 'biological entity tagging is not a trivial task because of several characteristics associated with biological entity names, namely : synonymy ( i. e., different terms refer to the same entity ), ambiguity ( i. e., one term is associated with different entities ), and coverage ( i. e., entity terms or entities are not present in databases or knowledge bases )']",0
"['inheritance in man ( omim )  #TAUTHOR_TAG, and']","['inheritance in man ( omim )  #TAUTHOR_TAG, and']","['in man ( omim )  #TAUTHOR_TAG, and']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase [ 19 ], yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim )  #TAUTHOR_TAG, and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
['##100 sequences into clusters based on the cd - hit algorithm  #TAUTHOR_TAG such that each'],['built by clustering uniref100 sequences into clusters based on the cd - hit algorithm  #TAUTHOR_TAG such that each'],['##50 are built by clustering uniref100 sequences into clusters based on the cd - hit algorithm  #TAUTHOR_TAG such that each cluster is composed of sequences that have'],"['resources - there are three databases in pir : the protein sequence database ( psd ), iproclass, and pir - nref.', 'psd database includes functionally annotated protein sequences.', 'the iproclass database is a central point for exploration of protein information, which provides summary descriptions of protein family, function and structure for all protein sequences from pir, swiss - prot, and trembl ( now uniprot ).', 'additionally, it links to over 70 biological databases in the world.', 'the pir - nref database is a comprehensive database for sequence searching and protein identification.', 'it contains non - redundant protein sequences from psd, swiss - prot, trembl, refseq, genpept, and pdb.', 'three uniref tables uniref100, uniref90 and uniref50 ) are available for download : uniref100 combines identical sequences and sub - fragments into a single uniref entry ; and uniref90 and uniref50 are built by clustering uniref100 sequences into clusters based on the cd - hit algorithm  #TAUTHOR_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity, respectively, to the representative sequence.', 'ncbi resources - three data sources from ncbi were used in this study : genpept, refseq, and entrez gene.', 'genpept entries are those translated from the genbanknucleotide sequence database.', 'refseq is a comprehensive, integrated, non - redundant set of sequences, including genomic dna, transcript ( rna ), and protein products, for major research organisms.', ""entrez gene provides a unified query environment for genes defined by sequence and / or in ncbi's map viewer."", 'it records gene names, symbols, and many other attributes associated with genes and the products they encode']",5
"[')  #TAUTHOR_TAG, worm - - wormbase [ 22 ], human nomenclature']","['( rgd )  #TAUTHOR_TAG, worm - - wormbase [ 22 ], human nomenclature']","[')  #TAUTHOR_TAG, worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ],']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase [ 19 ], yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd )  #TAUTHOR_TAG, worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
"[' #TAUTHOR_TAG, and', 'additionally, several model organism']","[' #TAUTHOR_TAG, and', 'additionally, several model organism']","[' #TAUTHOR_TAG, and', 'additionally, several model organism']","['system utilizes several large size biological databases including three ncbi databases ( genpept [ 11 ], refseq [ 12 ], and entrez gene [ 13 ] ), psd database from protein information resources ( pir )  #TAUTHOR_TAG, and', 'additionally, several model organism databases or nomenclature databases were used.', 'correspondences among records from these databases are identified using the rich cross - reference information provided by the iproclass database of pir [ 14 ].', 'the following provides a brief description of each of the database']",5
"['##base  #TAUTHOR_TAG, yeast saccharomyces genome']","['( mgd ) [ 18 ], fly flybase  #TAUTHOR_TAG, yeast saccharomyces genome']","[') [ 18 ], fly flybase  #TAUTHOR_TAG, yeast saccharomyces genome database ( sg']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase  #TAUTHOR_TAG, yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
['##num )  #TAUTHOR_TAG'],['( ecnum )  #TAUTHOR_TAG'],['##num )  #TAUTHOR_TAG'],"['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase [ 19 ], yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum )  #TAUTHOR_TAG']",5
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],['of our prior work  #TAUTHOR_TAG'],"['our experiments, we use naive bayes as the learning algorithm.', 'the knowledge sources we use include parts - of - speech, local collocations, and surrounding words.', 'these knowledge sources were effectively used to build a state - of - the - art wsd program in one of our prior work  #TAUTHOR_TAG']",2
"['( see  #AUTHOR_TAG ),  #TAUTHOR_TAG speculate that deeper linguistic']","['( see  #AUTHOR_TAG ),  #TAUTHOR_TAG speculate that deeper linguistic']","['( see  #AUTHOR_TAG ),  #TAUTHOR_TAG speculate that deeper linguistic knowledge needs to']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG ).', 'while these approaches have been reasonably successful ( see  #AUTHOR_TAG ),  #TAUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g.,  #AUTHOR_TAG ), their semantic similarity as computed using wordnet ( e. g.,  #AUTHOR_TAG ) or wikipedia  #AUTHOR_TAG, and the contextual role played by an np ( see  #AUTHOR_TAG )']",0
"['. g.,  #TAUTHOR_TAG ; and for those that do report per - forma']","['on perfect mentions ( e. g.,  #TAUTHOR_TAG ; and for those that do report per - formance on automatically']","['. g.,  #TAUTHOR_TAG ; and for those that do report per - formance on automatically']","['ace participants have also adopted a corpus - based approach to sc deter - mination that is investigated as part of the mention detection ( md ) task ( e. g.,  #AUTHOR_TAG ).', 'briefly, the goal of md is to identify the boundary of a mention, its mention type ( e. g., pronoun, name ), and its semantic type ( e. g., person, location ).', 'un - like them, ( 1 ) we do not perform the full md task, as our goal is to investigate the role of sc knowl - edge in coreference resolution ; and ( 2 ) we do not use the ace training data for acquiring our sc clas - sifier ; instead, we use the bbn entity type corpus  #AUTHOR_TAG, which consists of all the penn treebank wall street journal articles with the ace mentions manually identified and an - notated with their scs.', 'this provides us with a train - ing set that is approximately five times bigger than that of ace.', 'more importantly, the ace participants do not evaluate the role of induced sc knowledge in coreference resolution : many of them evaluate coreference performance on perfect mentions ( e. g.,  #TAUTHOR_TAG ; and for those that do report per - formance on automatically extracted mentions, they do not explain whether or how the induced sc information is used in their coreference algorithms']",1
"['.,  #AUTHOR_TAG,  #TAUTHOR_TAG']","[',  #AUTHOR_TAG,  #TAUTHOR_TAG']","['.,  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'while these approaches have been reasonably']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g.,  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'while these approaches have been reasonably successful ( see  #AUTHOR_TAG ),  #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g.,  #AUTHOR_TAG ), their semantic similarity as computed using wordnet ( e. g.,  #AUTHOR_TAG ) or wikipedia  #AUTHOR_TAG, and the contextual role played by an np ( see  #AUTHOR_TAG )']",0
"['type corpus  #TAUTHOR_TAG,']","['type corpus  #TAUTHOR_TAG,']","['acquiring our sc classifier ; instead, we use the bbn entity type corpus  #TAUTHOR_TAG,']","['ace participants have also adopted a corpus - based approach to sc determination that is investigated as part of the mention detection ( md ) task ( e. g.,  #AUTHOR_TAG ).', 'briefly, the goal of md is to identify the boundary of a mention, its mention type ( e. g., pronoun, name ), and its semantic type ( e. g., person, location ).', 'unlike them, ( 1 ) we do not perform the full md task, as our goal is to investigate the role of sc knowledge in coreference resolution ; and ( 2 ) we do not use the ace training data for acquiring our sc classifier ; instead, we use the bbn entity type corpus  #TAUTHOR_TAG, which consists of all the penn treebank wall street journal articles with the ace mentions manually identified and annotated with their scs.', 'this provides us with a training set that is approximately five times bigger than that of ace.', 'more importantly, the ace participants do not evaluate the role of induced sc knowledge in coreference resolution : many of them evaluate coreference performance on perfect mentions ( e. g.,  #AUTHOR_TAG ) ; and for those that do report per - formance on automatically extracted mentions, they do not explain whether or how the induced sc information is used in their coreference algorithms']",5
"['( see  #TAUTHOR_TAG,  #AUTHOR_TAG speculate that deeper linguistic']","['( see  #TAUTHOR_TAG,  #AUTHOR_TAG speculate that deeper linguistic']","['( see  #TAUTHOR_TAG,  #AUTHOR_TAG speculate that deeper linguistic knowledge needs to']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG ).', 'while these approaches have been reasonably successful ( see  #TAUTHOR_TAG,  #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g.,  #AUTHOR_TAG ), their semantic similarity as computed using wordnet ( e. g.,  #AUTHOR_TAG ) or wikipedia  #AUTHOR_TAG, and the contextual role played by an np ( see  #AUTHOR_TAG )']",0
['package  #TAUTHOR_TAG ) on these'],['( using the libsvm package  #TAUTHOR_TAG ) on these'],['( using the libsvm package  #TAUTHOR_TAG ) on these'],"['addition, we train an svm classifier for sc determination by combining the output of five clas - sification methods : dl, 1 - nn, me, nb, and soon et al. s method as described in the introduction, 8 with the goal of examining whether sc classifica - tion accuracy can be improved by combining the output of individual classifiers in a supervised man - ner.', 'specifically, we ( 1 ) use 80 % of the instances generated from the bbn entity type corpus to train the four classifiers ; ( 2 ) apply the four classifiers and soon et al. s method to independently make predictions for the remaining 20 % of the instances ; and ( 3 ) train an svm classifier ( using the libsvm package  #TAUTHOR_TAG ) on these 20 % of the instances, where each instance, i, is represented by a per org gpe fac loc oth training test 19. 8 9. 6 11. 4 1. 6 1. 2 56. 3 19. 5 9. 0 9. 6 1. 8 1. 1 59. 0 set of 31 binary features.', 'more specifically, let l = i li ; li2 ; li3 ; li4 ; li5 be the set of predictions that we obtained for i in step ( 2 ).', 'to represent i, we generate one feature from each non - empty subset of li']",5
"[""4 ) ne : we use bbn's identifinder  #TAUTHOR_TAG, a muc - style ne""]","[""4 ) ne : we use bbn's identifinder  #TAUTHOR_TAG, a muc - style ne""]","[""4 ) ne : we use bbn's identifinder  #TAUTHOR_TAG, a muc - style ne recognizer to""]","[""4 ) ne : we use bbn's identifinder  #TAUTHOR_TAG, a muc - style ne recognizer to determine the ne type of npz."", 'if npi is determined to be a person or organization, we create an ne feature whose value is simply its muc ne type.', 'however, if npi is determined to be a location, we create a feature with value gpe ( because most of the muc loca - tion nes are ace gpe nes ).', 'otherwise, no ne feature will be created ( because we are not interested in the other muc ne types )']",5
"[', most frequent ) wordnet sense as its sc ( e. g.,  #AUTHOR_TAG,  #TAUTHOR_TAG.', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]","['( i. e., most frequent ) wordnet sense as its sc ( e. g.,  #AUTHOR_TAG,  #TAUTHOR_TAG.', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]","[', most frequent ) wordnet sense as its sc ( e. g.,  #AUTHOR_TAG,  #TAUTHOR_TAG.', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]","['type of semantic knowledge that has been employed by coreference resolvers is the semantic class ( sc ) of an np, which can be used to disallow coreference between semantically incompatible nps.', 'however, learning - based resolvers have not been able to benefit from having an sc agreement feature, presumably because the method used to compute the sc of an np is too simplistic : while the sc of a proper name is computed fairly accurately using a named entity ( ne ) recognizer, many resolvers simply assign to a common noun the first ( i. e., most frequent ) wordnet sense as its sc ( e. g.,  #AUTHOR_TAG,  #TAUTHOR_TAG.', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]",0
"['dl ) : we use the dl learner as described in  #TAUTHOR_TAG, motivated by its']","['dl ) : we use the dl learner as described in  #TAUTHOR_TAG, motivated by its']","['dl ) : we use the dl learner as described in  #TAUTHOR_TAG, motivated by its']","['algorithms.', 'we experiment with four learners commonly employed in language learning : decision list ( dl ) : we use the dl learner as described in  #TAUTHOR_TAG, motivated by its success in the related tasks of word sense disambiguation  #AUTHOR_TAG and ne classification  #AUTHOR_TAG.', 'we apply add - one smoothing to smooth the class posteriors']",5
"[' #TAUTHOR_TAG, and ( 2 )']","[' #TAUTHOR_TAG, and ( 2 )']","['on the 97 test texts.', 'we report performance in terms of two metrics : ( 1 ) the fmeasure score as computed by the commonly - used muc scorer  #TAUTHOR_TAG, and ( 2 )']","['in sc induction, we use the ace phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.', 'we report performance in terms of two metrics : ( 1 ) the fmeasure score as computed by the commonly - used muc scorer  #TAUTHOR_TAG, and ( 2 ) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', ' #AUTHOR_TAG, we consider an anaphoric reference, np i, correctly resolved if np i and its closest antecedent are in the same coreference chain in the resulting partition.', 'in all of our experiments, we use nps automatically extracted by an in - house np chunker and identifinder']",5
"['', 'following  #TAUTHOR_TAG, we select as']","['np in a test text.', 'following  #TAUTHOR_TAG, we select as']","['each np in a test text.', 'following  #TAUTHOR_TAG, we select as']","['training, the decision tree classifier is used to select an antecedent for each np in a test text.', 'following  #TAUTHOR_TAG, we select as the antecedent of each np, npj, the closest preceding np that is classified as coreferent with npj.', 'if no such np exists, no antecedent is selected for npj']",4
"['. g.,  #TAUTHOR_TAG.', 'briefly, the goal of md is to']","['( md ) task ( e. g.,  #TAUTHOR_TAG.', 'briefly, the goal of md is to']","['. g.,  #TAUTHOR_TAG.', 'briefly, the goal of md is to']","['ace participants have also adopted a corpus - based approach to sc determination that is investigated as part of the mention detection ( md ) task ( e. g.,  #TAUTHOR_TAG.', 'briefly, the goal of md is to identify the boundary of a mention, its mention type ( e. g., pronoun, name ), and its semantic type ( e. g., person, location ).', 'unlike them, ( 1 ) we do not perform the full md task, as our goal is to investigate the role of sc knowledge in coreference resolution ; and ( 2 ) we do not use the ace training data for acquiring our sc classifier ; instead, we use the bbn entity type corpus  #AUTHOR_TAG, which consists of all the penn treebank wall street journal articles with the ace mentions manually identified and annotated with their scs.', 'this provides us with a training set that is approximately five times bigger than that of ace.', 'more importantly, the ace participants do not evaluate the role of induced sc knowledge in coreference resolution : many of them evaluate coreference performance on perfect mentions ( e. g.,  #AUTHOR_TAG ) ; and for those that do report performance on automatically extracted mentions, they do not explain whether or how the induced sc information is used in their coreference algorithms']",1
"[' #AUTHOR_TAG and  #TAUTHOR_TAG, as described below']","[' #AUTHOR_TAG and  #TAUTHOR_TAG, as described below']","[' #AUTHOR_TAG and  #TAUTHOR_TAG, as described below']","['baseline coreference system uses the c4. 5 deci - sion tree learner  #AUTHOR_TAG to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g.,  #AUTHOR_TAG and  #AUTHOR_TAG ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi +, npi + 2,, npj positional features that have been employed by high - performing resolvers such as  #AUTHOR_TAG and  #TAUTHOR_TAG, as described below']",1
['learner  #TAUTHOR_TAG'],['learner  #TAUTHOR_TAG'],['baseline coreference system uses the c4. 5 decision tree learner  #TAUTHOR_TAG'],"['baseline coreference system uses the c4. 5 decision tree learner  #TAUTHOR_TAG to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g.,  #AUTHOR_TAG and  #AUTHOR_TAG ), we generate training instances as follows : a positive instance is created for each anaphoric np, np j, and its closest antecedent, np i ; and a negative instance is created for np j paired with each of the intervening nps, np i + 1, np i + 2,..', '., np j1.', 'each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as  #AUTHOR_TAG and  #AUTHOR_TAG, as described below']",5
"[' #TAUTHOR_TAG and  #AUTHOR_TAG, as described below']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, as described below']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, as described below']","['baseline coreference system uses the c4. 5 decision tree learner  #AUTHOR_TAG to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g.,  #AUTHOR_TAG and  #AUTHOR_TAG ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi +, npi + 2,, npj.', 'each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high - performing resolvers such as  #TAUTHOR_TAG and  #AUTHOR_TAG, as described below']",1
"['.', 'following  #TAUTHOR_TAG, we consider an anaphoric reference, npi,']","['anaphoric references correctly resolved.', 'following  #TAUTHOR_TAG, we consider an anaphoric reference, npi,']","['.', 'following  #TAUTHOR_TAG, we consider an anaphoric reference, npi,']","['in sc induction, we use the ace phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.', 'we report performance in terms of two metrics : ( 1 ) the fmeasure score as computed by the commonly - used muc scorer  #AUTHOR_TAG, and ( 2 ) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', 'following  #TAUTHOR_TAG, we consider an anaphoric reference, npi, correctly resolved if npi and its closest antecedent are in the same coreference chain in the resulting partition.', 'in all of our experiments, we use nps automatically extracted by an in - house np chunker and identifinder']",5
"['( e. g.,  #AUTHOR_TAG ) or wikipedia  #AUTHOR_TAG, and the contextual role played by an np ( see  #TAUTHOR_TAG']","['( e. g.,  #AUTHOR_TAG ) or wikipedia  #AUTHOR_TAG, and the contextual role played by an np ( see  #TAUTHOR_TAG']","['( e. g.,  #AUTHOR_TAG ) or wikipedia  #AUTHOR_TAG, and the contextual role played by an np ( see  #TAUTHOR_TAG']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG ).', 'while these approaches have been reasonably successful ( see  #AUTHOR_TAG ),  #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g.,  #AUTHOR_TAG ), their semantic similarity as computed using wordnet ( e. g.,  #AUTHOR_TAG ) or wikipedia  #AUTHOR_TAG, and the contextual role played by an np ( see  #TAUTHOR_TAG']",0
"['. g.,  #TAUTHOR_TAG']","['( i. e., most frequent ) wordnet sense as its sc ( e. g.,  #TAUTHOR_TAG']","['. g.,  #TAUTHOR_TAG']","['type of semantic knowledge that has been employed by coreference resolvers is the semantic class ( sc ) of an np, which can be used to disallow coreference between semantically incompatible nps.', 'however, learning - based resolvers have not been able to benefit from having an sc agreement feature, presumably because the method used to compute the sc of an np is too simplistic : while the sc of a proper name is computed fairly accurately using a named entity ( ne ) recognizer, many resolvers simply assign to a common noun the first ( i. e., most frequent ) wordnet sense as its sc ( e. g.,  #TAUTHOR_TAG,  #AUTHOR_TAG ).', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]",0
"['. g.,  #TAUTHOR_TAG and  #AUTHOR_TAG ), we generate training instances as follows : a positive']","['( e. g.,  #TAUTHOR_TAG and  #AUTHOR_TAG ), we generate training instances as follows : a positive']","['. g.,  #TAUTHOR_TAG and  #AUTHOR_TAG ), we generate training instances as follows : a positive instance is created for each anaphoric np,']","['baseline coreference system uses the c4. 5 decision tree learner  #AUTHOR_TAG to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g.,  #TAUTHOR_TAG and  #AUTHOR_TAG ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi + 1, npi + 2,..., npj _ 1.', '., np j1.', 'each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as  #AUTHOR_TAG and  #AUTHOR_TAG, as described below']",5
"[' #TAUTHOR_TAG and ne classification  #AUTHOR_TAG.', 'we apply add - one smoothing to smooth the class posteriors']","[' #TAUTHOR_TAG and ne classification  #AUTHOR_TAG.', 'we apply add - one smoothing to smooth the class posteriors']","['in the related tasks of word sense disambiguation  #TAUTHOR_TAG and ne classification  #AUTHOR_TAG.', 'we apply add - one smoothing to smooth the class posteriors']","['algorithms.', 'we experiment with four learners commonly employed in language learning : decision list ( dl ) : we use the dl learner as described in  #AUTHOR_TAG, motivated by its success in the related tasks of word sense disambiguation  #TAUTHOR_TAG and ne classification  #AUTHOR_TAG.', 'we apply add - one smoothing to smooth the class posteriors']",4
"['( e. g.,  #AUTHOR_TAG ) or wikipedia  #TAUTHOR_TAG, and the contextual role played by an np ( see  #AUTHOR_TAG )']","['( e. g.,  #AUTHOR_TAG ) or wikipedia  #TAUTHOR_TAG, and the contextual role played by an np ( see  #AUTHOR_TAG )']","['( e. g.,  #AUTHOR_TAG ) or wikipedia  #TAUTHOR_TAG, and the contextual role played by an np ( see  #AUTHOR_TAG )']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG ).', 'while these approaches have been reasonably successful ( see  #AUTHOR_TAG ),  #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g.,  #AUTHOR_TAG ), their semantic similarity as computed using wordnet ( e. g.,  #AUTHOR_TAG ) or wikipedia  #TAUTHOR_TAG, and the contextual role played by an np ( see  #AUTHOR_TAG )']",0
"['. g.,  #TAUTHOR_TAG, their semantic']","['between two nps ( e. g.,  #TAUTHOR_TAG, their semantic']","['. g.,  #TAUTHOR_TAG, their semantic similarity as computed using']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG ).', 'while these approaches have been reasonably successful ( see  #AUTHOR_TAG ),  #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g.,  #TAUTHOR_TAG, their semantic similarity as computed using wordnet ( e. g.,  #AUTHOR_TAG ) or wikipedia  #AUTHOR_TAG, and the contextual role played by an np ( see  #AUTHOR_TAG )']",0
"['distributionally similar nps ( see  #TAUTHOR_TAG a ) ).', 'motivated by this']","['distributionally similar nps ( see  #TAUTHOR_TAG a ) ).', 'motivated by this observation, we create']","['distributionally similar nps ( see  #TAUTHOR_TAG a ) ).', 'motivated by this observation, we create']","['', 'we then infer the sc of a common noun as follows : ( 1 ) we compute the probability that the common noun co - occurs with each of the eight ne types 6 based on the extracted appositive relations, and ( 2 ) if the most likely ne type has a co - occurrence probability above a certain threshold ( we set it to 0. 7 ), we create a induced class fea - ture for np i whose value is the most likely ne type.', '( 7 ) neighbor : research in lexical semantics suggests that the sc of an np can be inferred from its distributionally similar nps ( see  #TAUTHOR_TAG a ) ).', ""motivated by this observation, we create for each of np i's ten most semantically similar nps a neigh - bor feature whose value is the surface string of the np."", ""to determine the ten nearest neighbors, we use the semantic similarity values provided by lin's dependency - based thesaurus, which is constructed using a distributional approach combined with an information - theoretic definition of similarity""]",4
"['. g.,  #TAUTHOR_TAG.', 'given a large, unannotated corpus 3, we use identi -']","['by research in lexical semantics ( e. g.,  #TAUTHOR_TAG.', 'given a large, unannotated corpus 3, we use identi - finder']","['. g.,  #TAUTHOR_TAG.', 'given a large, unannotated corpus 3, we use identi -']","['3 ) verb obj : a verb obj feature is created in a similar fashion as subj verb if np i participates in a verb - object relation.', 'again, this represents our attempt to coarsely model subcategorization.', ""( 4 ) ne : we use bbn's identifinder  #AUTHOR_TAG ( 5 ) wn class : for each keyword w shown in the right column of table 1, we determine whether the head noun of np i is a hyponym of w in wordnet, using only the first wordnet sense of np i. 1 if so, we create a wn class feature with w as its value."", 'these keywords are potentially useful features because some of them are subclasses of the ace scs shown in the left column of table 1, while others appear to be correlated with these ace scs. 2 ( 6 ) induced class : since the first - sense heuristic used in the previous feature may not be accurate in capturing the sc of an np, we employ a corpusbased method for inducing scs that is motivated by research in lexical semantics ( e. g.,  #TAUTHOR_TAG.', 'given a large, unannotated corpus 3, we use identi - finder to label each ne with its ne type and mini - par to extract all the appositive relations.', '']",4
"['and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our']","['predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our']","['and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', ' #AUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', ' #AUTHOR_TAG and  #AUTHOR_TAG study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues  #AUTHOR_TAG and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in  #AUTHOR_TAG, must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
"['and annotated with anvil using a subset of the attributes defined in the mumin annotation scheme  #TAUTHOR_TAG.', 'the mu - min scheme is a general']","['and annotated with anvil using a subset of the attributes defined in the mumin annotation scheme  #TAUTHOR_TAG.', 'the mu - min scheme is a general']","['communicative head gestures in the videos were found and annotated with anvil using a subset of the attributes defined in the mumin annotation scheme  #TAUTHOR_TAG.', 'the mu - min scheme is a general framework for the study of gestures in interpersonal communication.', 'in this study, we do not deal with functional classification of the gestures in themselves, but rather with how gestures']","['communicative head gestures in the videos were found and annotated with anvil using a subset of the attributes defined in the mumin annotation scheme  #TAUTHOR_TAG.', 'the mu - min scheme is a general framework for the study of gestures in interpersonal communication.', 'in this study, we do not deal with functional classification of the gestures in themselves, but rather with how gestures contribute to the semantic interpretations of linguistic expressions.', 'therefore, only a subset of the mumin attributes has been used, i. e.', 'smile, laughter, scowl, faceother for facial expressions, and nod, jerk, tilt, sideturn, shake, waggle, other for head movements']",5
"['in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #TAUTHOR_TAG examine the']","['in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #TAUTHOR_TAG examine the']","['of map - task dialogues, also targeted in this paper.', ' #AUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #TAUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', ' #AUTHOR_TAG and  #AUTHOR_TAG study the relation between eye']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', ' #AUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #TAUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', ' #AUTHOR_TAG and  #AUTHOR_TAG study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues  #AUTHOR_TAG and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication  #AUTHOR_TAG.', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in  #AUTHOR_TAG, must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
"['and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our']","['predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our']","['and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', ' #AUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', ' #AUTHOR_TAG and  #AUTHOR_TAG study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues  #AUTHOR_TAG and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in  #AUTHOR_TAG, must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see  #TAUTHOR_TAG for an'],['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see  #TAUTHOR_TAG for an'],"['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see  #TAUTHOR_TAG for an overview ).', '']","['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see  #TAUTHOR_TAG for an overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example,  #AUTHOR_TAG and  #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements, while  #AUTHOR_TAG show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den  #AUTHOR_TAG and  #AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']",0
"[' #TAUTHOR_TAG 2.', 'an']","[' #TAUTHOR_TAG 2.', '']","[' #AUTHOR_TAG 1 and corrected kappa  #TAUTHOR_TAG 2.', 'an']","['general, dialogue act, agreement and turn anno - tations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'however, one dialogue was coded independently and in parallel by two expert annotators to measure inter - coder agreement.', 'a measure was derived for each annotated feature using the agreement analysis facility provided in anvil.', ""agreement between two annotation sets is calculated here in terms of cohen's kappa  #AUTHOR_TAG 1 and corrected kappa  #TAUTHOR_TAG 2."", 'anvil divides the annotations in slices and compares each slice.', 'we used slices of 0. 04 seconds.', 'the inter - coder agreement figures obtained for the three types of annotation are given in table 2']",5
"[', pauses and dialogue structure in annotated english map - task dialogues  #TAUTHOR_TAG and find correlations between']","[', pauses and dialogue structure in annotated english map - task dialogues  #TAUTHOR_TAG and find correlations between']","['of map - task dialogues, also targeted in this paper.', ' #AUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', ' #AUTHOR_TAG and  #AUTHOR_TAG study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues  #TAUTHOR_TAG and find correlations between']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', ' #AUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', ' #AUTHOR_TAG and  #AUTHOR_TAG study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues  #TAUTHOR_TAG and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication  #AUTHOR_TAG.', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in  #AUTHOR_TAG, must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
"['system  #TAUTHOR_TAG.', 'we experimented with various weka classifiers, comprising hidden naive']","['system  #TAUTHOR_TAG.', 'we experimented with various weka classifiers, comprising hidden naive bayes, smo, id3, ladtree and decision table.', 'the best results on most of our data were']","['automatic dialogue act classification, which was run in the weka system  #TAUTHOR_TAG.', 'we experimented with various weka classifiers, comprising hidden naive bayes, smo, id3, ladtree and decision table.', 'the best results on most of our data were obtained using hidden naive bayes ( hnb )  #AUTHOR_TAG.', 'therefore, here we show the results of this classifier.', 'ten - folds crossvalidation was applied throughout']","['multimodal data we obtained by combining the linguistic annotations from danpass with the gesture annotation created in anvil, resulted into two different groups of data, one containing all yes and no expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in table 4.', 'these two sets of data were used for automatic dialogue act classification, which was run in the weka system  #TAUTHOR_TAG.', 'we experimented with various weka classifiers, comprising hidden naive bayes, smo, id3, ladtree and decision table.', 'the best results on most of our data were obtained using hidden naive bayes ( hnb )  #AUTHOR_TAG.', 'therefore, here we show the results of this classifier.', 'ten - folds crossvalidation was applied throughout']",5
"['of map - task dialogues, also targeted in this paper.', ' #TAUTHOR_TAG obtain promising']","['of map - task dialogues, also targeted in this paper.', ' #TAUTHOR_TAG obtain promising']","['of map - task dialogues, also targeted in this paper.', ' #TAUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', ' #AUTHOR_TAG and  #AUTHOR_TAG study the relation between eye']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', ' #TAUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', ' #AUTHOR_TAG and  #AUTHOR_TAG study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues  #AUTHOR_TAG and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication  #AUTHOR_TAG.', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in  #AUTHOR_TAG, must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
[' #TAUTHOR_TAG 1 and corrected'],[' #TAUTHOR_TAG 1 and corrected'],[' #TAUTHOR_TAG 1 and corrected'],"['general, dialogue act, agreement and turn annotations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'however, one dialogue was coded independently and in parallel by two expert annotators to measure inter - coder agreement.', 'a measure was derived for each annotated feature using the agreement analysis facility provided in anvil.', ""agreement between two annotation sets is calculated here in terms of cohen's kappa  #TAUTHOR_TAG 1 and corrected kappa  #AUTHOR_TAG 2."", 'anvil divides the annotations in slices and compares each slice.', 'we used slices of 0. 04 seconds.', 'the inter - coder agreement figures obtained for the three types of annotation are given in table 2']",5
"['and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our']","['predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our']","['and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', ' #AUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', ' #AUTHOR_TAG and  #AUTHOR_TAG study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues  #AUTHOR_TAG and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication  #TAUTHOR_TAG.', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in  #AUTHOR_TAG, must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
"['the resulting figures, see  #TAUTHOR_TAG, it is usually']","['the resulting figures, see  #TAUTHOR_TAG, it is usually']","['the resulting figures, see  #TAUTHOR_TAG, it is usually']","['should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'for example, a yes can be an answer to a question, an agree and a turnelicit at the same time, thus making the semantic classification very fine - grained.', ""table 1 although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see  #TAUTHOR_TAG, it is usually assumed that cohen's kappa figures over 60 are good while those over are excellent  #AUTHOR_TAG."", 'looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by anvil from the latest annotated element']",0
"['to recognise some of the functions of head movements, while  #AUTHOR_TAG show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den  #AUTHOR_TAG and  #TAUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']","['to recognise some of the functions of head movements, while  #AUTHOR_TAG show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den  #AUTHOR_TAG and  #TAUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']","['have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example,  #AUTHOR_TAG and  #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements, while  #AUTHOR_TAG show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den  #AUTHOR_TAG and  #TAUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']","['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see mc  #AUTHOR_TAG for an overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example,  #AUTHOR_TAG and  #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements, while  #AUTHOR_TAG show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den  #AUTHOR_TAG and  #TAUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']",0
"[',  #TAUTHOR_TAG and  #AUTHOR_TAG find that machine learning algorithms can be']","[',  #TAUTHOR_TAG and  #AUTHOR_TAG find that machine learning algorithms can be']","[',  #TAUTHOR_TAG and  #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements, while  #AUTHOR_TAG show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den  #AUTHOR_TAG and  #AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']","['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see mc  #AUTHOR_TAG for an overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example,  #TAUTHOR_TAG and  #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements, while  #AUTHOR_TAG show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den  #AUTHOR_TAG and  #AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']",0
"[',  #AUTHOR_TAG and  #TAUTHOR_TAG find that machine learning algorithms can be']","[',  #AUTHOR_TAG and  #TAUTHOR_TAG find that machine learning algorithms can be']","[',  #AUTHOR_TAG and  #TAUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements, while  #AUTHOR_TAG show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den  #AUTHOR_TAG and  #AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multi - modal corpus']","['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see mc  #AUTHOR_TAG for an overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example,  #AUTHOR_TAG and  #TAUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements, while  #AUTHOR_TAG show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den  #AUTHOR_TAG and  #AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multi - modal corpus']",0
"['figures over 60 are good while those over are excellent  #TAUTHOR_TAG.', 'looking at the cases of disagreement we could see that many of these']","['figures over 60 are good while those over are excellent  #TAUTHOR_TAG.', 'looking at the cases of disagreement we could see that many of these']","['figures over 60 are good while those over are excellent  #TAUTHOR_TAG.', 'looking at the cases of disagreement we could see that many of these']","['should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'for example, a yes can be an answer to a question, an agree and a turnelicit at the same time, thus making the semantic classification very fine - grained.', ""table 1 although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see  #AUTHOR_TAG, it is usually assumed that cohen's kappa figures over 60 are good while those over are excellent  #TAUTHOR_TAG."", 'looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by anvil from the latest annotated element']",0
"['in previous studies using the same annotation scheme  #TAUTHOR_TAG, but are still sat - isfactory given the high number of categories provided by the scheme']","['in previous studies using the same annotation scheme  #TAUTHOR_TAG, but are still sat - isfactory given the high number of categories provided by the scheme']","['in previous studies using the same annotation scheme  #TAUTHOR_TAG, but are still sat - isfactory given the high number of categories provided by the scheme']","['head gestures in the danpass data have been coded by non expert annotators ( one annotator per video ) and subsequently controlled by a second annotator, with the exception of one video which was annotated independently and in parallel by two annotators.', 'the annotations of this video were then used to measure inter - coder agreement in anvil as it was the case for the annotations on feedback expressions.', 'in the case of gestures we also measured agreement on gesture segmentation.', 'the figures obtained are given in table 3.', 'these results are slightly worse than those obtained in previous studies using the same annotation scheme  #TAUTHOR_TAG, but are still sat - isfactory given the high number of categories provided by the scheme']",1
"['obtained on a smaller dataset in  #TAUTHOR_TAG,']","['obtained on a smaller dataset in  #TAUTHOR_TAG,']","['obtained on a smaller dataset in  #TAUTHOR_TAG, must be seen in']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', ' #AUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while  #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', ' #AUTHOR_TAG and  #AUTHOR_TAG study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues  #AUTHOR_TAG and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication  #AUTHOR_TAG.', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in  #TAUTHOR_TAG, must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",1
"['using hidden naive bayes ( hnb )  #TAUTHOR_TAG.', 'therefore, here we show the results of this classifier.', 'ten - folds crossvalidation was applied throughout']","['using hidden naive bayes ( hnb )  #TAUTHOR_TAG.', 'therefore, here we show the results of this classifier.', 'ten - folds crossvalidation was applied throughout']","['automatic dialogue act classification, which was run in the weka system  #AUTHOR_TAG.', 'we experimented with various weka classifiers, comprising hidden naive bayes, smo, id3, ladtree and decision table.', 'the best results on most of our data were obtained using hidden naive bayes ( hnb )  #TAUTHOR_TAG.', 'therefore, here we show the results of this classifier.', 'ten - folds crossvalidation was applied throughout']","['multimodal data we obtained by combining the linguistic annotations from danpass with the gesture annotation created in anvil, resulted into two different groups of data, one containing all yes and no expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in table 4.', 'these two sets of data were used for automatic dialogue act classification, which was run in the weka system  #AUTHOR_TAG.', 'we experimented with various weka classifiers, comprising hidden naive bayes, smo, id3, ladtree and decision table.', 'the best results on most of our data were obtained using hidden naive bayes ( hnb )  #TAUTHOR_TAG.', 'therefore, here we show the results of this classifier.', 'ten - folds crossvalidation was applied throughout']",5
"['##l  #TAUTHOR_TAG.', 'to distinguish among the various functions']","['out using anvil  #TAUTHOR_TAG.', 'to distinguish among the various functions']","['them are marked with onset or offset hesitation, or both.', 'for this study, we added semantic labels - including dialogue acts - and gesture annotation.', 'both kinds of annotation were carried out using anvil  #TAUTHOR_TAG.', 'to distinguish among the various functions']","['already mentioned, all words in danpass are phonetically and prosodically annotated.', 'in the subset of the corpus considered here, 82 % of the feedback expressions bear stress or tone information, and 12 % are unstressed ; 7 % of them are marked with onset or offset hesitation, or both.', 'for this study, we added semantic labels - including dialogue acts - and gesture annotation.', 'both kinds of annotation were carried out using anvil  #TAUTHOR_TAG.', 'to distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging iso 24617 - 2 standard for semantic annotation of language resources.', 'this subset comprises the categories accept, decline, repeatrephrase and answer.', 'moreover, all feedback expressions were annotated with an agreement feature ( agree, nonagree ) where relevant.', 'finally, the two turn management categories turn - take and turnelicit were also coded']",5
"['expert annotator, for more information see grnnum ( 2006 ).', 'the praat tool was used  #TAUTHOR_TAG']","['expert annotator, for more information see grnnum ( 2006 ).', 'the praat tool was used  #TAUTHOR_TAG']","['upon version was produced with the supervision of an expert annotator, for more information see grnnum ( 2006 ).', 'the praat tool was used  #TAUTHOR_TAG']","['and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see grnnum ( 2006 ).', 'the praat tool was used  #TAUTHOR_TAG']",5
"[', based on  #TAUTHOR_TAG b ), outputs']","[', based on  #TAUTHOR_TAG b ), outputs']","[', based on  #TAUTHOR_TAG b ), outputs']","['', ""the diagnoser, based on  #TAUTHOR_TAG b ), outputs a diagnosis which consists of lists of correct, contradictory and non - mentioned objects and relations from the student's answer."", 'at present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to  #AUTHOR_TAG']",2
"['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
['surge  #TAUTHOR_TAG generation'],"[', and a fuf / surge  #TAUTHOR_TAG generation']","[', and a fuf / surge  #TAUTHOR_TAG generation system']","[""strategy decision made by the tutorial planner, together with relevant semantic content from the student's answer ( e. g., part of the answer to confirm ), is passed to content planning and generation."", 'the system uses a domain - specific content planner to produce input to the surface realizer based on the strategy decision, and a fuf / surge  #TAUTHOR_TAG generation system to produce the appropriate text.', 'templates are used to generate some stock phrases such as "" when you are ready, go on to the next slide.']",5
"['with higher learning gain  #TAUTHOR_TAG.', 'using a deep generator to automatically generate system feedback gives us a']","['with higher learning gain  #TAUTHOR_TAG.', 'using a deep generator to automatically generate system feedback gives us a']","['with higher learning gain  #TAUTHOR_TAG.', 'using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail']","['dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.', ""the analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example, using better terminology ) but doesn't explicitly explain the reason why different terminology is needed  #AUTHOR_TAG."", 'results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain  #TAUTHOR_TAG.', 'using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail']",3
"['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['dialogue manager which uses the informationstate approach  #TAUTHOR_TAG.', 'the dialogue state is represented by']","['dialogue manager which uses the informationstate approach  #TAUTHOR_TAG.', 'the dialogue state is represented by']","['between components is coordinated by the dialogue manager which uses the informationstate approach  #TAUTHOR_TAG.', 'the dialogue state is represented by']","['between components is coordinated by the dialogue manager which uses the informationstate approach  #TAUTHOR_TAG.', 'the dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not - yet - mentioned parts of the answer.', 'once the complete answer has been accumulated, the system accepts it and moves on.', 'tutor hints can contribute parts of the answer to the cumulative state as well, allowing the system to jointly construct the solution with the student']",5
"['##ing  #TAUTHOR_TAG.', 'however,']","['in humanhuman tutoring  #TAUTHOR_TAG.', 'however,']","['in developing tutorial dialogue systems that understand student explanations  #AUTHOR_TAG van  #AUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #TAUTHOR_TAG.', 'however,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations  #AUTHOR_TAG van  #AUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #TAUTHOR_TAG.', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['different terminology is needed  #TAUTHOR_TAG.', 'results from other systems show that measures']","['different terminology is needed  #TAUTHOR_TAG.', 'results from other systems show that measures']","['different terminology is needed  #TAUTHOR_TAG.', 'results from other systems show that measures']","['dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.', ""the analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example, using better terminology ) but doesn't explicitly explain the reason why different terminology is needed  #TAUTHOR_TAG."", 'results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain  #AUTHOR_TAG.', 'using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail']",3
"['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['##ing  #TAUTHOR_TAG.', 'however,']","['in humanhuman tutoring  #TAUTHOR_TAG.', 'however,']","['in developing tutorial dialogue systems that understand student explanations  #AUTHOR_TAG van  #AUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #TAUTHOR_TAG.', 'however,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations  #AUTHOR_TAG van  #AUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #TAUTHOR_TAG.', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['an error recovery policy  #TAUTHOR_TAG.', 'since']","['an error recovery policy  #TAUTHOR_TAG.', 'since']","[', the tutorial planner implements an error recovery policy  #TAUTHOR_TAG.', 'since']","['tutorial policy makes a high - level decision as to which strategy to use ( for example, "" acknowledge the correct part and give a high specificity hint "" ) based on the answer analysis and dialogue context.', 'at present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 in addition to a remediation policy, the tutorial planner implements an error recovery policy  #TAUTHOR_TAG.', 'since the system accepts unrestricted input, interpretation errors are unavoidable.', 'our recovery policy is modeled on the targetedhelp  #AUTHOR_TAG policy used in task - oriented dialogue.', '']",5
"['similar to  #TAUTHOR_TAG, and an']","['similar to  #TAUTHOR_TAG, and an']","['', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', 'the contextual interpreter then uses a reference resolution approach similar to  #TAUTHOR_TAG, and an ontology mapping mechanism  #AUTHOR_TAG a )']","['use the trips dialogue parser  #AUTHOR_TAG to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to  #TAUTHOR_TAG, and an ontology mapping mechanism  #AUTHOR_TAG a ) to produce a domain - specific semantic representation of the student's output."", '']",1
"['to overcome these limitations  #TAUTHOR_TAG.', '']","['to overcome these limitations  #TAUTHOR_TAG.', '']","['beetle ii system architecture is designed to overcome these limitations  #TAUTHOR_TAG.', 'it uses a deep parser']","['beetle ii system architecture is designed to overcome these limitations  #TAUTHOR_TAG.', 'it uses a deep parser and generator, together with a domain reasoner and a diagnoser, to produce detailed analyses of student utterances and generate feedback automatically.', 'this allows the system to consistently apply the same tutorial policy across a range of questions.', 'to some extent, this comes at the expense of being able to address individual student misconceptions.', ""however, the system's modular setup and extensibility make it a suitable testbed for both computational linguistics algorithms and more general questions about theories of learning""]",0
"['category, though in the future we may consider a classifier similar to  #TAUTHOR_TAG']","['category, though in the future we may consider a classifier similar to  #TAUTHOR_TAG']","['relations into the appropriate category, though in the future we may consider a classifier similar to  #TAUTHOR_TAG']","['', ""the diagnoser, based on  #AUTHOR_TAG b ), outputs a diagnosis which consists of lists of correct, contradictory and non - mentioned objects and relations from the student's answer."", 'at present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to  #TAUTHOR_TAG']",3
"['##ing  #TAUTHOR_TAG.', 'however,']","['in humanhuman tutoring  #TAUTHOR_TAG.', 'however,']","['in developing tutorial dialogue systems that understand student explanations  #AUTHOR_TAG van  #AUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #TAUTHOR_TAG.', 'however,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations  #AUTHOR_TAG van  #AUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #TAUTHOR_TAG.', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
[' #TAUTHOR_TAG a )'],[' #TAUTHOR_TAG a )'],"[' #AUTHOR_TAG, and an ontology mapping mechanism  #TAUTHOR_TAG a )']","['use the trips dialogue parser  #AUTHOR_TAG to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to  #AUTHOR_TAG, and an ontology mapping mechanism  #TAUTHOR_TAG a ) to produce a domain - specific semantic representation of the student's output."", '']",5
['factors such as student confidence could be considered as well  #TAUTHOR_TAG'],['factors such as student confidence could be considered as well  #TAUTHOR_TAG'],['factors such as student confidence could be considered as well  #TAUTHOR_TAG'],['factors such as student confidence could be considered as well  #TAUTHOR_TAG'],3
"['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG,']","['in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations  #TAUTHOR_TAG, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring  #AUTHOR_TAG.', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
['is modeled on the targetedhelp  #TAUTHOR_TAG'],['is modeled on the targetedhelp  #TAUTHOR_TAG'],"['.', 'our recovery policy is modeled on the targetedhelp  #TAUTHOR_TAG policy used in task - oriented dialogue.', 'if']","['tutorial policy makes a high - level decision as to which strategy to use ( for example, "" acknowledge the correct part and give a high specificity hint "" ) based on the answer analysis and dialogue context.', 'at present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 in addition to a remediation policy, the tutorial planner implements an error recovery policy.', 'since the system accepts unrestricted input, interpretation errors are unavoidable.', 'our recovery policy is modeled on the targetedhelp  #TAUTHOR_TAG policy used in task - oriented dialogue.', '']",2
['use the trips dialogue parser  #TAUTHOR_TAG to'],['use the trips dialogue parser  #TAUTHOR_TAG to'],['use the trips dialogue parser  #TAUTHOR_TAG to'],"['use the trips dialogue parser  #TAUTHOR_TAG to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to  #AUTHOR_TAG, and an ontology mapping mechanism  #AUTHOR_TAG a ) to produce a domain - specific semantic representation of the student's output."", '']",5
['implemented in the km representation language  #TAUTHOR_TAG to represent'],['implemented in the km representation language  #TAUTHOR_TAG to represent'],"['implemented in the km representation language  #TAUTHOR_TAG to represent the state of the world.', 'at present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits']","['system uses a knowledge base implemented in the km representation language  #TAUTHOR_TAG to represent the state of the world.', 'at present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits']",5
"['applying logical inference  #AUTHOR_TAG, or adopting transformation - based techniques  #TAUTHOR_TAG, incorporate different types of lexical knowledge to support textual inference']","['applying logical inference  #AUTHOR_TAG, or adopting transformation - based techniques  #TAUTHOR_TAG, incorporate different types of lexical knowledge to support textual inference']","['applying logical inference  #AUTHOR_TAG, or adopting transformation - based techniques  #TAUTHOR_TAG, incorporate different types of lexical knowledge to support textual inference.', 'such information ranges']","['current approaches to monolingual te, either syntactically oriented  #AUTHOR_TAG, or applying logical inference  #AUTHOR_TAG, or adopting transformation - based techniques  #TAUTHOR_TAG, incorporate different types of lexical knowledge to support textual inference.', '']",0
"['in another language  #TAUTHOR_TAG.', 'there are several methods to build phrase tables.', 'the one adopted in this']","['in another language  #TAUTHOR_TAG.', 'there are several methods to build phrase tables.', 'the one adopted in this']","['in another language  #TAUTHOR_TAG.', 'there are several methods to build phrase tables.', 'the one adopted in this work consists in learning phrase alignments from a word - aligned bilingual corpus.', 'in']","['tables ( pht ) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'they are widely used in mt as a way to figure out how to translate input in one language into output in another language  #TAUTHOR_TAG.', 'there are several methods to build phrase tables.', 'the one adopted in this work consists in learning phrase alignments from a word - aligned bilingual corpus.', 'in order to build english - spanish phrase tables for our experiments, we used the freely available europarl v. 4, news commentary and united nations spanish - english parallel corpora released for the wmt10 1.', 'we run treetagger  #AUTHOR_TAG for tokenization, and used the giza + +  #AUTHOR_TAG to align the tokenized corpora at the word level.', 'subsequently, we extracted the bilingual phrase table from the aligned corpora using the moses toolkit  #AUTHOR_TAG.', 'since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'in addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0. 01, 0. 05, 0. 1, 0. 2, 0. 3, 0. 4 and 0. 5.', 'the resulting 1 http : / / www. statmt. org / wmt10 /', 'phrase tables range from 76 to 48 million entries, with an average of 3. 9 words per phrase']",0
"[' #TAUTHOR_TAG, verboce']","[', dirt  #TAUTHOR_TAG, verbocean  #AUTHOR_TAG, framenet  #AUTHOR_TAG, and wikipedia  #AUTHOR_TAG.', 'dirt is a']","[', dirt  #TAUTHOR_TAG, verboce']","[', dirt  #TAUTHOR_TAG, verbocean  #AUTHOR_TAG, framenet  #AUTHOR_TAG, and wikipedia  #AUTHOR_TAG.', 'dirt is a collection of sta - tistically learned inference rules, that is often inte - grated as a source of lexical paraphrases and entail - ment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are fre - quently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similar - ity / relatedness scores']",0
"['applying logical inference ( tatu and  #TAUTHOR_TAG, or adopting transformation - based techniques  #AUTHOR_TAG bar -  #AUTHOR_TAG, incorporate different types of lexical knowledge to']","['applying logical inference ( tatu and  #TAUTHOR_TAG, or adopting transformation - based techniques  #AUTHOR_TAG bar -  #AUTHOR_TAG, incorporate different types of lexical knowledge to']","['applying logical inference ( tatu and  #TAUTHOR_TAG, or adopting transformation - based techniques  #AUTHOR_TAG bar -  #AUTHOR_TAG, incorporate different types of lexical knowledge to']","['current approaches to monolingual te, either syntactically oriented  #AUTHOR_TAG, or applying logical inference ( tatu and  #TAUTHOR_TAG, or adopting transformation - based techniques  #AUTHOR_TAG bar -  #AUTHOR_TAG, incorporate different types of lexical knowledge to support textual inference.', '']",0
"['by  #TAUTHOR_TAG.', 'the method relies on translation -']","['by  #TAUTHOR_TAG.', 'the method relies on translation - validation cycles, defined as separate jobs']","['by  #TAUTHOR_TAG.', 'the method relies on translation -']","['dataset used for our experiments is an english - spanish entailment corpus obtained from the original rte3 dataset by translating the english hypothesis into spanish.', 'it consists of 1600 pairs derived from the rte3 development and test sets ( 800 + 800 ).', 'translations have been generated by the crowdflower3 channel to amazon mechanical turk4 ( mturk ), adopting the methodology proposed by  #TAUTHOR_TAG.', ""the method relies on translation - validation cycles, defined as separate jobs routed to mturk's workforce."", 'translation jobs return one spanish version for each hypothesis.', 'validation jobs ask multiple workers to check the correctness of each translation using the original english sentence as reference.', 'at each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the clte corpus, while wrong translations are sent back to workers in a new translation job.', 'although the quality of the results is enhanced by the possibility to automatically weed out untrusted workers using gold units, we performed a manual quality check on a subset of the acquired clte corpus.', 'the validation, carried out by a spanish native speaker on 100 randomly selected pairs after two translation - validation cycles, showed the good quality of the collected material, with only 3 minor "" errors "" consisting in controversial but substantially acceptable translations reflecting regional spanish variations']",5
['( clte ) has been proposed by  #TAUTHOR_TAG as an extension of textual'],['( clte ) has been proposed by  #TAUTHOR_TAG as an extension of textual'],['( clte ) has been proposed by  #TAUTHOR_TAG as an extension of'],"['- lingual textual entailment ( clte ) has been proposed by  #TAUTHOR_TAG as an extension of textual entailment  #AUTHOR_TAG that consists in deciding, given two texts t and h in different languages, if the meaning of h can be inferred from the meaning of t.', 'the task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'for instance, the reliance of cur - rent monolingual te systems on lexical resources ( e. g. wordnet, verbocean, framenet ) and deep processing components ( e. g. syntactic and semantic parsers, co - reference resolution tools, temporal ex - pressions recognizers and normalizers ) has to con - front, at the cross - lingual level, with the limited availability of lexical / semantic resources covering multiple languages, the limited coverage of the ex - isting ones, and the burden of integrating language - specific components into the same cross - lingual ar - chitecture']",0
"[' #TAUTHOR_TAG as a term of comparison, we experiment with different sources of multilingual lexical knowledge to address']","[' #TAUTHOR_TAG as a term of comparison, we experiment with different sources of multilingual lexical knowledge to address']","[' #TAUTHOR_TAG as a term of comparison, we experiment with different sources of multilingual lexical knowledge to address']","['paper investigates the idea, still unexplored, of a tighter integration of mt and te algorithms and techniques.', 'our aim is to embed cross - lingual processing techniques inside the te recognition process in order to avoid any dependency on external mt components, and eventually gain full control of the systems behaviour.', 'along this direction, we start from the acquisition and use of lexical knowl - edge, which represents the basic building block of any te system.', 'using the basic solution proposed by  #TAUTHOR_TAG as a term of comparison, we experiment with different sources of multilingual lexical knowledge to address the following questions']",1
"[', and used the giza + +  #TAUTHOR_TAG']","['tokenization, and used the giza + +  #TAUTHOR_TAG']","[', and used the giza + +  #TAUTHOR_TAG']","['tables ( pht ) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'they are widely used in mt as a way to figure out how to translate input in one language into output in another language  #AUTHOR_TAG.', 'there are several methods to build phrase tables.', 'the one adopted in this work consists in learning phrase alignments from a word - aligned bilingual corpus.', 'in order to build english - spanish phrase tables for our experiments, we used the freely available europarl v. 4, news commentary and united nations spanish - english parallel corpora released for the wmt10 1.', 'we run treetagger  #AUTHOR_TAG for tokenization, and used the giza + +  #TAUTHOR_TAG to align the tokenized corpora at the word level.', 'subsequently, we extracted the bilingual phrase table from the aligned corpora using the moses toolkit  #AUTHOR_TAG.', 'since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'in addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0. 01, 0. 05, 0. 1, 0. 2, 0. 3, 0. 4 and 0. 5.', 'the resulting 1 http : / / www. statmt. org / wmt10 /', 'phrase tables range from 76 to 48 million entries, with an average of 3. 9 words per phrase']",5
"[' #TAUTHOR_TAG, automatic']","[' #TAUTHOR_TAG, automatic']","[' #TAUTHOR_TAG, automatic evaluation']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation  #AUTHOR_TAG, multidocument summarization  #TAUTHOR_TAG, automatic evaluation of mt  #AUTHOR_TAG, and te  #AUTHOR_TAG.', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison -  #AUTHOR_TAG.', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques  #AUTHOR_TAG can be applied to increase the precision of the extracted paraphrases']",4
"['.', 'some previous works  #TAUTHOR_TAG indicate, as main limitations of the mentioned resources,']","['others.', 'some previous works  #TAUTHOR_TAG indicate, as main limitations of the mentioned resources,']","['.', 'some previous works  #TAUTHOR_TAG indicate, as main limitations of the mentioned resources,']","['the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'as emerges from the ablation tests reported in  #AUTHOR_TAG, even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'some previous works  #TAUTHOR_TAG indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words']",0
"['relative weights, we trained a support vector machine classifier, svmlight  #TAUTHOR_TAG, using each score as a feature']","['relative weights, we trained a support vector machine classifier, svmlight  #TAUTHOR_TAG, using each score as a feature']","['relative weights, we trained a support vector machine classifier, svmlight  #TAUTHOR_TAG, using each score as a feature']","['combine the phrasal matching scores obtained at each n - gram level, and optimize their relative weights, we trained a support vector machine classifier, svmlight  #TAUTHOR_TAG, using each score as a feature']",5
"['proved to be useful in a number of nlp applications such as natural language generation  #TAUTHOR_TAG, multi']","['proved to be useful in a number of nlp applications such as natural language generation  #TAUTHOR_TAG, multidocument']","['tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation  #TAUTHOR_TAG, multi']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation  #TAUTHOR_TAG, multidocument summarization ( mc  #AUTHOR_TAG, automatic evaluation of mt  #AUTHOR_TAG, and te  #AUTHOR_TAG.', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison -  #AUTHOR_TAG.', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques  #AUTHOR_TAG can be applied to increase the precision of the extracted paraphrases']",4
"[' #TAUTHOR_TAG, framenet  #AUTHOR_TAG, and wikipedia  #AUTHOR_TAG.', 'dirt is a']","[' #AUTHOR_TAG, verbocean  #TAUTHOR_TAG, framenet  #AUTHOR_TAG, and wikipedia  #AUTHOR_TAG.', 'dirt is a']","[' #AUTHOR_TAG, verbocean  #TAUTHOR_TAG, framenet  #AUTHOR_TAG, and wikipedia  #AUTHOR_TAG.', 'dirt is a collection of sta - tistically']","['wordnet, the rte literature documents the use of a variety of lexical information sources  #AUTHOR_TAG.', 'these include, just to mention the most popular ones, dirt  #AUTHOR_TAG, verbocean  #TAUTHOR_TAG, framenet  #AUTHOR_TAG, and wikipedia  #AUTHOR_TAG.', 'dirt is a collection of sta - tistically learned inference rules, that is often inte - grated as a source of lexical paraphrases and entail - ment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similar - ity / relatedness scores']",0
"['##net  #TAUTHOR_TAG ) have been created for several languages, with different degrees of coverage']","['multiwordnet  #TAUTHOR_TAG ) have been created for several languages, with different degrees of coverage']","['. multiwordnet  #TAUTHOR_TAG ) have been created for several languages, with different degrees of coverage.', 'as an example, the 57, 424 synsets of the spanish section of multi']","['', 'as regards the first issue, its worth noting that in the monolingual scenario simple bag of words ( or bag of n - grams ) approaches are per se sufficient to achieve results above baseline.', 'in contrast, their application in the cross - lingual setting is not a viable solution due to the impossibility to perform direct lex - ical matches between texts and hypotheses in different languages.', 'this situation makes the availability of multilingual lexical knowledge a necessary condition to bridge the language gap.', 'however, with the only exceptions represented by wordnet and wikipedia, most of the aforementioned resources are available only for english.', 'multilingual lexical databases aligned with the english wordnet ( e. g. multiwordnet  #TAUTHOR_TAG ) have been created for several languages, with different degrees of coverage.', 'as an example, the 57, 424 synsets of the spanish section of multiwordnet aligned to english cover just around 50 % of the wordnets synsets, thus making the coverage issue even more problematic than for te.', 'as regards wikipedia, the cross - lingual links between pages in different languages offer a possibility to extract lexical knowledge useful for clte.', 'however, due to their relatively small number ( especially for some languages ), bilingual lexicons extracted from wikipedia are still inadequate to provide acceptable coverage.', 'in addition, featuring a bias towards named entities, the information acquired through cross - lingual links can at most complement the lexical knowledge extracted from more generic multilingual resources ( e. g bilingual dictionaries )']",0
"['mt  #AUTHOR_TAG, and te  #TAUTHOR_TAG.', 'one of the proposed methods to']","['mt  #AUTHOR_TAG, and te  #TAUTHOR_TAG.', 'one of the proposed methods to']","['mt  #AUTHOR_TAG, and te  #TAUTHOR_TAG.', 'one of the proposed methods to']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation  #AUTHOR_TAG, multidocument summarization ( mc  #AUTHOR_TAG, automatic evaluation of mt  #AUTHOR_TAG, and te  #TAUTHOR_TAG.', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison -  #AUTHOR_TAG.', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques  #AUTHOR_TAG can be applied to increase the precision of the extracted paraphrases']",4
"['using phrase alignments in a bilingual parallel corpus  #TAUTHOR_TAG.', 'with this method, all the different phrases in one language that are aligned with the']","['using phrase alignments in a bilingual parallel corpus  #TAUTHOR_TAG.', 'with this method, all the different phrases in one language that are aligned with the']","['on a pivot - based approach using phrase alignments in a bilingual parallel corpus  #TAUTHOR_TAG.', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation  #AUTHOR_TAG, multidocument summarization ( mc  #AUTHOR_TAG, automatic evaluation of mt  #AUTHOR_TAG, and te  #AUTHOR_TAG.', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus  #TAUTHOR_TAG.', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques  #AUTHOR_TAG can be applied to increase the precision of the extracted paraphrases']",0
"[' #AUTHOR_TAG, framenet  #AUTHOR_TAG, and wikipedia  #TAUTHOR_TAG.', 'dirt is a']","[' #AUTHOR_TAG, verbocean  #AUTHOR_TAG, framenet  #AUTHOR_TAG, and wikipedia  #TAUTHOR_TAG.', 'dirt is a']","[' #AUTHOR_TAG, framenet  #AUTHOR_TAG, and wikipedia  #TAUTHOR_TAG.', 'dirt is a collection of statistically']","['wordnet, the rte literature documents the use of a variety of lexical information sources  #AUTHOR_TAG.', 'these include, just to mention the most popular ones, dirt  #AUTHOR_TAG, verbocean  #AUTHOR_TAG, framenet  #AUTHOR_TAG, and wikipedia  #TAUTHOR_TAG.', '']",0
"[' #TAUTHOR_TAG that consists in deciding, given two texts t and h in different languages, if the']","[' #TAUTHOR_TAG that consists in deciding, given two texts t and h in different languages, if the']","[' #TAUTHOR_TAG that consists in deciding, given two texts t and h in different languages, if the meaning']","['- lingual textual entailment ( clte ) has been proposed by  #AUTHOR_TAG as an extension of textual entailment  #TAUTHOR_TAG that consists in deciding, given two texts t and h in different languages, if the meaning of h can be inferred from the meaning of t.', 'the task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'for instance, the reliance of cur - rent monolingual te systems on lexical resources ( e. g. wordnet, verbocean, framenet ) and deep processing components ( e. g. syntactic and semantic parsers, co - reference resolution tools, temporal expressions recognizers and normalizers ) has to confront, at the cross - lingual level, with the limited availability of lexical / semantic resources covering multiple languages, the limited coverage of the existing ones, and the burden of integrating language - specific components into the same cross - lingual architecture']",0
"[' #AUTHOR_TAG, framenet  #TAUTHOR_TAG, and wikipedia  #AUTHOR_TAG.', 'dirt is a']","[' #AUTHOR_TAG, verbocean  #AUTHOR_TAG, framenet  #TAUTHOR_TAG, and wikipedia  #AUTHOR_TAG.', 'dirt is a']","[' #AUTHOR_TAG, framenet  #TAUTHOR_TAG, and wikipedia  #AUTHOR_TAG.', 'dirt is a collection of statistically']","['wordnet, the rte literature documents the use of a variety of lexical information sources  #AUTHOR_TAG.', 'these include, just to mention the most popular ones, dirt  #AUTHOR_TAG, verbocean  #AUTHOR_TAG, framenet  #TAUTHOR_TAG, and wikipedia  #AUTHOR_TAG.', '']",0
"[', pruning techniques  #TAUTHOR_TAG can be applied to increase the']","[', pruning techniques  #TAUTHOR_TAG can be applied to increase the']","['as paraphrases.', 'after the extraction, pruning techniques  #TAUTHOR_TAG can be applied to increase the precision of the extracted paraphrases']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation  #AUTHOR_TAG, multidocument summarization ( mc  #AUTHOR_TAG, automatic evaluation of mt  #AUTHOR_TAG, and te  #AUTHOR_TAG.', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison -  #AUTHOR_TAG.', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques  #TAUTHOR_TAG can be applied to increase the precision of the extracted paraphrases']",0
"['mt  #TAUTHOR_TAG, and te  #AUTHOR_TAG.', 'one of the proposed methods to']","['mt  #TAUTHOR_TAG, and te  #AUTHOR_TAG.', 'one of the proposed methods to']","['mt  #TAUTHOR_TAG, and te  #AUTHOR_TAG.', 'one of the proposed methods to']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation  #AUTHOR_TAG, multidocument summarization ( mc  #AUTHOR_TAG, automatic evaluation of mt  #TAUTHOR_TAG, and te  #AUTHOR_TAG.', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison -  #AUTHOR_TAG.', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques  #AUTHOR_TAG can be applied to increase the precision of the extracted paraphrases']",4
"['.', 'some previous works  #TAUTHOR_TAG indicate, as main limitations of the mentioned resources,']","['others.', 'some previous works  #TAUTHOR_TAG indicate, as main limitations of the mentioned resources,']","['.', 'some previous works  #TAUTHOR_TAG indicate, as main limitations of the mentioned resources,']","['the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'as emerges from the ablation tests reported in  #AUTHOR_TAG, even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'some previous works  #TAUTHOR_TAG indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words']",0
"['by  #TAUTHOR_TAG.', 'although']","['by  #TAUTHOR_TAG.', 'although']","['by  #TAUTHOR_TAG.', 'although it was presented as an approach to clte, the proposed method brings the problem back to']","[""the sake of completeness, we report in this section also the results obtained adopting the ` ` basic solution'' proposed by  #TAUTHOR_TAG."", 'although it was presented as an approach to clte, the proposed method brings the problem back to the monolingual case by translating h into the language of t. the comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive mt system ( google translate ) in the same scenario']",1
['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],"['by  #TAUTHOR_TAG.', 'another interesting direction']","['future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for te and clte.', 'on one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'one possible direction is to consider linguistically motivated approaches, such as the extraction of syntactic phrase tables as proposed by  #TAUTHOR_TAG.', '']",3
"[') over wikipedia using the jlsi tool  #TAUTHOR_TAG to measure the relatedness between words in the dataset.', 'then, we filtered']","['( lsa ) over wikipedia using the jlsi tool  #TAUTHOR_TAG to measure the relatedness between words in the dataset.', 'then, we filtered']","[') over wikipedia using the jlsi tool  #TAUTHOR_TAG to measure the relatedness between words in the dataset.', 'then, we filtered all the pairs with similarity lower than']","['( wiki ).', 'we performed latent semantic analysis ( lsa ) over wikipedia using the jlsi tool  #TAUTHOR_TAG to measure the relatedness between words in the dataset.', 'then, we filtered all the pairs with similarity lower than 0. 7 as proposed by  #AUTHOR_TAG.', 'in this way we obtained 13760 word pairs']",5
"['of  #TAUTHOR_TAG, the']","['of  #TAUTHOR_TAG, the']","['of  #TAUTHOR_TAG, the results']",[' #TAUTHOR_TAG'],1
"['aligned corpora using the moses toolkit  #TAUTHOR_TAG.', 'since']","['aligned corpora using the moses toolkit  #TAUTHOR_TAG.', 'since']","['aligned corpora using the moses toolkit  #TAUTHOR_TAG.', 'since']","['tables ( pht ) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'they are widely used in mt as a way to figure out how to translate input in one language into output in another language  #AUTHOR_TAG.', 'there are several methods to build phrase tables.', 'the one adopted in this work consists in learning phrase alignments from a word - aligned bilingual corpus.', 'in order to build english - spanish phrase tables for our experiments, we used the freely available europarl v. 4, news commentary and united nations spanish - english parallel corpora released for the wmt10 1.', 'we run treetagger  #AUTHOR_TAG for tokenization, and used the giza + +  #AUTHOR_TAG to align the tokenized corpora at the word level.', 'subsequently, we extracted the bilingual phrase table from the aligned corpora using the moses toolkit  #TAUTHOR_TAG.', 'since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'in addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0. 01, 0. 05, 0. 1, 0. 2, 0. 3, 0. 4 and 0. 5.', 'the resulting 1 http : / / www. statmt. org / wmt10 /', 'phrase tables range from 76 to 48 million entries, with an average of 3. 9 words per phrase']",5
"['lexical information sources  #TAUTHOR_TAG.', 'these include, just to mention the most popular']","['lexical information sources  #TAUTHOR_TAG.', 'these include, just to mention the most popular ones,']","['lexical information sources  #TAUTHOR_TAG.', 'these include, just to mention the most']","['wordnet, the rte literature documents the use of a variety of lexical information sources  #TAUTHOR_TAG.', 'these include, just to mention the most popular ones, dirt  #AUTHOR_TAG, verbocean  #AUTHOR_TAG, framenet  #AUTHOR_TAG, and  #AUTHOR_TAG.', '']",0
"['by designing appropriate priming experiments  #TAUTHOR_TAG or other lexical decision tasks.', 'the reaction time of the subjects']","['by designing appropriate priming experiments  #TAUTHOR_TAG or other lexical decision tasks.', 'the reaction time of the subjects']","['by designing appropriate priming experiments  #TAUTHOR_TAG or other lexical decision tasks.', 'the reaction time of the subjects']","['questions are typically answered by designing appropriate priming experiments  #TAUTHOR_TAG or other lexical decision tasks.', 'the reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain.', '( see sec. 2 for models of morphological organization and access and related experiments )']",0
['inter annotator agreement using the fleiss kappa  #TAUTHOR_TAG measure ( x ) where the agreement lies'],['inter annotator agreement using the fleiss kappa  #TAUTHOR_TAG measure ( x ) where the agreement lies'],"['of them and rest 500 are overlapping.', 'we measure the inter annotator agreement using the fleiss kappa  #TAUTHOR_TAG measure ( x ) where the agreement lies']","['', 'for example, "" [UNK] [UNK] "" ( getting up ) is a compound verb where individual words do not entirely reflects the meaning of the whole expression.', 'however, not all v1 + v2 combinations are cvs.', 'for example, expressions like, "" [UNK] [UNK] "" ( take and then go ) and "" [UNK] [UNK] [UNK] "" ( return back ) are the examples of verb sequences where meaning of the whole expression can be derived from the meaning of the individual component and thus, these verb sequences are not considered as cv.', 'the key question linguists are trying to identify for a long time and debating a lot is whether to consider cvs as a single lexical units or consider them as two separate units.', 'since linguistic rules fails to explain the process, we for the first time tried to perform cognitive experiments to understand the organization and processing of such verb sequences in the human mind.', 'a clear understanding about these phenomena may help us to classify or extract actual cvs from other verb sequences.', 'in order to do so, presently we have applied three different techniques to collect user data.', 'in the first technique, we annotated 4500 v1 + v2 sequences, along with their example sentences, using a group of three linguists ( the expert subjects ).', 'we asked the experts to classify the verb sequences into three classes namely, cv, not a cv and not sure.', 'each linguist has received 2000 verb pairs along with their respective example sentences.', 'out of this, 1500 verb sequences are unique to each of them and rest 500 are overlapping.', 'we measure the inter annotator agreement using the fleiss kappa  #TAUTHOR_TAG measure ( x ) where the agreement lies around 0. 79.', 'next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 v1 + v2 pairs and presented them to 36 native bangla speakers.', 'we ask each subjects to give a compositionality score of each verb sequences under 1 - 10 point scale, 10 being highly compositional and 1 for noncompositional.', 'we found an agreement of  = 0. 69 among the subjects.', 'we also observe a continuum of compositionality score among the verb sequences.', 'this reflects that it is difficult to classify bangla verb sequences discretely into the classes of cv and not']",5
['proposed by  #TAUTHOR_TAG that points out'],['proposed by  #TAUTHOR_TAG that points out'],['by  #TAUTHOR_TAG that points out'],"['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', ' #AUTHOR_TAG considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', ' #AUTHOR_TAG argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', ' #AUTHOR_TAG tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by  #TAUTHOR_TAG that points out v1 and v2 are paired on the basis of their semantic compatibility, which is subject to syntactic constraints.', ' #AUTHOR_TAG tried to represent bangla cvs in terms of hpsg formalism.', 'she proposes that the selection of a v2 by a v1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'since none of the linguistic formalism could satisfactorily explain the unique phenomena of cv formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb - verb combinations in the ml and compare these responses with that of the existing models']",0
"['as discussed in  #TAUTHOR_TAG for english polymorphemic words. however,', 'rather than derived words, the']","['as discussed in  #TAUTHOR_TAG for english polymorphemic words. however,', 'rather than derived words, the']","['experimental procedure as discussed in  #TAUTHOR_TAG for english polymorphemic words. however,', 'rather than derived words, the subjects were shown a verb sequence and asked whether']","['the agreement lies around 0. 79. next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 v1 + v2 pairs and presented them to 36', 'native bangla speakers. we ask each subjects to give a compositionality score of', 'each verb sequences under 1 - 10 point scale, 10 being highly compositional and 1 for noncompositional. we found an agreement of  =', '0. 69 among the subjects. we also observe a continuum of compositional', ""##ity score among the verb sequences. this reflects that it is difficult to classify bangla verb sequences discretely into the classes of cv and not a cv. we then, compare the compositionality score with that of the expert user's annotation. we found a significant correlation between the expert annotation and the"", 'compositionality score. we observe verb sequences that are annotated as cvs ( like, [UNK] [UNK], [UNK] [UNK], [UNK] [UNK] ) have got low compositionality score ( average score ranges between', '1 - 4 ) on the other hand high compositional values are in general tagged as not a cv ( [UNK] [UNK] ( come and get ), [UNK] [UNK] ( return back )', ', [UNK] [UNK] [UNK] ( kept ), [UNK] [UNK] ( roll on floor ) ). this reflects that verb', 'sequences which are not cv shows high degree of compositionality. in other words non cv verbs', 'can directly interpret from their constituent verbs. this leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be', 'greater than the non - compositional verbs which maps to a single expression of meaning. in order to validate such claim we', 'perform a lexical decision experiment using native bangla speakers with 92 different verb sequences', '. we followed the same experimental procedure as discussed in  #TAUTHOR_TAG for english polymorphemic words. however,', 'rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination. the reaction time', '( rt ) of each subject is recorded. our preliminarily observation from the rt analysis shows that as per our claim, rt of verb sequences having high compositionality value is significantly higher than the', 'rts for low or noncompositional verbs. this proves our hypothesis that bangla compound verbs that show less compositionality are stored as a hole in the mental lexicon', 'and thus follows the full - listing model whereas compositional verb phrases are individually', 'parsed. however, we do believe that our experiment is composed of', 'a very small set of data and it is premature to conclude anything concrete based only on the current experimental results']",5
"['argument structure.', ' #TAUTHOR_TAG tried to construct a semantic']","['argument structure.', ' #TAUTHOR_TAG tried to construct a semantic']","['the argument structure.', ' #TAUTHOR_TAG tried to construct a semantic analysis']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', ' #AUTHOR_TAG considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', ' #AUTHOR_TAG argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', ' #TAUTHOR_TAG tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by  #AUTHOR_TAG that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', ' #AUTHOR_TAG tried to represent bangla cvs in terms of hpsg formalism.', 'she proposes that the selection of a v2 by a v1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'since none of the linguistic formalism could satisfactorily explain the unique phenomena of cv formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb - verb combinations in the ml and compare these responses with that of the existing models']",0
[' #AUTHOR_TAG taft 1975 ;  #TAUTHOR_TAG where it'],[' #AUTHOR_TAG taft 1975 ;  #TAUTHOR_TAG where it'],[' #AUTHOR_TAG taft 1975 ;  #TAUTHOR_TAG where'],"['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', ' #AUTHOR_TAG with his experiment on english inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'similar observation for surface word frequency was also observed by  #AUTHOR_TAG taft 1975 ;  #TAUTHOR_TAG where it has been claimed that words having low surface frequency tends to decompose.', ' #AUTHOR_TAG proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'if it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts']",0
"['mental lexicon  #TAUTHOR_TAG.', '']","['mental lexicon  #TAUTHOR_TAG.', '']","['a whole in the human mental lexicon  #TAUTHOR_TAG.', '']","['the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.', 'most of the studies are designed to support one of the two mutually exclusive paradigms : the full - listing and the morphemic model.', 'the full - listing model claims that polymorphic words are represented as a whole in the human mental lexicon  #TAUTHOR_TAG.', 'on the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.', 'the affixes are stripped away from the root form, which in turn are used to access the mental lexicon  #AUTHOR_TAG mac  #AUTHOR_TAG.', 'intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.', 'for instance, the derived morphological forms are believed to be represented as a whole, whereas the representation of the inflected forms follows the morphemic model  #AUTHOR_TAG']",0
"[', dutch, and few other languages  #TAUTHOR_TAG.', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than']","[', dutch, and few other languages  #TAUTHOR_TAG.', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than']","[', dutch, and few other languages  #TAUTHOR_TAG.', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of english, hebrew, italian, french, dutch, and few other languages  #TAUTHOR_TAG.', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than many of their indo - european cousins.', 'moreover, indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'on the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent  #AUTHOR_TAG.', 'therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages']",0
"['.', ' #TAUTHOR_TAG argues cv formations in hindi and urdu are']","['auxiliaries.', ' #TAUTHOR_TAG argues cv formations in hindi and urdu are']","['as an aspectual complex comparable to the auxiliaries.', ' #TAUTHOR_TAG argues cv formations in hindi and urdu are']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', ' #AUTHOR_TAG considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', ' #TAUTHOR_TAG argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', ' #AUTHOR_TAG tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by  #AUTHOR_TAG that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', ' #AUTHOR_TAG tried to represent bangla cvs in terms of hpsg formalism.', 'she proposes that the selection of a v2 by a v1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'since none of the linguistic formalism could satisfactorily explain the unique phenomena of cv formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb - verb combinations in the ml and compare these responses with that of the existing models']",0
"['the different priming and other lexical decision experiments, described in literature  #TAUTHOR_TAG ; bentin, s. and  #AUTHOR_TAG specifically for derivation']","['the different priming and other lexical decision experiments, described in literature  #TAUTHOR_TAG ; bentin, s. and  #AUTHOR_TAG specifically for derivationally suffixed polymorphemic words and compound verbs of bangla.', 'our cross - modal and masked priming experiment on bangla derivationally suffixed words shows that morphological relatedness between lexical items triggers a significant priming effect, even when']","['to this, we apply the different priming and other lexical decision experiments, described in literature  #TAUTHOR_TAG ; bentin, s. and  #AUTHOR_TAG specifically for derivationally suffixed polymorphemic words and compound verbs of bangla.', 'our cross - modal and masked priming experiment on bangla derivationally suffixed words shows that morphological relatedness between lexical items triggers a significant priming effect, even when the forms are phonologically / orthographically unrelated.', 'these observations are similar to']","['respect to this, we apply the different priming and other lexical decision experiments, described in literature  #TAUTHOR_TAG ; bentin, s. and  #AUTHOR_TAG specifically for derivationally suffixed polymorphemic words and compound verbs of bangla.', 'our cross - modal and masked priming experiment on bangla derivationally suffixed words shows that morphological relatedness between lexical items triggers a significant priming effect, even when the forms are phonologically / orthographically unrelated.', 'these observations are similar to those reported for english and indicate that derivationally suffixed words in bangla are in general accessed through decomposition of the word into its constituent morphemes.', 'further, based on the experimental data we have developed a series of computational models that can be used to predict the decomposition of bangla polymorphemic words.', 'our evaluation result shows that decomposition of a polymorphemic word depends on several factors like, frequency, productivity of the suffix and the compositionality between the stem and the suffix']",5
"['modal priming and masked priming experiment discussed in  #TAUTHOR_TAG for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were']","['modal priming and masked priming experiment discussed in  #TAUTHOR_TAG for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were']","['apply two different priming experiments namely, the cross modal priming and masked priming experiment discussed in  #TAUTHOR_TAG for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were asked to']","['apply two different priming experiments namely, the cross modal priming and masked priming experiment discussed in  #TAUTHOR_TAG for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'the same target word is again probed but with a different audio or visual probe called the control word.', 'the control shows no relationship with the target.', 'for example, bayaska ( aged ) and bayasa ( age ) is a prime - target pair, for which the corresponding control - target pair could be nayana ( eye ) and bayasa ( age )']",5
"[', dutch, and few other languages  #TAUTHOR_TAG ; grain']","[', dutch, and few other languages  #TAUTHOR_TAG ; grainger, et al.,']","[', dutch, and few other languages  #TAUTHOR_TAG ; grain']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of english, hebrew, italian, french, dutch, and few other languages  #TAUTHOR_TAG ; grainger, et al., 1991 ;  #AUTHOR_TAG.', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than many of their indo - european cousins.', 'moreover, indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'on the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent  #AUTHOR_TAG.', 'therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages']",0
"['modal priming and masked priming experiment discussed in  #TAUTHOR_TAG for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were']","['modal priming and masked priming experiment discussed in  #TAUTHOR_TAG for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were']","['apply two different priming experiments namely, the cross modal priming and masked priming experiment discussed in  #TAUTHOR_TAG for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were asked to']","['apply two different priming experiments namely, the cross modal priming and masked priming experiment discussed in  #TAUTHOR_TAG for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'the same target word is again probed but with a different audio or visual probe called the control word.', 'the control shows no relationship with the target.', 'for example, bayaska ( aged ) and bayasa ( age ) is a prime - target pair, for which the corresponding control - target pair could be nayana ( eye ) and bayasa ( age )']",5
"['##net  #TAUTHOR_TAG and raises the questions like, how to store morphologically complex words, in a lexical']","['like wordnet  #TAUTHOR_TAG and raises the questions like, how to store morphologically complex words, in a lexical']","['##net  #TAUTHOR_TAG and raises the questions like, how to store morphologically complex words, in a lexical resource like']","['clear understanding of the structure and the processing mechanism of the mental lexicon will further our knowledge of how the human brain processes language.', 'further, these linguistically important and interesting questions are also highly significant for computational linguistics ( cl ) and natural language processing ( nlp ) applications.', 'their computational significance arises from the issue of their storage in lexical resources like wordnet  #TAUTHOR_TAG and raises the questions like, how to store morphologically complex words, in a lexical resource like wordnet keeping in mind the storage and access efficiency']",0
"['processing of morphologically complex words are not quite language independent  #TAUTHOR_TAG.', 'therefore, the findings from experiments in one language cannot be generalized to all languages making it important to']","['processing of morphologically complex words are not quite language independent  #TAUTHOR_TAG.', 'therefore, the findings from experiments in one language cannot be generalized to all languages making it important to']","['processing of morphologically complex words are not quite language independent  #TAUTHOR_TAG.', 'therefore, the findings from experiments in one language cannot be generalized to all languages making it important to']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of english, hebrew, italian, french, dutch, and few other languages ( marslen -  #AUTHOR_TAG.', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than many of their indo - european cousins.', 'moreover, indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'on the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent  #TAUTHOR_TAG.', 'therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages']",0
['of the inflected forms follows the morphemic model  #TAUTHOR_TAG'],['of the inflected forms follows the morphemic model  #TAUTHOR_TAG'],"['argues that different types of morphological forms are processed separately.', 'for instance, the derived morphological forms are believed to be represented as a whole, whereas the representation of the inflected forms follows the morphemic model  #TAUTHOR_TAG']","['the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.', 'most of the studies are designed to support one of the two mutually exclusive paradigms : the full - listing and the morphemic model.', 'the full - listing model claims that polymorphic words are represented as a whole in the human mental lexicon  #AUTHOR_TAG.', 'on the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.', 'the affixes are stripped away from the root form, which in turn are used to access the mental lexicon  #AUTHOR_TAG mac  #AUTHOR_TAG.', 'intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.', 'for instance, the derived morphological forms are believed to be represented as a whole, whereas the representation of the inflected forms follows the morphemic model  #TAUTHOR_TAG']",0
"['', ' #TAUTHOR_TAG considers']","['to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', ' #TAUTHOR_TAG considers']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', ' #TAUTHOR_TAG considers']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', ' #TAUTHOR_TAG considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', ' #AUTHOR_TAG argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', ' #AUTHOR_TAG tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by  #AUTHOR_TAG that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', ' #AUTHOR_TAG tried to represent bangla cvs in terms of hpsg formalism.', 'she proposes that the selection of a v2 by a v1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'since none of the linguistic formalism could satisfactorily explain the unique phenomena of cv formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb - verb combinations in the ml and compare these responses with that of the existing models']",0
[' #TAUTHOR_TAG taft 1975 ;  #AUTHOR_TAG where it'],[' #TAUTHOR_TAG taft 1975 ;  #AUTHOR_TAG where it'],[' #TAUTHOR_TAG taft 1975 ;  #AUTHOR_TAG where'],"['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', ' #AUTHOR_TAG with his experiment on english inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'similar observation for surface word frequency was also observed by  #TAUTHOR_TAG taft 1975 ;  #AUTHOR_TAG where it has been claimed that words having low surface frequency tends to decompose.', ' #AUTHOR_TAG proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'if it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts']",0
